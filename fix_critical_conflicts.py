#!/usr/bin/env python3
"""
ä¿®å¤BMAæ¨¡å‹çš„å…³é”®å†²çªé—®é¢˜
1. ç»Ÿä¸€ç´¢å¼•ç­–ç•¥: å…¨éƒ¨ä½¿ç”¨MultiIndex(date, ticker)
2. æ”¹è¿›åˆå¹¶é€»è¾‘: ä½¿ç”¨pd.merge on=['date', 'ticker']æ›¿ä»£å­—ç¬¦ä¸²åˆå¹¶é”®
3. åˆ†ç¦»PCAå¤„ç†: Alphaå› å­å’Œä¼ ç»Ÿå› å­åˆ†åˆ«è¿›è¡Œé™ç»´ï¼Œæœ€ååˆå¹¶
4. ç»Ÿä¸€æ—¶é—´é…ç½®: ä½¿ç”¨å•ä¸€çš„æ»åå‚æ•°é…ç½®
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import re
import pandas as pd
import numpy as np
from typing import Dict, Any, Optional, List
from datetime import datetime

def fix_bma_critical_conflicts():
    """ä¿®å¤BMAæ¨¡å‹çš„æ‰€æœ‰å…³é”®å†²çªé—®é¢˜"""
    
    print("=== å¼€å§‹ä¿®å¤BMAæ¨¡å‹å…³é”®å†²çª ===\n")
    
    file_path = "bma_models/é‡åŒ–æ¨¡å‹_bma_ultra_enhanced.py"
    
    # è¯»å–åŸæ–‡ä»¶
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    print(f"åŸæ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
    
    # 1. ä¿®å¤é‡å¤æ–¹æ³•å®šä¹‰é—®é¢˜
    print("1. ä¿®å¤é‡å¤æ–¹æ³•å®šä¹‰...")
    fixed_content = fix_duplicate_methods(content)
    
    # 2. ç»Ÿä¸€æ—¶é—´é…ç½®
    print("2. ç»Ÿä¸€æ—¶é—´é…ç½®...")
    fixed_content = unify_temporal_config(fixed_content)
    
    # 3. ä¼˜åŒ–ç´¢å¼•å’Œåˆå¹¶é€»è¾‘
    print("3. ä¼˜åŒ–ç´¢å¼•å’Œåˆå¹¶é€»è¾‘...")
    fixed_content = optimize_indexing_and_merging(fixed_content)
    
    # 4. åˆ†ç¦»PCAå¤„ç†é€»è¾‘
    print("4. åˆ†ç¦»PCAå¤„ç†é€»è¾‘...")
    fixed_content = separate_pca_processing(fixed_content)
    
    # 5. æ¸…ç†ç‰ˆæœ¬æ··åˆä»£ç 
    print("5. æ¸…ç†ç‰ˆæœ¬æ··åˆä»£ç ...")
    fixed_content = clean_version_mixing(fixed_content)
    
    # åˆ›å»ºå¤‡ä»½å¹¶å†™å…¥ä¿®å¤åçš„æ–‡ä»¶
    backup_file = file_path + f".backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    with open(backup_file, 'w', encoding='utf-8') as f:
        f.write(content)
    print(f"åŸæ–‡ä»¶å¤‡ä»½è‡³: {backup_file}")
    
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(fixed_content)
    
    print(f"\nâœ“ ä¿®å¤å®Œæˆï¼ä¿®å¤åæ–‡ä»¶å¤§å°: {len(fixed_content)} å­—ç¬¦")
    print(f"âœ“ åŸæ–‡ä»¶å·²å¤‡ä»½è‡³: {backup_file}")
    
    return True

def fix_duplicate_methods(content: str) -> str:
    """ä¿®å¤é‡å¤æ–¹æ³•å®šä¹‰"""
    print("  - è¯†åˆ«å¹¶åˆå¹¶é‡å¤æ–¹æ³•...")
    
    # é‡å¤æ–¹æ³•çš„å¤„ç†ç­–ç•¥ï¼šä¿ç•™æœ€æ–°ç‰ˆæœ¬ï¼Œç§»é™¤æ—§ç‰ˆæœ¬
    duplicate_patterns = {
        # __init__ æ–¹æ³•å»é‡ï¼šä¿ç•™æœ€å®Œæ•´çš„ç‰ˆæœ¬
        r'(class \w+:.*?)def __init__\(self\):.*?(?=def|\n\n|\Z)': 'remove_duplicate_init',
        
        # validate_dataframe å»é‡ï¼šä¿ç•™å‚æ•°æ›´å®Œæ•´çš„ç‰ˆæœ¬
        r'def validate_dataframe\(self, df: pd\.DataFrame, source_name: str\).*?(?=def|\n\n|\Z)': 'keep_first',
        
        # calculate_all_signals å»é‡
        r'def calculate_all_signals\(self, symbol\):.*?(?=def|\n\n|\Z)': 'keep_first',
        
        # stats æ–¹æ³•å»é‡
        r'def stats\(self\):.*?(?=def|\n\n|\Z)': 'keep_first',
    }
    
    fixed_content = content
    
    # ç®€å•çš„æ–¹æ³•ï¼šç§»é™¤æ˜æ˜¾çš„é‡å¤defè¡Œ
    lines = fixed_content.split('\n')
    seen_methods = {}
    filtered_lines = []
    skip_until_next_def = False
    
    for i, line in enumerate(lines):
        if line.strip().startswith('def '):
            method_signature = line.strip()
            
            # æå–æ–¹æ³•å
            method_name = method_signature.split('(')[0].replace('def ', '').strip()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å·²çŸ¥çš„é‡å¤æ–¹æ³•
            if method_name in seen_methods:
                print(f"    ç§»é™¤é‡å¤æ–¹æ³•: {method_name} (è¡Œ {i+1})")
                skip_until_next_def = True
                continue
            else:
                seen_methods[method_name] = i
                skip_until_next_def = False
        
        if skip_until_next_def:
            # å¦‚æœå½“å‰è¡Œæ˜¯ä¸‹ä¸€ä¸ªæ–¹æ³•å®šä¹‰æˆ–ç±»å®šä¹‰ï¼Œåœæ­¢è·³è¿‡
            if line.strip().startswith(('def ', 'class ', '@')):
                skip_until_next_def = False
                filtered_lines.append(line)
            # å¦åˆ™è·³è¿‡è¿™è¡Œ
            continue
        
        filtered_lines.append(line)
    
    return '\n'.join(filtered_lines)

def unify_temporal_config(content: str) -> str:
    """ç»Ÿä¸€æ—¶é—´é…ç½®å‚æ•°"""
    print("  - ç»Ÿä¸€æ»åå’Œé—´éš”å‚æ•°...")
    
    # å®šä¹‰ç»Ÿä¸€çš„æ—¶é—´é…ç½®
    unified_config = {
        'feature_lag_days': 1,    # ç‰¹å¾æ»å1å¤©
        'safety_gap_days': 1,     # å®‰å…¨é—´éš”1å¤©  
        'cv_gap_days': 1,         # CVé—´éš”1å¤©
        'cv_embargo_days': 1,     # CVç¦è¿1å¤©
        'prediction_horizon_days': 10  # é¢„æµ‹æœŸ10å¤©
    }
    
    # æ›¿æ¢ç¡¬ç¼–ç çš„æ•°å€¼
    replacements = [
        # ç»Ÿä¸€ç‰¹å¾æ»å
        (r'feature_lag.*?=.*?[0-9]+', f"feature_lag_days = {unified_config['feature_lag_days']}"),
        (r'FEATURE_LAG.*?=.*?[0-9]+', f"FEATURE_LAG = {unified_config['feature_lag_days']}"),
        
        # ç»Ÿä¸€å®‰å…¨é—´éš”
        (r'safety_gap.*?=.*?[0-9]+', f"safety_gap_days = {unified_config['safety_gap_days']}"),
        (r'SAFETY_GAP.*?=.*?[0-9]+', f"SAFETY_GAP = {unified_config['safety_gap_days']}"),
        
        # ç»Ÿä¸€CVé—´éš”
        (r'cv_gap.*?=.*?[0-9]+', f"cv_gap_days = {unified_config['cv_gap_days']}"),
        (r'gap=.*?[0-9]+', f"gap={unified_config['cv_gap_days']}"),
        
        # ç»Ÿä¸€CVç¦è¿æœŸ
        (r'cv_embargo.*?=.*?[0-9]+', f"cv_embargo_days = {unified_config['cv_embargo_days']}"),
        (r'embargo=.*?[0-9]+', f"embargo={unified_config['cv_embargo_days']}"),
    ]
    
    fixed_content = content
    for pattern, replacement in replacements:
        fixed_content = re.sub(pattern, replacement, fixed_content, flags=re.IGNORECASE)
    
    # æ·»åŠ ç»Ÿä¸€çš„æ—¶é—´é…ç½®å¸¸é‡
    config_block = f'''
# === ç»Ÿä¸€æ—¶é—´é…ç½®å¸¸é‡ ===
UNIFIED_FEATURE_LAG_DAYS = {unified_config['feature_lag_days']}
UNIFIED_SAFETY_GAP_DAYS = {unified_config['safety_gap_days']}
UNIFIED_CV_GAP_DAYS = {unified_config['cv_gap_days']}
UNIFIED_CV_EMBARGO_DAYS = {unified_config['cv_embargo_days']}
UNIFIED_PREDICTION_HORIZON_DAYS = {unified_config['prediction_horizon_days']}

# å‘åå…¼å®¹åˆ«å
FEATURE_LAG = UNIFIED_FEATURE_LAG_DAYS
SAFETY_GAP = UNIFIED_SAFETY_GAP_DAYS
'''
    
    # æ’å…¥é…ç½®å—åˆ°å¯¼å…¥åé¢
    insert_pos = fixed_content.find('# === PROJECT PATH SETUP ===')
    if insert_pos > 0:
        fixed_content = fixed_content[:insert_pos] + config_block + '\n' + fixed_content[insert_pos:]
    
    return fixed_content

def optimize_indexing_and_merging(content: str) -> str:
    """ä¼˜åŒ–ç´¢å¼•å’Œåˆå¹¶é€»è¾‘"""
    print("  - ä¼˜åŒ–ç´¢å¼•å’Œåˆå¹¶é€»è¾‘...")
    
    # æ·»åŠ ç»Ÿä¸€çš„æ•°æ®åˆå¹¶è¾…åŠ©å‡½æ•°
    merge_helper_code = '''
# === ç»Ÿä¸€æ•°æ®åˆå¹¶è¾…åŠ©å‡½æ•° ===
def safe_merge_on_multiindex(left_df: pd.DataFrame, right_df: pd.DataFrame, 
                           how: str = 'left', suffixes: tuple = ('', '_right')) -> pd.DataFrame:
    """
    å®‰å…¨åˆå¹¶ä¸¤ä¸ªDataFrameï¼Œè‡ªåŠ¨å¤„ç†MultiIndexå’Œæ™®é€šç´¢å¼•
    
    Args:
        left_df: å·¦ä¾§DataFrame
        right_df: å³ä¾§DataFrame  
        how: åˆå¹¶æ–¹å¼ ('left', 'right', 'outer', 'inner')
        suffixes: é‡å¤åˆ—ååç¼€
        
    Returns:
        åˆå¹¶åçš„DataFrameï¼Œä¿æŒMultiIndex(date, ticker)ç»“æ„
    """
    try:
        # ç¡®ä¿ä¸¤ä¸ªDataFrameéƒ½æœ‰dateå’Œtickeråˆ—
        left_work = left_df.copy()
        right_work = right_df.copy()
        
        # é‡ç½®ç´¢å¼•ç¡®ä¿æœ‰dateå’Œtickeråˆ—
        if isinstance(left_work.index, pd.MultiIndex):
            left_work = left_work.reset_index()
        if isinstance(right_work.index, pd.MultiIndex):
            right_work = right_work.reset_index()
            
        # ç¡®ä¿æœ‰å¿…éœ€çš„åˆ—
        required_cols = {'date', 'ticker'}
        if not required_cols.issubset(left_work.columns):
            raise ValueError(f"å·¦ä¾§DataFrameç¼ºå°‘å¿…éœ€åˆ—: {required_cols - set(left_work.columns)}")
        if not required_cols.issubset(right_work.columns):
            raise ValueError(f"å³ä¾§DataFrameç¼ºå°‘å¿…éœ€åˆ—: {required_cols - set(right_work.columns)}")
        
        # æ‰§è¡Œæ ‡å‡†pandas merge
        merged = left_work.merge(right_work, on=['date', 'ticker'], how=how, suffixes=suffixes)
        
        # é‡æ–°è®¾ç½®MultiIndex
        if 'date' in merged.columns and 'ticker' in merged.columns:
            merged = merged.set_index(['date', 'ticker']).sort_index()
        
        return merged
        
    except Exception as e:
        print(f"åˆå¹¶å¤±è´¥: {e}")
        return left_df

def ensure_multiindex_structure(df: pd.DataFrame) -> pd.DataFrame:
    """
    ç¡®ä¿DataFrameå…·æœ‰æ­£ç¡®çš„MultiIndex(date, ticker)ç»“æ„
    
    Args:
        df: è¾“å…¥DataFrame
        
    Returns:
        å…·æœ‰æ­£ç¡®MultiIndexç»“æ„çš„DataFrame
    """
    if df is None or df.empty:
        return df
        
    # å¦‚æœå·²ç»æ˜¯æ­£ç¡®çš„MultiIndexï¼Œç›´æ¥è¿”å›
    if isinstance(df.index, pd.MultiIndex) and df.index.names == ['date', 'ticker']:
        return df
    
    # é‡ç½®ç´¢å¼•
    if isinstance(df.index, pd.MultiIndex):
        df = df.reset_index()
    
    # æ£€æŸ¥å¿…éœ€åˆ—
    if 'date' not in df.columns or 'ticker' not in df.columns:
        return df  # è¿”å›åŸDataFrameï¼Œä¸åšä¿®æ”¹
    
    # è®¾ç½®MultiIndex
    return df.set_index(['date', 'ticker']).sort_index()

'''
    
    # æŸ¥æ‰¾åˆé€‚çš„æ’å…¥ä½ç½®ï¼ˆåœ¨ç±»å®šä¹‰ä¹‹å‰ï¼‰
    class_pos = content.find('class DataContractManager:')
    if class_pos > 0:
        content = content[:class_pos] + merge_helper_code + '\n' + content[class_pos:]
    
    # æ›¿æ¢é—®é¢˜çš„åˆå¹¶ä»£ç 
    problematic_merge_pattern = r'''merged = merge_df\.merge\(alpha_df, on=\['date', 'ticker'\], how='left'\)'''
    replacement_merge = "merged = safe_merge_on_multiindex(merge_df, alpha_df, how='left')"
    
    content = re.sub(problematic_merge_pattern, replacement_merge, content)
    
    # æ›¿æ¢å…¶ä»–åˆå¹¶æ¨¡å¼
    merge_patterns = [
        (r'\.merge\([^)]*on=\[\'date\', \'ticker\'\][^)]*\)', 
         lambda m: f"safe_merge_on_multiindex({m.group().replace('.merge(', '').replace(')', '')})")
    ]
    
    for pattern, replacement in merge_patterns:
        content = re.sub(pattern, replacement, content)
    
    return content

def separate_pca_processing(content: str) -> str:
    """åˆ†ç¦»Alphaå’Œä¼ ç»Ÿå› å­çš„PCAå¤„ç†"""
    print("  - åˆ†ç¦»PCAå¤„ç†é€»è¾‘...")
    
    pca_separation_code = '''
# === åˆ†ç¦»çš„PCAå¤„ç†ç³»ç»Ÿ ===
def apply_separated_pca(feature_data: pd.DataFrame, alpha_data: pd.DataFrame = None,
                       traditional_n_components: int = None, alpha_n_components: int = None) -> pd.DataFrame:
    """
    å¯¹Alphaå› å­å’Œä¼ ç»Ÿå› å­åˆ†åˆ«åº”ç”¨PCAé™ç»´ï¼Œç„¶ååˆå¹¶
    
    Args:
        feature_data: ä¼ ç»Ÿç‰¹å¾æ•°æ®
        alpha_data: Alphaç‰¹å¾æ•°æ®
        traditional_n_components: ä¼ ç»Ÿç‰¹å¾PCAç»„ä»¶æ•°
        alpha_n_components: Alphaç‰¹å¾PCAç»„ä»¶æ•°
        
    Returns:
        åˆå¹¶åçš„é™ç»´ç‰¹å¾æ•°æ®
    """
    try:
        from sklearn.decomposition import PCA
        
        results = []
        
        # å¤„ç†ä¼ ç»Ÿç‰¹å¾
        if feature_data is not None and not feature_data.empty:
            traditional_features = feature_data.select_dtypes(include=[np.number]).fillna(0)
            
            if traditional_features.shape[1] > 1:
                n_comp = min(traditional_n_components or traditional_features.shape[1]//2, 
                           traditional_features.shape[1], 
                           traditional_features.shape[0]//2)
                n_comp = max(1, n_comp)
                
                pca_trad = PCA(n_components=n_comp, random_state=42)
                trad_pca_features = pca_trad.fit_transform(traditional_features)
                
                trad_pca_df = pd.DataFrame(
                    trad_pca_features,
                    index=traditional_features.index,
                    columns=[f'trad_pca_{i+1}' for i in range(trad_pca_features.shape[1])]
                )
                results.append(trad_pca_df)
                print(f"  ä¼ ç»Ÿç‰¹å¾PCA: {traditional_features.shape[1]} -> {n_comp}")
        
        # å¤„ç†Alphaç‰¹å¾
        if alpha_data is not None and not alpha_data.empty:
            alpha_features = alpha_data.select_dtypes(include=[np.number]).fillna(0)
            
            if alpha_features.shape[1] > 1:
                n_comp = min(alpha_n_components or alpha_features.shape[1]//2,
                           alpha_features.shape[1],
                           alpha_features.shape[0]//2)
                n_comp = max(1, n_comp)
                
                pca_alpha = PCA(n_components=n_comp, random_state=42)
                alpha_pca_features = pca_alpha.fit_transform(alpha_features)
                
                alpha_pca_df = pd.DataFrame(
                    alpha_pca_features,
                    index=alpha_features.index, 
                    columns=[f'alpha_pca_{i+1}' for i in range(alpha_pca_features.shape[1])]
                )
                results.append(alpha_pca_df)
                print(f"  Alphaç‰¹å¾PCA: {alpha_features.shape[1]} -> {n_comp}")
        
        # åˆå¹¶ç»“æœ
        if results:
            combined = pd.concat(results, axis=1)
            return ensure_multiindex_structure(combined)
        else:
            return feature_data if feature_data is not None else pd.DataFrame()
            
    except Exception as e:
        print(f"PCAå¤„ç†å¤±è´¥: {e}")
        return feature_data if feature_data is not None else pd.DataFrame()

'''
    
    # æ’å…¥PCAåˆ†ç¦»ä»£ç 
    pca_insert_pos = content.find('def apply_separated_pca')
    if pca_insert_pos == -1:
        # åœ¨ç±»å®šä¹‰ä¹‹å‰æ’å…¥
        class_pos = content.find('class ModuleManager:')
        if class_pos > 0:
            content = content[:class_pos] + pca_separation_code + '\n' + content[class_pos:]
    
    return content

def clean_version_mixing(content: str) -> str:
    """æ¸…ç†ç‰ˆæœ¬æ··åˆä»£ç """
    print("  - æ¸…ç†ç‰ˆæœ¬æ ‡è®°å’Œè¿‡æ—¶ä»£ç ...")
    
    # ç§»é™¤ç‰ˆæœ¬æ ‡è®°æ³¨é‡Š
    version_patterns = [
        r'#.*[Vv][0-9]+.*',
        r'#.*legacy.*',
        r'#.*deprecated.*',
        r'#.*old.*implementation.*',
    ]
    
    for pattern in version_patterns:
        content = re.sub(pattern, '', content, flags=re.IGNORECASE)
    
    # ç®€åŒ–ç‰ˆæœ¬ç›¸å…³å˜é‡å
    version_var_replacements = [
        (r'enable_v[0-9]+_enhancements', 'enable_enhancements'),
        (r'v[0-9]+_config', 'enhanced_config'),
        (r'v[0-9]+_performance_tracker', 'performance_tracker'),
    ]
    
    for pattern, replacement in version_var_replacements:
        content = re.sub(pattern, replacement, content, flags=re.IGNORECASE)
    
    return content

if __name__ == "__main__":
    try:
        success = fix_bma_critical_conflicts()
        if success:
            print("\nğŸ‰ æ‰€æœ‰å…³é”®å†²çªä¿®å¤å®Œæˆï¼")
            print("\nä¸»è¦ä¿®å¤å†…å®¹:")
            print("âœ“ ç»Ÿä¸€MultiIndex(date, ticker)ç´¢å¼•ç­–ç•¥")
            print("âœ“ æ”¹è¿›pd.merge on=['date', 'ticker']åˆå¹¶é€»è¾‘")  
            print("âœ“ åˆ†ç¦»Alphaå’Œä¼ ç»Ÿå› å­çš„PCAå¤„ç†")
            print("âœ“ ç»Ÿä¸€æ—¶é—´é…ç½®å‚æ•°(æ»å1å¤©)")
            print("âœ“ æ¸…ç†é‡å¤æ–¹æ³•å’Œç‰ˆæœ¬æ··åˆä»£ç ")
            print("\nè¯·è¿è¡Œæµ‹è¯•ä»¥éªŒè¯ä¿®å¤æ•ˆæœ!")
        else:
            print("âŒ ä¿®å¤è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜")
    except Exception as e:
        print(f"âŒ ä¿®å¤å¤±è´¥: {e}")