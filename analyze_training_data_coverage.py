#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æè®­ç»ƒæ•°æ®çš„æ—¶é—´è¦†ç›–èŒƒå›´
æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä¸‰å¹´æ•°æ®éƒ½è¢«çº³å…¥è®­ç»ƒï¼Œé™¤äº†CV gap
"""

import pandas as pd
import numpy as np
import sys
import os
from pathlib import Path

# Add BMA models path
sys.path.append(os.path.join(os.path.dirname(__file__), 'bma_models'))

def analyze_data_coverage():
    """åˆ†æè®­ç»ƒæ•°æ®è¦†ç›–èŒƒå›´"""

    print("=" * 80)
    print("è®­ç»ƒæ•°æ®æ—¶é—´è¦†ç›–åˆ†æ")
    print("=" * 80)

    # ä»é…ç½®è¯»å–å…³é”®å‚æ•°
    try:
        from bma_models.unified_config_loader import get_time_config
        time_config = get_time_config()

        print("é…ç½®å‚æ•°:")
        print(f"  CV Gapå¤©æ•°: {time_config.cv_gap_days}")
        print(f"  CV Embargoå¤©æ•°: {time_config.cv_embargo_days}")
        print(f"  é¢„æµ‹çª—å£: T+{time_config.prediction_horizon_days}")
        print(f"  ç‰¹å¾æ»å: T-{time_config.feature_lag_days}")

        # ä»é…ç½®æ–‡ä»¶è¯»å–è®­ç»ƒå‚æ•°
        cv_gap_days = time_config.cv_gap_days
        cv_embargo_days = time_config.cv_embargo_days
        min_train_size = 252  # ä»yamlé…ç½®è¯»å–çš„å€¼

        # å°è¯•ä»yamlè·å–å†å²æ•°æ®é…ç½®
        try:
            import yaml
            with open('bma_models/unified_config.yaml', 'r') as f:
                config_data = yaml.safe_load(f)

            data_config = config_data.get('data', {})
            print(f"  é£é™©æ¨¡å‹å†å²: {data_config.get('risk_model_history_days', 300)}å¤©")
            print(f"  Alphaæ•°æ®å†å²: {data_config.get('alpha_data_history_days', 200)}å¤©")

            training_config = config_data.get('training', {})
            min_train_size = training_config.get('cv_min_train_size', 252)
            print(f"  æœ€å°è®­ç»ƒé›†å¤§å°: {min_train_size}å¤©")

        except Exception as yaml_e:
            print(f"  ä½¿ç”¨é»˜è®¤å†å²æ•°æ®é…ç½®: {yaml_e}")

    except Exception as e:
        print(f"æ— æ³•åŠ è½½é…ç½®: {e}")
        # ä½¿ç”¨ç¡¬ç¼–ç é»˜è®¤å€¼
        cv_gap_days = 6
        cv_embargo_days = 5
        min_train_size = 252
        print(f"ä½¿ç”¨é»˜è®¤å€¼: gap={cv_gap_days}, embargo={cv_embargo_days}, min_train={min_train_size}")

    print("\n" + "=" * 80)
    print("æ•°æ®åˆ©ç”¨ç‡åˆ†æ")
    print("=" * 80)

    # åˆ†æä¸åŒæ—¶é—´èŒƒå›´çš„æ•°æ®åˆ©ç”¨
    total_days_3years = 252 * 3  # çº¦3å¹´äº¤æ˜“æ—¥

    # CVé—´éš”è®¡ç®—
    total_cv_gap = cv_gap_days + cv_embargo_days  # æ€»CVé—´éš”

    # æœ‰æ•ˆè®­ç»ƒæ•°æ®è®¡ç®—
    effective_training_days = total_days_3years - total_cv_gap
    training_utilization = effective_training_days / total_days_3years * 100

    print(f" ä¸‰å¹´æ•°æ®åˆ©ç”¨åˆ†æ:")
    print(f"  æ€»äº¤æ˜“æ—¥ (3å¹´): {total_days_3years}å¤©")
    print(f"  CV Gap + Embargo: {total_cv_gap}å¤©")
    print(f"  æœ‰æ•ˆè®­ç»ƒå¤©æ•°: {effective_training_days}å¤©")
    print(f"  æ•°æ®åˆ©ç”¨ç‡: {training_utilization:.1f}%")

    # æœ€å°è®­ç»ƒé›†è¦æ±‚
    print(f"\nğŸ“ˆ è®­ç»ƒé›†è§„æ¨¡åˆ†æ:")
    print(f"  æœ€å°è®­ç»ƒé›†è¦æ±‚: {min_train_size}å¤© ({min_train_size/252:.1f}å¹´)")
    print(f"  å®é™…å¯ç”¨å¤©æ•°: {effective_training_days}å¤© ({effective_training_days/252:.1f}å¹´)")

    if effective_training_days >= min_train_size:
        coverage_ratio = effective_training_days / min_train_size
        print(f"  âœ… æ»¡è¶³æœ€å°è¦æ±‚: {coverage_ratio:.1f}x å€æ•°")
    else:
        shortage = min_train_size - effective_training_days
        print(f"  âŒ ä¸æ»¡è¶³è¦æ±‚: ç¼ºå°‘{shortage}å¤©")

    # CVæŠ˜æ•°åˆ†æ
    cv_splits = 5
    test_size = 63  # 3ä¸ªæœˆæµ‹è¯•é›†

    print(f"\nğŸ”„ äº¤å‰éªŒè¯åˆ†æ:")
    print(f"  CVæŠ˜æ•°: {cv_splits}")
    print(f"  æµ‹è¯•é›†å¤§å°: {test_size}å¤© ({test_size/252:.1f}å¹´)")

    # è®¡ç®—æ¯æŠ˜çš„è®­ç»ƒæ•°æ®é‡
    fold_training_data = []
    for fold in range(cv_splits):
        # æ¯æŠ˜çš„è®­ç»ƒæ•°æ® = å‰é¢çš„æ•°æ® - gap
        fold_start = 0
        fold_train_end = min_train_size + fold * test_size
        fold_test_start = fold_train_end + cv_gap_days
        fold_test_end = fold_test_start + test_size

        if fold_test_end <= effective_training_days:
            actual_train_days = fold_train_end
            fold_training_data.append(actual_train_days)
            print(f"    Fold {fold+1}: è®­ç»ƒ{actual_train_days}å¤©, æµ‹è¯•{test_size}å¤©")
        else:
            print(f"    Fold {fold+1}: æ•°æ®ä¸è¶³ï¼Œè·³è¿‡")

    if fold_training_data:
        avg_training_days = np.mean(fold_training_data)
        total_unique_training_days = max(fold_training_data) if fold_training_data else 0
        print(f"  å¹³å‡è®­ç»ƒå¤©æ•°/æŠ˜: {avg_training_days:.0f}å¤©")
        print(f"  æ€»ç‹¬ç‰¹è®­ç»ƒå¤©æ•°: {total_unique_training_days}å¤©")

        # æ•°æ®é‡ç”¨åˆ†æ
        data_reuse_ratio = sum(fold_training_data) / total_unique_training_days if total_unique_training_days > 0 else 0
        print(f"  æ•°æ®é‡ç”¨å€æ•°: {data_reuse_ratio:.1f}x")

    print(f"\n" + "=" * 80)
    print("æ—¶é—´å®‰å…¨æ€§éªŒè¯")
    print("=" * 80)

    # éªŒè¯æ—¶é—´å®‰å…¨è®¾ç½®
    prediction_horizon = 5  # T+5

    print(f"â° æ—¶é—´éš”ç¦»éªŒè¯:")
    print(f"  é¢„æµ‹ç›®æ ‡: T+{prediction_horizon}")
    print(f"  CV Gap: {cv_gap_days}å¤© (>= {prediction_horizon}å¤© âœ…)" if cv_gap_days >= prediction_horizon else f"  CV Gap: {cv_gap_days}å¤© (< {prediction_horizon}å¤© âŒ)")
    print(f"  Embargo: {cv_embargo_days}å¤© (>= {prediction_horizon}å¤© âœ…)" if cv_embargo_days >= prediction_horizon else f"  Embargo: {cv_embargo_days}å¤© (< {prediction_horizon}å¤© âŒ)")

    # ç‰¹å¾æ»åéªŒè¯
    feature_lag = 1  # T-1
    print(f"  ç‰¹å¾æ»å: T-{feature_lag} (é˜²æ­¢æœªæ¥ä¿¡æ¯æ³„æ¼ âœ…)")

    # æ€»æ—¶é—´é—´éš”
    total_isolation = cv_gap_days + cv_embargo_days + feature_lag
    print(f"  æ€»æ—¶é—´éš”ç¦»: {total_isolation}å¤©")

    print(f"\n ç»“è®º:")
    if training_utilization >= 90:
        print(f"  âœ… ä¼˜ç§€: {training_utilization:.1f}%çš„ä¸‰å¹´æ•°æ®è¢«æœ‰æ•ˆåˆ©ç”¨")
    elif training_utilization >= 80:
        print(f"  âœ… è‰¯å¥½: {training_utilization:.1f}%çš„ä¸‰å¹´æ•°æ®è¢«æœ‰æ•ˆåˆ©ç”¨")
    else:
        print(f"  âš ï¸ å¯ä¼˜åŒ–: ä»…{training_utilization:.1f}%çš„ä¸‰å¹´æ•°æ®è¢«åˆ©ç”¨")

    gap_efficiency = 100 - (total_cv_gap / total_days_3years * 100)
    print(f"  Gapæ•ˆç‡: {gap_efficiency:.1f}% (éGapæ—¶é—´æ¯”ä¾‹)")

    if cv_gap_days >= prediction_horizon and cv_embargo_days >= prediction_horizon:
        print(f"  âœ… æ—¶é—´å®‰å…¨: CVè®¾ç½®é˜²æ­¢æ•°æ®æ³„æ¼")
    else:
        print(f"  âŒ æ—¶é—´é£é™©: CVè®¾ç½®å¯èƒ½å­˜åœ¨æ³„æ¼é£é™©")


def check_data_files():
    """æ£€æŸ¥å®é™…æ•°æ®æ–‡ä»¶çš„æ—¶é—´èŒƒå›´"""

    print(f"\n" + "=" * 80)
    print("å®é™…æ•°æ®æ–‡ä»¶åˆ†æ")
    print("=" * 80)

    # æ£€æŸ¥å¯èƒ½çš„æ•°æ®æ–‡ä»¶ä½ç½®
    data_paths = [
        "D:/trade/data",
        "D:/trade/cache",
        "D:/trade",
        "."
    ]

    data_files_found = []

    for data_path in data_paths:
        if os.path.exists(data_path):
            # æŸ¥æ‰¾CSVå’Œpickleæ–‡ä»¶
            for ext in ['*.csv', '*.pkl', '*.parquet']:
                import glob
                files = glob.glob(os.path.join(data_path, f"**/{ext}"), recursive=True)
                data_files_found.extend(files)

    if data_files_found:
        print(f"ğŸ“ å‘ç°{len(data_files_found)}ä¸ªæ•°æ®æ–‡ä»¶:")
        for file_path in data_files_found[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            file_size = os.path.getsize(file_path) / (1024*1024)  # MB
            mod_time = pd.Timestamp.fromtimestamp(os.path.getmtime(file_path))
            print(f"  {os.path.basename(file_path)} ({file_size:.1f}MB, {mod_time.strftime('%Y-%m-%d')})")

        if len(data_files_found) > 10:
            print(f"  ... è¿˜æœ‰{len(data_files_found)-10}ä¸ªæ–‡ä»¶")
    else:
        print("ğŸ“ æœªå‘ç°æ•°æ®æ–‡ä»¶")

    # æ£€æŸ¥è®­ç»ƒæ—¥å¿—
    log_paths = ["D:/trade/training_logs", "D:/trade/logs"]

    for log_path in log_paths:
        if os.path.exists(log_path):
            log_files = list(Path(log_path).glob("*.log"))
            if log_files:
                print(f"\nğŸ“„ è®­ç»ƒæ—¥å¿— ({log_path}):")
                for log_file in log_files[-5:]:  # æœ€è¿‘5ä¸ªæ—¥å¿—
                    mod_time = pd.Timestamp.fromtimestamp(log_file.stat().st_mtime)
                    print(f"  {log_file.name} ({mod_time.strftime('%Y-%m-%d %H:%M')})")


if __name__ == "__main__":
    analyze_data_coverage()
    check_data_files()