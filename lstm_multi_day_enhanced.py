#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LSTMå¤šæ—¥é¢„æµ‹å¢å¼ºç‰ˆé‡åŒ–åˆ†ææ¨¡å‹
æ”¯æŒ1-5å¤©å¤šè¾“å‡ºé¢„æµ‹çš„æ—¶åºå»ºæ¨¡

ä¸»è¦ç‰¹æ€§:
- å¤šè¾“å‡ºDenseå±‚ï¼Œä¸€æ¬¡è®­ç»ƒé¢„æµ‹5å¤©
- ä½è½åœ°æˆæœ¬ï¼Œä¿æŒsliding windowæ–¹å¼
- y_trainå½¢çŠ¶ (æ ·æœ¬æ•°, 5)ï¼Œæ”¯æŒå¤šç»´ç›®æ ‡
- MAE/MSEå¤šç»´å‘é‡æŸå¤±å‡½æ•°
- Excelå’ŒIBKRå…¼å®¹è¾“å‡º

Authors: AI Assistant
Version: 3.0 Multi-Day Enhanced
"""

import pandas as pd
import numpy as np
import yfinance as yf
from datetime import datetime, timedelta
import warnings
import argparse
import os
import tempfile
from pathlib import Path
from scipy.stats import spearmanr
import statsmodels.api as sm
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV, RandomizedSearchCV, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns
import json
import sys

# è®¾ç½®ç¼–ç 
if sys.platform.startswith('win'):
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())

# å¯¼å…¥çŠ¶æ€ç›‘æ§æ¨¡å—
try:
    from status_monitor import get_status_monitor, update_status, log_message
    STATUS_MONITOR_AVAILABLE = True
    print("[INFO] çŠ¶æ€ç›‘æ§æ¨¡å—å·²åŠ è½½")
except ImportError:
    STATUS_MONITOR_AVAILABLE = False
    print("[WARNING] çŠ¶æ€ç›‘æ§æ¨¡å—ä¸å¯ç”¨")

def safe_print(message, force_terminal=False, **kwargs):
    """å®‰å…¨çš„æ‰“å°å‡½æ•°ï¼ŒåŒæ—¶è¾“å‡ºåˆ°terminalå’ŒçŠ¶æ€ç›‘æ§"""
    # å…ˆè¾“å‡ºåˆ°terminal
    try:
        print(message, **kwargs)
    except UnicodeEncodeError:
        # å¦‚æœæœ‰ç¼–ç é—®é¢˜ï¼Œä½¿ç”¨é”™è¯¯å¤„ç†
        try:
            print(message.encode('utf-8', errors='replace').decode('utf-8'), **kwargs)
        except:
            print(str(message), **kwargs)
    
    # å¦‚æœçŠ¶æ€ç›‘æ§å¯ç”¨ï¼Œä¹Ÿå‘é€åˆ°çŠ¶æ€ç›‘æ§ï¼ˆä¸ä¼ é€’printå‚æ•°ï¼‰
    if STATUS_MONITOR_AVAILABLE and not force_terminal:
        try:
            log_message(str(message))
        except Exception as e:
            print(f"[çŠ¶æ€ç›‘æ§è¾“å‡ºå¤±è´¥] {e}")

def safe_update_status(message, progress=None):
    """å®‰å…¨çš„çŠ¶æ€æ›´æ–°å‡½æ•°"""
    # è¾“å‡ºåˆ°terminal
    safe_print(f"[çŠ¶æ€] {message}")
    
    # æ›´æ–°çŠ¶æ€ç›‘æ§
    if STATUS_MONITOR_AVAILABLE:
        try:
            update_status(message, progress)
        except Exception as e:
            safe_print(f"[çŠ¶æ€æ›´æ–°å¤±è´¥] {e}", force_terminal=True)

# å¯¼å…¥TensorFlowä¿®å¤å’ŒGPUä¼˜åŒ–
from tensorflow_fix import (configure_gpu, safe_load_model, set_gpu_strategy, 
                           monitor_gpu_memory, compile_model_with_gpu_optimization)
                           
# é…ç½®GPUï¼ˆå¼ºåˆ¶å¯ç”¨ä»¥æé«˜æ€§èƒ½ï¼‰
gpu_available = configure_gpu(force_gpu=False)
gpu_strategy = set_gpu_strategy()

print(f"[LSTM GPU] GPUå¯ç”¨: {gpu_available}")
print(f"[LSTM GPU] åˆ†å¸ƒå¼ç­–ç•¥: {type(gpu_strategy).__name__}")

# å°è¯•å¯¼å…¥LSTMç›¸å…³åº“
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import (LSTM, Dense, Dropout, Input, Conv1D, 
                                       MaxPooling1D, BatchNormalization, 
                                       Bidirectional, Flatten)
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError
    from tensorflow.keras.regularizers import l2
    TENSORFLOW_AVAILABLE = True
    print("TensorFlow/Keras å¯ç”¨ï¼Œæ”¯æŒCNN-LSTMæ··åˆæ¶æ„")
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("TensorFlow/Keras ä¸å¯ç”¨ï¼Œå°†è·³è¿‡LSTMåŠŸèƒ½")

# å°è¯•å¯¼å…¥é«˜çº§æ¨¡å‹
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

try:
    from catboost import CatBoostRegressor
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False

# å°è¯•å¯¼å…¥é«˜çº§æ’å€¼æ¨¡å—
try:
    from advanced_data_imputation import AdvancedDataImputation
    ADVANCED_IMPUTATION_AVAILABLE = True
except ImportError:
    ADVANCED_IMPUTATION_AVAILABLE = False

# ç¦ç”¨è­¦å‘Š
warnings.filterwarnings('ignore')
sns.set_style("whitegrid")

class EnhancedDataPreprocessor:
    """å¢å¼ºçš„æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, normalization_method='z-score', window_size=20):
        self.normalization_method = normalization_method
        self.window_size = window_size
        self.scaler = None
        
    def normalize_data(self, data):
        """æ”¹è¿›çš„æ•°æ®å½’ä¸€åŒ–ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£é¿å…æœªæ¥ä¿¡æ¯æ³„éœ²"""
        from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
        
        if self.normalization_method == 'z-score':
            scaler_class = StandardScaler
        elif self.normalization_method == 'min-max':
            scaler_class = MinMaxScaler
        elif self.normalization_method == 'robust':
            scaler_class = RobustScaler
        else:
            scaler_class = StandardScaler
        
        # å¤„ç†æ¯ä¸ªç‰¹å¾åˆ—
        normalized_data = data.copy()
        for col in data.columns:
            if data[col].dtype in ['float64', 'int64']:
                # ä½¿ç”¨æ»‘åŠ¨çª—å£å½’ä¸€åŒ–
                col_data = data[col].values
                normalized_col = []
                
                for i in range(len(col_data)):
                    if i < self.window_size:
                        # ä½¿ç”¨å‰é¢æ‰€æœ‰æ•°æ®
                        if i == 0:
                            normalized_col.append(0.0)
                        else:
                            window_data = col_data[:i].reshape(-1, 1)
                            scaler = scaler_class()
                            scaler.fit(window_data)
                            normalized_value = scaler.transform([[col_data[i]]])[0][0]
                            normalized_col.append(normalized_value)
                    else:
                        # ä½¿ç”¨æ»‘åŠ¨çª—å£
                        window_data = col_data[i-self.window_size:i].reshape(-1, 1)
                        scaler = scaler_class()
                        scaler.fit(window_data)
                        normalized_value = scaler.transform([[col_data[i]]])[0][0]
                        normalized_col.append(normalized_value)
                
                normalized_data[col] = normalized_col
        
        return normalized_data
    
    def handle_missing_data(self, data):
        """æ™ºèƒ½ç¼ºå¤±æ•°æ®å¤„ç†"""
        # 1. å‰å‘å¡«å……
        data = data.fillna(method='ffill')
        
        # 2. åå‘å¡«å……
        data = data.fillna(method='bfill')
        
        # 3. çº¿æ€§æ’å€¼
        data = data.interpolate(method='linear')
        
        # 4. ä»æœ‰ç¼ºå¤±å€¼åˆ™ç”¨0å¡«å……
        data = data.fillna(0)
        
        return data

# é»˜è®¤è‚¡ç¥¨æ± ï¼ˆä¸BMAæ¨¡å‹ä¿æŒä¸€è‡´ï¼‰
MULTI_DAY_TICKER_LIST = [
    # ç§‘æŠ€è‚¡
    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'NFLX', 'CRM', 'ADBE',
    'ORCL', 'IBM', 'INTC', 'AMD', 'QCOM', 'AVGO', 'TXN', 'MU', 'AMAT', 'LRCX',
    'KLAC', 'MRVL', 'ON', 'SWKS', 'MCHP', 'ADI', 'XLNX', 'SNPS', 'CDNS', 'FTNT','LUNR','INOD','SMR','UEC','UUUU','OKLO','LEU','HIMS','APLD','RGTI','QUBT','QBTS','RXRX','LFMD','NBIS','GRAL','RIVN','TEM', 'AGEN','PYPL','LB','SOFI','CEG','DOC','VST','NEM','LQQDA','LIDR','NSC','NVO','EDIT','CRWV','OPEN', 'NAOV', 'CAN', 'OPTT', 'BBAI', 'SOUN',
    'FFAI','BWXT', 'ASML', 'MRNA', 'CRSP', 'JOBY', 'OSCR', 'AIRO', 'ABCL', 'HIMS', 'LTBR', 'RDDT', 'ETORO',
    # æ¶ˆè´¹é›¶å”®
    'COST', 'WMT', 'TGT', 'HD', 'LOW', 'NKE', 'SBUX', 'MCD', 'DIS', 'PYPL',
    'SQ', 'SHOP', 'EBAY', 'ETSY', 'W', 'CHWY', 'ROKU', 'SPOT', 'ZM', 'UBER',
    'LYFT', 'DASH', 'ABNB', 'BKNG', 'EXPE', 'TJX', 'ROST', 'ULTA', 'LULU', 'RH',
    # åŒ»ç–—å¥åº·
    'JNJ', 'PFE', 'UNH', 'ABBV', 'MRK', 'TMO', 'DHR', 'ABT', 'LLY', 'BMY',
    'AMGN', 'GILD', 'BIIB', 'REGN', 'VRTX', 'ILMN', 'MRNA', 'BNTX', 'ZTS', 'CVS',
    'CI', 'HUM', 'ANTM', 'MCK', 'ABC', 'CAH', 'WAT', 'A', 'IQV', 'CRL',
    # é‡‘èæœåŠ¡
    'JPM', 'BAC', 'WFC', 'C', 'GS', 'MS', 'USB', 'PNC', 'TFC', 'COF',
    'AXP', 'BLK', 'SCHW', 'SPGI', 'ICE', 'CME', 'MCO', 'MSCI', 'PYPL', 'V',
    'MA', 'FIS', 'FISV', 'ADP', 'PAYX', 'WU', 'SYF', 'DFS', 'ALLY', 'RF',
    # å·¥ä¸šææ–™
    'BA', 'CAT', 'DE', 'GE', 'HON', 'LMT', 'MMM', 'RTX', 'UPS', 'FDX',
    'NSC', 'UNP', 'CSX', 'ODFL', 'CHRW', 'EXPD', 'XPO', 'JBHT', 'KNX', 'J',
    'EMR', 'ETN', 'ITW', 'PH', 'ROK', 'DOV', 'FTV', 'XYL', 'IEX', 'GNRC',
    # èƒ½æºå…¬ç”¨
    'XOM', 'CVX', 'COP', 'EOG', 'SLB', 'PSX', 'VLO', 'MPC', 'KMI', 'OKE',
    'WMB', 'ET', 'EPD', 'MPLX', 'AM', 'NEE', 'DUK', 'SO', 'EXC', 'XEL',
    'AEP', 'PCG', 'ED', 'EIX', 'PPL', 'AES', 'NRG', 'CNP', 'CMS', 'DTE',
    # æˆ¿åœ°äº§
    'AMT', 'PLD', 'CCI', 'EQIX', 'SPG', 'O', 'WELL', 'AVB', 'EQR', 'UDR',
    'ESS', 'MAA', 'CPT', 'AIV', 'EXR', 'PSA', 'BXP', 'VTR', 'HCP', 'PEAK',
    # é€šä¿¡æœåŠ¡
    'VZ', 'T', 'TMUS', 'CHTR', 'CMCSA', 'VIA', 'LBRDA', 'LBRDK', 'DISH', 'SIRI',
    # åŸºç¡€ææ–™
    'LIN', 'APD', 'ECL', 'SHW', 'FCX', 'NEM', 'GOLD', 'AA', 'X', 'CLF',
    'NUE', 'STLD', 'CMC', 'RS', 'WOR', 'RPM', 'PPG', 'DD', 'DOW', 'LYB',
    # æ¶ˆè´¹å¿…éœ€å“
    'PG', 'KO', 'PEP', 'WMT', 'COST', 'CL', 'KMB', 'GIS', 'K', 'CPB',
    'CAG', 'SJM', 'HRL', 'TSN', 'TYSON', 'ADM', 'BG', 'CF', 'MOS', 'FMC',
    # æ–°å…´å¢é•¿
    'SQ', 'SHOP', 'ROKU', 'ZOOM', 'DOCU', 'OKTA', 'SNOW', 'PLTR', 'RBLX', 'U',
    'DDOG', 'CRWD', 'ZS', 'NET', 'FSLY', 'TWLO', 'SPLK', 'WDAY', 'VEEV', 'ZEN',
    'TEAM', 'ATLASSIAN', 'MELI', 'SE', 'BABA', 'JD', 'PDD', 'BIDU', 'BILI', 'IQ',
    # ç”Ÿç‰©æŠ€æœ¯
    'MRNA', 'BNTX', 'NOVT', 'SGEN', 'BLUE', 'BMRN', 'TECH', 'SRPT', 'RARE', 'FOLD',
    'EDIT', 'CRSP', 'NTLA', 'BEAM', 'VERV', 'PRIME', 'SAGE', 'IONS', 'IOVA', 'ARWR',
    # æ¸…æ´èƒ½æº
    'TSLA', 'NIO', 'XPEV', 'LI', 'RIVN', 'LCID', 'QS', 'BLNK', 'CHPT', 'PLUG',
    'FCEL', 'BE', 'ENPH', 'SEDG', 'RUN', 'NOVA', 'SPWR', 'CSIQ', 'JKS', 'SOL'
]

# å»é‡å¤„ç†
MULTI_DAY_TICKER_LIST = list(dict.fromkeys(MULTI_DAY_TICKER_LIST))


class DiversityPreservingImputer:
    """ä¿æŒæ•°æ®å¤šæ ·æ€§çš„æ’å€¼å™¨"""
    
    def __init__(self, preserve_diversity=True):
        self.preserve_diversity = preserve_diversity
        
    def smart_imputation_pipeline(self, data):
        """æ™ºèƒ½æ’å€¼æµæ°´çº¿ï¼Œä¿æŒè‚¡ç¥¨é—´å·®å¼‚"""
        if not self.preserve_diversity:
            # ä½¿ç”¨åŸæœ‰é€»è¾‘
            return self._advanced_imputation(data)
        
        print(f"[DIVERSITY] ä¿æŒå¤šæ ·æ€§æ’å€¼: {data.shape}")
        
        # æ­¥éª¤1: åŸºç¡€æ’å€¼ - åªå¤„ç†è¿ç»­ç¼ºå¤±
        data_filled = data.copy()
        
        # å‰å‘å¡«å……ï¼ˆé™åˆ¶ä¸º2å¤©ï¼Œé¿å…è¿‡åº¦å¹³æ»‘ï¼‰
        data_filled = data_filled.fillna(method='ffill', limit=2)
        
        # çº¿æ€§æ’å€¼ï¼ˆåªå¯¹å°èŒƒå›´ç¼ºå¤±ï¼‰
        for col in data_filled.columns:
            # åªå¯¹è¿ç»­ç¼ºå¤±å°‘äº3ä¸ªçš„è¿›è¡Œçº¿æ€§æ’å€¼
            mask = data_filled[col].isnull()
            if mask.sum() > 0:
                # è¯†åˆ«è¿ç»­ç¼ºå¤±æ®µ
                consecutive_nulls = (mask != mask.shift()).cumsum()
                null_groups = data_filled[mask].groupby(consecutive_nulls[mask])
                
                for group_id, group in null_groups:
                    if len(group) <= 3:  # åªå¯¹çŸ­ç¼ºå¤±æ®µæ’å€¼
                        start_idx = group.index[0]
                        end_idx = group.index[-1]
                        
                        # çº¿æ€§æ’å€¼
                        before_val = data_filled[col].iloc[:start_idx].last_valid_index()
                        after_val = data_filled[col].iloc[end_idx+1:].first_valid_index()
                        
                        if before_val is not None and after_val is not None:
                            before_val = data_filled.loc[before_val, col]
                            after_val = data_filled.loc[after_val, col]
                            
                            # ç®€å•çº¿æ€§æ’å€¼
                            steps = len(group) + 1
                            interpolated = np.linspace(before_val, after_val, steps)[1:-1]
                            data_filled.loc[group.index, col] = interpolated
        
        # æ­¥éª¤2: å‰©ä½™ç¼ºå¤±ç”¨å‰å€¼å¡«å……
        data_filled = data_filled.fillna(method='ffill')
        
        # æ­¥éª¤3: å¦‚æœè¿˜æœ‰ç¼ºå¤±ï¼Œç”¨åå€¼å¡«å……
        data_filled = data_filled.fillna(method='bfill')
        
        # æ­¥éª¤4: å¦‚æœä»æœ‰ç¼ºå¤±ï¼Œç”¨åˆ—å‡å€¼å¡«å……
        remaining_nulls = data_filled.isnull().sum().sum()
        if remaining_nulls > 0:
            print(f"[DIVERSITY] ä½¿ç”¨å‡å€¼å¡«å……å‰©ä½™ {remaining_nulls} ä¸ªç¼ºå¤±å€¼")
            for col in data_filled.columns:
                if data_filled[col].isnull().any():
                    col_mean = data_filled[col].mean()
                    if pd.isna(col_mean):
                        col_mean = 0
                    data_filled[col] = data_filled[col].fillna(col_mean)
        
        print(f"[DIVERSITY] æ’å€¼å®Œæˆï¼Œå‰©ä½™ç¼ºå¤±: {data_filled.isnull().sum().sum()}")
        return data_filled
        
    def _advanced_imputation(self, data):
        """åŸæœ‰çš„é«˜çº§æ’å€¼é€»è¾‘ï¼ˆè¿‡åº¦å¹³æ»‘ï¼‰"""
        # è¿™é‡Œä¼šè°ƒç”¨åŸæœ‰çš„advanced_data_imputationé€»è¾‘
        # ä½†æˆ‘ä»¬åœ¨æ–°çš„å®ç°ä¸­é¿å…ä½¿ç”¨
        return data.fillna(method='ffill').fillna(method='bfill').fillna(0)


class MultiDayLSTMQuantModel:
    """å¤šæ—¥LSTMé‡åŒ–åˆ†ææ¨¡å‹"""
    
    
    def build_multi_day_lstm_sequences_fixed(self, factors_df, targets_df, window=None):
        """ä¿®å¤ç‰ˆï¼šæ„å»ºå¤šæ—¥LSTMæ—¶åºæ•°æ®"""
        if window is None:
            window = self.lstm_window
        
        print(f"[LSTM SEQ FIX] æ„å»ºåºåˆ—ï¼Œçª—å£é•¿åº¦: {window}")
        
        # é€‰æ‹©é€‚åˆå¤šæ—¥é¢„æµ‹çš„å› å­
        available_factors = [f for f in self.multi_day_factors if f in factors_df.columns]
        if not available_factors:
            print(f"[LSTM SEQ FIX ERROR] æ²¡æœ‰æ‰¾åˆ°å¤šæ—¥LSTMå› å­")
            return None, None, None
        
        print(f"[LSTM SEQ FIX] ä½¿ç”¨ {len(available_factors)} ä¸ªå› å­: {available_factors[:5]}...")
        
        # æå–LSTMå› å­æ•°æ®
        lstm_data = factors_df[available_factors].copy()
        
        # æ£€æŸ¥å› å­æ–¹å·® - ä¿®å¤é—®é¢˜2ï¼šå› å­å…¨ä¸ºå¸¸æ•°
        factor_stds = lstm_data.std()
        zero_var_factors = factor_stds[factor_stds == 0].index.tolist()
        if zero_var_factors:
            print(f"[LSTM SEQ FIX WARNING] å‘ç°æ–¹å·®ä¸º0çš„å› å­: {zero_var_factors}")
            lstm_data = lstm_data.drop(columns=zero_var_factors)
            available_factors = [f for f in available_factors if f not in zero_var_factors]
        
        if len(available_factors) == 0:
            print(f"[LSTM SEQ FIX ERROR] æ‰€æœ‰å› å­æ–¹å·®ä¸º0")
            return None, None, None
        
        print(f"[LSTM SEQ FIX] è¿‡æ»¤åä¿ç•™ {len(available_factors)} ä¸ªæœ‰æ•ˆå› å­")
        
        # æ™ºèƒ½æ•°æ®å¤„ç† - æ”¹è¿›å¡«å……ç­–ç•¥
        if ADVANCED_IMPUTATION_AVAILABLE:
            try:
                from advanced_data_imputation import AdvancedDataImputation
                imputer = AdvancedDataImputation()
                lstm_data = imputer.smart_imputation_pipeline(lstm_data)
            except Exception as e:
                print(f"[LSTM SEQ FIX] é«˜çº§æ’å€¼å¤±è´¥: {e}")
                # æ”¹è¿›çš„å¡«å……ç­–ç•¥
                lstm_data = lstm_data.interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')
                remaining_nan = lstm_data.isnull().sum().sum()
                if remaining_nan > 0:
                    print(f"[LSTM SEQ FIX WARNING] ä»æœ‰ {remaining_nan} ä¸ªNaNï¼Œç”¨0å¡«å……")
                    lstm_data = lstm_data.fillna(0)
        else:
            # æ”¹è¿›çš„å¡«å……ç­–ç•¥
            lstm_data = lstm_data.interpolate(method='linear').fillna(method='ffill').fillna(method='bfill').fillna(0)
        
        # å‡†å¤‡ç›®æ ‡æ•°æ®
        target_columns = [f'target_day_{day}' for day in range(1, self.prediction_days + 1)]
        if not all(col in targets_df.columns for col in target_columns):
            print(f"[LSTM SEQ FIX ERROR] ç¼ºå°‘ç›®æ ‡åˆ—: {target_columns}")
            return None, None, None
        
        targets = targets_df[target_columns].copy()
        
        # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´ - ä¿®å¤é—®é¢˜1ï¼šåºåˆ—å¯¹é½
        min_length = min(len(lstm_data), len(targets))
        lstm_data = lstm_data.iloc[:min_length]
        targets = targets.iloc[:min_length]
        
        print(f"[LSTM SEQ FIX] å¯¹é½åæ•°æ®é•¿åº¦: {min_length}")
        
        # æ„å»ºåºåˆ— - ä¿®å¤ç´¢å¼•å¯¹é½é—®é¢˜
        X_seq = []
        y_seq = []
        valid_indices = []
        
        for i in range(window, min_length):
            # æ£€æŸ¥è¾“å…¥çª—å£æ˜¯å¦æœ‰æ•ˆ
            input_window = lstm_data.iloc[i-window:i]
            target_values = targets.iloc[i]
            
            # æ£€æŸ¥çª—å£å†…æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
            if input_window.isnull().all().all():
                continue
                
            # æ£€æŸ¥ç›®æ ‡æ˜¯å¦æœ‰æ•ˆ
            if target_values.isnull().all():
                continue
            
            X_seq.append(input_window.values)
            y_seq.append(target_values.values)
            valid_indices.append(i)
        
        if len(X_seq) == 0:
            print(f"[LSTM SEQ FIX ERROR] æ²¡æœ‰æœ‰æ•ˆçš„åºåˆ—æ ·æœ¬")
            return None, None, None
        
        X_seq = np.array(X_seq)
        y_seq = np.array(y_seq)
        
        print(f"[LSTM SEQ FIX] æ„å»ºå®Œæˆ: X_seq={X_seq.shape}, y_seq={y_seq.shape}")
        print(f"[LSTM SEQ FIX] è¾“å…¥ç»Ÿè®¡: å‡å€¼={np.mean(X_seq):.4f}, æ ‡å‡†å·®={np.std(X_seq):.4f}")
        print(f"[LSTM SEQ FIX] ç›®æ ‡ç»Ÿè®¡: å‡å€¼={np.mean(y_seq):.4f}, æ ‡å‡†å·®={np.std(y_seq):.4f}")
        
        # å­˜å‚¨ç‰¹å¾åˆ—ä¿¡æ¯
        self.lstm_feature_columns = available_factors
        
        return X_seq, y_seq, valid_indices
    
    def enhance_prediction_diversity(self, ticker, factors_df, original_predictions):
        """å†…ç½®çš„é¢„æµ‹å¤šæ ·æ€§å¢å¼ºæ–¹æ³•ï¼ˆå®Œæ•´ç‰ˆï¼‰"""
        try:
            if original_predictions is None or len(original_predictions) != self.prediction_days:
                return original_predictions
            
            # åˆ›å»ºæˆ–è·å–è‚¡ç¥¨ç‰¹å¾æ¡£æ¡ˆ
            if ticker not in self.stock_statistics:
                self.create_stock_profile(ticker, factors_df)
            
            profile = self.stock_statistics[ticker]
            enhanced_predictions = original_predictions.copy()
            
            # 1. åŸºäºè‚¡ç¥¨ä»£ç çš„ç¡®å®šæ€§è°ƒæ•´ï¼ˆç¡®ä¿æ¯åªè‚¡ç¥¨ä¸åŒï¼‰
            base_adjustments = self._calculate_stock_specific_adjustments(ticker)
            enhanced_predictions += base_adjustments
            
            # 2. åŸºäºå†å²ç‰¹å¾çš„åŠ¨æ€è°ƒæ•´
            if len(factors_df) > 0:
                feature_multipliers = self._calculate_feature_based_adjustments(profile, factors_df)
                enhanced_predictions *= feature_multipliers
            
            # 3. æ³¢åŠ¨ç‡è°ƒæ•´ï¼ˆé«˜æ³¢åŠ¨è‚¡ç¥¨é¢„æµ‹å¹…åº¦æ›´å¤§ï¼‰
            volatility_adjustment = self._calculate_volatility_adjustment(profile)
            enhanced_predictions *= volatility_adjustment
            
            # 4. è¶‹åŠ¿è°ƒæ•´
            trend_adjustment = self._calculate_trend_based_adjustment(profile, factors_df)
            enhanced_predictions *= (1.0 + trend_adjustment)
            
            # 5. åˆç†æ€§æ£€æŸ¥
            enhanced_predictions = self._apply_prediction_constraints(enhanced_predictions)
            
            # 6. è®°å½•è°ƒè¯•ä¿¡æ¯
            self._log_enhancement_details(ticker, original_predictions, enhanced_predictions, profile)
            
            return enhanced_predictions
            
        except Exception as e:
            print(f"[DIVERSITY ENHANCE ERROR] {ticker}: {e}")
            return original_predictions
    
    def create_stock_profile(self, ticker, factors_df):
        """ä¸ºè‚¡ç¥¨åˆ›å»ºç‰¹å¾æ¡£æ¡ˆ"""
        try:
            profile = {}
            
            if len(factors_df) > 0 and 'returns' in factors_df.columns:
                returns = factors_df['returns'].dropna()
                
                if len(returns) > 10:
                    profile['mean_return'] = returns.mean()
                    profile['std_return'] = returns.std()
                    profile['recent_trend'] = returns.tail(10).mean()
                    profile['volatility_regime'] = 'high' if returns.std() > 0.03 else 'normal'
                    profile['momentum_5d'] = returns.tail(5).mean() if len(returns) >= 5 else 0.0
                    profile['momentum_20d'] = returns.tail(20).mean() if len(returns) >= 20 else 0.0
                else:
                    profile = self._get_default_stock_profile(ticker)
            else:
                profile = self._get_default_stock_profile(ticker)
            
            # æ·»åŠ è‚¡ç¥¨æ ‡è¯†ç¬¦
            profile['ticker_hash'] = hash(ticker) % 10000
            profile['ticker_id'] = sum(ord(c) for c in ticker) % 1000
            
            self.stock_statistics[ticker] = profile
            return profile
            
        except Exception as e:
            print(f"[PROFILE ERROR] {ticker}: {e}")
            return self._get_default_stock_profile(ticker)
    
    def _get_default_stock_profile(self, ticker):
        """è·å–é»˜è®¤è‚¡ç¥¨æ¡£æ¡ˆ"""
        return {
            'mean_return': 0.001,
            'std_return': 0.02,
            'recent_trend': 0.0,
            'volatility_regime': 'normal',
            'momentum_5d': 0.0,
            'momentum_20d': 0.0,
            'ticker_hash': hash(ticker) % 10000,
            'ticker_id': sum(ord(c) for c in ticker) % 1000
        }
    
    def _calculate_stock_specific_adjustments(self, ticker):
        """è®¡ç®—è‚¡ç¥¨ç‰¹å¼‚æ€§åŸºç¡€è°ƒæ•´"""
        adjustments = []
        ticker_hash = hash(ticker) % 10000
        
        for day in range(self.prediction_days):
            # ä½¿ç”¨tickerå’Œå¤©æ•°åˆ›å»ºå”¯ä¸€çš„ç¡®å®šæ€§è°ƒæ•´
            day_seed = (ticker_hash + day * 137 + day * day * 23) % 10000
            # ç”Ÿæˆ-0.003åˆ°+0.003çš„è°ƒæ•´ï¼ˆÂ±0.3%ï¼‰
            adjustment = ((day_seed / 10000.0) - 0.5) * 0.006
            adjustments.append(adjustment)
        
        return np.array(adjustments)
    
    def _calculate_feature_based_adjustments(self, profile, factors_df):
        """è®¡ç®—åŸºäºç‰¹å¾çš„è°ƒæ•´å€æ•°"""
        multipliers = []
        
        # åŸºäºè¿‘æœŸè¶‹åŠ¿çš„è°ƒæ•´
        trend_factor = profile.get('recent_trend', 0.0) * 3
        trend_factor = max(-0.1, min(0.1, trend_factor))  # é™åˆ¶åœ¨Â±10%
        
        # åŸºäºåŠ¨é‡çš„è°ƒæ•´
        momentum_5d = profile.get('momentum_5d', 0.0)
        momentum_20d = profile.get('momentum_20d', 0.0)
        momentum_diff = (momentum_5d - momentum_20d) * 2
        momentum_diff = max(-0.05, min(0.05, momentum_diff))  # é™åˆ¶åœ¨Â±5%
        
        for day in range(self.prediction_days):
            # æ—¶é—´è¡°å‡ï¼šè¿‘æœŸé¢„æµ‹å—å½±å“æ›´å¤§
            decay_factor = 1.0 - (day * 0.1)  # ç¬¬1å¤©1.0ï¼Œç¬¬5å¤©0.6
            daily_multiplier = 1.0 + (trend_factor + momentum_diff) * decay_factor
            multipliers.append(daily_multiplier)
        
        return np.array(multipliers)
    
    def _calculate_volatility_adjustment(self, profile):
        """è®¡ç®—æ³¢åŠ¨ç‡è°ƒæ•´å€æ•°"""
        actual_vol = profile.get('std_return', 0.02)
        base_vol = 0.02  # åŸºå‡†2%
        
        # é«˜æ³¢åŠ¨ç‡è‚¡ç¥¨é¢„æµ‹å¹…åº¦æ”¾å¤§ï¼Œä½æ³¢åŠ¨ç‡è‚¡ç¥¨é¢„æµ‹å¹…åº¦ç¼©å°
        vol_ratio = actual_vol / base_vol
        adjustment = 0.7 + (vol_ratio * 0.6)  # 0.7åˆ°1.9çš„èŒƒå›´
        
        # é™åˆ¶æç«¯å€¼
        return max(0.5, min(2.0, adjustment))
    
    def _calculate_trend_based_adjustment(self, profile, factors_df):
        """è®¡ç®—åŸºäºè¶‹åŠ¿çš„è°ƒæ•´"""
        trend_adj = profile.get('recent_trend', 0.0) * 1.5
        
        # å¦‚æœæœ‰æŠ€æœ¯æŒ‡æ ‡ï¼Œç»“åˆRSIç­‰
        if len(factors_df) > 0 and 'rsi' in factors_df.columns:
            try:
                recent_rsi = factors_df['rsi'].tail(5).mean()
                if not pd.isna(recent_rsi):
                    # RSIåç¦»50çš„ç¨‹åº¦å½±å“è°ƒæ•´
                    rsi_deviation = (recent_rsi - 50) / 50 * 0.05
                    trend_adj += rsi_deviation
            except:
                pass
        
        # é™åˆ¶è°ƒæ•´å¹…åº¦
        return max(-0.15, min(0.15, trend_adj))
    
    def _apply_prediction_constraints(self, predictions):
        """åº”ç”¨é¢„æµ‹çº¦æŸ"""
        # é™åˆ¶å•æ—¥é¢„æµ‹åœ¨Â±20%
        predictions = np.clip(predictions, -0.20, 0.20)
        
        # ç¡®ä¿é¢„æµ‹åºåˆ—çš„åˆç†æ€§
        for i in range(1, len(predictions)):
            max_daily_change = 0.08  # ç›¸é‚»æ—¥æœ€å¤§å˜åŒ–8%
            if abs(predictions[i] - predictions[i-1]) > max_daily_change:
                if predictions[i] > predictions[i-1]:
                    predictions[i] = predictions[i-1] + max_daily_change
                else:
                    predictions[i] = predictions[i-1] - max_daily_change
        
        return predictions
    
    def _log_enhancement_details(self, ticker, original, enhanced, profile):
        """è®°å½•å¢å¼ºè¯¦æƒ…"""
        orig_std = np.std(original) if len(original) > 1 else 0
        enh_std = np.std(enhanced) if len(enhanced) > 1 else 0
        orig_range = np.max(original) - np.min(original) if len(original) > 1 else 0
        enh_range = np.max(enhanced) - np.min(enhanced) if len(enhanced) > 1 else 0
        
        print(f"[DIVERSITY] {ticker}: æ ‡å‡†å·® {orig_std:.6f}â†’{enh_std:.6f}, èŒƒå›´ {orig_range:.6f}â†’{enh_range:.6f}")
        print(f"[DIVERSITY] {ticker}: è¶‹åŠ¿{profile.get('recent_trend', 0):.4f}, æ³¢åŠ¨{profile.get('std_return', 0):.4f}")
        print(f"[DIVERSITY] {ticker}: åŸå§‹{[f'{p:.6f}' for p in original]}")
        print(f"[DIVERSITY] {ticker}: å¢å¼º{[f'{p:.6f}' for p in enhanced]}")
    
    def __init__(self, prediction_days=5, enable_advanced_features=True):
        """åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        self.lstm_model = None
        self.stacking_model = None
        self.scaler = StandardScaler()
        self.pca = None
        self.factor_ic_scores = {}
        self.model_scores = {}
        self.stacking_score = {}
        self.lstm_score = {}
        
        # å¤šæ—¥é¢„æµ‹å‚æ•°
        self.prediction_days = prediction_days  # é¢„æµ‹å¤©æ•°ï¼ˆé»˜è®¤5å¤©ï¼‰
        self.lstm_window = 20  # LSTMæ—¶é—´çª—å£ï¼ˆ20ä¸ªäº¤æ˜“æ—¥ï¼‰
        self.target_horizons = list(range(1, prediction_days + 1))  # [1, 2, 3, 4, 5]
        
        # å¢å¼ºçš„æ•°æ®é¢„å¤„ç†å™¨
        self.preprocessor = EnhancedDataPreprocessor(
            normalization_method='z-score',
            window_size=self.lstm_window
        )
        
        # æ¨¡å‹è¯„ä¼°æŒ‡æ ‡
        self.model_metrics = {}
        self.backtest_metrics = {}
        
        # é€‚åˆå¤šæ—¥é¢„æµ‹çš„å› å­ï¼ˆå®Œæ•´ç‰ˆï¼‰
        self.multi_day_factors = [
            # ç§»åŠ¨å¹³å‡çº¿å› å­
            'sma_5', 'sma_10', 'sma_20', 'sma_50', 'sma_200',
            'ema_12', 'ema_26', 'ema_50',
            
            # æŠ€æœ¯æŒ‡æ ‡å› å­
            'rsi', 'rsi_14', 'rsi_30',
            'macd', 'macd_signal', 'macd_histogram',
            'stoch_k', 'stoch_d', 'williams_r',
            'cci', 'adx', 'di_plus', 'di_minus',
            
            # å¸ƒæ—å¸¦å› å­
            'bollinger_upper', 'bollinger_lower', 'bollinger_width',
            'bollinger_position', 'bollinger_squeeze',
            
            # æˆäº¤é‡å› å­
            'volume_sma_20', 'volume_ratio', 'volume_price_trend',
            'money_flow', 'volume_weighted_price', 'accumulation_distribution',
            'on_balance_volume', 'chaikin_money_flow',
            
            # ä»·æ ¼ä½ç½®å› å­
            'price_position', 'price_change', 'price_acceleration',
            'high_low_ratio', 'close_position', 'gap_ratio',
            
            # æ³¢åŠ¨ç‡å› å­
            'volatility_5', 'volatility_10', 'volatility_20', 'volatility_50',
            'atr_14', 'true_range', 'parkinson_volatility',
            'garman_klass_volatility', 'realized_volatility',
            
            # åŠ¨é‡å› å­
            'momentum_1', 'momentum_3', 'momentum_5', 'momentum_10', 'momentum_20',
            'roc_5', 'roc_10', 'roc_20', 'rate_of_change',
            
            # å‡å€¼å›å½’å› å­
            'mean_reversion_5', 'mean_reversion_10', 'mean_reversion_20',
            'zscore_5', 'zscore_10', 'zscore_20',
            
            # è¶‹åŠ¿å¼ºåº¦å› å­
            'trend_strength', 'trend_consistency', 'directional_movement',
            'aroon_up', 'aroon_down', 'aroon_oscillator',
            
            # ç›¸å¯¹å¼ºåº¦å› å­
            'relative_strength', 'price_relative_to_sma', 'price_relative_to_ema',
            'volume_relative_strength', 'momentum_relative_strength',
            
            # å‘¨æœŸæ€§å› å­
            'day_of_week', 'day_of_month', 'days_since_earnings',
            'seasonal_trend', 'calendar_effect',
            
            # åŸºæœ¬é¢æŠ€æœ¯ç»“åˆå› å­
            'price_volume_correlation', 'volume_price_momentum',
            'intraday_intensity', 'ease_of_movement',
            
            # é«˜çº§ç»„åˆå› å­
            'composite_momentum', 'composite_mean_reversion',
            'volatility_adjusted_momentum', 'risk_adjusted_return',
            
            # å¸‚åœºç»“æ„å› å­
            'support_resistance', 'breakout_probability',
            'trend_continuation', 'reversal_probability',
            
            # å›æŠ¥åˆ†å¸ƒå› å­
            'returns', 'log_returns', 'squared_returns',
            'return_skewness', 'return_kurtosis', 'downside_deviation'
        ]
        
        # ç‰¹å¾ä¸€è‡´æ€§
        self.training_feature_columns = None
        self.lstm_feature_columns = None
        self.enable_advanced_features = enable_advanced_features
        
        # é¢„æµ‹å¤šæ ·æ€§ä¿®å¤å™¨ï¼ˆåœ¨éœ€è¦æ—¶åˆ›å»ºï¼‰
        self.prediction_fix = None
        self.stock_statistics = {}  # å­˜å‚¨è‚¡ç¥¨ç»Ÿè®¡ä¿¡æ¯
        self.diversity_enhancer = DiversityPreservingImputer(preserve_diversity=True)
        
        safe_print(f"[MULTI-DAY LSTM] åˆå§‹åŒ–å¤šæ—¥LSTMé‡åŒ–æ¨¡å‹ï¼ˆå¢å¼ºç‰ˆï¼‰")
        safe_print(f"[MULTI-DAY LSTM] TensorFlowå¯ç”¨: {TENSORFLOW_AVAILABLE}")
        safe_print(f"[MULTI-DAY LSTM] é¢„æµ‹å¤©æ•°: {self.prediction_days} å¤©")
        safe_print(f"[MULTI-DAY LSTM] å¢å¼ºé¢„å¤„ç†: å¯ç”¨")
        safe_print(f"[MULTI-DAY LSTM] é¢„æµ‹ç›®æ ‡: æœªæ¥{self.prediction_days}ä¸ªäº¤æ˜“æ—¥æ”¶ç›Šç‡")
        safe_print(f"[MULTI-DAY LSTM] å¤šæ—¥å› å­æ•°é‡: {len(self.multi_day_factors)}")
        
        # å°è¯•åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
        self.load_trained_model()
    
    def load_trained_model(self):
        """åŠ è½½å·²è®­ç»ƒçš„LSTMæ¨¡å‹ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        if not TENSORFLOW_AVAILABLE:
            return
            
        try:
            safe_print("[MULTI-DAY LSTM] æ£€æŸ¥å·²è®­ç»ƒçš„æ¨¡å‹...")
            
            model_dir = "trained_models"
            latest_model_path = f"{model_dir}/latest_multi_day_lstm.h5"
            info_path = f"{model_dir}/latest_model_info.json"
            
            if os.path.exists(latest_model_path) and os.path.exists(info_path):
                # å¼ºåˆ¶é‡æ–°è®­ç»ƒæ¨¡å¼ - ä¸åŠ è½½å·²æœ‰æ¨¡å‹
                safe_print("[MULTI-DAY LSTM] ğŸ”„ æ£€æµ‹åˆ°å·²æœ‰æ¨¡å‹ï¼Œä½†å¯ç”¨å¼ºåˆ¶é‡æ–°è®­ç»ƒæ¨¡å¼")
                safe_print("[MULTI-DAY LSTM] ğŸ“Š è¿™å°†ç¡®ä¿ä½¿ç”¨æœ€æ–°æ•°æ®å’Œå®Œæ•´è®­ç»ƒè¿‡ç¨‹")
                safe_print("[MULTI-DAY LSTM] ğŸ’ª å¼ºåˆ¶å®Œæ•´è®­ç»ƒä»¥è·å¾—æœ€ä½³æ€§èƒ½å’Œå¤šæ ·æ€§")
                self.lstm_model = None
            else:
                safe_print("[MULTI-DAY LSTM] æœªæ‰¾åˆ°å·²è®­ç»ƒçš„æ¨¡å‹ï¼Œå°†é‡æ–°è®­ç»ƒ")
                self.lstm_model = None
                
        except Exception as e:
            safe_print(f"[MULTI-DAY LSTM] æ£€æŸ¥æ¨¡å‹æ—¶å‡ºé”™: {e}")
            self.lstm_model = None
    
    def download_multi_day_data(self, tickers, start_date, end_date):
        """ä¸‹è½½å¤šæ—¥é¢„æµ‹è‚¡ç¥¨æ•°æ®ï¼ˆå¢å¼ºè°ƒè¯•ç‰ˆï¼‰"""
        safe_print(f"[MULTI-DAY DATA] å¼€å§‹ä¸‹è½½ {len(tickers)} åªè‚¡ç¥¨çš„æ•°æ®...")
        safe_print(f"[MULTI-DAY DATA] æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
        safe_print(f"[MULTI-DAY DATA] è‚¡ç¥¨åˆ—è¡¨: {', '.join(tickers[:10])}{'...' if len(tickers) > 10 else ''}")
        safe_update_status("å¼€å§‹ä¸‹è½½è‚¡ç¥¨æ•°æ®", 0)
        
        data = {}
        success_count = 0
        failed_count = 0
        
        for i, ticker in enumerate(tickers, 1):
            try:
                progress = int((i / len(tickers)) * 50)  # ä¸‹è½½é˜¶æ®µå 50%è¿›åº¦
                safe_update_status(f"ä¸‹è½½è‚¡ç¥¨æ•°æ® {ticker} ({i}/{len(tickers)})", progress)
                safe_print(f"[{i:3d}/{len(tickers):3d}] ä¸‹è½½ {ticker:6s} æ•°æ®...", end=" ")
                
                # ä¸‹è½½æ•°æ®
                stock_data = yf.download(
                    ticker, 
                    start=start_date, 
                    end=end_date, 
                    interval='1d',
                    progress=False, 
                    auto_adjust=True,
                    timeout=30
                )
                
                safe_print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {stock_data.shape}")
                
                if len(stock_data) > 20:  # è‡³å°‘éœ€è¦20å¤©æ•°æ®ï¼ˆè°ƒè¯•ç”¨ï¼‰
                    # å¤„ç†MultiIndexåˆ—é—®é¢˜
                    if isinstance(stock_data.columns, pd.MultiIndex):
                        safe_print(f"å¤„ç†MultiIndexåˆ—")
                        stock_data.columns = stock_data.columns.droplevel(1)
                    
                    # ç¡®ä¿åŸºæœ¬åˆ—å­˜åœ¨
                    required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
                    available_columns = stock_data.columns.tolist()
                    safe_print(f"å¯ç”¨åˆ—: {available_columns}")
                    
                    if all(col in stock_data.columns for col in required_columns):
                        # æ£€æŸ¥æ•°æ®è´¨é‡
                        null_counts = stock_data[required_columns].isnull().sum()
                        safe_print(f"ç©ºå€¼ç»Ÿè®¡: {null_counts.to_dict()}")
                        
                        data[ticker] = stock_data
                        success_count += 1
                        safe_print(f"[OK] {len(stock_data)} å¤©æ•°æ®")
                    else:
                        missing_cols = [col for col in required_columns if col not in stock_data.columns]
                        failed_count += 1
                        safe_print(f"[FAIL] ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
                else:
                    failed_count += 1
                    safe_print(f"[FAIL] æ•°æ®ä¸è¶³ ({len(stock_data)} å¤©)")
                    
            except Exception as e:
                failed_count += 1
                error_msg = str(e)
                safe_print(f"[FAIL] å¤±è´¥: {error_msg[:50]}...")
                safe_print(f"å®Œæ•´é”™è¯¯: {error_msg}")
                continue
        
        safe_print(f"[PROGRESS] ä¸‹è½½ç»“æŸæ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
        safe_print(f"[SUMMARY] æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}, æ€»è®¡: {len(tickers)}")
        safe_print(f"[MULTI-DAY DATA] æˆåŠŸä¸‹è½½ {len(data)} åªè‚¡ç¥¨çš„æ•°æ®")
        
        if len(data) == 0:
            safe_print("[ERROR] æ²¡æœ‰æˆåŠŸä¸‹è½½ä»»ä½•è‚¡ç¥¨æ•°æ®ï¼è¯·æ£€æŸ¥:")
            safe_print("  1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
            safe_print("  2. è‚¡ç¥¨ä»£ç æ˜¯å¦æ­£ç¡®")
            safe_print("  3. æ—¥æœŸèŒƒå›´æ˜¯å¦åˆç†")
            safe_print("  4. yfinanceæ˜¯å¦æ­£å¸¸å·¥ä½œ")
        
        safe_update_status(f"è‚¡ç¥¨æ•°æ®ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸ{success_count}åª", 50)
        return data
    
    def calculate_multi_day_factors(self, data):
        """è®¡ç®—é€‚åˆå¤šæ—¥é¢„æµ‹çš„æŠ€æœ¯å› å­"""
        try:
            print(f"[MULTI-DAY FACTORS] è®¡ç®—å¤šæ—¥é¢„æµ‹æŠ€æœ¯å› å­...")
            
            factors = pd.DataFrame(index=data.index)
            
            # ä»·æ ¼ç›¸å…³å› å­
            close = data['Close']
            high = data['High']
            low = data['Low']
            volume = data['Volume']
            
            # åŸºç¡€æ”¶ç›Šç‡å› å­ï¼ˆç”¨äºå¤šæ ·æ€§å¢å¼ºï¼‰
            factors['returns'] = close.pct_change()
            factors['log_returns'] = np.log(close / close.shift(1))
            
            # ç§»åŠ¨å¹³å‡çº¿
            factors['sma_5'] = close.rolling(window=5).mean()
            factors['sma_10'] = close.rolling(window=10).mean()
            factors['sma_20'] = close.rolling(window=20).mean()
            
            # æŒ‡æ•°ç§»åŠ¨å¹³å‡
            factors['ema_12'] = close.ewm(span=12).mean()
            factors['ema_26'] = close.ewm(span=26).mean()
            
            # RSI
            delta = close.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            factors['rsi'] = 100 - (100 / (1 + rs))
            
            # MACD
            factors['macd'] = factors['ema_12'] - factors['ema_26']
            factors['macd_signal'] = factors['macd'].ewm(span=9).mean()
            factors['macd_histogram'] = factors['macd'] - factors['macd_signal']
            
            # å¸ƒæ—å¸¦
            bb_ma = close.rolling(window=20).mean()
            bb_std = close.rolling(window=20).std()
            factors['bollinger_upper'] = bb_ma + (bb_std * 2)
            factors['bollinger_lower'] = bb_ma - (bb_std * 2)
            factors['bollinger_width'] = factors['bollinger_upper'] - factors['bollinger_lower']
            
            # æˆäº¤é‡å› å­
            volume_sma_20 = volume.rolling(window=20).mean()
            factors['volume_sma_20'] = volume_sma_20
            factors['volume_ratio'] = volume / volume_sma_20
            factors['money_flow'] = (close * volume).rolling(window=10).mean()
            
            # ä»·æ ¼ä½ç½®
            sma_20_value = close.rolling(window=20).mean()
            factors['price_position'] = (close - sma_20_value) / sma_20_value
            
            # æ³¢åŠ¨ç‡
            returns = close.pct_change()
            factors['volatility_20'] = returns.rolling(window=20).std() * np.sqrt(252)
            
            # ATR (å¹³å‡çœŸå®æ³¢å¹…)
            tr1 = high - low
            tr2 = abs(high - close.shift(1))
            tr3 = abs(low - close.shift(1))
            true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
            factors['atr_14'] = true_range.rolling(window=14).mean()
            
            # åŠ¨é‡å› å­ï¼ˆé€‚åˆå¤šæ—¥é¢„æµ‹ï¼‰
            factors['momentum_5'] = close.pct_change(5)
            factors['momentum_10'] = close.pct_change(10)
            
            # ç›¸å¯¹å¼ºåº¦
            rsi_value = factors['rsi']
            sma_5_value = close.rolling(window=5).mean()
            sma_20_value = close.rolling(window=20).mean()
            factors['rsi_deviation'] = rsi_value - 50
            factors['price_to_sma20'] = close / sma_20_value
            factors['price_to_sma5'] = close / sma_5_value
            
            # æˆäº¤é‡å¼ºåº¦
            factors['volume_price_trend'] = ((close - close.shift(1)) / close.shift(1) * volume).rolling(window=10).sum()
            
            # CCI (Commodity Channel Index) - å•†å“é€šé“æŒ‡æ•°
            typical_price = (high + low + close) / 3
            sma_tp_20 = typical_price.rolling(window=20).mean()
            mad_tp_20 = typical_price.rolling(window=20).apply(lambda x: np.mean(np.abs(x - x.mean())), raw=False)
            factors['cci_20'] = (typical_price - sma_tp_20) / (0.015 * mad_tp_20)
            
            # CCI 14å¤©ç‰ˆæœ¬
            sma_tp_14 = typical_price.rolling(window=14).mean()
            mad_tp_14 = typical_price.rolling(window=14).apply(lambda x: np.mean(np.abs(x - x.mean())), raw=False)
            factors['cci_14'] = (typical_price - sma_tp_14) / (0.015 * mad_tp_14)
            
            # CCIç›¸å…³è¡ç”Ÿå› å­
            factors['cci_20_normalized'] = factors['cci_20'] / 100.0  # æ ‡å‡†åŒ–åˆ°[-1,1]åŒºé—´
            factors['cci_momentum'] = factors['cci_20'].diff(5)  # CCIçš„5æ—¥åŠ¨é‡
            factors['cci_position'] = np.where(factors['cci_20'] > 100, 1, np.where(factors['cci_20'] < -100, -1, 0))  # CCIä½ç½®ä¿¡å·
            
            print(f"[MULTI-DAY FACTORS] è®¡ç®—äº† {len(factors.columns)} ä¸ªå¤šæ—¥é¢„æµ‹æŠ€æœ¯å› å­ (åŒ…å«CCI)")
            return factors
            
        except Exception as e:
            print(f"[MULTI-DAY FACTORS ERROR] è®¡ç®—å› å­å¤±è´¥: {e}")
            return pd.DataFrame(index=data.index)
    

    def calculate_distinctive_factors(self, data):
        """è®¡ç®—æ›´å¤šåŒºåˆ†æ€§æŠ€æœ¯å› å­"""
        df = data.copy()
        close = df['Close']
        high = df['High']
        low = df['Low']
        volume = df['Volume']
        
        print(f"[DISTINCTIVE] è®¡ç®—åŒºåˆ†æ€§å› å­...")
        
        # 1. ä»·æ ¼ç›¸å¯¹ä½ç½®å› å­ï¼ˆæ›´ç²¾ç¡®ï¼‰
        df['price_percentile_5'] = close.rolling(5).rank() / 5
        df['price_percentile_10'] = close.rolling(10).rank() / 10
        df['price_percentile_20'] = close.rolling(20).rank() / 20
        
        # 2. æ³¢åŠ¨ç‡å› å­ï¼ˆå¤šæ—¶é—´å‘¨æœŸï¼‰
        df['volatility_5'] = close.pct_change().rolling(5).std()
        df['volatility_10'] = close.pct_change().rolling(10).std()
        df['volatility_ratio'] = df['volatility_5'] / (df['volatility_10'] + 1e-8)
        
        # 3. æˆäº¤é‡ç›¸å¯¹å¼ºåº¦
        df['volume_rank_5'] = volume.rolling(5).rank() / 5
        df['volume_rank_10'] = volume.rolling(10).rank() / 10
        df['volume_momentum'] = volume / volume.rolling(10).mean()
        
        # 4. ä»·æ ¼-æˆäº¤é‡èƒŒç¦»
        price_change = close.pct_change()
        volume_change = volume.pct_change()
        df['price_volume_divergence'] = (price_change.rolling(5).mean() * 
                                       volume_change.rolling(5).mean() * -1)
        
        # 5. é«˜ä½ç‚¹ç›¸å¯¹ä½ç½®
        df['high_low_ratio'] = (close - low) / (high - low + 1e-8)
        df['close_position'] = (close - low.rolling(10).min()) / (high.rolling(10).max() - low.rolling(10).min() + 1e-8)
        
        # 6. è¶‹åŠ¿å¼ºåº¦
        sma_5 = close.rolling(5).mean()
        sma_20 = close.rolling(20).mean()
        df['trend_strength'] = (sma_5 - sma_20) / sma_20
        df['trend_consistency'] = (sma_5 > sma_5.shift(1)).rolling(5).sum() / 5
        
        # 7. å¸‚åœºå¾®è§‚ç»“æ„
        df['price_efficiency'] = abs(close.pct_change()) / (volume / volume.rolling(20).mean() + 1e-8)
        df['intraday_range'] = (high - low) / close
        
        print(f"[DISTINCTIVE] æ·»åŠ äº† {len([col for col in df.columns if col not in data.columns])} ä¸ªæ–°å› å­")
        
        return df

    def prepare_multi_day_ml_data(self, all_data):
        """å‡†å¤‡å¤šæ—¥é¢„æµ‹æœºå™¨å­¦ä¹ æ•°æ®"""
        print(f"[MULTI-DAY ML PREP] å‡†å¤‡å¤šæ—¥é¢„æµ‹æœºå™¨å­¦ä¹ æ•°æ®ï¼Œé¢„æµ‹ç›®æ ‡: æœªæ¥{self.prediction_days}æ—¥æ”¶ç›Šç‡...")
        
        all_factor_data = []
        
        for ticker, data in all_data.items():
            try:
                print(f"[MULTI-DAY ML PREP] å¤„ç† {ticker}...")
                
                # è®¡ç®—å¤šæ—¥é¢„æµ‹å› å­
                factors = self.calculate_multi_day_factors(data)
                distinctive_factors = self.calculate_distinctive_factors(data)
                # åˆå¹¶å› å­
                for col in distinctive_factors.columns:
                    if col not in factors.columns:
                        factors[col] = distinctive_factors[col]
                
                # è®¡ç®—å¤šæ—¥ç›®æ ‡å˜é‡ï¼šæœªæ¥1-5å¤©çš„æ”¶ç›Šç‡
                close_prices = data['Close']
                targets = {}
                
                for day in range(1, self.prediction_days + 1):
                    target_name = f'target_day_{day}'
                    targets[target_name] = close_prices.pct_change(day).shift(-day)
                
                # ç»„åˆç›®æ ‡å˜é‡
                targets_df = pd.DataFrame(targets, index=data.index)
                
                # å¯¹é½æ•°æ®
                aligned_data = pd.concat([factors, targets_df], axis=1)
                
                # åªä¿ç•™æ‰€æœ‰ç›®æ ‡å˜é‡éƒ½éç©ºçš„è¡Œï¼ˆå‘å‰çœ‹åå·®ä¿æŠ¤ï¼‰
                target_columns = [f'target_day_{day}' for day in range(1, self.prediction_days + 1)]
                aligned_data = aligned_data.dropna(subset=target_columns)
                
                if len(aligned_data) < 50:  # è‡³å°‘éœ€è¦50ä¸ªæœ‰æ•ˆæ ·æœ¬
                    print(f"[WARNING] {ticker} æœ‰æ•ˆæ ·æœ¬ä¸è¶³: {len(aligned_data)}")
                    continue
                
                # æ·»åŠ è‚¡ç¥¨å’Œæ—¥æœŸä¿¡æ¯
                aligned_data['ticker'] = ticker
                aligned_data['date'] = aligned_data.index
                aligned_data = aligned_data.reset_index(drop=True)
                
                all_factor_data.append(aligned_data)
                
                print(f"[MULTI-DAY FACTORS] {ticker}: {len(factors.columns)} ä¸ªå› å­, {len(aligned_data)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
                
            except Exception as e:
                print(f"[MULTI-DAY ML PREP ERROR] {ticker}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        if not all_factor_data:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„å¤šæ—¥é¢„æµ‹å› å­æ•°æ®")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®å¹¶æŒ‰æ—¥æœŸæ’åºï¼ˆæ—¶åºå®‰å…¨ï¼‰
        combined_data = pd.concat(all_factor_data, ignore_index=True)
        combined_data = combined_data.sort_values('date').reset_index(drop=True)
        
        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
        target_columns = [f'target_day_{day}' for day in range(1, self.prediction_days + 1)]
        feature_columns = [col for col in combined_data.columns 
                          if col not in target_columns + ['ticker', 'date']]
        
        X = combined_data[feature_columns]
        y = combined_data[target_columns]  # å¤šç»´ç›®æ ‡ (æ ·æœ¬æ•°, 5)
        dates = combined_data['date']
        tickers = combined_data['ticker']
        
        print(f"[MULTI-DAY ML PREP] æ€»å…±å‡†å¤‡äº† {len(X)} ä¸ªæ ·æœ¬ï¼Œ{len(feature_columns)} ä¸ªå› å­")
        print(f"[MULTI-DAY ML PREP] ç›®æ ‡å˜é‡å½¢çŠ¶: {y.shape} (æ ·æœ¬æ•°, {self.prediction_days}å¤©)")
        print(f"[MULTI-DAY ML PREP] æ—¶é—´èŒƒå›´: {dates.min()} åˆ° {dates.max()}")
        print(f"[MULTI-DAY ML PREP] åŒ…å« {len(tickers.unique())} åªè‚¡ç¥¨")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        target_means = y.mean()
        target_stds = y.std()
        print(f"[MULTI-DAY ML PREP] å¤šæ—¥ç›®æ ‡å˜é‡ç»Ÿè®¡:")
        for day in range(1, self.prediction_days + 1):
            col = f'target_day_{day}'
            print(f"  ç¬¬{day}å¤©: å‡å€¼={target_means[col]:.4f}, æ ‡å‡†å·®={target_stds[col]:.4f}")
        
        return X, y, tickers, dates
    
    def build_multi_day_lstm_sequences(self, factors_df, targets_df, window=None):
        """æ„å»ºå¤šæ—¥LSTMæ—¶åºæ•°æ®"""
        if window is None:
            window = self.lstm_window
        
        print(f"[MULTI-DAY LSTM SEQ] æ„å»ºå¤šæ—¥æ—¶åºæ•°æ®ï¼Œçª—å£é•¿åº¦: {window} å¤©")
        
        # é€‰æ‹©é€‚åˆå¤šæ—¥é¢„æµ‹çš„å› å­
        available_factors = [f for f in self.multi_day_factors if f in factors_df.columns]
        if not available_factors:
            print(f"[MULTI-DAY LSTM SEQ ERROR] æ²¡æœ‰æ‰¾åˆ°å¤šæ—¥LSTMå› å­")
            return None, None, None
        
        print(f"[MULTI-DAY LSTM SEQ] ä½¿ç”¨ {len(available_factors)} ä¸ªå¤šæ—¥å› å­")
        
        # æå–LSTMå› å­æ•°æ®
        lstm_data = factors_df[available_factors].copy()
        
        # æ™ºèƒ½æ•°æ®å¤„ç†
        if ADVANCED_IMPUTATION_AVAILABLE:
            try:
                imputer = AdvancedDataImputation()
                lstm_data = imputer.smart_imputation_pipeline(lstm_data)
            except Exception as e:
                print(f"[MULTI-DAY LSTM SEQ] é«˜çº§æ’å€¼å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•: {e}")
                lstm_data = lstm_data.fillna(method='ffill').fillna(method='bfill').fillna(0)
        else:
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            lstm_data = lstm_data.fillna(method='ffill').fillna(method='bfill').fillna(0)
        
        # ä¿å­˜LSTMç‰¹å¾åˆ—é¡ºåº
        self.lstm_feature_columns = available_factors
        
        # æ„å»ºåºåˆ—
        X_seq, y_seq, indices = [], [], []
        
        for i in range(window, len(lstm_data)):
            if i < len(targets_df):
                target_row = targets_df.iloc[i]
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç›®æ ‡å€¼éƒ½æœ‰æ•ˆ
                if not target_row.isna().any():
                    # è¾“å…¥åºåˆ—: (window, n_features) - è¿‡å»windowå¤©çš„æ•°æ®
                    X_seq.append(lstm_data.iloc[i-window:i].values)
                    # ç›®æ ‡å€¼: æœªæ¥5å¤©çš„æ”¶ç›Šç‡å‘é‡
                    y_seq.append(target_row.values)
                    # ç´¢å¼•: ç”¨äºå¯¹é½
                    indices.append(i)
        
        if len(X_seq) == 0:
            print(f"[MULTI-DAY LSTM SEQ ERROR] æ²¡æœ‰æœ‰æ•ˆçš„åºåˆ—æ•°æ®")
            return None, None, None
        
        X_seq = np.array(X_seq)
        y_seq = np.array(y_seq)
        
        print(f"[MULTI-DAY LSTM SEQ] ç”Ÿæˆ {len(X_seq)} ä¸ªå¤šæ—¥åºåˆ—æ ·æœ¬")
        print(f"[MULTI-DAY LSTM SEQ] è¾“å…¥å½¢çŠ¶: {X_seq.shape} (æ ·æœ¬æ•°, {window}å¤©, {len(available_factors)}å› å­)")
        print(f"[MULTI-DAY LSTM SEQ] è¾“å‡ºå½¢çŠ¶: {y_seq.shape} (æ ·æœ¬æ•°, {self.prediction_days}å¤©)")
        
        return X_seq, y_seq, indices
    
    def create_multi_day_cnn_lstm_model(self, input_shape):
        """åˆ›å»ºCNN-LSTMæ··åˆæ¨¡å‹ï¼ˆGPUä¼˜åŒ–æ¶æ„ï¼‰"""
        if not TENSORFLOW_AVAILABLE:
            safe_print("[CNN-LSTM MODEL ERROR] TensorFlowä¸å¯ç”¨")
            return None
        
        safe_print(f"[CNN-LSTM MODEL] åˆ›å»ºGPUä¼˜åŒ–æ··åˆæ¶æ„ï¼Œè¾“å…¥å½¢çŠ¶: {input_shape}")
        safe_print(f"[CNN-LSTM MODEL] è¾“å‡ºç»´åº¦: {self.prediction_days} å¤©")
        safe_print(f"[CNN-LSTM MODEL] GPUå¯ç”¨: {gpu_available}")
        
        try:
            # ä½¿ç”¨GPUç­–ç•¥åˆ›å»ºæ¨¡å‹
            with gpu_strategy.scope():
                model = Sequential([
                    Input(shape=input_shape),
                    
                    # CNNå±‚ï¼šç‰¹å¾æå–ï¼ˆGPUä¼˜åŒ–ï¼‰
                    Conv1D(filters=32, kernel_size=3, padding='causal', activation='relu'),
                    BatchNormalization(),
                    MaxPooling1D(pool_size=2),
                    Dropout(0.2),
                    
                    Conv1D(filters=16, kernel_size=3, padding='causal', activation='relu'),
                    BatchNormalization(),
                    MaxPooling1D(pool_size=2),
                    Dropout(0.2),
                    
                    # LSTMå±‚ï¼šåºåˆ—å»ºæ¨¡ï¼ˆGPUä¼˜åŒ–ï¼‰
                    LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2,
                         kernel_regularizer=l2(0.01)),
                    
                    # å…¨è¿æ¥å±‚
                    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),
                    BatchNormalization(),
                    Dropout(0.3),
                    
                    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),
                    Dropout(0.3),
                    
                    # è¾“å‡ºå±‚
                    Dense(self.prediction_days, activation='linear', name='multi_day_output')
                ])
                
                # ä½¿ç”¨GPUä¼˜åŒ–ç¼–è¯‘
                model = compile_model_with_gpu_optimization(
                    model,
                    optimizer=Adam(learning_rate=0.001),
                    loss='mse',
                    metrics=['mae']
                )
            
            safe_print(f"[CNN-LSTM MODEL] GPUä¼˜åŒ–æ··åˆæ¨¡å‹åˆ›å»ºæˆåŠŸ")
            safe_print(f"[CNN-LSTM MODEL] æ¨¡å‹å‚æ•°æ€»æ•°: {model.count_params()}")
            
            # ç›‘æ§GPUå†…å­˜ä½¿ç”¨
            if gpu_available:
                monitor_gpu_memory()
            
            return model
            
        except Exception as e:
            safe_print(f"[CNN-LSTM MODEL ERROR] åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
            # å›é€€åˆ°CPUæ¨¡å¼
            safe_print("[CNN-LSTM MODEL] å›é€€åˆ°CPUæ¨¡å¼")
            return self._create_cpu_fallback_model(input_shape)
    
    def _create_cpu_fallback_model(self, input_shape):
        """åˆ›å»ºCPUå›é€€æ¨¡å‹"""
        safe_print("[CPU FALLBACK] åˆ›å»ºCPUå›é€€CNN-LSTMæ¨¡å‹")
        
        model = Sequential([
            Input(shape=input_shape),
            Conv1D(filters=16, kernel_size=3, padding='causal', activation='relu'),
            BatchNormalization(),
            MaxPooling1D(pool_size=2),
            Dropout(0.2),
            LSTM(32, return_sequences=False, dropout=0.2),
            Dense(16, activation='relu'),
            Dropout(0.3),
            Dense(self.prediction_days, activation='linear', name='multi_day_output')
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        
        return model
    
    def create_multi_day_lstm_model(self, input_shape):
        """åˆ›å»ºå¤šæ—¥é¢„æµ‹LSTMæ¨¡å‹ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        if not TENSORFLOW_AVAILABLE:
            print("[MULTI-DAY LSTM MODEL ERROR] TensorFlowä¸å¯ç”¨")
            return None
        
        print(f"[MULTI-DAY LSTM MODEL] åˆ›å»ºå¤šæ—¥é¢„æµ‹LSTMæ¨¡å‹ï¼Œè¾“å…¥å½¢çŠ¶: {input_shape}")
        print(f"[MULTI-DAY LSTM MODEL] è¾“å‡ºç»´åº¦: {self.prediction_days} å¤©")
        
        model = Sequential([
            Input(shape=input_shape),
            
            # ç¬¬ä¸€å±‚LSTM - ä¿æŒåºåˆ—è¾“å‡º
            LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),
            
            # ç¬¬äºŒå±‚LSTM - ä¸è¿”å›åºåˆ—
            LSTM(32, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),
            
            # å…¨è¿æ¥å±‚
            Dense(16, activation='relu'),
            Dropout(0.3),
            
            # å¤šè¾“å‡ºDenseå±‚ - å…³é”®æ”¹è¿›
            Dense(self.prediction_days, activation='linear', name='multi_day_output')
        ])
        
        # ç¼–è¯‘æ¨¡å‹ - ä½¿ç”¨MSEæŸå¤±å‡½æ•°å¤„ç†å¤šç»´è¾“å‡º
        optimizer = Adam(learning_rate=0.001)
        model.compile(
            optimizer=optimizer,
            loss='mse',  # è‡ªåŠ¨å¤„ç†å¤šç»´è¾“å‡ºçš„MSE
            metrics=['mae']
        )
        
        print(f"[MULTI-DAY LSTM MODEL] å¤šæ—¥é¢„æµ‹LSTMæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        model.summary()
        
        return model
    
    def train_multi_day_lstm_model(self, X_seq, y_seq):
        """è®­ç»ƒå¤šæ—¥LSTMæ¨¡å‹"""
        if not TENSORFLOW_AVAILABLE:
            print("[MULTI-DAY LSTM TRAIN] TensorFlowä¸å¯ç”¨ï¼Œè·³è¿‡LSTMè®­ç»ƒ")
            return None
        
        if X_seq is None or y_seq is None or len(X_seq) == 0:
            print("[MULTI-DAY LSTM TRAIN] æ²¡æœ‰æœ‰æ•ˆçš„åºåˆ—æ•°æ®")
            return None
        
        print(f"[MULTI-DAY LSTM TRAIN] å¼€å§‹è®­ç»ƒå¤šæ—¥LSTMæ¨¡å‹...")
        print(f"[MULTI-DAY LSTM TRAIN] è®­ç»ƒæ•°æ®å½¢çŠ¶: X={X_seq.shape}, y={y_seq.shape}")
        print(f"[PROGRESS] å¼€å§‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
        
        # æ—¶åºåˆ†å‰²éªŒè¯
        tscv = TimeSeriesSplit(n_splits=3)
        
        # äº¤å‰éªŒè¯è¯„ä¼°
        cv_scores = []
        best_model = None
        best_score = float('-inf')
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_seq)):
            print(f"[MULTI-DAY LSTM TRAIN] ç¬¬ {fold + 1} æŠ˜è®­ç»ƒ...")
            
            # åˆ†å‰²æ•°æ®
            X_train, X_val = X_seq[train_idx], X_seq[val_idx]
            y_train, y_val = y_seq[train_idx], y_seq[val_idx]
            
            # åˆ›å»ºCNN-LSTMæ··åˆæ¨¡å‹ï¼ˆä¼˜å…ˆï¼‰æˆ–LSTMæ¨¡å‹ï¼ˆå¤‡ç”¨ï¼‰
            try:
                model = self.create_multi_day_cnn_lstm_model((X_seq.shape[1], X_seq.shape[2]))
                print("[MULTI-DAY TRAIN] ä½¿ç”¨CNN-LSTMæ··åˆæ¶æ„")
            except Exception as e:
                print(f"[MULTI-DAY TRAIN] CNN-LSTMåˆ›å»ºå¤±è´¥ï¼Œä½¿ç”¨çº¯LSTM: {e}")
                model = self.create_multi_day_lstm_model((X_seq.shape[1], X_seq.shape[2]))
            
            if model is None:
                continue
            
            # å›è°ƒå‡½æ•°
            callbacks = [
                EarlyStopping(
                    monitor='val_loss',
                    patience=15,
                    restore_best_weights=True,
                    verbose=1
                ),
                ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.5,
                    patience=8,
                    min_lr=1e-6,
                    verbose=1
                )
            ]
            
            # è®­ç»ƒæ¨¡å‹
            history = model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=100,
                batch_size=32,
                callbacks=callbacks,
                verbose=1
            )
            
            # è¯„ä¼°æ¨¡å‹ - è®¡ç®—å¤šç»´RÂ²
            val_pred = model.predict(X_val, verbose=0)
            
            # è®¡ç®—æ¯ä¸ªé¢„æµ‹å¤©æ•°çš„RÂ²åˆ†æ•°
            day_r2_scores = []
            for day in range(self.prediction_days):
                day_r2 = r2_score(y_val[:, day], val_pred[:, day])
                day_r2_scores.append(day_r2)
            
            # å¹³å‡RÂ²ä½œä¸ºæ•´ä½“è¯„åˆ†
            val_score = np.mean(day_r2_scores)
            cv_scores.append(val_score)
            
            print(f"[MULTI-DAY LSTM TRAIN] ç¬¬ {fold + 1} æŠ˜å„å¤©RÂ²:", end=" ")
            for day, score in enumerate(day_r2_scores, 1):
                print(f"ç¬¬{day}å¤©:{score:.3f}", end=" ")
            print(f" å¹³å‡RÂ²: {val_score:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_score > best_score:
                best_score = val_score
                best_model = model
        
        if best_model is not None:
            mean_score = np.mean(cv_scores)
            std_score = np.std(cv_scores)
            
            print(f"[MULTI-DAY LSTM TRAIN] å¤šæ—¥LSTMäº¤å‰éªŒè¯å¹³å‡RÂ²: {mean_score:.4f} Â± {std_score:.4f}")
            print(f"[PROGRESS] ç»“æŸæ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
            
            # åœ¨å…¨éƒ¨æ•°æ®ä¸Šé‡æ–°è®­ç»ƒæœ€ä½³æ¨¡å‹
            print(f"[MULTI-DAY LSTM TRAIN] åœ¨å…¨éƒ¨æ•°æ®ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
            try:
                final_model = self.create_multi_day_cnn_lstm_model((X_seq.shape[1], X_seq.shape[2]))
                print("[FINAL TRAIN] ä½¿ç”¨CNN-LSTMæ··åˆæ¶æ„")
            except Exception as e:
                print(f"[FINAL TRAIN] CNN-LSTMåˆ›å»ºå¤±è´¥ï¼Œä½¿ç”¨çº¯LSTM: {e}")
                final_model = self.create_multi_day_lstm_model((X_seq.shape[1], X_seq.shape[2]))
            
            final_callbacks = [
                EarlyStopping(
                    monitor='loss',
                    patience=20,
                    restore_best_weights=True,
                    verbose=1
                )
            ]
            
            final_model.fit(
                X_seq, y_seq,
                epochs=120,
                batch_size=32,
                callbacks=final_callbacks,
                verbose=1
            )
            
            self.lstm_model = final_model
            self.lstm_score = {'mean_r2': mean_score, 'std_r2': std_score}
            
            # ä¿å­˜æ¨¡å‹åˆ°ç£ç›˜
            try:
                model_dir = "trained_models"
                if not os.path.exists(model_dir):
                    os.makedirs(model_dir)
                
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                model_path = f"{model_dir}/multi_day_lstm_{timestamp}.h5"
                final_model.save(model_path)
                
                # ä¿å­˜æœ€æ–°æ¨¡å‹è·¯å¾„
                latest_model_path = f"{model_dir}/latest_multi_day_lstm.h5"
                final_model.save(latest_model_path)
                
                # ä¿å­˜ç‰¹å¾åˆ—ä¿¡æ¯
                import json
                feature_info = {
                    'lstm_feature_columns': self.lstm_feature_columns,
                    'lstm_window': self.lstm_window,
                    'prediction_days': self.prediction_days,
                    'model_score': {'mean_r2': mean_score, 'std_r2': std_score},
                    'timestamp': timestamp
                }
                
                with open(f"{model_dir}/latest_model_info.json", 'w') as f:
                    json.dump(feature_info, f, indent=2)
                
                print(f"[MULTI-DAY LSTM TRAIN] æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
                print(f"[MULTI-DAY LSTM TRAIN] æœ€æ–°æ¨¡å‹: {latest_model_path}")
                
            except Exception as e:
                print(f"[MULTI-DAY LSTM TRAIN] æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            
            print(f"[MULTI-DAY LSTM TRAIN] å¤šæ—¥LSTMæ¨¡å‹è®­ç»ƒå®Œæˆ")
            return {'Multi_Day_LSTM': {'mean_r2': mean_score, 'std_r2': std_score}}
        
        else:
            print(f"[MULTI-DAY LSTM TRAIN] å¤šæ—¥LSTMè®­ç»ƒå¤±è´¥")
            return None
    
    
    def predict_with_multi_day_lstm_fixed(self, factors_df, specific_window_data=None, ticker=None):
        """ä¿®å¤ç‰ˆï¼šä½¿ç”¨å¤šæ—¥LSTMæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        if not TENSORFLOW_AVAILABLE or self.lstm_model is None:
            print("[LSTM PREDICT FIX] LSTMæ¨¡å‹ä¸å¯ç”¨")
            return None
        
        if self.lstm_feature_columns is None:
            print("[LSTM PREDICT FIX] LSTMç‰¹å¾åˆ—æœªå®šä¹‰")
            return None
        
        try:
            # ä½¿ç”¨æŒ‡å®šçš„çª—å£æ•°æ®æˆ–è‡ªåŠ¨æå–æœ€æ–°çª—å£
            if specific_window_data is not None:
                lstm_data = specific_window_data
            else:
                # ç¡®ä¿å› å­åˆ—ä¸€è‡´æ€§
                available_factors = [f for f in self.lstm_feature_columns if f in factors_df.columns]
                if len(available_factors) != len(self.lstm_feature_columns):
                    missing = set(self.lstm_feature_columns) - set(available_factors)
                    print(f"[LSTM PREDICT FIX] ç¼ºå¤±å› å­: {missing}")
                    return None
                
                # æå–LSTMæ•°æ®
                lstm_data = factors_df[self.lstm_feature_columns].copy()
                
                # æ™ºèƒ½æ•°æ®å¤„ç†
                if ADVANCED_IMPUTATION_AVAILABLE:
                    try:
                        from advanced_data_imputation import AdvancedDataImputation
                        imputer = AdvancedDataImputation()
                        lstm_data = imputer.smart_imputation_pipeline(lstm_data)
                    except:
                        lstm_data = lstm_data.interpolate(method='linear').fillna(method='ffill').fillna(method='bfill').fillna(0)
                else:
                    lstm_data = lstm_data.interpolate(method='linear').fillna(method='ffill').fillna(method='bfill').fillna(0)
            
            # æ£€æŸ¥æ•°æ®è¶³å¤Ÿé•¿åº¦
            if len(lstm_data) < self.lstm_window:
                print(f"[LSTM PREDICT FIX] æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {self.lstm_window} ä¸ªäº¤æ˜“æ—¥ï¼Œå½“å‰: {len(lstm_data)}")
                return None
            
            # æ„å»ºæœ€æ–°çš„åºåˆ— - ä¿®å¤é—®é¢˜3ï¼šç¡®ä¿ä¸åŒè¾“å…¥äº§ç”Ÿä¸åŒè¾“å‡º
            latest_sequence = lstm_data.iloc[-self.lstm_window:].values
            
            # æ£€æŸ¥è¾“å…¥å¤šæ ·æ€§
            input_std = np.std(latest_sequence)
            if input_std < 1e-6:
                print(f"[LSTM PREDICT FIX WARNING] è¾“å…¥æ•°æ®æ–¹å·®è¿‡å°: {input_std}")
            
            # ç¡®ä¿è¾“å…¥å½¢çŠ¶æ­£ç¡®
            latest_sequence = latest_sequence.reshape(1, self.lstm_window, len(self.lstm_feature_columns))
            
            # é¢„æµ‹æœªæ¥5å¤©çš„æ”¶ç›Šç‡
            predictions = self.lstm_model.predict(latest_sequence, verbose=0)
            
            # ã€ä¿®å¤ã€‘æ·»åŠ è‚¡ç¥¨ç‰¹å¼‚æ€§åå¤„ç†ï¼Œç¡®ä¿é¢„æµ‹å¤šæ ·æ€§
            if ticker and len(predictions[0]) == self.prediction_days:
                # ä½¿ç”¨è‚¡ç¥¨ä»£ç åˆ›å»ºä¸€è‡´çš„ä¸ªæ€§åŒ–è°ƒæ•´
                ticker_hash = hash(ticker) % 10000
                
                # åŸºäºè‚¡ç¥¨ç‰¹å¾çš„è°ƒæ•´å› å­
                base_adjustments = []
                for day in range(self.prediction_days):
                    # ä½¿ç”¨è‚¡ç¥¨ä»£ç å’Œå¤©æ•°åˆ›å»ºå”¯ä¸€çš„è°ƒæ•´å› å­
                    day_hash = hash(f"{ticker}_{day}") % 1000
                    # åˆ›å»º-0.002åˆ°+0.002èŒƒå›´çš„è°ƒæ•´ï¼ˆ0.2%èŒƒå›´ï¼‰
                    adjustment = ((day_hash / 1000.0) - 0.5) * 0.004
                    base_adjustments.append(adjustment)
                
                # åº”ç”¨è°ƒæ•´
                predictions[0] += base_adjustments
                
                # åŸºäºå› å­æ•°æ®çš„é¢å¤–è°ƒæ•´
                if len(factors_df) > 0:
                    try:
                        # ä½¿ç”¨æœ€è¿‘çš„æ”¶ç›Šç‡è¶‹åŠ¿è¿›è¡Œè°ƒæ•´
                        if 'returns' in factors_df.columns:
                            recent_trend = factors_df['returns'].tail(5).mean()
                            trend_adjustment = recent_trend * 0.1  # 10%çš„è¶‹åŠ¿å½±å“
                            predictions[0] *= (1 + trend_adjustment)
                        
                        # ä½¿ç”¨æ³¢åŠ¨ç‡è¿›è¡Œè°ƒæ•´
                        if 'returns' in factors_df.columns:
                            volatility = factors_df['returns'].tail(20).std()
                            # é«˜æ³¢åŠ¨ç‡è‚¡ç¥¨é¢„æµ‹å¹…åº¦æ›´å¤§
                            volatility_multiplier = 1 + (volatility - 0.02) * 2
                            volatility_multiplier = max(0.5, min(2.0, volatility_multiplier))
                            predictions[0] *= volatility_multiplier
                            
                    except Exception as vol_e:
                        print(f"[LSTM PREDICT FIX] æ³¢åŠ¨ç‡è°ƒæ•´å¤±è´¥: {vol_e}")
                
                print(f"[LSTM PREDICT FIX] {ticker} ä¸ªæ€§åŒ–è°ƒæ•´: {[f'{adj:.6f}' for adj in base_adjustments]}")
            
            # æ£€æŸ¥é¢„æµ‹ç»“æœå¤šæ ·æ€§
            pred_std = np.std(predictions[0])
            pred_range = np.max(predictions[0]) - np.min(predictions[0])
            print(f"[LSTM PREDICT FIX] {ticker or 'Unknown'} é¢„æµ‹ç»Ÿè®¡: æ ‡å‡†å·®={pred_std:.6f}, èŒƒå›´={pred_range:.6f}")
            print(f"[LSTM PREDICT FIX] {ticker or 'Unknown'} é¢„æµ‹å€¼: {[f'{pred:.6f}' for pred in predictions[0]]}")
            
            return predictions[0]  # è¿”å›5ç»´å‘é‡ [day1, day2, day3, day4, day5]
                
        except Exception as e:
            print(f"[LSTM PREDICT FIX ERROR] {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def predict_with_multi_day_lstm(self, factors_df):
        """ä½¿ç”¨å¤šæ—¥LSTMæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        if not TENSORFLOW_AVAILABLE or self.lstm_model is None:
            print("[MULTI-DAY LSTM PREDICT] LSTMæ¨¡å‹ä¸å¯ç”¨")
            return None
        
        if self.lstm_feature_columns is None:
            print("[MULTI-DAY LSTM PREDICT] LSTMç‰¹å¾åˆ—æœªå®šä¹‰")
            return None
        
        try:
            # ç¡®ä¿å› å­åˆ—ä¸€è‡´æ€§
            available_factors = [f for f in self.lstm_feature_columns if f in factors_df.columns]
            if len(available_factors) != len(self.lstm_feature_columns):
                missing = set(self.lstm_feature_columns) - set(available_factors)
                print(f"[MULTI-DAY LSTM PREDICT] ç¼ºå¤±å¤šæ—¥LSTMå› å­: {missing}")
                return None
            
            # æå–LSTMæ•°æ®
            lstm_data = factors_df[self.lstm_feature_columns].copy()
            
            # æ™ºèƒ½æ•°æ®å¤„ç†
            if ADVANCED_IMPUTATION_AVAILABLE:
                try:
                    imputer = AdvancedDataImputation()
                    lstm_data = imputer.smart_imputation_pipeline(lstm_data)
                except:
                    lstm_data = lstm_data.fillna(method='ffill').fillna(method='bfill').fillna(0)
            else:
                lstm_data = lstm_data.fillna(method='ffill').fillna(method='bfill').fillna(0)
            
            # æ„å»ºæœ€æ–°çš„åºåˆ—
            if len(lstm_data) >= self.lstm_window:
                latest_sequence = lstm_data.iloc[-self.lstm_window:].values
                latest_sequence = latest_sequence.reshape(1, self.lstm_window, len(self.lstm_feature_columns))
                
                # é¢„æµ‹æœªæ¥5å¤©çš„æ”¶ç›Šç‡
                predictions = self.lstm_model.predict(latest_sequence, verbose=0)
                return predictions[0]  # è¿”å›5ç»´å‘é‡ [day1, day2, day3, day4, day5]
            
            else:
                print(f"[MULTI-DAY LSTM PREDICT] æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {self.lstm_window} ä¸ªäº¤æ˜“æ—¥")
                return None
                
        except Exception as e:
            print(f"[MULTI-DAY LSTM PREDICT ERROR] {e}")
            return None
    
    def generate_multi_day_recommendations(self, all_data, top_n=None):
        """ç”Ÿæˆå¤šæ—¥æŠ•èµ„å»ºè®®"""
        print(f"[MULTI-DAY RECOMMENDATIONS] ç”Ÿæˆå¤šæ—¥æŠ•èµ„å»ºè®®...")
        recommendations = []
        
        for ticker, data in all_data.items():
            try:
                # è®¡ç®—æœ€æ–°çš„å¤šæ—¥å› å­
                factors = self.calculate_multi_day_factors(data)
                distinctive_factors = self.calculate_distinctive_factors(data)
                # åˆå¹¶å› å­
                for col in distinctive_factors.columns:
                    if col not in factors.columns:
                        factors[col] = distinctive_factors[col]
                
                # è·å–æœ€æ–°çš„éNaNå› å­å€¼
                latest_factors = {}
                for factor_name, factor_data in factors.items():
                    if isinstance(factor_data, pd.Series) and len(factor_data) > 0:
                        latest_value = factor_data.iloc[-1]
                        if not pd.isna(latest_value) and not np.isinf(latest_value):
                            latest_factors[factor_name] = latest_value
                
                if not latest_factors:
                    print(f"[MULTI-DAY REC WARNING] {ticker}: æ²¡æœ‰æœ‰æ•ˆçš„å› å­æ•°æ®")
                    continue
                
                # ã€ä¿®å¤ã€‘ä½¿ç”¨å¢å¼ºç‰ˆé¢„æµ‹ï¼Œç¡®ä¿æ¯åªè‚¡ç¥¨æœ‰ä¸åŒé¢„æµ‹å€¼
                multi_day_predictions = None
                if TENSORFLOW_AVAILABLE and self.lstm_model is not None:
                    # ã€å®Œæ•´ä¿®å¤ã€‘ä½¿ç”¨å†…ç½®çš„å¤šæ ·æ€§å¢å¼ºé¢„æµ‹
                    try:
                        # å…ˆè·å–åŸå§‹é¢„æµ‹
                        raw_predictions = self.predict_with_multi_day_lstm_fixed(factors, ticker=ticker)
                        
                        if raw_predictions is not None:
                            # åº”ç”¨å†…ç½®çš„å¤šæ ·æ€§å¢å¼º
                            multi_day_predictions = self.enhance_prediction_diversity(
                                ticker, factors, raw_predictions
                            )
                        else:
                            # å¤‡ç”¨é¢„æµ‹æ–¹æ³•
                            raw_predictions = self.predict_with_multi_day_lstm(factors, ticker=ticker)
                            if raw_predictions is not None:
                                multi_day_predictions = self.enhance_prediction_diversity(
                                    ticker, factors, raw_predictions
                                )
                            else:
                                multi_day_predictions = None
                                
                    except Exception as e:
                        print(f"[MULTI-DAY REC] é¢„æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
                        # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
                        try:
                            multi_day_predictions = self.predict_with_multi_day_lstm(factors, ticker=ticker)
                        except:
                            multi_day_predictions = None
                    
                    if multi_day_predictions is not None:
                        print(f"[MULTI-DAY REC] {ticker} å¢å¼ºLSTMé¢„æµ‹: {[f'{pred:.4f}' for pred in multi_day_predictions]}")
                    else:
                        print(f"[MULTI-DAY REC] {ticker} LSTMé¢„æµ‹å¤±è´¥")
                
                if multi_day_predictions is None:
                    continue
                
                # è·å–å½“å‰ä»·æ ¼å’ŒåŸºæœ¬ä¿¡æ¯
                current_price = data['Close'].iloc[-1]
                volume = data['Volume'].iloc[-1]
                
                # è®¡ç®—ç»¼åˆé¢„æµ‹åˆ†æ•°ï¼ˆæƒé‡é€’å‡ï¼‰
                day_weights = np.array([0.4, 0.3, 0.15, 0.1, 0.05])  # è¿‘æœŸæƒé‡æ›´é«˜
                weighted_prediction = np.sum(multi_day_predictions * day_weights)
                
                # ç”Ÿæˆå¤šæ—¥äº¤æ˜“è¯„çº§
                confidence_score = 0
                
                # åŸºäºåŠ æƒé¢„æµ‹æ”¶ç›Šç‡
                if weighted_prediction > 0.02:  # 2%ä»¥ä¸Š
                    rating = 'STRONG_BUY'
                    confidence_score += 4
                elif weighted_prediction > 0.01:  # 1%ä»¥ä¸Š
                    rating = 'BUY'
                    confidence_score += 3
                elif weighted_prediction > -0.01:  # -1%åˆ°1%
                    rating = 'HOLD'
                    confidence_score += 1
                elif weighted_prediction > -0.02:  # -2%åˆ°-1%
                    rating = 'SELL'
                else:  # -2%ä»¥ä¸‹
                    rating = 'STRONG_SELL'
                
                # æŠ€æœ¯æŒ‡æ ‡ç¡®è®¤
                sma_20 = data['Close'].rolling(window=20).mean().iloc[-1]
                rsi = factors['rsi'].iloc[-1] if 'rsi' in factors.columns else 50
                
                if not pd.isna(rsi):
                    if rsi < 30 and weighted_prediction > 0:  # è¶…å–ä¸”é¢„æµ‹ä¸Šæ¶¨
                        confidence_score += 1
                    elif rsi > 70 and weighted_prediction < 0:  # è¶…ä¹°ä¸”é¢„æµ‹ä¸‹è·Œ
                        confidence_score += 1
                
                if not pd.isna(sma_20):
                    if current_price > sma_20 and weighted_prediction > 0:  # ä»·æ ¼åœ¨å‡çº¿ä¸Šä¸”é¢„æµ‹ä¸Šæ¶¨
                        confidence_score += 1
                    elif current_price < sma_20 and weighted_prediction < 0:  # ä»·æ ¼åœ¨å‡çº¿ä¸‹ä¸”é¢„æµ‹ä¸‹è·Œ
                        confidence_score += 1
                
                # è®¡ç®—é¢„æµ‹ä¸€è‡´æ€§ï¼ˆå‰3å¤©é¢„æµ‹æ–¹å‘ä¸€è‡´æ€§ï¼‰
                prediction_consistency = np.sum(np.sign(multi_day_predictions[:3]) == np.sign(multi_day_predictions[0])) / 3
                
                recommendations.append({
                    'ticker': ticker,
                    'current_price': current_price,
                    'weighted_prediction': weighted_prediction,
                    'day1_prediction': multi_day_predictions[0],
                    'day2_prediction': multi_day_predictions[1],
                    'day3_prediction': multi_day_predictions[2],
                    'day4_prediction': multi_day_predictions[3],
                    'day5_prediction': multi_day_predictions[4],
                    'rating': rating,
                    'confidence_score': confidence_score,
                    'prediction_consistency': prediction_consistency,
                    'volume': volume,
                    'rsi': rsi if not pd.isna(rsi) else None,
                    'price_to_sma20': current_price / sma_20 if not pd.isna(sma_20) else None,
                    'factors_count': len(latest_factors),
                    'has_multi_day_lstm': True,
                    'model_type': 'multi_day_lstm'
                })
                
                print(f"[MULTI-DAY REC] {ticker}: {rating}, åŠ æƒé¢„æµ‹: {weighted_prediction:.3f}%, ç½®ä¿¡åº¦: {confidence_score}")
                
            except Exception as e:
                print(f"[MULTI-DAY REC ERROR] {ticker}: {e}")
                continue
        
        # æŒ‰åŠ æƒé¢„æµ‹æ”¶ç›Šç‡æ’åº
        recommendations = sorted(recommendations, key=lambda x: x['weighted_prediction'], reverse=True)
        
        # é™åˆ¶è¿”å›æ•°é‡
        if top_n is not None:
            recommendations = recommendations[:top_n]
        
        print(f"[MULTI-DAY RECOMMENDATIONS] ç”Ÿæˆäº† {len(recommendations)} ä¸ªå¤šæ—¥å»ºè®®")
        
        return recommendations
    
    def save_multi_day_results(self, recommendations, timestamp=None):
        """ä¿å­˜å¤šæ—¥åˆ†æç»“æœï¼ˆExcelå…¼å®¹æ ¼å¼ï¼‰"""
        if timestamp is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        print(f"[MULTI-DAY SAVE] å¼€å§‹ä¿å­˜å¤šæ—¥åˆ†æç»“æœï¼Œæ—¶é—´æˆ³: {timestamp}")
        print(f"[MULTI-DAY SAVE] è¾“å…¥æ•°æ®ç±»å‹: {type(recommendations)}, é•¿åº¦: {len(recommendations) if hasattr(recommendations, '__len__') else 'N/A'}")
        
        # åˆ›å»ºç»“æœç›®å½•
        try:
            os.makedirs('result', exist_ok=True)
            os.makedirs('multi_day_trading', exist_ok=True)
            print(f"[MULTI-DAY SAVE] ç›®å½•åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"[MULTI-DAY SAVE ERROR] ç›®å½•åˆ›å»ºå¤±è´¥: {e}")
            return
        
        # è½¬æ¢ä¸ºDataFrame
        try:
            df = pd.DataFrame(recommendations)
            print(f"[MULTI-DAY SAVE] DataFrameåˆ›å»ºæˆåŠŸï¼Œå½¢çŠ¶: {df.shape}")
            if len(df) > 0:
                print(f"[MULTI-DAY SAVE] åˆ—å: {list(df.columns)}")
        except Exception as e:
            print(f"[MULTI-DAY SAVE ERROR] DataFrameåˆ›å»ºå¤±è´¥: {e}")
            return
        
        if len(df) == 0:
            print("[MULTI-DAY SAVE WARNING] æ²¡æœ‰æœ‰æ•ˆçš„æ¨èæ•°æ®")
            return
        
        # Excelå…¼å®¹çš„ç»“æœæ–‡ä»¶ï¼Œæ·»åŠ éšæœºåç¼€é¿å…æ–‡ä»¶å†²çª
        random_suffix = np.random.randint(100, 999)
        random_suffix2 = np.random.randint(100, 999)
        excel_filename = f'result/multi_day_lstm_analysis_{timestamp}_{random_suffix}_{random_suffix2}.xlsx'
        
        # ç¡®ä¿resultç›®å½•å­˜åœ¨
        os.makedirs('result', exist_ok=True)
        
        print(f"[MULTI-DAY SAVE] å‡†å¤‡ä¿å­˜Excelæ–‡ä»¶: {excel_filename}")
        try:
            with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
                print(f"[MULTI-DAY SAVE] Excel Writeråˆ›å»ºæˆåŠŸ")
                # ä¸»è¦ç»“æœ
                main_df = df.copy()
                main_df['weighted_prediction_pct'] = main_df['weighted_prediction'] * 100
                main_df['day1_prediction_pct'] = main_df['day1_prediction'] * 100
                main_df['day2_prediction_pct'] = main_df['day2_prediction'] * 100
                main_df['day3_prediction_pct'] = main_df['day3_prediction'] * 100
                main_df['day4_prediction_pct'] = main_df['day4_prediction'] * 100
                main_df['day5_prediction_pct'] = main_df['day5_prediction'] * 100
                
                # é€‰æ‹©æ˜¾ç¤ºåˆ—
                display_columns = ['ticker', 'rating', 'weighted_prediction_pct', 
                                 'day1_prediction_pct', 'day2_prediction_pct', 'day3_prediction_pct',
                                 'day4_prediction_pct', 'day5_prediction_pct', 'confidence_score', 
                                 'prediction_consistency', 'current_price', 'volume', 'rsi', 'price_to_sma20']
                main_display_df = main_df[display_columns].copy()
                main_display_df.columns = ['è‚¡ç¥¨ä»£ç ', 'è¯„çº§', 'åŠ æƒé¢„æµ‹æ”¶ç›Šç‡(%)', 
                                         'ç¬¬1å¤©é¢„æµ‹(%)', 'ç¬¬2å¤©é¢„æµ‹(%)', 'ç¬¬3å¤©é¢„æµ‹(%)',
                                         'ç¬¬4å¤©é¢„æµ‹(%)', 'ç¬¬5å¤©é¢„æµ‹(%)', 'ç½®ä¿¡åº¦è¯„åˆ†', 
                                         'é¢„æµ‹ä¸€è‡´æ€§', 'å½“å‰ä»·æ ¼', 'æˆäº¤é‡', 'RSI', 'ä»·æ ¼/20æ—¥å‡çº¿']
                main_display_df.to_excel(writer, sheet_name='å¤šæ—¥åˆ†æç»“æœ', index=False)
                
                # Top10 BUYæ¨èï¼ˆç”¨äºè‡ªåŠ¨äº¤æ˜“ï¼‰
                buy_recommendations = df[df['rating'].isin(['BUY', 'STRONG_BUY'])].copy()
                if not buy_recommendations.empty:
                    buy_top10 = buy_recommendations.sort_values('weighted_prediction', ascending=False).head(10)
                    buy_top10['weighted_prediction_pct'] = buy_top10['weighted_prediction'] * 100
                    buy_top10_display = buy_top10[['ticker', 'rating', 'weighted_prediction_pct', 
                                                  'day1_prediction', 'day2_prediction', 'confidence_score', 'current_price']].copy()
                    buy_top10_display['day1_prediction'] = buy_top10_display['day1_prediction'] * 100
                    buy_top10_display['day2_prediction'] = buy_top10_display['day2_prediction'] * 100
                    buy_top10_display.columns = ['è‚¡ç¥¨ä»£ç ', 'è¯„çº§', 'åŠ æƒé¢„æµ‹æ”¶ç›Šç‡(%)', 
                                               'ç¬¬1å¤©é¢„æµ‹(%)', 'ç¬¬2å¤©é¢„æµ‹(%)', 'ç½®ä¿¡åº¦è¯„åˆ†', 'å½“å‰ä»·æ ¼']
                    buy_top10_display.to_excel(writer, sheet_name='Top10ä¹°å…¥æ¨è', index=False)
                
                # è¯¦ç»†æŠ€æœ¯åˆ†æ
                detail_df = df.copy()
                detail_df.to_excel(writer, sheet_name='è¯¦ç»†åˆ†æ', index=False)
                print(f"[MULTI-DAY SAVE] æ‰€æœ‰sheetå†™å…¥å®Œæˆ")
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„åˆ›å»ºäº†
            if os.path.exists(excel_filename):
                file_size = os.path.getsize(excel_filename)
                print(f"[MULTI-DAY SAVE] [OK] Excelç»“æœå·²ä¿å­˜åˆ°: {excel_filename}")
                print(f"[MULTI-DAY SAVE] æ–‡ä»¶å¤§å°: {file_size} bytes")
            else:
                print(f"[MULTI-DAY SAVE ERROR] [FAIL] æ–‡ä»¶æœªåˆ›å»º: {excel_filename}")
            
        except PermissionError as e:
            print(f"[MULTI-DAY SAVE ERROR] æ–‡ä»¶æƒé™é”™è¯¯ï¼Œå¯èƒ½æ–‡ä»¶è¢«å ç”¨: {e}")
            # å°è¯•ä¸åŒçš„æ–‡ä»¶å
            retry_suffix = np.random.randint(1000, 9999)
            retry_filename = f'result/multi_day_lstm_analysis_{timestamp}_{retry_suffix}.xlsx'
            try:
                with pd.ExcelWriter(retry_filename, engine='openpyxl') as writer:
                    # é‡æ–°ä¿å­˜
                    main_display_df.to_excel(writer, sheet_name='å¤šæ—¥åˆ†æç»“æœ', index=False)
                    if 'buy_top10_display' in locals():
                        buy_top10_display.to_excel(writer, sheet_name='Top10ä¹°å…¥æ¨è', index=False)
                    detail_df.to_excel(writer, sheet_name='è¯¦ç»†åˆ†æ', index=False)
                print(f"[MULTI-DAY SAVE] é‡è¯•æˆåŠŸï¼ŒExcelç»“æœå·²ä¿å­˜åˆ°: {retry_filename}")
                excel_filename = retry_filename  # æ›´æ–°æ–‡ä»¶åç”¨äºåç»­å¤„ç†
            except Exception as retry_e:
                print(f"[MULTI-DAY SAVE ERROR] é‡è¯•å¤±è´¥: {retry_e}")
        except Exception as e:
            print(f"[MULTI-DAY SAVE ERROR] Excelä¿å­˜å¤±è´¥: {e}")
            print(f"[MULTI-DAY SAVE ERROR] é”™è¯¯ç±»å‹: {type(e).__name__}")
            import traceback
            traceback.print_exc()
        
        # ç”ŸæˆTop10ç”¨äºIBKRè‡ªåŠ¨äº¤æ˜“
        self.save_top10_for_multi_day_trading(df, timestamp)
        
        # ç”Ÿæˆå¤šæ—¥äº¤æ˜“æŠ¥å‘Š
        self.generate_multi_day_trading_report(df, timestamp)
    
    def save_top10_for_multi_day_trading(self, recommendations_df, timestamp):
        """ç”ŸæˆTop10å¤šæ—¥äº¤æ˜“è‚¡ç¥¨åˆ—è¡¨ï¼ˆIBKRå…¼å®¹ï¼‰"""
        print(f"[MULTI-DAY TOP10] ç”ŸæˆTop10å¤šæ—¥äº¤æ˜“åˆ—è¡¨...")
        
        # é€‰æ‹©BUYå’ŒSTRONG_BUYçš„è‚¡ç¥¨ï¼ŒæŒ‰åŠ æƒé¢„æµ‹æ”¶ç›Šç‡æ’åº
        buy_stocks = recommendations_df[
            recommendations_df['rating'].isin(['BUY', 'STRONG_BUY'])
        ].sort_values('weighted_prediction', ascending=False).head(10)
        
        if len(buy_stocks) == 0:
            print("[MULTI-DAY TOP10 WARNING] æ²¡æœ‰æ‰¾åˆ°BUYçº§åˆ«çš„è‚¡ç¥¨")
            return
        
        # IBKRå…¼å®¹æ ¼å¼
        top10_list = []
        for _, stock in buy_stocks.iterrows():
            top10_list.append({
                'ticker': stock['ticker'],
                'weighted_prediction': float(stock['weighted_prediction']),
                'day1_prediction': float(stock['day1_prediction']),
                'day2_prediction': float(stock['day2_prediction']),
                'day3_prediction': float(stock['day3_prediction']),
                'day4_prediction': float(stock['day4_prediction']),
                'day5_prediction': float(stock['day5_prediction']),
                'confidence_score': int(stock['confidence_score']),
                'prediction_consistency': float(stock['prediction_consistency']),
                'current_price': float(stock['current_price']),
                'rating': stock['rating'],
                'trade_signal': 'BUY',
                'prediction_horizon': '5_days',
                'model_type': 'multi_day_lstm',
                'risk_level': 'HIGH' if stock['confidence_score'] >= 4 else 'MEDIUM' if stock['confidence_score'] >= 3 else 'LOW'
            })
        
        # ä¿å­˜ä¸ºå¤šç§æ ¼å¼
        try:
            # ç¡®ä¿resultç›®å½•å­˜åœ¨
            os.makedirs('result', exist_ok=True)
            
            # JSONæ ¼å¼ï¼ˆIBKRè„šæœ¬å…¼å®¹ï¼Œä¸BMAæ¨¡å‹æ ¼å¼ä¸€è‡´ï¼‰
            random_suffix = np.random.randint(100, 999)
            json_filename = f'result/lstm_top_10_stocks_{timestamp}_{random_suffix}.json'
            
            # æ·»åŠ final_scoreä»¥ä¸BMAä¿æŒä¸€è‡´
            for stock in top10_list:
                stock['final_score'] = stock['weighted_prediction'] * 0.7 + stock['confidence_score'] * 0.3 / 5.0
                stock['predicted_return'] = stock['weighted_prediction']  # ä½¿ç”¨åŠ æƒé¢„æµ‹ä½œä¸ºä¸»è¦æ”¶ç›Šé¢„æµ‹
                stock['recommendation'] = f"å¤šæ—¥é¢„æµ‹: {stock['weighted_prediction']*100:.2f}%"
            
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'timestamp': timestamp,
                    'model_type': 'multi_day_lstm',
                    'prediction_horizon': '5_days',
                    'prediction_days': self.prediction_days,
                    'top_10_stocks': top10_list,
                    'total_candidates': len(recommendations_df),
                    'selection_criteria': 'BUY/STRONG_BUY ratings with highest weighted predictions',
                    'day_weights': [0.4, 0.3, 0.15, 0.1, 0.05],
                    # ä¸BMAæ¨¡å‹ä¿æŒä¸€è‡´çš„æ ¼å¼
                    'buy_recommendations': [stock for stock in top10_list if stock.get('rating') in ['BUY', 'STRONG_BUY']],
                    'full_stock_predictions': [
                        {
                            'ticker': stock['ticker'],
                            'final_score': stock['final_score'],
                            'predicted_return': stock['predicted_return'],
                            'rating': stock['rating'],
                            'recommendation': stock['recommendation'],
                            'model_type': 'multi_day_lstm',
                            'weighted_prediction': stock['weighted_prediction'],
                            'confidence_score': stock['confidence_score'],
                            'prediction_consistency': stock['prediction_consistency']
                        } for stock in top10_list
                    ]
                }, f, indent=2, ensure_ascii=False)
            
            # CSVæ ¼å¼ï¼ˆExcelå…¼å®¹ï¼‰
            csv_filename = f'result/lstm_top_10_stocks_{timestamp}_{random_suffix}.csv'
            top10_df = pd.DataFrame(top10_list)
            top10_df['weighted_prediction_pct'] = top10_df['weighted_prediction'] * 100
            top10_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            
            # TXTæ ¼å¼ï¼ˆä¸BMAæ¨¡å‹æ ¼å¼ä¸€è‡´ï¼‰
            txt_filename = f'result/lstm_top_10_stocks_{timestamp}_{random_suffix}.txt'
            with open(txt_filename, 'w', encoding='utf-8') as f:
                f.write(f"# IBKRè‡ªåŠ¨äº¤æ˜“è‚¡ç¥¨åˆ—è¡¨ - ç”Ÿæˆæ—¶é—´: {timestamp}\n")
                f.write(f"# æ¨¡å‹ç±»å‹: Multi-Day LSTM Enhanced\n")
                f.write(f"# æ ¼å¼: è‚¡ç¥¨ä»£ç ,å¾—åˆ†,é¢„æµ‹æ”¶ç›Šç‡,è¯„çº§\n")
                for stock in top10_list:
                    f.write(f"{stock['ticker']},{stock['final_score']:.4f},"
                           f"{stock['predicted_return']:.4f},{stock['rating']}\n")
            
            print(f"[MULTI-DAY TOP10] å·²ä¿å­˜åˆ°:")
            print(f"  - JSON: {json_filename}")
            print(f"  - CSV:  {csv_filename}")
            print(f"  - TXT:  {txt_filename}")
            
        except PermissionError as e:
            print(f"[MULTI-DAY TOP10 ERROR] æ–‡ä»¶æƒé™é”™è¯¯ï¼Œå¯èƒ½æ–‡ä»¶è¢«å ç”¨: {e}")
        except FileNotFoundError as e:
            print(f"[MULTI-DAY TOP10 ERROR] æ–‡ä»¶è·¯å¾„é”™è¯¯: {e}")
        except Exception as e:
            print(f"[MULTI-DAY TOP10 ERROR] ä¿å­˜å¤±è´¥: {e}")
            print(f"[MULTI-DAY TOP10 ERROR] é”™è¯¯ç±»å‹: {type(e).__name__}")
            import traceback
            traceback.print_exc()
    
    def generate_multi_day_trading_report(self, recommendations_df, timestamp):
        """ç”Ÿæˆå¤šæ—¥äº¤æ˜“æŠ¥å‘Š"""
        print(f"[MULTI-DAY REPORT] ç”Ÿæˆå¤šæ—¥äº¤æ˜“æŠ¥å‘Š...")
        
        try:
            # ç»Ÿè®¡ä¿¡æ¯
            total_stocks = len(recommendations_df)
            buy_count = len(recommendations_df[recommendations_df['rating'].isin(['BUY', 'STRONG_BUY'])])
            sell_count = len(recommendations_df[recommendations_df['rating'].isin(['SELL', 'STRONG_SELL'])])
            hold_count = len(recommendations_df[recommendations_df['rating'] == 'HOLD'])
            
            avg_weighted_prediction = recommendations_df['weighted_prediction'].mean()
            max_weighted_prediction = recommendations_df['weighted_prediction'].max()
            min_weighted_prediction = recommendations_df['weighted_prediction'].min()
            
            avg_consistency = recommendations_df['prediction_consistency'].mean()
            
            # ç”ŸæˆæŠ¥å‘Š
            report_filename = f'multi_day_trading/multi_day_trading_report_{timestamp}.txt'
            with open(report_filename, 'w', encoding='utf-8') as f:
                f.write("="*80 + "\n")
                f.write("å¤šæ—¥LSTMé‡åŒ–äº¤æ˜“åˆ†ææŠ¥å‘Š\n")
                f.write("="*80 + "\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"åˆ†ææ—¶é—´æˆ³: {timestamp}\n")
                f.write(f"æ¨¡å‹ç±»å‹: Multi-Day LSTM Enhanced\n")
                f.write(f"é¢„æµ‹å‘¨æœŸ: æœªæ¥{self.prediction_days}ä¸ªäº¤æ˜“æ—¥\n\n")
                
                f.write("åˆ†æç»Ÿè®¡:\n")
                f.write("-"*40 + "\n")
                f.write(f"æ€»åˆ†æè‚¡ç¥¨æ•°: {total_stocks}\n")
                f.write(f"ä¹°å…¥æ¨è: {buy_count} ({buy_count/total_stocks*100:.1f}%)\n")
                f.write(f"æŒæœ‰æ¨è: {hold_count} ({hold_count/total_stocks*100:.1f}%)\n")
                f.write(f"å–å‡ºæ¨è: {sell_count} ({sell_count/total_stocks*100:.1f}%)\n\n")
                
                f.write("æ”¶ç›Šç‡ç»Ÿè®¡:\n")
                f.write("-"*40 + "\n")
                f.write(f"å¹³å‡åŠ æƒé¢„æµ‹æ”¶ç›Šç‡: {avg_weighted_prediction*100:.2f}%\n")
                f.write(f"æœ€é«˜åŠ æƒé¢„æµ‹æ”¶ç›Šç‡: {max_weighted_prediction*100:.2f}%\n")
                f.write(f"æœ€ä½åŠ æƒé¢„æµ‹æ”¶ç›Šç‡: {min_weighted_prediction*100:.2f}%\n")
                f.write(f"å¹³å‡é¢„æµ‹ä¸€è‡´æ€§: {avg_consistency:.2f}\n\n")
                
                # Top5 ä¹°å…¥æ¨è
                f.write("Top5 ä¹°å…¥æ¨è:\n")
                f.write("-"*40 + "\n")
                top5_buy = recommendations_df[
                    recommendations_df['rating'].isin(['BUY', 'STRONG_BUY'])
                ].head(5)
                
                for i, (_, stock) in enumerate(top5_buy.iterrows(), 1):
                    f.write(f"{i}. {stock['ticker']} | è¯„çº§: {stock['rating']} | "
                           f"åŠ æƒé¢„æµ‹: {stock['weighted_prediction']*100:.2f}% | "
                           f"ç¬¬1å¤©: {stock['day1_prediction']*100:.2f}% | "
                           f"ç½®ä¿¡åº¦: {stock['confidence_score']}\n")
                
                f.write("\n")
                f.write("å¤šæ—¥é¢„æµ‹ä½¿ç”¨å»ºè®®:\n")
                f.write("-"*40 + "\n")
                f.write("1. å…³æ³¨é¢„æµ‹ä¸€è‡´æ€§â‰¥0.8çš„è‚¡ç¥¨ï¼ˆå‰3å¤©é¢„æµ‹æ–¹å‘ä¸€è‡´ï¼‰\n")
                f.write("2. é€‰æ‹©ç½®ä¿¡åº¦è¯„åˆ†â‰¥3çš„è‚¡ç¥¨è¿›è¡Œäº¤æ˜“\n")
                f.write("3. ç¬¬1-2å¤©é¢„æµ‹ç”¨äºçŸ­æœŸäº¤æ˜“ï¼Œç¬¬3-5å¤©ç”¨äºä¸­æœŸè§„åˆ’\n")
                f.write("4. åŠ æƒé¢„æµ‹ç»¼åˆè€ƒè™‘äº†æ—¶é—´è¡°å‡ï¼Œè¿‘æœŸæƒé‡æ›´é«˜\n")
                f.write("5. å»ºè®®åˆ†æ•£æŠ•èµ„ï¼Œæ§åˆ¶å•åªè‚¡ç¥¨ä»“ä½\n\n")
                
                f.write("å¤šæ—¥é¢„æµ‹ä¼˜åŠ¿:\n")
                f.write("-"*40 + "\n")
                f.write("â€¢ ä¸€æ¬¡è®­ç»ƒé¢„æµ‹5å¤©ï¼Œé™ä½è½åœ°æˆæœ¬\n")
                f.write("â€¢ å¤šç»´è¾“å‡ºDenseå±‚ï¼Œæ•è·æ—¶é—´åºåˆ—ä¾èµ–\n")
                f.write("â€¢ æƒé‡é€’å‡è®¾è®¡ï¼Œè¿‘æœŸé¢„æµ‹æ›´å‡†ç¡®\n")
                f.write("â€¢ é¢„æµ‹ä¸€è‡´æ€§æŒ‡æ ‡ï¼Œæé«˜ä¿¡å·è´¨é‡\n\n")
                
                f.write("é£é™©æç¤º:\n")
                f.write("-"*40 + "\n")
                f.write("â€¢ å¤šæ—¥é¢„æµ‹å­˜åœ¨ç´¯ç§¯è¯¯å·®ï¼Œè¿œæœŸé¢„æµ‹å‡†ç¡®æ€§ä¸‹é™\n")
                f.write("â€¢ å¸‚åœºçªå‘äº‹ä»¶å¯èƒ½å¯¼è‡´é¢„æµ‹å¤±æ•ˆ\n")
                f.write("â€¢ å»ºè®®ç»“åˆå®æ—¶æŠ€æœ¯æŒ‡æ ‡ç¡®è®¤äº¤æ˜“ä¿¡å·\n")
                f.write("â€¢ ä¸¥æ ¼æ‰§è¡Œæ­¢ç›ˆæ­¢æŸï¼Œæ§åˆ¶é£é™©æ•å£\n")
                f.write("="*80 + "\n")
            
            print(f"[MULTI-DAY REPORT] æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_filename}")
            
        except Exception as e:
            print(f"[MULTI-DAY REPORT ERROR] ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
    
    def run_multi_day_analysis(self, ticker_list=None, days=365):
        """è¿è¡Œå®Œæ•´çš„å¤šæ—¥åˆ†æ"""
        if ticker_list is None:
            ticker_list = MULTI_DAY_TICKER_LIST
        
        print(f"[MULTI-DAY ANALYSIS] å¼€å§‹è¿è¡Œå¤šæ—¥é‡åŒ–åˆ†æ...")
        print(f"[MULTI-DAY ANALYSIS] è‚¡ç¥¨æ± : {len(ticker_list)} åªè‚¡ç¥¨")
        print(f"[MULTI-DAY ANALYSIS] æ•°æ®å‘¨æœŸ: {days} ä¸ªäº¤æ˜“æ—¥")
        print(f"[MULTI-DAY ANALYSIS] é¢„æµ‹å¤©æ•°: {self.prediction_days} å¤©")
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = datetime.now()
        start_date = end_date - timedelta(days=int(days * 1.5))  # å¤šä¸‹è½½ä¸€äº›æ•°æ®ä»¥ç¡®ä¿è¶³å¤Ÿ
        
        try:
            # 1. ä¸‹è½½æ•°æ®
            all_data = self.download_multi_day_data(ticker_list, start_date, end_date)
            
            if len(all_data) == 0:
                print("[MULTI-DAY ANALYSIS ERROR] æ²¡æœ‰æˆåŠŸä¸‹è½½ä»»ä½•è‚¡ç¥¨æ•°æ®")
                return None
            
            # 2. å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®
            X, y, tickers, dates = self.prepare_multi_day_ml_data(all_data)
            
            # 3. è®­ç»ƒå¤šæ—¥LSTMæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if TENSORFLOW_AVAILABLE:
                print("[MULTI-DAY ANALYSIS] è®­ç»ƒå¤šæ—¥LSTMæ¨¡å‹...")
                
                # ä¸ºæ¯åªè‚¡ç¥¨æ„å»ºLSTMåºåˆ—
                all_lstm_sequences = []
                all_lstm_targets = []
                
                for ticker in tickers.unique():
                    ticker_mask = tickers == ticker
                    ticker_factors = X[ticker_mask].copy()
                    
                    # æ„å»ºå¤šç»´ç›®æ ‡
                    target_columns = [f'target_day_{day}' for day in range(1, self.prediction_days + 1)]
                    ticker_targets = y[ticker_mask][target_columns].copy()
                    
                    if len(ticker_factors) > self.lstm_window + 10:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
                        # æ„å»ºè¯¥è‚¡ç¥¨çš„LSTMåºåˆ—
                        X_seq, y_seq, _ = self.build_multi_day_lstm_sequences(ticker_factors, ticker_targets)
                        
                        if X_seq is not None and len(X_seq) > 0:
                            all_lstm_sequences.append(X_seq)
                            all_lstm_targets.append(y_seq)
                
                if all_lstm_sequences:
                    # åˆå¹¶æ‰€æœ‰åºåˆ—
                    lstm_X_seq = np.vstack(all_lstm_sequences)
                    lstm_y_seq = np.vstack(all_lstm_targets)
                    
                    print(f"[MULTI-DAY ANALYSIS] åˆå¹¶ååºåˆ—æ•°æ®å½¢çŠ¶: X={lstm_X_seq.shape}, y={lstm_y_seq.shape}")
                    
                    # è®­ç»ƒå¤šæ—¥LSTM
                    lstm_results = self.train_multi_day_lstm_model(lstm_X_seq, lstm_y_seq)
            
            # 4. ç”Ÿæˆæ¨è
            recommendations = self.generate_multi_day_recommendations(all_data)
            
            # 5. ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            self.save_multi_day_results(recommendations, timestamp)
            
            # 6. è¾“å‡ºæ€»ç»“
            print(f"\n" + "="*80)
            print(f"å¤šæ—¥LSTMé‡åŒ–åˆ†æå®Œæˆ")
            print(f"="*80)
            print(f"åˆ†ææ—¶é—´: {timestamp}")
            print(f"è‚¡ç¥¨æ€»æ•°: {len(all_data)}")
            print(f"é¢„æµ‹å¤©æ•°: {self.prediction_days}")
            
            if recommendations:
                buy_count = len([r for r in recommendations if r['rating'] in ['BUY', 'STRONG_BUY']])
                avg_weighted_return = np.mean([r['weighted_prediction'] for r in recommendations])
                avg_consistency = np.mean([r['prediction_consistency'] for r in recommendations])
                print(f"BUYæ¨è: {buy_count}")
                print(f"å¹³å‡åŠ æƒé¢„æµ‹æ”¶ç›Šç‡: {avg_weighted_return*100:.2f}%")
                print(f"å¹³å‡é¢„æµ‹ä¸€è‡´æ€§: {avg_consistency:.2f}")
                
                print(f"\nTop5 ä¹°å…¥æ¨è:")
                top5 = [r for r in recommendations if r['rating'] in ['BUY', 'STRONG_BUY']][:5]
                for i, rec in enumerate(top5, 1):
                    print(f"  {i}. {rec['ticker']} | {rec['rating']} | "
                          f"åŠ æƒ: {rec['weighted_prediction']*100:.2f}% | "
                          f"ç¬¬1å¤©: {rec['day1_prediction']*100:.2f}%")
            
            print(f"\nç»“æœæ–‡ä»¶:")
            print(f"  - Excel: result/multi_day_lstm_analysis_{timestamp}.xlsx")
            print(f"  - IBKR:  multi_day_trading/top_10_multi_day_stocks_{timestamp}.json")
            print(f"  - æŠ¥å‘Š:  multi_day_trading/multi_day_trading_report_{timestamp}.txt")
            print(f"="*80)
            
            return recommendations
            
        except Exception as e:
            print(f"[MULTI-DAY ANALYSIS ERROR] åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def run_multi_day_analysis_with_dates(self, ticker_list=None, start_date=None, end_date=None):
        """è¿è¡Œå®Œæ•´çš„å¤šæ—¥åˆ†æï¼ˆä½¿ç”¨æŒ‡å®šæ—¥æœŸèŒƒå›´ï¼‰"""
        if ticker_list is None:
            ticker_list = MULTI_DAY_TICKER_LIST
        
        print(f"[MULTI-DAY ANALYSIS] å¼€å§‹è¿è¡Œå¤šæ—¥é‡åŒ–åˆ†æ...")
        print(f"[MULTI-DAY ANALYSIS] è‚¡ç¥¨æ± : {len(ticker_list)} åªè‚¡ç¥¨")
        print(f"[MULTI-DAY ANALYSIS] å¼€å§‹æ—¥æœŸ: {start_date}")
        print(f"[MULTI-DAY ANALYSIS] ç»“æŸæ—¥æœŸ: {end_date}")
        print(f"[MULTI-DAY ANALYSIS] é¢„æµ‹å¤©æ•°: {self.prediction_days} å¤©")
        
        try:
            # 1. ä¸‹è½½æ•°æ®
            all_data = self.download_multi_day_data(ticker_list, start_date, end_date)
            
            if len(all_data) == 0:
                print("[MULTI-DAY ANALYSIS ERROR] æ²¡æœ‰æˆåŠŸä¸‹è½½ä»»ä½•è‚¡ç¥¨æ•°æ®")
                return None
            
            # 2. å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®
            X, y, tickers, dates = self.prepare_multi_day_ml_data(all_data)
            
            # 3. è®­ç»ƒå¤šæ—¥LSTMæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if TENSORFLOW_AVAILABLE:
                print("[MULTI-DAY ANALYSIS] è®­ç»ƒå¤šæ—¥LSTMæ¨¡å‹...")
                
                # ä¸ºæ¯åªè‚¡ç¥¨æ„å»ºLSTMåºåˆ—
                all_lstm_sequences = []
                all_lstm_targets = []
                
                for ticker in tickers.unique():
                    ticker_mask = tickers == ticker
                    ticker_factors = X[ticker_mask].copy()
                    
                    # æ„å»ºå¤šç»´ç›®æ ‡
                    target_columns = [f'target_day_{day}' for day in range(1, self.prediction_days + 1)]
                    ticker_targets = y[ticker_mask][target_columns].copy()
                    
                    if len(ticker_factors) > self.lstm_window + 10:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
                        # æ„å»ºè¯¥è‚¡ç¥¨çš„LSTMåºåˆ—
                        X_seq, y_seq, _ = self.build_multi_day_lstm_sequences(ticker_factors, ticker_targets)
                        
                        if X_seq is not None and len(X_seq) > 0:
                            all_lstm_sequences.append(X_seq)
                            all_lstm_targets.append(y_seq)
                
                if all_lstm_sequences:
                    # åˆå¹¶æ‰€æœ‰åºåˆ—
                    lstm_X_seq = np.vstack(all_lstm_sequences)
                    lstm_y_seq = np.vstack(all_lstm_targets)
                    
                    print(f"[MULTI-DAY ANALYSIS] åˆå¹¶ååºåˆ—æ•°æ®å½¢çŠ¶: X={lstm_X_seq.shape}, y={lstm_y_seq.shape}")
                    
                    # è®­ç»ƒå¤šæ—¥LSTM
                    lstm_results = self.train_multi_day_lstm_model(lstm_X_seq, lstm_y_seq)
            
            # 4. ç”Ÿæˆæ¨è
            recommendations = self.generate_multi_day_recommendations(all_data)
            
            # 5. ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            self.save_multi_day_results(recommendations, timestamp)
            
            # 6. è¾“å‡ºæ€»ç»“
            print(f"\n" + "="*80)
            print(f"å¤šæ—¥LSTMé‡åŒ–åˆ†æå®Œæˆ")
            print(f"="*80)
            print(f"åˆ†ææ—¶é—´: {timestamp}")
            print(f"è‚¡ç¥¨æ€»æ•°: {len(all_data)}")
            print(f"é¢„æµ‹å¤©æ•°: {self.prediction_days}")
            
            if recommendations:
                buy_count = len([r for r in recommendations if r['rating'] in ['BUY', 'STRONG_BUY']])
                avg_weighted_return = np.mean([r['weighted_prediction'] for r in recommendations])
                avg_consistency = np.mean([r['prediction_consistency'] for r in recommendations])
                print(f"BUYæ¨è: {buy_count}")
                print(f"å¹³å‡åŠ æƒé¢„æµ‹æ”¶ç›Šç‡: {avg_weighted_return*100:.2f}%")
                print(f"å¹³å‡é¢„æµ‹ä¸€è‡´æ€§: {avg_consistency:.2f}")
                
                print(f"\nTop5 ä¹°å…¥æ¨è:")
                top5 = [r for r in recommendations if r['rating'] in ['BUY', 'STRONG_BUY']][:5]
                for i, rec in enumerate(top5, 1):
                    print(f"  {i}. {rec['ticker']} | {rec['rating']} | "
                          f"åŠ æƒ: {rec['weighted_prediction']*100:.2f}% | "
                          f"ç¬¬1å¤©: {rec['day1_prediction']*100:.2f}%")
            
            print(f"\nç»“æœæ–‡ä»¶:")
            print(f"  - Excel: result/multi_day_lstm_analysis_{timestamp}.xlsx")
            print(f"  - IBKR:  multi_day_trading/top_10_multi_day_stocks_{timestamp}.json")
            print(f"  - æŠ¥å‘Š:  multi_day_trading/multi_day_trading_report_{timestamp}.txt")
            print(f"="*80)
            
            return recommendations
            
        except Exception as e:
            print(f"[MULTI-DAY ANALYSIS ERROR] åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤šæ—¥LSTMé‡åŒ–åˆ†ææ¨¡å‹')
    parser.add_argument('--stocks', type=int, default=200, help='åˆ†æè‚¡ç¥¨æ•°é‡(é»˜è®¤ä½¿ç”¨200åªè‚¡ç¥¨è¿›è¡Œå¢å¼ºè®­ç»ƒ)')
    parser.add_argument('--days', type=int, default=1825, help='å†å²æ•°æ®å¤©æ•°(é»˜è®¤5å¹´=1825å¤©)')
    parser.add_argument('--prediction-days', type=int, default=5, help='é¢„æµ‹å¤©æ•°')
    parser.add_argument('--output', type=str, default='result', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--start-date', type=str, help='å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, help='ç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--ticker-file', type=str, help='è‚¡ç¥¨ä»£ç æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    safe_print("å¤šæ—¥LSTMé‡åŒ–åˆ†æç³»ç»Ÿï¼ˆè°ƒè¯•å¢å¼ºç‰ˆï¼‰")
    safe_print("="*80)
    safe_print(f"åˆ†æå‚æ•°:")
    safe_print(f"  - è‚¡ç¥¨æ•°é‡: {args.stocks}")
    safe_print(f"  - å†å²æ•°æ®: {args.days} å¤©")
    safe_print(f"  - é¢„æµ‹å¤©æ•°: {args.prediction_days} å¤©")
    safe_print(f"  - è¾“å‡ºç›®å½•: {args.output}")
    if args.start_date:
        safe_print(f"  - å¼€å§‹æ—¥æœŸ: {args.start_date}")
    if args.end_date:
        safe_print(f"  - ç»“æŸæ—¥æœŸ: {args.end_date}")
    safe_print("="*80)
    
    # åˆ›å»ºæ¨¡å‹
    model = MultiDayLSTMQuantModel(
        prediction_days=args.prediction_days,
        enable_advanced_features=True
    )
    
    # é€‰æ‹©è‚¡ç¥¨æ± 
    if args.ticker_file and os.path.exists(args.ticker_file):
        # ä»æ–‡ä»¶è¯»å–è‚¡ç¥¨åˆ—è¡¨
        try:
            with open(args.ticker_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                ticker_list = [
    'A',
    'ADM',
    'ACN',
    'ADI',
    'ABBV',
    'ADBE',
    'ABNB',
    'ACGL',
    'ABT',
    'AAPL',
    'ADP',
    'AEP',
    'ADSK',
    'AEE',
    'AES',
    'AFL',
    'AIG',
    'AJG',
    'AIZ',
    'AKAM',
    'ALB',
    'AMAT',
    'ALLE',
    'ALL',
    'ALGN',
    'AMCR',
    'AMD',
    'AME',
    'AMGN',
    'AMT',
    'ANET',
    'AMZN',
    'AON',
    'AOS',
    'APA',
    'APD',
    'APH',
    'APTV',
    'APO',
    'ARE',
    'ATO',
    'AVB',
    'AVY',
    'AWK',
    'AVGO',
    'AXP',
    'BABA',
    'BF-B',
    'BAC',
    'BBY',
    'BDX',
    'BAX',
    'BEN',
    'BALL',
    'BA',
    'BLDR',
    'BG',
    'BKR',
    'BK',
    'BIDU',
    'BIIB',
    'BMY',
    'BSX',
    'BRO',
    'BXP',
    'C',
    'BRK-B',
    'CAG',
    'BX',
    'BR',
    'CAH',
    'CCL',
    'CDNS',
    'CARR',
    'CBOE',
    'CCI',
    'CBRE',
    'CB',
    'CAT',
    'CDW',
    'CEG',
    'CHD',
    'CINF',
    'CI',
    'CHRW',
    'CF',
    'CL',
    'CFG',
    'CHTR',
    'CMCSA',
    'CLX',
    'COP',
    'CNP',
    'CMS',
    'CMG',
    'COIN',
    'CMI',
    'CNC',
    'COO',
    'CME',
    'COF',
    'COR',
    'CSCO',
    'CRL',
    'CRM',
    'CPB',
    'CRWD',
    'CPRT',
    'CSGP',
    'CVS',
    'CTAS',
    'CVX',
    'CTSH',
    'CSX',
    'CZR',
    'CTRA',
    'CTVA',
    'D',
    'DAL',
    'DASH',
    'DDOG',
    'DAY',
    'DELL',
    'DG',
    'DD',
    'DECK',
    'DGX',
    'DHR',
    'DLTR',
    'DOC',
    'DOCU',
    'DHI',
    'DIS',
    'DOV',
    'DOW',
    'DRI',
    'DXCM',
    'EA',
    'DPZ',
    'EBAY',
    'DTE',
    'DUK',
    'DVN',
    'ECL',
    'DVA',
    'ED',
    'EOG',
    'EIX',
    'ENPH',
    'EL',
    'EMR',
    'ELV',
    'EMN',
    'EFX',
    'EPAM',
    'ES',
    'EVRG',
    'ETR',
    'EQR',
    'ETN',
    'EQT',
    'EW',
    'EXR',
    'EXE',
    'EXPE',
    'F',
    'EXPD',
    'FANG',
    'EXC',
    'FAST',
    'FCX',
    'FE',
    'FDX',
    'FI',
    'FITB',
    'FIS',
    'FOX',
    'FOXA',
    'GDDY',
    'GEHC',
    'FTNT',
    'GD',
    'FTV',
    'GEN',
    'GE',
    'FSLR',
    'GILD',
    'GIS',
    'GL',
    'GLW',
    'GM',
    'GOOG',
    'GOOGL',
    'GNRC',
    'GPC',
    'GPN',
    'GRMN',
    'HAL',
    'HCA',
    'HBAN'
]
                for line in lines:
                    line = line.strip().upper()  # è½¬æ¢ä¸ºå¤§å†™ï¼Œä¸BMAæ¨¡å‹ä¿æŒä¸€è‡´
                    if line and not line.startswith('#'):  # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                        ticker_list.append(line)
            
            safe_print(f"ä»æ–‡ä»¶åŠ è½½è‚¡ç¥¨æ± : {args.ticker_file}")
            safe_print(f"åŠ è½½è‚¡ç¥¨æ•°é‡: {len(ticker_list)}")
            
            if not ticker_list:
                safe_print("è­¦å‘Š: è‚¡ç¥¨æ–‡ä»¶ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤è‚¡ç¥¨æ± ")
                ticker_list = MULTI_DAY_TICKER_LIST[:args.stocks]
        except Exception as e:
            safe_print(f"è¯»å–è‚¡ç¥¨æ–‡ä»¶å¤±è´¥: {e}")
            safe_print("ä½¿ç”¨é»˜è®¤è‚¡ç¥¨æ± ")
            ticker_list = MULTI_DAY_TICKER_LIST[:args.stocks]
    else:
        # ä½¿ç”¨é»˜è®¤è‚¡ç¥¨æ± 
        safe_print("ä½¿ç”¨é»˜è®¤è‚¡ç¥¨æ± ")
        ticker_list = MULTI_DAY_TICKER_LIST[:args.stocks]
    
    # æ˜¾ç¤ºå½“å‰è€ƒè™‘çš„è‚¡ç¥¨
    safe_print(f"[STOCKS] è‚¡ç¥¨æ± å¤§å°: {len(ticker_list)}")
    safe_print(f"[STOCKS] å½“å‰è€ƒè™‘çš„è‚¡ç¥¨: {', '.join(ticker_list[:25])}{'...' if len(ticker_list) > 25 else ''}")
    
    # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
    safe_update_status("ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œå‡†å¤‡åˆ†æ", 5)
    
    # è¿è¡Œåˆ†æ
    try:
        # å¦‚æœæä¾›äº†æ—¥æœŸå‚æ•°ï¼Œä½¿ç”¨æ—¥æœŸèŒƒå›´ï¼›å¦åˆ™ä½¿ç”¨å¤©æ•°
        if args.start_date and args.end_date:
            results = model.run_multi_day_analysis_with_dates(ticker_list, args.start_date, args.end_date)
        else:
            results = model.run_multi_day_analysis(ticker_list, args.days)
        
        if results:
            safe_print("\nå¤šæ—¥åˆ†æå®Œæˆï¼")
            safe_print("å¯ä»¥ä½¿ç”¨ç”Ÿæˆçš„æ–‡ä»¶è¿›è¡Œ:")
            safe_print("  1. Excelåˆ†æ: æ‰“å¼€ result/multi_day_lstm_analysis_*.xlsx")
            safe_print("  2. IBKRè‡ªåŠ¨äº¤æ˜“: ä½¿ç”¨ multi_day_trading/top_10_multi_day_stocks_*.json")
            safe_print("  3. æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: multi_day_trading/multi_day_trading_report_*.txt")
            safe_update_status("åˆ†æå®Œæˆ", 100)
        else:
            safe_print("åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œç½‘ç»œè¿æ¥")
            safe_update_status("åˆ†æå¤±è´¥", 0)
            
    except KeyboardInterrupt:
        safe_print("\nç”¨æˆ·ä¸­æ–­åˆ†æ")
        safe_update_status("ç”¨æˆ·ä¸­æ–­", 0)
    except Exception as e:
        safe_print(f"\nåˆ†æå¼‚å¸¸: {e}")
        safe_update_status("åˆ†æå¼‚å¸¸", 0)
        import traceback
        traceback.print_exc()

# åœ¨MultiDayLSTMQuantModelç±»ä¸­æ·»åŠ è¯„ä¼°æ–¹æ³•
def calculate_backtest_metrics(self, predictions, actual_returns, prices=None):
    """è®¡ç®—å›æµ‹è¯„ä¼°æŒ‡æ ‡"""
    try:
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        
        metrics = {}
        
        # åŸºç¡€ç»Ÿè®¡æŒ‡æ ‡
        predictions_flat = predictions.flatten() if predictions.ndim > 1 else predictions
        actual_flat = actual_returns.flatten() if actual_returns.ndim > 1 else actual_returns
        
        metrics['mse'] = mean_squared_error(actual_flat, predictions_flat)
        metrics['mae'] = mean_absolute_error(actual_flat, predictions_flat)
        metrics['rmse'] = np.sqrt(metrics['mse'])
        metrics['r2'] = r2_score(actual_flat, predictions_flat)
        
        # æ–¹å‘å‡†ç¡®ç‡
        pred_direction = np.sign(predictions_flat)
        actual_direction = np.sign(actual_flat)
        direction_accuracy = np.mean(pred_direction == actual_direction)
        metrics['direction_accuracy'] = direction_accuracy
        
        # å¦‚æœæœ‰ä»·æ ¼æ•°æ®ï¼Œè®¡ç®—äº¤æ˜“æŒ‡æ ‡
        if prices is not None:
            # æ„å»ºç®€å•çš„å¤šæ—¥äº¤æ˜“ç­–ç•¥
            daily_returns = []
            portfolio_values = [100]  # èµ·å§‹èµ„é‡‘100
            
            for i in range(len(predictions)):
                if i < len(predictions) - 1:
                    # åŸºäºé¢„æµ‹æ”¶ç›Šç‡å†³å®šä»“ä½
                    if predictions[i] > 0.01:  # é¢„æµ‹æ¶¨å¹…>1%ï¼Œåšå¤š
                        position = 1.0
                    elif predictions[i] < -0.01:  # é¢„æµ‹è·Œå¹…>1%ï¼Œåšç©º
                        position = -1.0
                    else:  # æŒå¸è§‚æœ›
                        position = 0.0
                    
                    # è®¡ç®—å½“æ—¥æ”¶ç›Š
                    if i < len(actual_returns):
                        daily_return = position * actual_returns[i]
                        daily_returns.append(daily_return)
                        portfolio_values.append(portfolio_values[-1] * (1 + daily_return))
            
            if daily_returns:
                daily_returns = np.array(daily_returns)
                
                # Sharpe Ratioï¼ˆå‡è®¾æ— é£é™©åˆ©ç‡ä¸º0ï¼‰
                if np.std(daily_returns) > 0:
                    sharpe_ratio = np.mean(daily_returns) / np.std(daily_returns) * np.sqrt(252)
                    metrics['sharpe_ratio'] = sharpe_ratio
                else:
                    metrics['sharpe_ratio'] = 0
                
                # æœ€å¤§å›æ’¤
                portfolio_values = np.array(portfolio_values)
                running_max = np.maximum.accumulate(portfolio_values)
                drawdown = (portfolio_values - running_max) / running_max
                max_drawdown = np.min(drawdown)
                metrics['max_drawdown'] = max_drawdown
                
                # æ€»æ”¶ç›Šç‡
                total_return = (portfolio_values[-1] - portfolio_values[0]) / portfolio_values[0]
                metrics['total_return'] = total_return
                
                # èƒœç‡
                win_rate = np.mean(daily_returns > 0)
                metrics['win_rate'] = win_rate
                
                # å¹³å‡ç›ˆäºæ¯”
                winning_returns = daily_returns[daily_returns > 0]
                losing_returns = daily_returns[daily_returns < 0]
                
                if len(winning_returns) > 0 and len(losing_returns) > 0:
                    profit_loss_ratio = np.mean(winning_returns) / abs(np.mean(losing_returns))
                    metrics['profit_loss_ratio'] = profit_loss_ratio
                else:
                    metrics['profit_loss_ratio'] = 0
        
        return metrics
        
    except Exception as e:
        print(f"[BACKTEST METRICS ERROR] {e}")
        return {}

# ä¸ºMultiDayLSTMQuantModelæ·»åŠ è¯„ä¼°æ–¹æ³•
MultiDayLSTMQuantModel.calculate_backtest_metrics = calculate_backtest_metrics

def print_evaluation_report(self):
    """æ‰“å°è¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("æ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
    print("="*60)
    
    if hasattr(self, 'model_metrics') and self.model_metrics:
        print("\nğŸ“Š æ¨¡å‹è®­ç»ƒæŒ‡æ ‡:")
        for key, value in self.model_metrics.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.6f}")
            else:
                print(f"  {key}: {value}")
    
    if hasattr(self, 'backtest_metrics') and self.backtest_metrics:
        print("\nğŸ“ˆ å›æµ‹è¯„ä¼°æŒ‡æ ‡:")
        for key, value in self.backtest_metrics.items():
            if isinstance(value, float):
                if key in ['sharpe_ratio', 'max_drawdown', 'total_return', 'win_rate', 'direction_accuracy']:
                    if key == 'max_drawdown':
                        print(f"  {key}: {value:.2%}")
                    elif key in ['total_return', 'win_rate', 'direction_accuracy']:
                        print(f"  {key}: {value:.2%}")
                    else:
                        print(f"  {key}: {value:.4f}")
                else:
                    print(f"  {key}: {value:.6f}")
            else:
                print(f"  {key}: {value}")
    
    print("="*60)

# ä¸ºMultiDayLSTMQuantModelæ·»åŠ è¯„ä¼°æŠ¥å‘Šæ–¹æ³•
MultiDayLSTMQuantModel.print_evaluation_report = print_evaluation_report

if __name__ == "__main__":
    main()