#!/usr/bin/env python3
"""
BMA Ultra Enhanced çœŸå®æ•°æ®æµ‹è¯•è„šæœ¬
å®Œæ•´æµç¨‹æµ‹è¯•ï¼šæ•°æ®è·å– â†’ ç‰¹å¾å·¥ç¨‹ â†’ æ¨¡å‹è®­ç»ƒ â†’ é¢„æµ‹ç”Ÿæˆ
"""

import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# è®¾ç½®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def create_realistic_test_data():
    """åˆ›å»ºçœŸå®çš„æµ‹è¯•æ•°æ®"""
    print("ğŸ“Š åˆ›å»ºçœŸå®æµ‹è¯•æ•°æ®...")
    
    # ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®
    start_date = pd.Timestamp('2023-01-01')
    end_date = pd.Timestamp('2024-01-31') 
    dates = pd.date_range(start=start_date, end=end_date, freq='D')
    
    # è‚¡ç¥¨åˆ—è¡¨
    tickers = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA']
    
    # ç”Ÿæˆå¤šé‡ç´¢å¼•æ•°æ®
    data_list = []
    for ticker in tickers:
        # ç”Ÿæˆä»·æ ¼æ•°æ®ï¼ˆæ¨¡æ‹ŸçœŸå®è‚¡ä»·èµ°åŠ¿ï¼‰
        np.random.seed(42 + hash(ticker) % 100)  # æ¯ä¸ªè‚¡ç¥¨ä¸åŒçš„éšæœºç§å­
        
        n_days = len(dates)
        returns = np.random.normal(0.001, 0.02, n_days)  # æ—¥æ”¶ç›Šç‡
        prices = 100 * np.exp(np.cumsum(returns))  # ç´¯ç§¯ä»·æ ¼
        
        # åˆ›å»ºOHLCVæ•°æ®
        for i, date in enumerate(dates):
            high = prices[i] * (1 + abs(np.random.normal(0, 0.01)))
            low = prices[i] * (1 - abs(np.random.normal(0, 0.01)))
            volume = np.random.randint(1000000, 10000000)
            
            data_list.append({
                'date': date,
                'ticker': ticker,
                'open': prices[i] * (1 + np.random.normal(0, 0.005)),
                'high': high,
                'low': low, 
                'close': prices[i],
                'volume': volume,
                'returns': returns[i],
                # æ·»åŠ ä¸€äº›åŸºæœ¬çš„æŠ€æœ¯æŒ‡æ ‡
                'rsi': 30 + 40 * np.sin(i * 0.1) + np.random.normal(0, 5),
                'ma_5': np.mean(prices[max(0, i-4):i+1]),
                'ma_20': np.mean(prices[max(0, i-19):i+1])
            })
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(data_list)
    df['date'] = pd.to_datetime(df['date'])
    df = df.set_index(['date', 'ticker']).sort_index()
    
    print(f"âœ… æµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆ: {df.shape}")
    print(f"   æ—¥æœŸèŒƒå›´: {df.index.get_level_values('date').min()} - {df.index.get_level_values('date').max()}")
    print(f"   è‚¡ç¥¨æ•°é‡: {len(df.index.get_level_values('ticker').unique())}")
    print(f"   ç‰¹å¾åˆ—: {list(df.columns)}")
    
    return df

def run_complete_bma_test():
    """è¿è¡Œå®Œæ•´çš„BMAæµ‹è¯•"""
    print("Starting BMA Ultra Enhanced Complete Test")
    print("="*60)
    
    try:
        # 1. å¯¼å…¥æ¨¡å‹
        print("Step 1: Import BMA Model")
        from bma_models.bma_ultra_enhanced_refactored import UltraEnhancedQuantitativeModel
        print("âœ… æ¨¡å‹å¯¼å…¥æˆåŠŸ")
        
        # 2. åˆå§‹åŒ–æ¨¡å‹
        print("\nğŸ”§ æ­¥éª¤2: åˆå§‹åŒ–æ¨¡å‹")
        model = UltraEnhancedQuantitativeModel(
            config_path='bma_models/unified_config.yaml',
            enable_optimization=True,
            enable_v6_enhancements=True
        )
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # 3. åˆ›å»ºæµ‹è¯•æ•°æ®
        print("\nğŸ“Š æ­¥éª¤3: å‡†å¤‡æµ‹è¯•æ•°æ®")
        test_data = create_realistic_test_data()
        
        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
        feature_columns = ['open', 'high', 'low', 'close', 'volume', 'rsi', 'ma_5', 'ma_20']
        X = test_data[feature_columns].copy()
        
        # åˆ›å»ºæœªæ¥10å¤©æ”¶ç›Šä½œä¸ºç›®æ ‡å˜é‡
        y = test_data.groupby('ticker')['returns'].shift(-10).fillna(0)
        
        # åˆ é™¤ç¼ºå¤±å€¼
        valid_mask = ~(X.isnull().any(axis=1) | y.isnull())
        X_clean = X[valid_mask]
        y_clean = y[valid_mask]
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: X={X_clean.shape}, y={len(y_clean)}")
        
        # 4. æµ‹è¯•æ•°æ®è·å–åŠŸèƒ½
        print("\nğŸŒ æ­¥éª¤4: æµ‹è¯•æ•°æ®è·å–åŠŸèƒ½")
        try:
            test_tickers = ['AAPL', 'MSFT']
            stock_data = model.download_stock_data(
                tickers=test_tickers, 
                start_date='2024-01-01', 
                end_date='2024-01-31'
            )
            print(f"âœ… æ•°æ®è·å–æµ‹è¯•: æˆåŠŸè·å– {len(stock_data)} åªè‚¡ç¥¨æ•°æ®")
        except Exception as e:
            print(f"âš ï¸ æ•°æ®è·å–æµ‹è¯•å¤±è´¥: {e} (è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºAPIé™åˆ¶)")
        
        # 5. æµ‹è¯•ç‰¹å¾å·¥ç¨‹
        print("\nğŸ”§ æ­¥éª¤5: æµ‹è¯•ç‰¹å¾å·¥ç¨‹")
        X_processed, y_processed = model._safe_data_preprocessing(X_clean, y_clean)
        print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ: {X_processed.shape}")
        
        X_lagged = model._apply_feature_lag_optimization(X_processed)
        print(f"âœ… ç‰¹å¾æ»åä¼˜åŒ–å®Œæˆ: {X_lagged.shape}")
        
        X_decayed = model._apply_adaptive_factor_decay(X_lagged)
        print(f"âœ… å› å­è¡°å‡å®Œæˆ: {X_decayed.shape}")
        
        X_selected, selected_features = model._apply_robust_feature_selection(X_decayed, y_processed)
        print(f"âœ… ç‰¹å¾é€‰æ‹©å®Œæˆ: {X_selected.shape}, é€‰æ‹©äº† {len(selected_features)} ä¸ªç‰¹å¾")
        
        # 6. æµ‹è¯•æ¨¡å‹è®­ç»ƒ
        print("\nğŸ¤– æ­¥éª¤6: æµ‹è¯•æ¨¡å‹è®­ç»ƒ")
        
        # ä¼ ç»ŸMLæ¨¡å‹è®­ç»ƒ
        traditional_results = model._train_standard_models(X_selected, y_processed, validation_split=0.2)
        print(f"âœ… ä¼ ç»ŸMLè®­ç»ƒå®Œæˆ: {len(traditional_results.get('models', {}))} ä¸ªæ¨¡å‹")
        
        # åˆ¶åº¦æ„ŸçŸ¥æ¨¡å‹è®­ç»ƒ
        regime_results = model._train_enhanced_regime_aware_models(X_selected, y_processed)
        print(f"âœ… åˆ¶åº¦æ„ŸçŸ¥è®­ç»ƒå®Œæˆ: {len(regime_results.get('models', {}))} ä¸ªæ¨¡å‹")
        
        # Stackingè®­ç»ƒ
        if len(traditional_results.get('models', {})) >= 2:
            stacking_results = model._train_stacking_models_modular(
                X_selected, y_processed,
                traditional_results['models'],
                regime_results.get('models', {})
            )
            print(f"âœ… Stackingè®­ç»ƒå®Œæˆ: {len(stacking_results.get('models', {}))} ä¸ªå…ƒå­¦ä¹ å™¨")
        else:
            stacking_results = {'models': {}}
            print("âš ï¸ Stackingè·³è¿‡: åŸºç¡€æ¨¡å‹æ•°é‡ä¸è¶³")
        
        # 7. æµ‹è¯•å®Œæ•´è®­ç»ƒæµç¨‹
        print("\nğŸ”„ æ­¥éª¤7: æµ‹è¯•å®Œæ•´è®­ç»ƒæµç¨‹")
        training_results = model.train_enhanced_models(X_selected, y_processed, validation_split=0.2)
        print(f"âœ… å®Œæ•´è®­ç»ƒå®Œæˆ: æˆåŠŸ={training_results.get('success', False)}")
        
        # 8. æµ‹è¯•é¢„æµ‹ç”Ÿæˆ
        print("\nğŸ”® æ­¥éª¤8: æµ‹è¯•é¢„æµ‹ç”Ÿæˆ")
        
        # å‡†å¤‡é¢„æµ‹æ•°æ®ï¼ˆä½¿ç”¨è®­ç»ƒæ•°æ®çš„ä¸€éƒ¨åˆ†ï¼‰
        X_pred = X_selected.iloc[:100]  # å–å‰100è¡Œä½œä¸ºé¢„æµ‹æ ·æœ¬
        
        predictions = model.generate_enhanced_predictions(X_pred)
        if predictions is not None and not predictions.empty:
            print(f"âœ… é¢„æµ‹ç”Ÿæˆå®Œæˆ: {predictions.shape}")
            print(f"   é¢„æµ‹åˆ—: {list(predictions.columns)}")
            print(f"   é¢„æµ‹æ ·æœ¬: {len(predictions)}")
        else:
            print("âš ï¸ é¢„æµ‹ç”Ÿæˆå¤±è´¥")
        
        # 9. æµ‹è¯•å®Œæ•´åˆ†ææµç¨‹
        print("\nğŸ“ˆ æ­¥éª¤9: æµ‹è¯•å®Œæ•´åˆ†ææµç¨‹")
        analysis_results = model.run_complete_analysis(X_selected, y_processed, test_size=0.2)
        print(f"âœ… å®Œæ•´åˆ†æå®Œæˆ:")
        print(f"   æˆåŠŸçŠ¶æ€: {analysis_results.get('success', False)}")
        print(f"   è®­ç»ƒæ¨¡å‹: {len(analysis_results.get('trained_models', {}))}")
        print(f"   é¢„æµ‹ç»“æœ: {'æœ‰' if analysis_results.get('predictions') is not None else 'æ— '}")
        
        # 10. æµ‹è¯•æ—¶åºé…ç½®éªŒè¯
        print("\nâ° æ­¥éª¤10: æµ‹è¯•æ—¶åºé…ç½®éªŒè¯")
        temporal_valid = model.validate_temporal_configuration()
        print(f"âœ… æ—¶åºé…ç½®éªŒè¯: {'é€šè¿‡' if temporal_valid else 'å¤±è´¥'}")
        
        # 11. æµ‹è¯•æ¨¡å‹æ‘˜è¦
        print("\nğŸ“‹ æ­¥éª¤11: ç”Ÿæˆæ¨¡å‹æ‘˜è¦")
        summary = model.get_model_summary()
        print("âœ… æ¨¡å‹æ‘˜è¦ç”Ÿæˆå®Œæˆ:")
        print(f"   é…ç½®å‚æ•°: {len(summary.get('config', {}))}")
        print(f"   æ€§èƒ½æŒ‡æ ‡: {summary.get('performance_metrics', {})}")
        
        # 12. å†…å­˜æ¸…ç†æµ‹è¯•
        print("\nğŸ§¹ æ­¥éª¤12: å†…å­˜æ¸…ç†æµ‹è¯•")
        model._cleanup_training_memory()
        print("âœ… å†…å­˜æ¸…ç†å®Œæˆ")
        
        print("\n" + "="*60)
        print("ğŸ‰ BMA Ultra Enhanced å®Œæ•´æµ‹è¯•æˆåŠŸ!")
        print("="*60)
        
        # æœ€ç»ˆç»Ÿè®¡
        print("\nğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
        print(f"   æ•°æ®æ ·æœ¬: {len(X_clean)} è¡Œ")
        print(f"   ç‰¹å¾æ•°é‡: {X_clean.shape[1]} â†’ {X_selected.shape[1]} (é€‰æ‹©å)")
        print(f"   è®­ç»ƒæ¨¡å‹: {len(analysis_results.get('trained_models', {}))}")
        print(f"   é¢„æµ‹å‡†ç¡®æ€§: {'å¯ç”¨' if predictions is not None else 'ä¸å¯ç”¨'}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
        return False

if __name__ == "__main__":
    print("BMA Ultra Enhanced - Real Data Test")
    print("çœŸå®æ•°æ®å®Œæ•´æµç¨‹æµ‹è¯•")
    print("="*60)
    
    success = run_complete_bma_test()
    
    if success:
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ - BMAæ¨¡å‹å®Œå…¨å¯ç”¨!")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ - éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")