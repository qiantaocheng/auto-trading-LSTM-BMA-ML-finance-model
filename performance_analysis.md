# BMAç³»ç»Ÿæ€§èƒ½ç“¶é¢ˆæ·±åº¦åˆ†æ

## ğŸ”¥ **å…³é”®æ€§èƒ½ç“¶é¢ˆè¯†åˆ«**

### 1. **æ•°æ®å¤„ç†ç“¶é¢ˆ** - ğŸ”´ ä¸¥é‡

**é—®é¢˜å‘ç°**:
```python
# Line 2120: æ‰¹é‡å¤„ç†å¾ªç¯ï¼Œå¯èƒ½æ•ˆç‡ä½ä¸‹
for i in range(0, len(tickers), batch_size):
    # æ¯æ‰¹æ¬¡éƒ½è¿›è¡Œå®Œæ•´çš„æ•°æ®åŠ è½½å’Œå¤„ç†

# Lines 855-901: å¤§é‡é‡å¤fillnaæ“ä½œ
group[col] = group[col].fillna(fill_value)
cleaned_data[numeric_cols] = cleaned_data[numeric_cols].fillna(method='ffill').fillna(0)
# ... é‡å¤å¤šæ¬¡fillnaè°ƒç”¨
```

**æ€§èƒ½å½±å“**:
- å¤§é‡è‚¡ç¥¨æ‰¹é‡å¤„ç†æ—¶ï¼Œæ¯æ‰¹éƒ½é‡æ–°åˆå§‹åŒ–æ•°æ®ç®¡é“
- å¤šæ¬¡fillnaæ“ä½œæ²¡æœ‰ä¼˜åŒ–ï¼Œé€ æˆå†…å­˜é‡å¤åˆ†é…
- é¢„è®¡å¯¹1000+è‚¡ç¥¨å¤„ç†é€ æˆ3-5å€æ€§èƒ½æŸå¤±

### 2. **ç‰¹å¾å·¥ç¨‹ç“¶é¢ˆ** - ğŸ”´ ä¸¥é‡

**æ ¸å¿ƒé—®é¢˜**:
```python
# Line 2447: ç‰¹å¾ç®¡é“åå¤fit_transform
processed_features, transform_info = self.feature_pipeline.fit_transform(...)

# Line 1359-1960: å¤§é‡pd.concatæ“ä½œ
combined = pd.concat(all_data, axis=0, ignore_index=True)
combined_features = pd.concat(all_features, ignore_index=False)
```

**æ€§èƒ½é—®é¢˜**:
- `fit_transform`æ¯æ¬¡é‡æ–°è®¡ç®—ç»Ÿè®¡é‡ï¼Œæ²¡æœ‰ç¼“å­˜æœºåˆ¶
- `pd.concat`åœ¨å¾ªç¯ä¸­ä½¿ç”¨ï¼Œé€ æˆO(nÂ²)å¤æ‚åº¦
- ç‰¹å¾æ ‡å‡†åŒ–é‡å¤è®¡ç®—å‡å€¼/æ ‡å‡†å·®

**ä¼˜åŒ–æ½œåŠ›**: 60-80%æ€§èƒ½æå‡

### 3. **æ¨¡å‹è®­ç»ƒç“¶é¢ˆ** - ğŸŸ¡ é‡è¦

**å‘ç°çš„é—®é¢˜**:
```python
# Line 6962-7024: CVè®­ç»ƒå¾ªç¯
for train_idx, val_idx in cv.split(...):
    model.fit(X_train, y_train)  # æ¯æŠ˜éƒ½é‡æ–°è®­ç»ƒ
    temp_model.fit(X_train, y_train)  # é‡å¤è®­ç»ƒç›¸ä¼¼æ¨¡å‹
```

**æ€§èƒ½åˆ†æ**:
- CVè¿‡ç¨‹ä¸­é‡å¤è®­ç»ƒç›¸ä¼¼è¶…å‚æ•°çš„æ¨¡å‹
- ç¼ºå°‘æ—©åœæœºåˆ¶ï¼Œè®­ç»ƒæ—¶é—´è¿‡é•¿
- æ²¡æœ‰åˆ©ç”¨GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰

### 4. **å†…å­˜ç®¡ç†ç“¶é¢ˆ** - ğŸŸ¡ é‡è¦

**å†…å­˜é—®é¢˜è¯†åˆ«**:
```python
# Lines 795-796: æ•°æ®æ¸…ç†ä½†å†…å­˜æœªé‡Šæ”¾
data = data.dropna(how='all', axis=0) 
data = data.dropna(how='all', axis=1)
# åŸæ•°æ®ä»åœ¨å†…å­˜ä¸­

# Line 3651: å¤§çŸ©é˜µè¿ç®—
factor_cov_matrix = cov_estimator.fit(risk_factors.fillna(0)).covariance_
# åæ–¹å·®çŸ©é˜µè®¡ç®—å†…å­˜å³°å€¼é«˜
```

**å†…å­˜å½±å“**:
- å³°å€¼å†…å­˜ä½¿ç”¨é‡å¯èƒ½è¶…è¿‡4-8GB
- åƒåœ¾æ”¶é›†é¢‘ç¹ï¼Œå½±å“æ€§èƒ½
- å¤§æ•°æ®é›†å¤„ç†æ—¶å¯èƒ½å†…å­˜æº¢å‡º

## ğŸ’¡ **ä¼˜åŒ–æ–¹æ¡ˆ**

### ç«‹å³ä¼˜åŒ–ï¼ˆé«˜ROIï¼‰:

1. **æ‰¹é‡å¤„ç†ä¼˜åŒ–**:
```python
# æ›¿æ¢ä½æ•ˆå¾ªç¯
# BEFORE:
for i in range(0, len(tickers), batch_size):
    # é‡å¤åˆå§‹åŒ–

# AFTER:  
def process_batch_vectorized(tickers_batch):
    # å‘é‡åŒ–å¤„ç†ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
```

2. **æ•°æ®ç¼“å­˜ç­–ç•¥**:
```python
# æ·»åŠ ç‰¹å¾ç¼“å­˜
@lru_cache(maxsize=128)
def cached_feature_transform(data_hash):
    return processed_features
```

3. **å¡«å……æ“ä½œä¼˜åŒ–**:
```python
# ä¸€æ¬¡æ€§å¡«å……æ›¿ä»£å¤šæ¬¡fillna
cleaned_data = data.fillna(value=fill_dict)  # å•æ¬¡æ“ä½œ
```

### ä¸­æœŸä¼˜åŒ–ï¼ˆä¸­ç­‰æ”¶ç›Šï¼‰:

4. **å¹¶è¡Œå¤„ç†**:
```python
from concurrent.futures import ThreadPoolExecutor
with ThreadPoolExecutor(max_workers=4) as executor:
    results = executor.map(process_stock, tickers)
```

5. **å†…å­˜æ± ç®¡ç†**:
```python
# æ˜¾å¼å†…å­˜ç®¡ç†
del intermediate_data
gc.collect()
```

### é•¿æœŸä¼˜åŒ–ï¼ˆæ¶æ„çº§ï¼‰:

6. **å¢é‡æ›´æ–°æœºåˆ¶**:
- åªé‡æ–°è®¡ç®—å˜åŒ–çš„ç‰¹å¾
- ä¿å­˜æ¨¡å‹ä¸­é—´çŠ¶æ€

7. **GPUåŠ é€Ÿ**:
- ä½¿ç”¨CuDFæ›¿ä»£Pandas
- GPUåŠ é€Ÿçš„MLæ¨¡å‹è®­ç»ƒ

## ğŸ“Š **é¢„æœŸæ€§èƒ½æå‡**

| ä¼˜åŒ–ç±»å‹ | æ€§èƒ½æå‡ | å®æ–½éš¾åº¦ | ä¼˜å…ˆçº§ |
|----------|----------|----------|--------|
| æ‰¹é‡å¤„ç†ä¼˜åŒ– | 40-60% | ä½ | ğŸ”´ é«˜ |
| å¡«å……æ“ä½œä¼˜åŒ– | 20-30% | ä½ | ğŸ”´ é«˜ |
| ç‰¹å¾ç¼“å­˜ | 30-50% | ä¸­ | ğŸŸ¡ ä¸­ |
| å¹¶è¡Œå¤„ç† | 50-100% | ä¸­ | ğŸŸ¡ ä¸­ |
| GPUåŠ é€Ÿ | 200-500% | é«˜ | ğŸŸ¢ ä½ |

æ€»ä½“é¢„æœŸ: **2-4å€æ€§èƒ½æå‡**

## ğŸš¨ **å†…å­˜ä½¿ç”¨åˆ†æ**

å½“å‰ä¼°è®¡å†…å­˜å³°å€¼:
- å•è‚¡ç¥¨å¤„ç†: ~50-100MB
- 1000è‚¡ç¥¨æ‰¹å¤„ç†: ~2-4GB
- CVè®­ç»ƒé˜¶æ®µ: ~4-8GB

ä¼˜åŒ–åé¢„æœŸ:
- å†…å­˜å³°å€¼å‡å°‘50-70%
- å¤„ç†é€Ÿåº¦æå‡2-4å€
- æ”¯æŒæ›´å¤§æ•°æ®é›†