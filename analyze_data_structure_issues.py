#!/usr/bin/env python3
"""
æ·±åº¦åˆ†æBMAæ¨¡å‹ä¸­æ•°æ®ç»“æ„å¯èƒ½å­˜åœ¨çš„é—®é¢˜
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import re
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Tuple
from collections import defaultdict, Counter

def analyze_dataframe_usage_patterns():
    """åˆ†æDataFrameä½¿ç”¨æ¨¡å¼ä¸­çš„æ½œåœ¨é—®é¢˜"""
    
    print("=== åˆ†æDataFrameä½¿ç”¨æ¨¡å¼ ===\n")
    
    file_path = "bma_models/é‡åŒ–æ¨¡å‹_bma_ultra_enhanced.py"
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    lines = content.split('\n')
    
    issues = {
        'index_inconsistencies': [],      # ç´¢å¼•ä¸ä¸€è‡´é—®é¢˜
        'data_type_conflicts': [],        # æ•°æ®ç±»å‹å†²çª  
        'memory_inefficiencies': [],      # å†…å­˜æ•ˆç‡é—®é¢˜
        'alignment_issues': [],           # æ•°æ®å¯¹é½é—®é¢˜
        'column_naming_conflicts': [],    # åˆ—åå†²çª
        'shape_assumptions': [],          # å½¢çŠ¶å‡è®¾é—®é¢˜
        'copy_inefficiencies': [],        # å¤åˆ¶æ•ˆç‡é—®é¢˜
        'nan_handling_inconsistencies': [] # NaNå¤„ç†ä¸ä¸€è‡´
    }
    
    # è·Ÿè¸ªDataFrameå˜é‡
    dataframe_vars = set()
    index_operations = []
    data_type_operations = []
    
    for i, line in enumerate(lines, 1):
        line_clean = line.strip()
        
        if line_clean.startswith('#'):
            continue
        
        # 1. æ£€æµ‹ç´¢å¼•æ“ä½œé—®é¢˜
        index_patterns = [
            (r'\.reset_index\(\)', 'reset_index'),
            (r'\.set_index\([^)]+\)', 'set_index'), 
            (r'\.reindex\([^)]+\)', 'reindex'),
            (r'\.loc\[[^]]+\]', 'loc_access'),
            (r'\.iloc\[[^]]+\]', 'iloc_access'),
            (r'\.index\s*=', 'index_assignment'),
            (r'MultiIndex', 'multiindex_usage')
        ]
        
        for pattern, operation_type in index_patterns:
            if re.search(pattern, line):
                issues['index_inconsistencies'].append({
                    'line': i,
                    'operation': operation_type,
                    'context': line_clean[:100],
                    'potential_issue': get_index_issue_description(operation_type)
                })
        
        # 2. æ£€æµ‹æ•°æ®ç±»å‹é—®é¢˜
        dtype_patterns = [
            (r'\.astype\([^)]+\)', 'explicit_conversion'),
            (r'pd\.to_datetime\([^)]+\)', 'datetime_conversion'),
            (r'pd\.to_numeric\([^)]+\)', 'numeric_conversion'),
            (r'\.fillna\([^)]+\)', 'fillna_operation'),
            (r'\.dropna\([^)]+\)', 'dropna_operation'),
            (r'dtype\s*=', 'dtype_specification')
        ]
        
        for pattern, operation_type in dtype_patterns:
            if re.search(pattern, line):
                issues['data_type_conflicts'].append({
                    'line': i,
                    'operation': operation_type,
                    'context': line_clean[:100],
                    'potential_issue': get_dtype_issue_description(operation_type)
                })
        
        # 3. æ£€æµ‹å†…å­˜æ•ˆç‡é—®é¢˜
        memory_patterns = [
            (r'\.copy\(\)', 'unnecessary_copy'),
            (r'pd\.concat\([^)]+\)', 'concat_operation'),
            (r'\.append\([^)]+\)', 'append_operation'),
            (r'for.*in.*\.iterrows\(\)', 'inefficient_iteration'),
            (r'\.apply\(lambda.*\)', 'lambda_apply'),
            (r'\+.*pd\.DataFrame', 'dataframe_addition')
        ]
        
        for pattern, operation_type in memory_patterns:
            if re.search(pattern, line):
                issues['memory_inefficiencies'].append({
                    'line': i,
                    'operation': operation_type, 
                    'context': line_clean[:100],
                    'potential_issue': get_memory_issue_description(operation_type)
                })
        
        # 4. æ£€æµ‹æ•°æ®å¯¹é½é—®é¢˜
        alignment_patterns = [
            (r'\.join\([^)]+\)', 'join_operation'),
            (r'\.merge\([^)]+\)', 'merge_operation'),
            (r'\.align\([^)]+\)', 'align_operation'),
            (r'\[.*\]\s*=.*\[.*\]', 'index_assignment_mismatch'),
            (r'\.reindex_like\([^)]+\)', 'reindex_like')
        ]
        
        for pattern, operation_type in alignment_patterns:
            if re.search(pattern, line):
                issues['alignment_issues'].append({
                    'line': i,
                    'operation': operation_type,
                    'context': line_clean[:100],
                    'potential_issue': get_alignment_issue_description(operation_type)
                })
        
        # 5. æ£€æµ‹åˆ—åå†²çª
        if 'columns' in line and ('=' in line or 'rename' in line):
            issues['column_naming_conflicts'].append({
                'line': i,
                'context': line_clean[:100],
                'potential_issue': 'åˆ—åæ“ä½œå¯èƒ½å¯¼è‡´å‘½åå†²çª'
            })
        
        # 6. æ£€æµ‹å½¢çŠ¶å‡è®¾
        shape_patterns = [
            (r'\.shape\[0\]', 'row_count_assumption'),
            (r'\.shape\[1\]', 'column_count_assumption'),
            (r'len\([^)]+\)', 'length_assumption'),
            (r'\.empty', 'emptiness_check')
        ]
        
        for pattern, assumption_type in shape_patterns:
            if re.search(pattern, line):
                issues['shape_assumptions'].append({
                    'line': i,
                    'assumption': assumption_type,
                    'context': line_clean[:100],
                    'potential_issue': get_shape_issue_description(assumption_type)
                })
    
    # åˆ†æç»“æœ
    print("1. [CRITICAL] ç´¢å¼•æ“ä½œé—®é¢˜:")
    if issues['index_inconsistencies']:
        index_operations_count = Counter(item['operation'] for item in issues['index_inconsistencies'])
        print(f"   å‘ç° {len(issues['index_inconsistencies'])} ä¸ªç´¢å¼•æ“ä½œï¼Œç±»å‹åˆ†å¸ƒ:")
        for op_type, count in index_operations_count.most_common(5):
            print(f"     {op_type}: {count} æ¬¡")
        
        print(f"   å‰5ä¸ªæ½œåœ¨é—®é¢˜:")
        for item in issues['index_inconsistencies'][:5]:
            print(f"     Line {item['line']}: {item['operation']} - {item['potential_issue']}")
    else:
        print("   [OK] æœªå‘ç°æ˜æ˜¾ç´¢å¼•é—®é¢˜")
    
    print(f"\n2. [HIGH] æ•°æ®ç±»å‹å†²çª:")
    if issues['data_type_conflicts']:
        dtype_operations_count = Counter(item['operation'] for item in issues['data_type_conflicts'])
        print(f"   å‘ç° {len(issues['data_type_conflicts'])} ä¸ªæ•°æ®ç±»å‹æ“ä½œ:")
        for op_type, count in dtype_operations_count.most_common(3):
            print(f"     {op_type}: {count} æ¬¡")
        
        print(f"   å‰3ä¸ªæ½œåœ¨é—®é¢˜:")
        for item in issues['data_type_conflicts'][:3]:
            print(f"     Line {item['line']}: {item['potential_issue']}")
    else:
        print("   [OK] æ•°æ®ç±»å‹æ“ä½œç›¸å¯¹å®‰å…¨")
    
    print(f"\n3. [MEDIUM] å†…å­˜æ•ˆç‡é—®é¢˜:")
    if issues['memory_inefficiencies']:
        memory_operations_count = Counter(item['operation'] for item in issues['memory_inefficiencies'])
        print(f"   å‘ç° {len(issues['memory_inefficiencies'])} ä¸ªå†…å­˜æ•ˆç‡é—®é¢˜:")
        for op_type, count in memory_operations_count.most_common(3):
            print(f"     {op_type}: {count} æ¬¡")
        
        high_impact_memory_issues = [
            item for item in issues['memory_inefficiencies'] 
            if item['operation'] in ['inefficient_iteration', 'unnecessary_copy', 'append_operation']
        ]
        if high_impact_memory_issues:
            print(f"   é«˜å½±å“å†…å­˜é—®é¢˜ ({len(high_impact_memory_issues)} ä¸ª):")
            for item in high_impact_memory_issues[:3]:
                print(f"     Line {item['line']}: {item['potential_issue']}")
    else:
        print("   [OK] å†…å­˜ä½¿ç”¨ç›¸å¯¹é«˜æ•ˆ")
    
    print(f"\n4. [MEDIUM] æ•°æ®å¯¹é½é—®é¢˜:")
    if issues['alignment_issues']:
        print(f"   å‘ç° {len(issues['alignment_issues'])} ä¸ªæ•°æ®å¯¹é½æ“ä½œ")
        merge_operations = [item for item in issues['alignment_issues'] if 'merge' in item['operation']]
        join_operations = [item for item in issues['alignment_issues'] if 'join' in item['operation']]
        
        print(f"     Mergeæ“ä½œ: {len(merge_operations)} ä¸ª")
        print(f"     Joinæ“ä½œ: {len(join_operations)} ä¸ª")
        
        if merge_operations:
            print(f"   Mergeæ“ä½œç¤ºä¾‹:")
            for item in merge_operations[:2]:
                print(f"     Line {item['line']}: {item['context'][:60]}...")
    else:
        print("   [OK] æ•°æ®å¯¹é½æ“ä½œè¾ƒå°‘")
    
    print(f"\n5. [LOW] å…¶ä»–ç»“æ„é—®é¢˜:")
    print(f"   åˆ—åæ“ä½œ: {len(issues['column_naming_conflicts'])} ä¸ª")
    print(f"   å½¢çŠ¶å‡è®¾: {len(issues['shape_assumptions'])} ä¸ª")
    
    return issues

def get_index_issue_description(operation_type):
    """è·å–ç´¢å¼•æ“ä½œé—®é¢˜æè¿°"""
    descriptions = {
        'reset_index': 'é¢‘ç¹reset_indexå¯èƒ½å¯¼è‡´ç´¢å¼•ä¸¢å¤±å’Œæ€§èƒ½ä¸‹é™',
        'set_index': 'é‡å¤set_indexæ“ä½œå¯èƒ½å¯¼è‡´ç´¢å¼•ä¸ä¸€è‡´',
        'reindex': 'reindexæ“ä½œå¯èƒ½å¼•å…¥NaNå’Œæ•°æ®ä¸å¯¹é½',
        'loc_access': 'locè®¿é—®å¯èƒ½å› ç´¢å¼•ä¸åŒ¹é…è€Œå¤±è´¥',
        'iloc_access': 'ilocè®¿é—®å‡è®¾ç‰¹å®šçš„è¡Œåºï¼Œå¯èƒ½è„†å¼±',
        'index_assignment': 'ç›´æ¥ç´¢å¼•èµ‹å€¼å¯èƒ½ç ´åç´¢å¼•å®Œæ•´æ€§',
        'multiindex_usage': 'MultiIndexä½¿ç”¨ä¸å½“å¯èƒ½å¯¼è‡´å¤æ‚æ€§'
    }
    return descriptions.get(operation_type, 'æœªçŸ¥ç´¢å¼•é—®é¢˜')

def get_dtype_issue_description(operation_type):
    """è·å–æ•°æ®ç±»å‹é—®é¢˜æè¿°"""
    descriptions = {
        'explicit_conversion': 'æ˜¾å¼ç±»å‹è½¬æ¢å¯èƒ½ä¸¢å¤±æ•°æ®æˆ–å¼•å‘é”™è¯¯',
        'datetime_conversion': 'æ—¥æœŸæ—¶é—´è½¬æ¢å¯èƒ½å› æ ¼å¼ä¸ä¸€è‡´è€Œå¤±è´¥',
        'numeric_conversion': 'æ•°å€¼è½¬æ¢å¯èƒ½å› éæ•°å€¼æ•°æ®è€Œå¤±è´¥',
        'fillna_operation': 'fillnaç­–ç•¥ä¸ä¸€è‡´å¯èƒ½å¯¼è‡´æ•°æ®è´¨é‡é—®é¢˜',
        'dropna_operation': 'dropnaå¯èƒ½æ„å¤–åˆ é™¤é‡è¦æ•°æ®',
        'dtype_specification': 'dtypeè§„èŒƒä¸å½“å¯èƒ½å¯¼è‡´å†…å­˜æµªè´¹'
    }
    return descriptions.get(operation_type, 'æœªçŸ¥æ•°æ®ç±»å‹é—®é¢˜')

def get_memory_issue_description(operation_type):
    """è·å–å†…å­˜é—®é¢˜æè¿°"""
    descriptions = {
        'unnecessary_copy': 'ä¸å¿…è¦çš„copy()æ“ä½œæµªè´¹å†…å­˜',
        'concat_operation': 'concatæ“ä½œå¯èƒ½å¯¼è‡´å†…å­˜ç¢ç‰‡',
        'append_operation': 'appendæ“ä½œåœ¨å¾ªç¯ä¸­éå¸¸ä½æ•ˆ',
        'inefficient_iteration': 'iterrows()æ¯”å‘é‡åŒ–æ“ä½œæ…¢æ•°å€',
        'lambda_apply': 'lambda applyæ¯”å†…ç½®å‡½æ•°æ…¢',
        'dataframe_addition': 'DataFrameç›´æ¥ç›¸åŠ å¯èƒ½å¯¼è‡´æ„å¤–è¡Œä¸º'
    }
    return descriptions.get(operation_type, 'æœªçŸ¥å†…å­˜é—®é¢˜')

def get_alignment_issue_description(operation_type):
    """è·å–å¯¹é½é—®é¢˜æè¿°"""
    descriptions = {
        'join_operation': 'joinæ“ä½œå¯èƒ½å¯¼è‡´æ•°æ®é”™ä½æˆ–ä¸¢å¤±',
        'merge_operation': 'mergeå‚æ•°ä¸å½“å¯èƒ½å¯¼è‡´æ„å¤–ç»“æœ',
        'align_operation': 'alignæ“ä½œå¯èƒ½å¼•å…¥ä¸å¿…è¦çš„NaN',
        'index_assignment_mismatch': 'ç´¢å¼•èµ‹å€¼ä¸åŒ¹é…å¯èƒ½å¯¼è‡´æ•°æ®é”™ä½',
        'reindex_like': 'reindex_likeå¯èƒ½æ”¹å˜æ•°æ®ç»“æ„'
    }
    return descriptions.get(operation_type, 'æœªçŸ¥å¯¹é½é—®é¢˜')

def get_shape_issue_description(assumption_type):
    """è·å–å½¢çŠ¶å‡è®¾é—®é¢˜æè¿°"""
    descriptions = {
        'row_count_assumption': 'å‡è®¾ç‰¹å®šè¡Œæ•°å¯èƒ½åœ¨æ•°æ®å˜åŒ–æ—¶å¤±è´¥',
        'column_count_assumption': 'å‡è®¾ç‰¹å®šåˆ—æ•°å¯èƒ½å¯¼è‡´ç´¢å¼•é”™è¯¯',
        'length_assumption': 'len()å‡è®¾å¯èƒ½åœ¨ç©ºæ•°æ®æ—¶å¤±è´¥',
        'emptiness_check': 'emptyæ£€æŸ¥å¯èƒ½ä¸è¶³ä»¥å¤„ç†æ‰€æœ‰è¾¹ç•Œæƒ…å†µ'
    }
    return descriptions.get(assumption_type, 'æœªçŸ¥å½¢çŠ¶å‡è®¾é—®é¢˜')

def analyze_data_flow_patterns():
    """åˆ†ææ•°æ®æµæ¨¡å¼ä¸­çš„é—®é¢˜"""
    
    print(f"\n=== æ•°æ®æµæ¨¡å¼åˆ†æ ===\n")
    
    file_path = "bma_models/é‡åŒ–æ¨¡å‹_bma_ultra_enhanced.py"
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æŸ¥æ‰¾æ•°æ®æµæ¨¡å¼
    data_flow_issues = {
        'circular_dependencies': [],
        'data_leakage_risks': [], 
        'inconsistent_transformations': [],
        'pipeline_bottlenecks': []
    }
    
    lines = content.split('\n')
    
    # è·Ÿè¸ªæ•°æ®å˜æ¢åºåˆ—
    transformations = []
    current_dataframe = None
    
    for i, line in enumerate(lines, 1):
        line_clean = line.strip()
        
        # æ£€æµ‹æ•°æ®æµæ¨¡å¼
        if '=' in line and any(df_op in line for df_op in ['.drop', '.fillna', '.transform', '.apply']):
            transformations.append({
                'line': i,
                'transformation': line_clean,
                'type': 'modification'
            })
        
        # æ£€æµ‹æ½œåœ¨çš„æ•°æ®æ³„æ¼
        leakage_patterns = [
            r'shift\(-\d+\)',           # å‘å‰shiftï¼ˆä½¿ç”¨æœªæ¥æ•°æ®ï¼‰
            r'\.expanding\(\)',         # expanding windowï¼ˆå¯èƒ½åŒ…å«æœªæ¥ä¿¡æ¯ï¼‰
            r'fillna.*method.*forward', # å‰å‘å¡«å……
            r'\.rolling\(.*center=True' # ä¸­å¿ƒåŒ–æ»šåŠ¨çª—å£
        ]
        
        for pattern in leakage_patterns:
            if re.search(pattern, line):
                data_flow_issues['data_leakage_risks'].append({
                    'line': i,
                    'pattern': pattern,
                    'context': line_clean,
                    'risk_level': 'HIGH' if 'shift(-' in line else 'MEDIUM'
                })
    
    # åˆ†æç»“æœ
    print("æ•°æ®æµé£é™©è¯„ä¼°:")
    print(f"æ½œåœ¨æ•°æ®æ³„æ¼é£é™©: {len(data_flow_issues['data_leakage_risks'])} ä¸ª")
    
    if data_flow_issues['data_leakage_risks']:
        high_risk = [item for item in data_flow_issues['data_leakage_risks'] if item['risk_level'] == 'HIGH']
        if high_risk:
            print(f"é«˜é£é™©æ•°æ®æ³„æ¼ ({len(high_risk)} ä¸ª):")
            for item in high_risk[:3]:
                print(f"  Line {item['line']}: {item['context'][:60]}...")
    
    return data_flow_issues

def detect_data_structure_antipatterns():
    """æ£€æµ‹æ•°æ®ç»“æ„åæ¨¡å¼"""
    
    print(f"\n=== æ•°æ®ç»“æ„åæ¨¡å¼æ£€æµ‹ ===\n")
    
    file_path = "bma_models/é‡åŒ–æ¨¡å‹_bma_ultra_enhanced.py"
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    antipatterns = {
        'god_dataframe': [],        # è¿‡å¤§çš„DataFrame
        'magic_columns': [],        # ç¡¬ç¼–ç åˆ—å
        'inconsistent_dtypes': [],  # æ•°æ®ç±»å‹ä¸ä¸€è‡´
        'nested_loops_on_df': [],   # DataFrameä¸Šçš„åµŒå¥—å¾ªç¯
        'string_operations_on_categorical': []  # åˆ†ç±»æ•°æ®ä¸Šçš„å­—ç¬¦ä¸²æ“ä½œ
    }
    
    lines = content.split('\n')
    
    for i, line in enumerate(lines, 1):
        line_clean = line.strip()
        
        # æ£€æµ‹ç¡¬ç¼–ç åˆ—åï¼ˆé­”æ³•å­—ç¬¦ä¸²ï¼‰
        column_patterns = [
            r"'[A-Za-z_][A-Za-z0-9_]*'",  # å¼•å·ä¸­çš„åˆ—å
            r'"[A-Za-z_][A-Za-z0-9_]*"'   # åŒå¼•å·ä¸­çš„åˆ—å
        ]
        
        for pattern in column_patterns:
            matches = re.findall(pattern, line)
            if matches and any(keyword in line.lower() for keyword in ['column', 'col', '[', 'drop']):
                antipatterns['magic_columns'].append({
                    'line': i,
                    'columns': matches,
                    'context': line_clean[:80]
                })
        
        # æ£€æµ‹DataFrameä¸Šçš„åµŒå¥—å¾ªç¯
        if 'for' in line and any(df_indicator in line for df_indicator in ['.iterrows', '.itertuples', '.items']):
            # æŸ¥çœ‹æ¥ä¸‹æ¥å‡ è¡Œæ˜¯å¦è¿˜æœ‰forå¾ªç¯
            context_lines = lines[i:i+5] if i < len(lines)-5 else lines[i:]
            if any('for' in context_line for context_line in context_lines[1:]):
                antipatterns['nested_loops_on_df'].append({
                    'line': i,
                    'context': line_clean,
                    'issue': 'åœ¨DataFrameä¸Šä½¿ç”¨åµŒå¥—å¾ªç¯ï¼Œæ€§èƒ½æå·®'
                })
    
    # æŠ¥å‘Šåæ¨¡å¼
    print("æ£€æµ‹åˆ°çš„åæ¨¡å¼:")
    
    if antipatterns['magic_columns']:
        print(f"ç¡¬ç¼–ç åˆ—å: {len(antipatterns['magic_columns'])} å¤„")
        print("  ç¤ºä¾‹:")
        for item in antipatterns['magic_columns'][:3]:
            print(f"    Line {item['line']}: {item['columns']}")
    
    if antipatterns['nested_loops_on_df']:
        print(f"DataFrameåµŒå¥—å¾ªç¯: {len(antipatterns['nested_loops_on_df'])} å¤„")
        for item in antipatterns['nested_loops_on_df']:
            print(f"  Line {item['line']}: {item['issue']}")
    
    return antipatterns

def run_comprehensive_data_structure_analysis():
    """è¿è¡Œå…¨é¢çš„æ•°æ®ç»“æ„åˆ†æ"""
    
    print("ğŸ” å¼€å§‹BMAç³»ç»Ÿæ•°æ®ç»“æ„æ·±åº¦åˆ†æ")
    print("=" * 60)
    
    # 1. DataFrameä½¿ç”¨æ¨¡å¼åˆ†æ
    print("ç¬¬ä¸€é˜¶æ®µï¼šDataFrameä½¿ç”¨æ¨¡å¼åˆ†æ")
    df_issues = analyze_dataframe_usage_patterns()
    
    # 2. æ•°æ®æµæ¨¡å¼åˆ†æ  
    print("ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®æµæ¨¡å¼åˆ†æ")
    flow_issues = analyze_data_flow_patterns()
    
    # 3. åæ¨¡å¼æ£€æµ‹
    print("ç¬¬ä¸‰é˜¶æ®µï¼šæ•°æ®ç»“æ„åæ¨¡å¼æ£€æµ‹") 
    antipatterns = detect_data_structure_antipatterns()
    
    # ç»¼åˆè¯„ä¼°
    print("\n" + "=" * 60)
    print("ğŸ“Š æ•°æ®ç»“æ„é—®é¢˜ç»¼åˆè¯„ä¼°")
    
    critical_issues = len(df_issues['index_inconsistencies'])
    high_issues = len(df_issues['data_type_conflicts']) + len(flow_issues['data_leakage_risks'])
    medium_issues = len(df_issues['memory_inefficiencies']) + len(df_issues['alignment_issues'])
    
    total_issues = critical_issues + high_issues + medium_issues
    
    print(f"CRITICALçº§åˆ«: {critical_issues} (ç´¢å¼•é—®é¢˜)")
    print(f"HIGHçº§åˆ«: {high_issues} (æ•°æ®ç±»å‹ + æ•°æ®æ³„æ¼)")  
    print(f"MEDIUMçº§åˆ«: {medium_issues} (å†…å­˜ + å¯¹é½)")
    print(f"æ€»é—®é¢˜æ•°: {total_issues}")
    
    # é£é™©è¯„çº§
    if critical_issues > 20:
        risk_level = "CRITICAL"
        print(f"\nğŸš¨ [CRITICAL] æ•°æ®ç»“æ„å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼")
    elif high_issues > 15:
        risk_level = "HIGH" 
        print(f"\nâš ï¸ [HIGH] æ•°æ®ç»“æ„é—®é¢˜è¾ƒå¤šï¼Œéœ€è¦ä¼˜åŒ–")
    elif total_issues > 50:
        risk_level = "MEDIUM"
        print(f"\nğŸ“ [MEDIUM] æ•°æ®ç»“æ„æœ‰æ”¹è¿›ç©ºé—´")
    else:
        risk_level = "LOW"
        print(f"\nâœ… [GOOD] æ•°æ®ç»“æ„ç›¸å¯¹å¥åº·")
    
    # ä¼˜å…ˆä¿®å¤å»ºè®®
    print(f"\nğŸ”§ ä¼˜å…ˆä¿®å¤å»ºè®®:")
    
    if critical_issues > 0:
        print("1. ç«‹å³ä¿®å¤ç´¢å¼•æ“ä½œä¸ä¸€è‡´é—®é¢˜")
    if len(flow_issues['data_leakage_risks']) > 0:
        print("2. æ£€æŸ¥å¹¶ä¿®å¤æ•°æ®æ³„æ¼é£é™©")
    if len(df_issues['memory_inefficiencies']) > 5:
        print("3. ä¼˜åŒ–å†…å­˜ä½¿ç”¨æ•ˆç‡")
    if len(antipatterns['magic_columns']) > 10:
        print("4. é‡æ„ç¡¬ç¼–ç åˆ—åï¼Œä½¿ç”¨é…ç½®ç®¡ç†")
    
    return {
        'risk_level': risk_level,
        'total_issues': total_issues,
        'critical': critical_issues,
        'high': high_issues,
        'medium': medium_issues,
        'detailed_issues': {
            'dataframe_issues': df_issues,
            'flow_issues': flow_issues,
            'antipatterns': antipatterns
        }
    }

if __name__ == "__main__":
    try:
        result = run_comprehensive_data_structure_analysis()
        print(f"\nğŸ“‹ åˆ†æå®Œæˆï¼æ•°æ®ç»“æ„é£é™©ç­‰çº§: {result['risk_level']}")
        
        if result['total_issues'] > 30:
            print(f"\nğŸ’¡ å»ºè®®ï¼šæ•°æ®ç»“æ„é—®é¢˜è¾ƒå¤šï¼Œè€ƒè™‘è¿›è¡Œç³»ç»Ÿæ€§é‡æ„")
        else:
            print(f"\nâœ¨ æ•°æ®ç»“æ„æ•´ä½“è´¨é‡å¯æ¥å—ï¼Œè¿›è¡Œé’ˆå¯¹æ€§ä¼˜åŒ–å³å¯")
            
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()