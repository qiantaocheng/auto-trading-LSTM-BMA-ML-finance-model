#!/usr/bin/env python3
"""
æ¨¡å‹å®æ—¶ç›‘æ§å’Œå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ
"""

import numpy as np
import pandas as pd
import warnings
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
from collections import deque
import logging
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.metrics import mean_squared_error

logger = logging.getLogger(__name__)

@dataclass
class Alert:
    """å‘Šè­¦ä¿¡æ¯"""
    timestamp: datetime
    level: str  # 'INFO', 'WARNING', 'ERROR', 'CRITICAL'
    component: str
    message: str
    metrics: Dict[str, float]
    suggested_action: str

@dataclass
class ModelMetrics:
    """æ¨¡å‹æŒ‡æ ‡"""
    timestamp: datetime
    prediction_count: int
    prediction_mean: float
    prediction_std: float
    prediction_range: Tuple[float, float]
    feature_drift_score: float
    prediction_drift_score: float
    outlier_ratio: float
    processing_time_ms: float

class ModelMonitor:
    """æ¨¡å‹å®æ—¶ç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self, window_size: int = 1000, alert_thresholds: Optional[Dict] = None):
        """
        åˆå§‹åŒ–ç›‘æ§ç³»ç»Ÿ
        
        Args:
            window_size: æ»‘åŠ¨çª—å£å¤§å°
            alert_thresholds: å‘Šè­¦é˜ˆå€¼é…ç½®
        """
        self.window_size = window_size
        
        # é»˜è®¤å‘Šè­¦é˜ˆå€¼
        self.alert_thresholds = {
            'prediction_drift_high': 0.3,
            'prediction_drift_critical': 0.5,
            'feature_drift_high': 0.2,
            'feature_drift_critical': 0.4,
            'outlier_ratio_high': 0.15,
            'outlier_ratio_critical': 0.25,
            'processing_time_high': 5000,  # ms
            'processing_time_critical': 10000,  # ms
            'prediction_std_low': 0.001,  # é¢„æµ‹æ–¹å·®è¿‡ä½
            'prediction_std_high': 1.0    # é¢„æµ‹æ–¹å·®è¿‡é«˜
        }
        
        if alert_thresholds:
            self.alert_thresholds.update(alert_thresholds)
        
        # å†å²æ•°æ®å­˜å‚¨
        self.prediction_history = deque(maxlen=window_size)
        self.feature_history = deque(maxlen=window_size)
        self.metrics_history = deque(maxlen=window_size) 
        self.alerts = deque(maxlen=1000)
        
        # åŸºçº¿ç»Ÿè®¡ï¼ˆç”¨äºæ¼‚ç§»æ£€æµ‹ï¼‰
        self.baseline_stats = {
            'prediction_mean': None,
            'prediction_std': None,
            'feature_means': None,
            'feature_stds': None
        }
        
        # å¼‚å¸¸æ£€æµ‹æ¨¡å‹
        self.outlier_detector = IsolationForest(
            contamination=0.1,
            random_state=42,
            n_estimators=100
        )
        self.outlier_detector_fitted = False
        
        # æ€§èƒ½è®¡æ•°å™¨
        self.counters = {
            'total_predictions': 0,
            'total_alerts': 0,
            'drift_detections': 0,
            'outlier_detections': 0,
            'processing_errors': 0
        }
        
        logger.info("æ¨¡å‹ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def set_baseline(self, baseline_predictions: np.ndarray, 
                     baseline_features: Optional[np.ndarray] = None):
        """
        è®¾ç½®åŸºçº¿ç»Ÿè®¡
        
        Args:
            baseline_predictions: åŸºçº¿é¢„æµ‹å€¼
            baseline_features: åŸºçº¿ç‰¹å¾å€¼
        """
        self.baseline_stats['prediction_mean'] = np.mean(baseline_predictions)
        self.baseline_stats['prediction_std'] = np.std(baseline_predictions)
        
        if baseline_features is not None:
            self.baseline_stats['feature_means'] = np.mean(baseline_features, axis=0)
            self.baseline_stats['feature_stds'] = np.std(baseline_features, axis=0)
            
            # è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹
            try:
                self.outlier_detector.fit(baseline_features)
                self.outlier_detector_fitted = True
                logger.info("å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒå®Œæˆ")
            except Exception as e:
                logger.warning(f"å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        
        logger.info("åŸºçº¿ç»Ÿè®¡è®¾ç½®å®Œæˆ")
    
    def detect_prediction_drift(self, current_predictions: np.ndarray) -> float:
        """
        æ£€æµ‹é¢„æµ‹æ¼‚ç§»
        
        Args:
            current_predictions: å½“å‰é¢„æµ‹å€¼
            
        Returns:
            æ¼‚ç§»åˆ†æ•° (0-1, è¶Šé«˜è¶Šä¸¥é‡)
        """
        if (self.baseline_stats['prediction_mean'] is None or 
            self.baseline_stats['prediction_std'] is None):
            return 0.0
        
        try:
            # ä½¿ç”¨Kolmogorov-Smirnovæ£€éªŒ
            baseline_samples = np.random.normal(
                self.baseline_stats['prediction_mean'],
                self.baseline_stats['prediction_std'],
                len(current_predictions)
            )
            
            ks_stat, p_value = stats.ks_2samp(baseline_samples, current_predictions)
            
            # è½¬æ¢ä¸º0-1èŒƒå›´çš„æ¼‚ç§»åˆ†æ•°
            drift_score = min(ks_stat * 2, 1.0)  # KSç»Ÿè®¡é‡é€šå¸¸0-0.5
            
            return drift_score
            
        except Exception as e:
            logger.warning(f"é¢„æµ‹æ¼‚ç§»æ£€æµ‹å¤±è´¥: {e}")
            return 0.0
    
    def detect_feature_drift(self, current_features: np.ndarray) -> float:
        """
        æ£€æµ‹ç‰¹å¾æ¼‚ç§»
        
        Args:
            current_features: å½“å‰ç‰¹å¾å€¼
            
        Returns:
            æ¼‚ç§»åˆ†æ•° (0-1, è¶Šé«˜è¶Šä¸¥é‡)
        """
        if (self.baseline_stats['feature_means'] is None or 
            self.baseline_stats['feature_stds'] is None):
            return 0.0
        
        try:
            current_means = np.mean(current_features, axis=0)
            baseline_means = self.baseline_stats['feature_means']
            baseline_stds = self.baseline_stats['feature_stds']
            
            # è®¡ç®—æ ‡å‡†åŒ–å·®å¼‚
            normalized_diffs = np.abs(current_means - baseline_means) / (baseline_stds + 1e-8)
            
            # å¹³å‡æ¼‚ç§»åˆ†æ•°
            drift_score = np.mean(np.clip(normalized_diffs / 3, 0, 1))  # 3ä¸ªæ ‡å‡†å·®ä¸ºæœ€å¤§
            
            return drift_score
            
        except Exception as e:
            logger.warning(f"ç‰¹å¾æ¼‚ç§»æ£€æµ‹å¤±è´¥: {e}")
            return 0.0
    
    def detect_outliers(self, features: np.ndarray) -> Tuple[np.ndarray, float]:
        """
        æ£€æµ‹å¼‚å¸¸å€¼
        
        Args:
            features: ç‰¹å¾æ•°æ®
            
        Returns:
            (outlier_mask, outlier_ratio) å¼‚å¸¸å€¼æ©ç å’Œæ¯”ä¾‹
        """
        if not self.outlier_detector_fitted:
            return np.zeros(len(features), dtype=bool), 0.0
        
        try:
            outlier_scores = self.outlier_detector.decision_function(features)
            outlier_mask = self.outlier_detector.predict(features) == -1
            outlier_ratio = np.mean(outlier_mask)
            
            return outlier_mask, outlier_ratio
            
        except Exception as e:
            logger.warning(f"å¼‚å¸¸å€¼æ£€æµ‹å¤±è´¥: {e}")
            return np.zeros(len(features), dtype=bool), 0.0
    
    def monitor_predictions(self, predictions: np.ndarray, 
                          features: Optional[np.ndarray] = None,
                          processing_time_ms: float = 0.0) -> List[Alert]:
        """
        ç›‘æ§é¢„æµ‹ç»“æœå¹¶ç”Ÿæˆå‘Šè­¦
        
        Args:
            predictions: é¢„æµ‹å€¼
            features: ç‰¹å¾å€¼ï¼ˆå¯é€‰ï¼‰
            processing_time_ms: å¤„ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
            
        Returns:
            å‘Šè­¦åˆ—è¡¨
        """
        start_time = datetime.now()
        alerts = []
        
        try:
            # æ›´æ–°è®¡æ•°å™¨
            self.counters['total_predictions'] += len(predictions)
            
            # åŸºæœ¬ç»Ÿè®¡
            pred_mean = np.mean(predictions)
            pred_std = np.std(predictions)
            pred_min, pred_max = np.min(predictions), np.max(predictions)
            
            # æ¼‚ç§»æ£€æµ‹
            prediction_drift = self.detect_prediction_drift(predictions)
            feature_drift = 0.0
            outlier_ratio = 0.0
            
            if features is not None:
                feature_drift = self.detect_feature_drift(features)
                _, outlier_ratio = self.detect_outliers(features)
                self.feature_history.extend(features.tolist())
            
            # è®°å½•å†å²
            self.prediction_history.extend(predictions.tolist())
            
            # åˆ›å»ºæŒ‡æ ‡å¯¹è±¡
            metrics = ModelMetrics(
                timestamp=start_time,
                prediction_count=len(predictions),
                prediction_mean=pred_mean,
                prediction_std=pred_std,
                prediction_range=(pred_min, pred_max),
                feature_drift_score=feature_drift,
                prediction_drift_score=prediction_drift,
                outlier_ratio=outlier_ratio,
                processing_time_ms=processing_time_ms
            )
            
            self.metrics_history.append(metrics)
            
            # ç”Ÿæˆå‘Šè­¦
            alerts.extend(self._check_drift_alerts(prediction_drift, feature_drift))
            alerts.extend(self._check_outlier_alerts(outlier_ratio))
            alerts.extend(self._check_performance_alerts(pred_std, processing_time_ms))
            alerts.extend(self._check_distribution_alerts(predictions))
            
            # è®°å½•å‘Šè­¦
            for alert in alerts:
                self.alerts.append(alert)
                self.counters['total_alerts'] += 1
            
            return alerts
            
        except Exception as e:
            self.counters['processing_errors'] += 1
            error_alert = Alert(
                timestamp=datetime.now(),
                level='ERROR',
                component='ModelMonitor',
                message=f"ç›‘æ§å¤„ç†å¤±è´¥: {e}",
                metrics={'error_count': self.counters['processing_errors']},
                suggested_action='æ£€æŸ¥ç›‘æ§ç³»ç»Ÿé…ç½®å’Œè¾“å…¥æ•°æ®'
            )
            return [error_alert]
    
    def _check_drift_alerts(self, prediction_drift: float, feature_drift: float) -> List[Alert]:
        """æ£€æŸ¥æ¼‚ç§»å‘Šè­¦"""
        alerts = []
        
        # é¢„æµ‹æ¼‚ç§»å‘Šè­¦
        if prediction_drift >= self.alert_thresholds['prediction_drift_critical']:
            alerts.append(Alert(
                timestamp=datetime.now(),
                level='CRITICAL',
                component='PredictionDrift',
                message=f'ä¸¥é‡é¢„æµ‹æ¼‚ç§»æ£€æµ‹: {prediction_drift:.3f}',
                metrics={'drift_score': prediction_drift},
                suggested_action='ç«‹å³é‡æ–°è®­ç»ƒæ¨¡å‹æˆ–æ£€æŸ¥æ•°æ®è´¨é‡'
            ))
            self.counters['drift_detections'] += 1
        elif prediction_drift >= self.alert_thresholds['prediction_drift_high']:
            alerts.append(Alert(
                timestamp=datetime.now(),
                level='WARNING',
                component='PredictionDrift',
                message=f'é¢„æµ‹æ¼‚ç§»è­¦å‘Š: {prediction_drift:.3f}',
                metrics={'drift_score': prediction_drift},
                suggested_action='è€ƒè™‘é‡æ–°è®­ç»ƒæ¨¡å‹'
            ))
        
        # ç‰¹å¾æ¼‚ç§»å‘Šè­¦
        if feature_drift >= self.alert_thresholds['feature_drift_critical']:
            alerts.append(Alert(
                timestamp=datetime.now(),
                level='CRITICAL',
                component='FeatureDrift',
                message=f'ä¸¥é‡ç‰¹å¾æ¼‚ç§»æ£€æµ‹: {feature_drift:.3f}',
                metrics={'drift_score': feature_drift},
                suggested_action='æ£€æŸ¥æ•°æ®æºå’Œç‰¹å¾å·¥ç¨‹æµç¨‹'
            ))
        elif feature_drift >= self.alert_thresholds['feature_drift_high']:
            alerts.append(Alert(
                timestamp=datetime.now(),
                level='WARNING',
                component='FeatureDrift',
                message=f'ç‰¹å¾æ¼‚ç§»è­¦å‘Š: {feature_drift:.3f}',
                metrics={'drift_score': feature_drift},
                suggested_action='ç›‘æ§ç‰¹å¾åˆ†å¸ƒå˜åŒ–'
            ))
        
        return alerts
    
    def _check_outlier_alerts(self, outlier_ratio: float) -> List[Alert]:
        """æ£€æŸ¥å¼‚å¸¸å€¼å‘Šè­¦"""
        alerts = []
        
        if outlier_ratio >= self.alert_thresholds['outlier_ratio_critical']:
            alerts.append(Alert(
                timestamp=datetime.now(),
                level='CRITICAL',
                component='OutlierDetection',
                message=f'ä¸¥é‡å¼‚å¸¸å€¼æ¯”ä¾‹: {outlier_ratio:.3f}',
                metrics={'outlier_ratio': outlier_ratio},
                suggested_action='æ£€æŸ¥æ•°æ®è´¨é‡å’Œé¢„å¤„ç†æµç¨‹'
            ))
            self.counters['outlier_detections'] += 1
        elif outlier_ratio >= self.alert_thresholds['outlier_ratio_high']:
            alerts.append(Alert(
                timestamp=datetime.now(),
                level='WARNING',
                component='OutlierDetection',
                message=f'å¼‚å¸¸å€¼æ¯”ä¾‹è¾ƒé«˜: {outlier_ratio:.3f}',
                metrics={'outlier_ratio': outlier_ratio},
                suggested_action='ç›‘æ§æ•°æ®å¼‚å¸¸'
            ))
        
        return alerts
    
    def _check_performance_alerts(self, pred_std: float, processing_time: float) -> List[Alert]:
        """æ£€æŸ¥æ€§èƒ½å‘Šè­¦"""
        alerts = []
        
        # å¤„ç†æ—¶é—´å‘Šè­¦
        if processing_time >= self.alert_thresholds['processing_time_critical']:
            alerts.append(Alert(
                timestamp=datetime.now(),
                level='CRITICAL',
                component='Performance',
                message=f'å¤„ç†æ—¶é—´è¿‡é•¿: {processing_time:.1f}ms',
                metrics={'processing_time_ms': processing_time},
                suggested_action='ä¼˜åŒ–æ¨¡å‹æˆ–å¢åŠ è®¡ç®—èµ„æº'
            ))
        elif processing_time >= self.alert_thresholds['processing_time_high']:
            alerts.append(Alert(
                timestamp=datetime.now(),
                level='WARNING',
                component='Performance',
                message=f'å¤„ç†æ—¶é—´è¾ƒé•¿: {processing_time:.1f}ms',
                metrics={'processing_time_ms': processing_time},
                suggested_action='ç›‘æ§ç³»ç»Ÿæ€§èƒ½'
            ))
        
        # é¢„æµ‹æ–¹å·®å‘Šè­¦
        if pred_std <= self.alert_thresholds['prediction_std_low']:
            alerts.append(Alert(
                timestamp=datetime.now(),
                level='WARNING',
                component='PredictionVariance',
                message=f'é¢„æµ‹æ–¹å·®è¿‡ä½: {pred_std:.4f}',
                metrics={'prediction_std': pred_std},
                suggested_action='æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¿‡åº¦å¹³æ»‘æˆ–ç¼ºä¹å¤šæ ·æ€§'
            ))
        elif pred_std >= self.alert_thresholds['prediction_std_high']:
            alerts.append(Alert(
                timestamp=datetime.now(),
                level='WARNING',
                component='PredictionVariance',
                message=f'é¢„æµ‹æ–¹å·®è¿‡é«˜: {pred_std:.4f}',
                metrics={'prediction_std': pred_std},
                suggested_action='æ£€æŸ¥æ¨¡å‹ç¨³å®šæ€§å’Œè¾“å…¥æ•°æ®è´¨é‡'
            ))
        
        return alerts
    
    def _check_distribution_alerts(self, predictions: np.ndarray) -> List[Alert]:
        """æ£€æŸ¥åˆ†å¸ƒå¼‚å¸¸å‘Šè­¦"""
        alerts = []
        
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–æ— ç©·å€¼
            nan_count = np.isnan(predictions).sum()
            inf_count = np.isinf(predictions).sum()
            
            if nan_count > 0:
                alerts.append(Alert(
                    timestamp=datetime.now(),
                    level='ERROR',
                    component='PredictionQuality',
                    message=f'é¢„æµ‹åŒ…å«NaNå€¼: {nan_count}ä¸ª',
                    metrics={'nan_count': nan_count},
                    suggested_action='æ£€æŸ¥æ¨¡å‹è¾“å…¥å’Œè®¡ç®—æµç¨‹'
                ))
            
            if inf_count > 0:
                alerts.append(Alert(
                    timestamp=datetime.now(),
                    level='ERROR',
                    component='PredictionQuality',
                    message=f'é¢„æµ‹åŒ…å«æ— ç©·å€¼: {inf_count}ä¸ª',
                    metrics={'inf_count': inf_count},
                    suggested_action='æ£€æŸ¥æ•°å€¼è®¡ç®—ç¨³å®šæ€§'
                ))
                
        except Exception as e:
            logger.warning(f"åˆ†å¸ƒæ£€æŸ¥å¤±è´¥: {e}")
        
        return alerts
    
    def get_recent_alerts(self, hours: int = 24, level: Optional[str] = None) -> List[Alert]:
        """è·å–æœ€è¿‘çš„å‘Šè­¦"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        recent_alerts = [
            alert for alert in self.alerts 
            if alert.timestamp >= cutoff_time
        ]
        
        if level:
            recent_alerts = [
                alert for alert in recent_alerts
                if alert.level == level
            ]
        
        return sorted(recent_alerts, key=lambda x: x.timestamp, reverse=True)
    
    def get_monitoring_summary(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§æ€»ç»“"""
        recent_metrics = list(self.metrics_history)[-100:] if self.metrics_history else []
        recent_alerts = self.get_recent_alerts(hours=24)
        
        summary = {
            'counters': self.counters.copy(),
            'recent_24h_alerts': len(recent_alerts),
            'alert_breakdown': {},
            'current_health': 'HEALTHY',
            'recommendations': []
        }
        
        # å‘Šè­¦åˆ†ç±»ç»Ÿè®¡
        for alert in recent_alerts:
            summary['alert_breakdown'][alert.level] = summary['alert_breakdown'].get(alert.level, 0) + 1
        
        # å¥åº·çŠ¶æ€è¯„ä¼°
        critical_alerts = summary['alert_breakdown'].get('CRITICAL', 0)
        warning_alerts = summary['alert_breakdown'].get('WARNING', 0)
        
        if critical_alerts > 0:
            summary['current_health'] = 'CRITICAL'
            summary['recommendations'].append('å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œéœ€è¦ç«‹å³å¤„ç†')
        elif warning_alerts > 5:
            summary['current_health'] = 'WARNING'
            summary['recommendations'].append('å­˜åœ¨å¤šä¸ªè­¦å‘Šï¼Œå»ºè®®æ£€æŸ¥ç³»ç»ŸçŠ¶æ€')
        elif warning_alerts > 0:
            summary['current_health'] = 'CAUTION'
            summary['recommendations'].append('å­˜åœ¨è­¦å‘Šï¼Œå»ºè®®æŒç»­ç›‘æ§')
        
        # æ€§èƒ½ç»Ÿè®¡
        if recent_metrics:
            summary['avg_processing_time'] = np.mean([m.processing_time_ms for m in recent_metrics])
            summary['avg_prediction_std'] = np.mean([m.prediction_std for m in recent_metrics])
            summary['avg_drift_score'] = np.mean([m.prediction_drift_score for m in recent_metrics])
        
        return summary
    
    def export_monitoring_data(self, output_file: str):
        """å¯¼å‡ºç›‘æ§æ•°æ®"""
        data = {
            'metrics_history': [
                {
                    'timestamp': m.timestamp.isoformat(),
                    'prediction_count': m.prediction_count,
                    'prediction_mean': m.prediction_mean,
                    'prediction_std': m.prediction_std,
                    'feature_drift_score': m.feature_drift_score,
                    'prediction_drift_score': m.prediction_drift_score,
                    'outlier_ratio': m.outlier_ratio,
                    'processing_time_ms': m.processing_time_ms
                }
                for m in self.metrics_history
            ],
            'alerts_history': [
                {
                    'timestamp': a.timestamp.isoformat(),
                    'level': a.level,
                    'component': a.component,
                    'message': a.message,
                    'metrics': a.metrics,
                    'suggested_action': a.suggested_action
                }
                for a in self.alerts
            ],
            'counters': self.counters,
            'export_timestamp': datetime.now().isoformat()
        }
        
        import json
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ç›‘æ§æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")


def example_usage():
    """ç¤ºä¾‹ç”¨æ³•"""
    print("ğŸ” æ¨¡å‹ç›‘æ§ç³»ç»Ÿç¤ºä¾‹")
    
    # åˆå§‹åŒ–ç›‘æ§ç³»ç»Ÿ
    monitor = ModelMonitor(window_size=500)
    
    # è®¾ç½®åŸºçº¿ï¼ˆæ¨¡æ‹Ÿå†å²æ•°æ®ï¼‰
    baseline_predictions = np.random.normal(0, 0.1, 1000)
    baseline_features = np.random.randn(1000, 10)
    
    monitor.set_baseline(baseline_predictions, baseline_features)
    
    # æ¨¡æ‹Ÿæ­£å¸¸é¢„æµ‹
    print("\nğŸ“ˆ ç›‘æ§æ­£å¸¸é¢„æµ‹...")
    normal_predictions = np.random.normal(0.02, 0.12, 100)  # è½»å¾®æ¼‚ç§»
    normal_features = np.random.randn(100, 10) + 0.1        # è½»å¾®ç‰¹å¾æ¼‚ç§»
    
    alerts = monitor.monitor_predictions(
        predictions=normal_predictions,
        features=normal_features,
        processing_time_ms=50
    )
    
    print(f"æ­£å¸¸é¢„æµ‹å‘Šè­¦æ•°: {len(alerts)}")
    for alert in alerts:
        print(f"  {alert.level}: {alert.message}")
    
    # æ¨¡æ‹Ÿå¼‚å¸¸é¢„æµ‹
    print("\nğŸš¨ ç›‘æ§å¼‚å¸¸é¢„æµ‹...")
    abnormal_predictions = np.random.normal(0.5, 0.3, 100)  # ä¸¥é‡æ¼‚ç§»
    abnormal_features = np.random.randn(100, 10) + 2        # ä¸¥é‡ç‰¹å¾æ¼‚ç§»
    
    alerts = monitor.monitor_predictions(
        predictions=abnormal_predictions,
        features=abnormal_features,
        processing_time_ms=8000  # å¤„ç†æ—¶é—´è¿‡é•¿
    )
    
    print(f"å¼‚å¸¸é¢„æµ‹å‘Šè­¦æ•°: {len(alerts)}")
    for alert in alerts:
        print(f"  {alert.level}: {alert.message}")
    
    # è·å–ç›‘æ§æ€»ç»“
    print("\nğŸ“Š ç›‘æ§æ€»ç»“:")
    summary = monitor.get_monitoring_summary()
    print(f"  ç³»ç»Ÿå¥åº·çŠ¶æ€: {summary['current_health']}")
    print(f"  24å°æ—¶å‘Šè­¦æ•°: {summary['recent_24h_alerts']}")
    print(f"  æ€»é¢„æµ‹æ•°: {summary['counters']['total_predictions']}")
    
    return monitor


if __name__ == "__main__":
    example_usage()
