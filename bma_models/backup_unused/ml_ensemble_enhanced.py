#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MLé›†æˆå­¦ä¹ å¢å¼ºæ¨¡å— - é«˜çº§æ¨¡å‹é›†æˆç³»ç»Ÿ
åŒ…å«Votingã€Stackingã€Boostingå’ŒåŠ¨æ€BMAæƒé‡å­¦ä¹ 

Features:
- Voting/Stackingé›†æˆ
- AdaBoost/GradientBoosting
- BMAæƒé‡MLåŠ¨æ€å­¦ä¹ 
- å¤šå±‚Stacking
- æ¨¡å‹å¤šæ ·æ€§åˆ†æ
- åŠ¨æ€æƒé‡è°ƒæ•´
"""

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from sklearn.base import BaseEstimator, RegressorMixin, clone
from sklearn.ensemble import (
    VotingRegressor, StackingRegressor,
    AdaBoostRegressor, GradientBoostingRegressor,
    RandomForestRegressor, ExtraTreesRegressor,
    BaggingRegressor
)
from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression
from sklearn.model_selection import cross_val_predict, TimeSeriesSplit, KFold
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from scipy.stats import spearmanr
import warnings
warnings.filterwarnings('ignore')

# æ¡ä»¶å¯¼å…¥
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class EnsembleConfig:
    """é›†æˆå­¦ä¹ é…ç½®"""
    # é›†æˆæ–¹æ³•
    ensemble_methods: List[str] = field(default_factory=lambda: [
        'voting', 'stacking', 'boosting', 'dynamic_bma'
    ])
    
    # Votingé…ç½®
    voting_type: str = 'soft'  # 'hard' or 'soft'
    voting_weights: Optional[List[float]] = None  # è‡ªåŠ¨å­¦ä¹ å¦‚æœä¸ºNone
    
    # Stackingé…ç½®
    stacking_meta_model: str = 'Ridge'  # å…ƒå­¦ä¹ å™¨
    stacking_cv_folds: int = 5
    stacking_use_probas: bool = False
    stacking_passthrough: bool = True  # æ˜¯å¦å°†åŸå§‹ç‰¹å¾ä¼ é€’ç»™å…ƒå­¦ä¹ å™¨
    multi_level_stacking: bool = True  # å¤šå±‚stacking
    
    # Boostingé…ç½®
    boosting_n_estimators: int = 100
    boosting_learning_rate: float = 0.1
    boosting_loss: str = 'square'  # 'linear', 'square', 'exponential'
    
    # BMAé…ç½®
    bma_learning_rate: float = 0.01
    bma_momentum: float = 0.9
    bma_weight_decay: float = 0.001
    bma_update_frequency: int = 10  # æ¯Nä¸ªé¢„æµ‹æ›´æ–°ä¸€æ¬¡æƒé‡
    
    # æ¨¡å‹æ± é…ç½® - ä¼˜åŒ–ä¸ºäº’è¡¥æ€§ä¸‰ç±»æ¨¡å‹
    base_models: List[str] = field(default_factory=lambda: [
        'ElasticNet', 'LightGBM', 'ExtraTrees'  # çº¿æ€§æ”¶ç¼© + æ¢¯åº¦æå‡ + è¢‹è£…æ ‘
    ])
    
    # å¤šæ ·æ€§é…ç½®
    diversity_threshold: float = 0.7  # ç›¸å…³æ€§é˜ˆå€¼
    min_model_performance: float = 0.3  # æœ€ä½æ¨¡å‹æ€§èƒ½è¦æ±‚
    
    # äº¤å‰éªŒè¯
    cv_strategy: str = 'time_series'  # 'time_series' or 'kfold'
    n_splits: int = 5
    
    # æ€§èƒ½é…ç½®
    n_jobs: int = -1
    random_state: int = 42
    verbose: int = 0


class DynamicBMAWeightLearner(BaseEstimator, RegressorMixin):
    """åŠ¨æ€BMAæƒé‡å­¦ä¹ å™¨ - ä½¿ç”¨MLå­¦ä¹ æœ€ä¼˜æƒé‡"""
    
    def __init__(self, base_models: List, learning_rate: float = 0.01, 
                 momentum: float = 0.9, weight_decay: float = 0.001):
        self.base_models = base_models
        self.learning_rate = learning_rate
        self.momentum = momentum
        self.weight_decay = weight_decay
        self.weights_ = None
        self.weight_history_ = []
        self.velocity_ = None
        self.logger = logging.getLogger(self.__class__.__name__)
        
    def fit(self, X, y):
        """è®­ç»ƒBMAæƒé‡"""
        n_models = len(self.base_models)
        
        # åˆå§‹åŒ–æƒé‡ï¼ˆå‡ç­‰æƒé‡ï¼‰
        self.weights_ = np.ones(n_models) / n_models
        self.velocity_ = np.zeros(n_models)
        
        # è·å–æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹
        predictions = self._get_base_predictions(X, y)
        
        # ä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æƒé‡
        n_iterations = 100
        for iteration in range(n_iterations):
            # è®¡ç®—åŠ æƒé¢„æµ‹
            weighted_pred = np.sum(predictions * self.weights_[:, np.newaxis], axis=0)
            
            # è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
            loss = np.mean((weighted_pred - y) ** 2)
            gradients = self._compute_gradients(predictions, y, weighted_pred)
            
            # å¸¦åŠ¨é‡çš„æ¢¯åº¦ä¸‹é™
            self.velocity_ = self.momentum * self.velocity_ - self.learning_rate * gradients
            self.weights_ += self.velocity_
            
            # L2æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰
            self.weights_ -= self.weight_decay * self.weights_
            
            # ç¡®ä¿æƒé‡éè´Ÿä¸”å’Œä¸º1
            self.weights_ = np.maximum(self.weights_, 0)
            self.weights_ /= (np.sum(self.weights_) + 1e-8)
            
            # è®°å½•æƒé‡å†å²
            self.weight_history_.append(self.weights_.copy())
            
            if iteration % 20 == 0:
                self.logger.debug(f"è¿­ä»£ {iteration}, æŸå¤±: {loss:.6f}")
        
        # è®­ç»ƒåŸºæ¨¡å‹
        for model in self.base_models:
            model.fit(X, y)
        
        self.logger.info(f"BMAæƒé‡å­¦ä¹ å®Œæˆ: {self.weights_}")
        
        return self
    
    def predict(self, X):
        """ä½¿ç”¨å­¦ä¹ çš„æƒé‡è¿›è¡Œé¢„æµ‹"""
        predictions = np.array([model.predict(X) for model in self.base_models])
        return np.sum(predictions * self.weights_[:, np.newaxis], axis=0)
    
    def _get_base_predictions(self, X, y):
        """è·å–åŸºæ¨¡å‹çš„äº¤å‰éªŒè¯é¢„æµ‹"""
        # ğŸš¨ ç¦æ­¢å†…éƒ¨åˆ›å»ºCVï¼Œå¿…é¡»ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„ç»Ÿä¸€CV
        if not hasattr(self, 'cv_factory') or self.cv_factory is None:
            raise NotImplementedError(
                "è¿åSSOTåŸåˆ™ï¼šé›†æˆç³»ç»Ÿä¸å…è®¸å†…éƒ¨åˆ›å»ºCVï¼\n"
                "ä¿®å¤æŒ‡å—ï¼šå¿…é¡»é€šè¿‡å¤–éƒ¨ä¼ å…¥cv_factoryï¼Œä½¿ç”¨UnifiedCVFactoryã€‚\n"
                "ç¦æ­¢TimeSeriesSplitç­‰å†…éƒ¨CVåˆ›å»ºã€‚"
            )
        
        cv = self.cv_factory
        predictions = []
        
        for model in self.base_models:
            model_clone = clone(model)
            pred = cross_val_predict(model_clone, X, y, cv=cv)
            predictions.append(pred)
        
        return np.array(predictions)
    
    def _compute_gradients(self, predictions, y, weighted_pred):
        """è®¡ç®—æƒé‡çš„æ¢¯åº¦"""
        error = weighted_pred - y
        gradients = np.array([
            2 * np.mean(error * pred) for pred in predictions
        ])
        return gradients
    
    def update_weights(self, X, y):
        """åœ¨çº¿æ›´æ–°æƒé‡"""
        predictions = np.array([model.predict(X) for model in self.base_models])
        weighted_pred = np.sum(predictions * self.weights_[:, np.newaxis], axis=0)
        
        # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°
        gradients = self._compute_gradients(predictions, y, weighted_pred)
        self.velocity_ = self.momentum * self.velocity_ - self.learning_rate * gradients
        self.weights_ += self.velocity_
        
        # æ­£åˆ™åŒ–å’Œå½’ä¸€åŒ–
        self.weights_ -= self.weight_decay * self.weights_
        self.weights_ = np.maximum(self.weights_, 0)
        self.weights_ /= (np.sum(self.weights_) + 1e-8)
        
        self.weight_history_.append(self.weights_.copy())


class MLEnsembleEnhanced:
    """å¢å¼ºçš„é›†æˆå­¦ä¹ ç³»ç»Ÿ"""
    
    def __init__(self, config: EnsembleConfig = None):
        self.config = config or EnsembleConfig()
        self.ensemble_models = {}
        self.model_performances = {}
        self.diversity_matrix = None
        self.logger = logging.getLogger(self.__class__.__name__)
        
    def build_ensemble(self, X: np.ndarray, y: np.ndarray,
                       cv_factory: callable, 
                       feature_names: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        æ„å»ºå®Œæ•´çš„é›†æˆå­¦ä¹ ç³»ç»Ÿ
        
        Args:
            X: ç‰¹å¾æ•°æ®
            y: ç›®æ ‡å˜é‡
            cv_factory: ç»Ÿä¸€CVå·¥å‚ï¼ˆå¿…é¡»ï¼‰
            feature_names: ç‰¹å¾åç§°
            
        Returns:
            é›†æˆæ¨¡å‹å­—å…¸å’Œæ€§èƒ½æŒ‡æ ‡
        """
        # ä¿å­˜CVå·¥å‚
        self.cv_factory = cv_factory
        self.logger.info("å¼€å§‹æ„å»ºé›†æˆå­¦ä¹ ç³»ç»Ÿ")
        
        # 1. åˆ›å»ºåŸºç¡€æ¨¡å‹æ± 
        base_models = self._create_base_models()
        
        # 2. è¯„ä¼°æ¨¡å‹å¤šæ ·æ€§
        self.diversity_matrix = self._evaluate_model_diversity(base_models, X, y)
        
        # 3. é€‰æ‹©å¤šæ ·åŒ–çš„æ¨¡å‹å­é›†
        selected_models = self._select_diverse_models(base_models, self.diversity_matrix)
        
        # 4. æ„å»ºå„ç§é›†æˆ
        results = {}
        
        if 'voting' in self.config.ensemble_methods:
            results['voting'] = self._build_voting_ensemble(selected_models, X, y)
        
        if 'stacking' in self.config.ensemble_methods:
            if self.config.multi_level_stacking:
                results['stacking'] = self._build_multi_level_stacking(selected_models, X, y)
            else:
                results['stacking'] = self._build_stacking_ensemble(selected_models, X, y)
        
        if 'boosting' in self.config.ensemble_methods:
            results['boosting'] = self._build_boosting_ensemble(X, y)
        
        if 'dynamic_bma' in self.config.ensemble_methods:
            results['dynamic_bma'] = self._build_dynamic_bma(selected_models, X, y)
        
        # 5. è¯„ä¼°å„é›†æˆæ–¹æ³•æ€§èƒ½
        performances = self._evaluate_ensembles(results, X, y)
        
        # 6. é€‰æ‹©æœ€ä¼˜é›†æˆ
        best_method = max(performances.keys(), key=lambda k: performances[k]['r2_score'])
        
        self.logger.info(f"æœ€ä¼˜é›†æˆæ–¹æ³•: {best_method}, R2: {performances[best_method]['r2_score']:.6f}")
        
        return {
            'models': results,
            'performances': performances,
            'best_method': best_method,
            'best_model': results[best_method],
            'diversity_matrix': self.diversity_matrix
        }
    
    def _create_base_models(self) -> List:
        """åˆ›å»ºä¼˜åŒ–çš„äº’è¡¥æ€§ä¸‰ç±»æ¨¡å‹æ± """
        models = []
        
        for model_name in self.config.base_models:
            if model_name == 'ElasticNet':
                # çº¿æ€§æ”¶ç¼©æ¨¡å‹ - é‡‘èå¼±ä¿¡å·ç¨³å¥ï¼ŒL1_ratioç½‘æ ¼æ¶µç›–LASSOåˆ°Ridge
                models.append(ElasticNet(
                    alpha=0.1, 
                    l1_ratio=0.5,  # å¹³è¡¡L1/L2
                    random_state=self.config.random_state,
                    max_iter=2000
                ))
            elif model_name == 'LightGBM' and LIGHTGBM_AVAILABLE:
                # LGBM: æµ…+å¼ºæ­£åˆ™+å­é‡‡æ · - æ§åˆ¶è¿‡æ‹Ÿåˆä¼˜å…ˆ
                models.append(lgb.LGBMRegressor(
                    n_estimators=80,         # å‡å°‘æ ‘æ•°é‡
                    max_depth=4,             # æµ…æ ‘
                    learning_rate=0.05,      # ä¿å®ˆå­¦ä¹ ç‡
                    num_leaves=20,           # å°‘å¶å­èŠ‚ç‚¹
                    feature_fraction=0.7,    # å¼ºç‰¹å¾å­é‡‡æ ·
                    bagging_fraction=0.7,    # å¼ºæ ·æœ¬å­é‡‡æ ·
                    lambda_l1=0.3,           # å¼ºL1æ­£åˆ™
                    lambda_l2=0.3,           # å¼ºL2æ­£åˆ™
                    min_child_samples=30,    # å¤§æœ€å°å¶èŠ‚ç‚¹æ ·æœ¬
                    random_state=self.config.random_state,
                    verbosity=-1
                ))
            elif model_name == 'ExtraTrees':
                # ET: æ·±+é«˜éšæœº - å¤šæ ·æ€§ä¼˜å…ˆ
                models.append(ExtraTreesRegressor(
                    n_estimators=120,        # æ›´å¤šæ ‘
                    max_depth=12,            # æ·±æ ‘
                    min_samples_split=2,     # æ¿€è¿›åˆ†è£‚
                    min_samples_leaf=2,      # å°å¶èŠ‚ç‚¹
                    max_features=0.5,        # é«˜ç‰¹å¾éšæœºæ€§
                    bootstrap=False,         # ETé»˜è®¤ä¸bootstrap
                    random_state=self.config.random_state
                ))
            elif model_name == 'XGBoost' and XGBOOST_AVAILABLE:
                # å¯é€‰å¤‡èƒ - å·®å¼‚åŒ–è¶…å‚å¢åŠ å¤šæ ·æ€§
                models.append(xgb.XGBRegressor(
                    n_estimators=80,  # ä¸LGBMä¸åŒ
                    max_depth=4,      # ç¨æµ…
                    learning_rate=0.08,  # ä¸åŒå­¦ä¹ ç‡
                    subsample=0.85,   # ä¸åŒé‡‡æ ·æ¯”ä¾‹
                    colsample_bytree=0.75,
                    random_state=self.config.random_state
                ))
        
        return models
    
    def _evaluate_model_diversity(self, models: List, X: np.ndarray, y: np.ndarray) -> np.ndarray:
        """è¯„ä¼°æ¨¡å‹å¤šæ ·æ€§ï¼ˆé€šè¿‡é¢„æµ‹ç›¸å…³æ€§ï¼‰"""
        cv = self._get_cv_strategy(X)
        predictions = []
        
        for i, model in enumerate(models):
            try:
                model_clone = clone(model)
                pred = cross_val_predict(model_clone, X, y, cv=cv)
                predictions.append(pred)
            except Exception as e:
                self.logger.warning(f"æ¨¡å‹ {i} é¢„æµ‹å¤±è´¥: {e}")
                predictions.append(np.zeros_like(y))
        
        # è®¡ç®—é¢„æµ‹ä¹‹é—´çš„ç›¸å…³çŸ©é˜µ
        n_models = len(predictions)
        diversity_matrix = np.zeros((n_models, n_models))
        
        for i in range(n_models):
            for j in range(n_models):
                if i != j:
                    corr, _ = spearmanr(predictions[i], predictions[j])
                    diversity_matrix[i, j] = 1 - abs(corr)  # å¤šæ ·æ€§ = 1 - ç›¸å…³æ€§
                else:
                    diversity_matrix[i, j] = 0
        
        return diversity_matrix
    
    def _select_diverse_models(self, models: List, diversity_matrix: np.ndarray) -> List:
        """é€‰æ‹©å¤šæ ·åŒ–çš„æ¨¡å‹å­é›†"""
        n_models = len(models)
        selected_indices = []
        
        # è´ªå©ªé€‰æ‹©ï¼šæ¯æ¬¡é€‰æ‹©èƒ½æœ€å¤§åŒ–æ€»ä½“å¤šæ ·æ€§çš„æ¨¡å‹
        for _ in range(min(n_models, 5)):  # æœ€å¤šé€‰æ‹©5ä¸ªæ¨¡å‹
            best_diversity = -1
            best_idx = -1
            
            for i in range(n_models):
                if i not in selected_indices:
                    if not selected_indices:
                        # ç¬¬ä¸€ä¸ªæ¨¡å‹éšæœºé€‰æ‹©
                        diversity_score = np.random.random()
                    else:
                        # è®¡ç®—ä¸å·²é€‰æ¨¡å‹çš„å¹³å‡å¤šæ ·æ€§
                        diversity_score = np.mean([diversity_matrix[i, j] for j in selected_indices])
                    
                    if diversity_score > best_diversity:
                        best_diversity = diversity_score
                        best_idx = i
            
            if best_idx >= 0:
                selected_indices.append(best_idx)
        
        selected_models = [models[i] for i in selected_indices]
        
        self.logger.info(f"é€‰æ‹©äº† {len(selected_models)} ä¸ªå¤šæ ·åŒ–æ¨¡å‹")
        
        return selected_models
    
    def _build_voting_ensemble(self, models: List, X: np.ndarray, y: np.ndarray):
        """æ„å»ºæŠ•ç¥¨é›†æˆ"""
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæƒé‡ï¼Œå­¦ä¹ æœ€ä¼˜æƒé‡
        if self.config.voting_weights is None:
            weights = self._learn_voting_weights(models, X, y)
        else:
            weights = self.config.voting_weights
        
        # åˆ›å»ºæŠ•ç¥¨é›†æˆ
        estimators = [(f'model_{i}', model) for i, model in enumerate(models)]
        
        voting = VotingRegressor(
            estimators=estimators,
            weights=weights,
            n_jobs=self.config.n_jobs
        )
        
        voting.fit(X, y)
        
        self.logger.info(f"æŠ•ç¥¨é›†æˆæ„å»ºå®Œæˆï¼Œæƒé‡: {weights}")
        
        return voting
    
    def _build_stacking_ensemble(self, models: List, X: np.ndarray, y: np.ndarray):
        """æ„å»ºStackingé›†æˆ"""
        # è·å–å…ƒå­¦ä¹ å™¨
        meta_model = self._get_meta_learner()
        
        # åˆ›å»ºStackingé›†æˆ
        estimators = [(f'model_{i}', model) for i, model in enumerate(models)]
        
        stacking = StackingRegressor(
            estimators=estimators,
            final_estimator=meta_model,
            cv=self._get_cv_strategy(X),
            n_jobs=self.config.n_jobs,
            passthrough=self.config.stacking_passthrough
        )
        
        stacking.fit(X, y)
        
        self.logger.info("Stackingé›†æˆæ„å»ºå®Œæˆ")
        
        return stacking
    
    def _build_multi_level_stacking(self, models: List, X: np.ndarray, y: np.ndarray):
        """æ„å»ºå¤šå±‚Stacking"""
        # ç¬¬ä¸€å±‚ï¼šåŸå§‹æ¨¡å‹
        level1_models = models[:3] if len(models) > 3 else models
        
        # ç¬¬äºŒå±‚ï¼šä½¿ç”¨ç¬¬ä¸€å±‚çš„é¢„æµ‹ä½œä¸ºç‰¹å¾
        level1_estimators = [(f'l1_model_{i}', model) for i, model in enumerate(level1_models)]
        
        level2_model = StackingRegressor(
            estimators=level1_estimators,
            final_estimator=Ridge(alpha=1.0),
            cv=self._get_cv_strategy(X),
            n_jobs=self.config.n_jobs
        )
        
        # ç¬¬ä¸‰å±‚ï¼šæœ€ç»ˆå…ƒå­¦ä¹ å™¨
        remaining_models = models[3:] if len(models) > 3 else []
        if remaining_models:
            level2_estimators = [('level2', level2_model)] + \
                              [(f'extra_{i}', m) for i, m in enumerate(remaining_models)]
        else:
            level2_estimators = [('level2', level2_model)]
        
        final_stacking = StackingRegressor(
            estimators=level2_estimators,
            final_estimator=self._get_meta_learner(),
            cv=self._get_cv_strategy(X),
            n_jobs=self.config.n_jobs,
            passthrough=self.config.stacking_passthrough
        )
        
        final_stacking.fit(X, y)
        
        self.logger.info("å¤šå±‚Stackingé›†æˆæ„å»ºå®Œæˆ")
        
        return final_stacking
    
    def _build_boosting_ensemble(self, X: np.ndarray, y: np.ndarray):
        """æ„å»ºBoostingé›†æˆ"""
        # ä½¿ç”¨AdaBoostå’ŒGradientBoostingçš„ç»„åˆ
        ada_boost = AdaBoostRegressor(
            n_estimators=self.config.boosting_n_estimators,
            learning_rate=self.config.boosting_learning_rate,
            loss=self.config.boosting_loss,
            random_state=self.config.random_state
        )
        
        gradient_boost = GradientBoostingRegressor(
            n_estimators=self.config.boosting_n_estimators,
            learning_rate=self.config.boosting_learning_rate,
            max_depth=5,
            random_state=self.config.random_state
        )
        
        # åˆ›å»ºBoostingé›†æˆçš„é›†æˆ
        boosting_ensemble = VotingRegressor(
            estimators=[
                ('ada', ada_boost),
                ('gradient', gradient_boost)
            ],
            n_jobs=self.config.n_jobs
        )
        
        boosting_ensemble.fit(X, y)
        
        self.logger.info("Boostingé›†æˆæ„å»ºå®Œæˆ")
        
        return boosting_ensemble
    
    def _build_dynamic_bma(self, models: List, X: np.ndarray, y: np.ndarray):
        """æ„å»ºåŠ¨æ€BMAæƒé‡å­¦ä¹ """
        bma_learner = DynamicBMAWeightLearner(
            base_models=models,
            learning_rate=self.config.bma_learning_rate,
            momentum=self.config.bma_momentum,
            weight_decay=self.config.bma_weight_decay
        )
        
        bma_learner.fit(X, y)
        
        self.logger.info(f"åŠ¨æ€BMAæ„å»ºå®Œæˆï¼Œå­¦ä¹ çš„æƒé‡: {bma_learner.weights_}")
        
        return bma_learner
    
    def _learn_voting_weights(self, models: List, X: np.ndarray, y: np.ndarray) -> List[float]:
        """å­¦ä¹ æŠ•ç¥¨æƒé‡"""
        cv = self._get_cv_strategy(X)
        model_scores = []
        
        for model in models:
            try:
                model_clone = clone(model)
                scores = []
                
                for train_idx, val_idx in cv.split(X):
                    X_train, X_val = X[train_idx], X[val_idx]
                    y_train, y_val = y[train_idx], y[val_idx]
                    
                    model_clone.fit(X_train, y_train)
                    pred = model_clone.predict(X_val)
                    score = r2_score(y_val, pred)
                    scores.append(max(0, score))  # ç¡®ä¿éè´Ÿ
                
                model_scores.append(np.mean(scores))
                
            except Exception as e:
                self.logger.warning(f"æ¨¡å‹è¯„åˆ†å¤±è´¥: {e}")
                model_scores.append(0.1)  # ç»™äºˆæœ€å°æƒé‡
        
        # è½¬æ¢åˆ†æ•°ä¸ºæƒé‡
        model_scores = np.array(model_scores)
        if model_scores.sum() > 0:
            weights = model_scores / model_scores.sum()
        else:
            weights = np.ones(len(models)) / len(models)
        
        return weights.tolist()
    
    def _get_meta_learner(self):
        """è·å–å…ƒå­¦ä¹ å™¨"""
        meta_learners = {
            'Ridge': Ridge(alpha=1.0),
            'Lasso': Lasso(alpha=0.1),
            'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5),
            'Linear': LinearRegression()
        }
        
        return meta_learners.get(self.config.stacking_meta_model, Ridge(alpha=1.0))
    
    def _get_cv_strategy(self, X: np.ndarray):
        """è·å–äº¤å‰éªŒè¯ç­–ç•¥ - å¼ºåˆ¶ä½¿ç”¨å¤–éƒ¨CVå·¥å‚"""
        # ğŸš¨ ç¦æ­¢å†…éƒ¨åˆ›å»ºCVï¼Œå¿…é¡»ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„ç»Ÿä¸€CV
        if not hasattr(self, 'cv_factory') or self.cv_factory is None:
            raise NotImplementedError(
                "è¿åSSOTåŸåˆ™ï¼šé›†æˆç³»ç»Ÿä¸å…è®¸å†…éƒ¨åˆ›å»ºCVï¼\n"
                "ä¿®å¤æŒ‡å—ï¼šå¿…é¡»é€šè¿‡å¤–éƒ¨ä¼ å…¥cv_factoryï¼Œä½¿ç”¨UnifiedCVFactoryã€‚\n"
                "ç¦æ­¢TimeSeriesSplitç­‰å†…éƒ¨CVåˆ›å»ºã€‚"
            )
        return self.cv_factory
    
    def _evaluate_ensembles(self, models: Dict, X: np.ndarray, y: np.ndarray) -> Dict:
        """è¯„ä¼°æ‰€æœ‰é›†æˆæ–¹æ³•"""
        performances = {}
        cv = self._get_cv_strategy(X)
        
        for name, model in models.items():
            try:
                # äº¤å‰éªŒè¯è¯„ä¼°
                predictions = cross_val_predict(model, X, y, cv=cv)
                
                mse = mean_squared_error(y, predictions)
                r2 = r2_score(y, predictions)
                
                performances[name] = {
                    'mse': mse,
                    'rmse': np.sqrt(mse),
                    'r2_score': r2
                }
                
                self.logger.info(f"{name} - MSE: {mse:.6f}, R2: {r2:.6f}")
                
            except Exception as e:
                self.logger.warning(f"è¯„ä¼° {name} å¤±è´¥: {e}")
                performances[name] = {
                    'mse': float('inf'),
                    'rmse': float('inf'),
                    'r2_score': -float('inf')
                }
        
        return performances
    
    def predict_with_uncertainty(self, models: Dict, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ä½¿ç”¨é›†æˆé¢„æµ‹å¹¶ä¼°è®¡ä¸ç¡®å®šæ€§"""
        predictions = []
        
        for name, model in models.items():
            try:
                pred = model.predict(X)
                predictions.append(pred)
            except Exception as e:
                self.logger.warning(f"æ¨¡å‹ {name} é¢„æµ‹å¤±è´¥: {e}")
        
        if predictions:
            predictions = np.array(predictions)
            mean_pred = np.mean(predictions, axis=0)
            std_pred = np.std(predictions, axis=0)
        else:
            mean_pred = np.zeros(len(X))
            std_pred = np.ones(len(X))
        
        return mean_pred, std_pred


# ä¾¿æ·å‡½æ•°
def build_advanced_ensemble(X: np.ndarray, y: np.ndarray, 
                          config: Optional[EnsembleConfig] = None) -> Dict[str, Any]:
    """
    æ„å»ºé«˜çº§é›†æˆå­¦ä¹ ç³»ç»Ÿä¾¿æ·å‡½æ•°
    
    Args:
        X: ç‰¹å¾æ•°æ®
        y: ç›®æ ‡å˜é‡
        config: é…ç½®ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        é›†æˆæ¨¡å‹å’Œæ€§èƒ½æŒ‡æ ‡
    """
    ensemble_builder = MLEnsembleEnhanced(config)
    return ensemble_builder.build_ensemble(X, y)