#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Rank-aware Blending - æ™ºèƒ½èåˆRidgeå’ŒLambdaRanké¢„æµ‹

æ ¸å¿ƒæ€è·¯ï¼š
- Ridgeå›å½’ï¼šè¿ç»­é¢„æµ‹ï¼Œä¿ç•™åˆ»åº¦ä¿¡æ¯
- LambdaRankï¼šæ’åºä¼˜åŒ–ï¼Œæå‡Top-Kæ€§èƒ½
- è‡ªé€‚åº”æƒé‡ï¼šåŸºäºå†å²RankICå’ŒNDCGåŠ¨æ€è°ƒæ•´

èåˆç­–ç•¥ï¼š
1. æŒ‰äº¤æ˜“æ—¥ç»„å†…æ ‡å‡†åŒ–
2. å†å²60dçª—å£è®¡ç®—æ€§èƒ½æŒ‡æ ‡
3. è‡ªé€‚åº”æƒé‡ï¼šwR âˆ RankIC@K, wL âˆ NDCG@K
4. Copulaæ­£æ€åŒ–å¢å¼ºé²æ£’æ€§
5. èåˆåˆ†æ•°ï¼šs* = wRÂ·zR + wLÂ·zL
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, Any, Tuple, Optional
from scipy import stats
from scipy.special import ndtr, ndtri  # æ ‡å‡†æ­£æ€åˆ†å¸ƒCDFå’Œé€†CDF
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class RankGateConfig:
    """
    é—¨æ§å¢ç›Šé…ç½®ç±»

    å®ç°LTRä¸“æ³¨æ’åé—¨æ§ï¼ŒRidgeä¸“æ³¨å¹…åº¦åˆ»åº¦çš„åˆ†ç¦»è®¾è®¡
    """

    def __init__(self,
                 tau_long: float = 0.7,          # é•¿å‡†å…¥é˜ˆå€¼
                 tau_short: float = 0.3,         # çŸ­å‡†å…¥é˜ˆå€¼
                 alpha_long: float = 0.2,        # é•¿ä¾§å¢ç›Šç³»æ•°
                 alpha_short: float = 0.2,       # çŸ­ä¾§å¢ç›Šç³»æ•°
                 min_coverage: float = 0.3,      # æœ€å°è¦†ç›–ç‡å…œåº•
                 neutral_band: bool = True,      # æ˜¯å¦å¯ç”¨ä¸­æ€§å¸¦ç½®é›¶
                 top_k_list: list = None,        # Top-Kç›‘æ§åˆ—è¡¨
                 ewma_beta: float = 0.1,         # EWMAå¹³æ»‘ç³»æ•°
                 max_gain: float = 1.3):         # æœ€å¤§å¢ç›Šä¸Šé™

        self.tau_long = tau_long
        self.tau_short = tau_short
        self.alpha_long = alpha_long
        self.alpha_short = alpha_short
        self.min_coverage = min_coverage
        self.neutral_band = neutral_band
        # ä¼˜åŒ–Kå€¼è®¾ç½®ï¼šé€‚åº”2600åªè‚¡ç¥¨çš„æŠ•èµ„å®‡å®™
        # åˆ†å±‚ç­–ç•¥ï¼šç²¾é€‰(5,10,20) + æŠ•èµ„ç»„åˆ(50,100) + é£é™©åˆ†æ•£(200)
        self.top_k_list = top_k_list or [5, 10, 20, 50, 100, 200]
        self.ewma_beta = ewma_beta
        self.max_gain = max_gain

        # è¿è¡Œæ—¶çŠ¶æ€
        self.coverage_history = []
        self.gain_stats_history = []

        logger.info(f"ğŸšª é—¨æ§é…ç½®åˆå§‹åŒ–: é•¿å‡†å…¥â‰¥{tau_long}, çŸ­å‡†å…¥â‰¤{tau_short}")
        logger.info(f"   å¢ç›Šç³»æ•°: Î±_long={alpha_long}, Î±_short={alpha_short}")
        logger.info(f"   æœ€å°è¦†ç›–: {min_coverage}, ä¸­æ€§å¸¦: {neutral_band}")

class RankAwareBlender:
    """
    Rank-awareæ™ºèƒ½èåˆå™¨

    æ ¸å¿ƒä¼˜åŠ¿ï¼š
    - è‡ªé€‚åº”æƒé‡ï¼Œæ ¹æ®å†å²è¡¨ç°åŠ¨æ€è°ƒæ•´
    - Copulaæ­£æ€åŒ–ï¼Œå¯¹é‡å°¾åˆ†å¸ƒæ›´é²æ£’
    - æŒ‰äº¤æ˜“æ—¥ç»„å†…å¤„ç†ï¼Œç¬¦åˆå®é™…äº¤æ˜“åœºæ™¯
    - å¹³æ»‘æƒé‡å˜åŒ–ï¼Œé¿å…æç«¯æ¼‚ç§»
    """

    def __init__(self,
                 lookback_window: int = 60,  # å†å²çª—å£å¤©æ•°
                 min_weight: float = 0.3,   # æœ€å°æƒé‡ï¼ˆé˜²æç«¯ï¼‰
                 max_weight: float = 0.7,   # æœ€å¤§æƒé‡ï¼ˆé˜²æç«¯ï¼‰
                 weight_smoothing: float = 0.3,  # æƒé‡å¹³æ»‘ç³»æ•°
                 use_copula: bool = True,    # æ˜¯å¦ä½¿ç”¨Copulaæ­£æ€åŒ–
                 use_decorrelation: bool = True,  # æ˜¯å¦ä½¿ç”¨å»ç›¸å…³èåˆ
                 top_k_list: list = None):   # Top-Kè¯„ä¼°åˆ—è¡¨
        """
        åˆå§‹åŒ–Rank-aware Blender

        Args:
            lookback_window: å†å²æ€§èƒ½è®¡ç®—çª—å£
            min_weight: LambdaRankæœ€å°æƒé‡
            max_weight: LambdaRankæœ€å¤§æƒé‡
            weight_smoothing: æƒé‡EWMAå¹³æ»‘ç³»æ•°
            use_copula: æ˜¯å¦ä½¿ç”¨Copulaæ­£æ€åŒ–
            top_k_list: Top-Kè¯„ä¼°æŒ‡æ ‡
        """
        self.lookback_window = lookback_window
        self.min_weight = min_weight
        self.max_weight = max_weight
        self.weight_smoothing = weight_smoothing
        self.use_copula = use_copula
        self.use_decorrelation = use_decorrelation

        if top_k_list is None:
            # ä¼˜åŒ–Kå€¼è®¾ç½®ï¼šé€‚åº”2600åªè‚¡ç¥¨çš„æŠ•èµ„å®‡å®™
            # åˆ†å±‚ç­–ç•¥ï¼šç²¾é€‰(5,10,20) + æŠ•èµ„ç»„åˆ(50,100) + é£é™©åˆ†æ•£(200)
            self.top_k_list = [5, 10, 20, 50, 100, 200]
        else:
            self.top_k_list = top_k_list

        # å†å²æƒé‡è®°å½•
        self.weight_history = []
        self.current_lambda_weight = 0.5  # åˆå§‹æƒé‡

        # å¯ç”¨é«˜çº§ç‰¹æ€§
        self.enable_advanced_blending = True  # å¯ç”¨é«˜çº§èåˆ
        self.enable_insightful_metrics = True  # å¯ç”¨æ·±åº¦æŒ‡æ ‡

        # é—¨æ§+æ®‹å·®å¾®èåˆå‚æ•°ï¼ˆ2600è‚¡ç¥¨å»ºè®®å‚æ•°ï¼‰
        self.tau_long = 0.65
        self.tau_short = 0.35
        self.alpha_long = 0.15
        self.alpha_short = 0.15
        self.max_gain = 1.25
        self.min_coverage = 0.30

        # æ®‹å·®å¾®èåˆå‚æ•°ç®¡ç†
        self.current_beta = 0.08  # Î²åˆå§‹å€¼
        self.beta_range = [0.0, 0.15]  # Î²å–å€¼èŒƒå›´
        self.beta_ewma_alpha = 0.3  # EWMAå¹³æ»‘ç³»æ•°
        self.beta_history = []  # Î²å†å²è®°å½•

        # æ€§èƒ½ç›‘æ§
        self._recent_performance_improved = True  # æ€§èƒ½æ”¹å–„æ ‡å¿—
        self._flip_ratio_history = []  # æ–¹å‘ç¿»è½¬å†å²
        self._coverage_history = []  # è¦†ç›–ç‡å†å²
        self._ndcg_history = []  # NDCGå†å²

        logger.info("ğŸ¤ æ™ºèƒ½Rank-aware Blender V2.0 åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   å†å²çª—å£: {self.lookback_window}å¤©")
        logger.info(f"   æƒé‡èŒƒå›´: [{self.min_weight}, {self.max_weight}]")
        logger.info(f"   Top-Kè¯„ä¼°: {self.top_k_list}")
        logger.info(f"   ç‰¹æ€§é…ç½®:")
        logger.info(f"     - Copulaæ­£æ€åŒ–: {self.use_copula}")
        logger.info(f"     - æ™ºèƒ½å»ç›¸å…³: {self.use_decorrelation}")
        logger.info(f"     - é«˜çº§èåˆ: {self.enable_advanced_blending}")
        logger.info(f"     - æ·±åº¦æŒ‡æ ‡: {self.enable_insightful_metrics}")
        logger.info(f"   é—¨æ§å‚æ•°: é•¿ä¾§â‰¥{self.tau_long}, çŸ­ä¾§â‰¤{self.tau_short}, å¢ç›Šä¸Šé™{self.max_gain}")

    def blend_predictions(self,
                         ridge_predictions: pd.DataFrame,
                         lambda_predictions: pd.DataFrame,
                         targets: Optional[pd.DataFrame] = None) -> pd.DataFrame:
        """
        èåˆRidgeå’ŒLambdaRanké¢„æµ‹

        Args:
            ridge_predictions: Ridgeé¢„æµ‹ç»“æœï¼ŒåŒ…å«'score'åˆ—
            lambda_predictions: LambdaRanké¢„æµ‹ç»“æœï¼ŒåŒ…å«'lambda_score'åˆ—
            targets: å†å²ç›®æ ‡å˜é‡ï¼ˆç”¨äºè®¡ç®—æƒé‡ï¼‰ï¼Œå¯é€‰

        Returns:
            èåˆåçš„é¢„æµ‹ç»“æœ
        """
        logger.info("ğŸ”„ å¼€å§‹æ™ºèƒ½Rank-awareèåˆ...")
        logger.info(f"   èåˆæ¨¡å¼: {'Advanced' if self.enable_advanced_blending else 'Standard'}")

        # éªŒè¯è¾“å…¥
        if not isinstance(ridge_predictions.index, pd.MultiIndex):
            raise ValueError("é¢„æµ‹æ•°æ®å¿…é¡»æœ‰MultiIndex(date, ticker)")

        # å¯¹é½ä¸¤ä¸ªé¢„æµ‹ç»“æœ
        ridge_aligned = ridge_predictions.reindex(ridge_predictions.index)
        lambda_aligned = lambda_predictions.reindex(ridge_predictions.index)

        # åˆå¹¶æ•°æ®
        combined_df = pd.DataFrame(index=ridge_predictions.index)
        # å¤„ç†Ridgeé¢„æµ‹çš„å¤šåˆ—è¾“å‡º - åªæå–'score'åˆ—ï¼ˆå®‰å…¨å¤„ç†Serieså’ŒDataFrameï¼‰
        if hasattr(ridge_aligned, 'columns') and 'score' in ridge_aligned.columns:
            combined_df['ridge_score'] = ridge_aligned['score']
        elif hasattr(ridge_aligned, 'columns') and 'score_z' in ridge_aligned.columns:
            combined_df['ridge_score'] = ridge_aligned['score_z']
        else:
            # å¦‚æœridge_alignedæœ¬èº«æ˜¯Seriesæˆ–å•åˆ—DataFrameï¼Œç›´æ¥ä½¿ç”¨
            if isinstance(ridge_aligned, pd.Series):
                combined_df['ridge_score'] = ridge_aligned
            elif len(ridge_aligned.columns) == 1:
                combined_df['ridge_score'] = ridge_aligned.iloc[:, 0]
            else:
                combined_df['ridge_score'] = ridge_aligned.get('score', np.nan)
        # å®‰å…¨å¤„ç†lambda_alignedçš„lambda_scoreåˆ—
        if isinstance(lambda_aligned, pd.Series):
            combined_df['lambda_score'] = lambda_aligned
        elif hasattr(lambda_aligned, 'columns') and 'lambda_score' in lambda_aligned.columns:
            combined_df['lambda_score'] = lambda_aligned['lambda_score']
        else:
            combined_df['lambda_score'] = lambda_aligned.get('lambda_score', np.nan) if hasattr(lambda_aligned, 'get') else np.nan

        # åˆ é™¤ä»»ä¸€æ¨¡å‹ç¼ºå¤±çš„æ ·æœ¬
        valid_mask = combined_df['ridge_score'].notna() & combined_df['lambda_score'].notna()
        total_samples = len(combined_df)
        ridge_valid = combined_df['ridge_score'].notna().sum()
        lambda_valid = combined_df['lambda_score'].notna().sum()
        both_valid = valid_mask.sum()

        logger.info(f"   é¢„æµ‹æ ·æœ¬ç»Ÿè®¡: æ€»æ•°={total_samples}, Ridgeæœ‰æ•ˆ={ridge_valid}, Lambdaæœ‰æ•ˆ={lambda_valid}, åŒæ–¹æœ‰æ•ˆ={both_valid}")

        combined_df = combined_df[valid_mask]

        if len(combined_df) == 0:
            # ğŸ”§ FIX: ä¼˜é›…å¤„ç†å•æ¨¡å‹æƒ…å†µ - ä½¿ç”¨æœ‰æ•ˆçš„å•ä¸€æ¨¡å‹
            if ridge_valid > 0 and lambda_valid == 0:
                logger.warning("LambdaRanké¢„æµ‹å…¨ä¸ºNaNï¼Œé€€åŒ–ä¸ºçº¯Ridgeé¢„æµ‹")
                result_df = pd.DataFrame(index=ridge_predictions.index)
                if 'score' in ridge_predictions.columns:
                    result_df['blended_score'] = ridge_predictions['score']
                else:
                    result_df['blended_score'] = ridge_predictions.iloc[:, 0]
                result_df['blended_rank'] = result_df['blended_score'].rank(ascending=False)
                # é˜²æ­¢é™¤ä»¥0ï¼šstd() è¿”å›æ ‡é‡ï¼Œä½¿ç”¨maxè¿›è¡Œä¸‹é™è£å‰ª
                result_df['blended_z'] = (result_df['blended_score'] - result_df['blended_score'].mean()) / max(result_df['blended_score'].std(), 1e-8)
                return result_df
            elif lambda_valid > 0 and ridge_valid == 0:
                logger.warning("Ridgeé¢„æµ‹å…¨ä¸ºNaNï¼Œé€€åŒ–ä¸ºçº¯LambdaRanké¢„æµ‹")
                result_df = pd.DataFrame(index=lambda_predictions.index)
                if 'lambda_score' in lambda_predictions.columns:
                    result_df['blended_score'] = lambda_predictions['lambda_score']
                else:
                    result_df['blended_score'] = lambda_predictions.iloc[:, 0]
                result_df['blended_rank'] = result_df['blended_score'].rank(ascending=False)
                # é˜²æ­¢é™¤ä»¥0ï¼šstd() è¿”å›æ ‡é‡ï¼Œä½¿ç”¨maxè¿›è¡Œä¸‹é™è£å‰ª
                result_df['blended_z'] = (result_df['blended_score'] - result_df['blended_score'].mean()) / max(result_df['blended_score'].std(), 1e-8)
                return result_df
            else:
                error_msg = f"ä¸¤ä¸ªæ¨¡å‹é¢„æµ‹éƒ½æ— æ•ˆ (æ€»æ•°={total_samples}, Ridgeæœ‰æ•ˆ={ridge_valid}, Lambdaæœ‰æ•ˆ={lambda_valid})"
                logger.error(error_msg)
                raise ValueError(error_msg)

        logger.info(f"   æœ‰æ•ˆæ ·æœ¬: {len(combined_df)}")

        # è®¡ç®—è‡ªé€‚åº”æƒé‡ï¼ˆå¦‚æœæœ‰å†å²ç›®æ ‡æ•°æ®ï¼‰
        if targets is not None:
            lambda_weight = self._calculate_adaptive_weight(combined_df, targets)
        else:
            lambda_weight = self.current_lambda_weight
            logger.info(f"   ä½¿ç”¨å½“å‰æƒé‡: Î»={lambda_weight:.3f}")

        # æ›´æ–°å½“å‰æƒé‡
        self.current_lambda_weight = lambda_weight
        ridge_weight = 1.0 - lambda_weight

        # è®¡ç®—åŸå§‹ä¿¡å·ç»Ÿè®¡
        original_corr = combined_df['ridge_score'].corr(combined_df['lambda_score'])
        logger.info(f"   åŸå§‹ä¿¡å·ç›¸å…³æ€§: {original_corr:.4f}")

        # æŒ‰äº¤æ˜“æ—¥ç»„å†…æ ‡å‡†åŒ–
        if self.use_copula:
            # Copulaæ­£æ€åŒ–
            combined_df = self._apply_copula_normalization(combined_df)
            ridge_col, lambda_col = 'ridge_norm', 'lambda_norm'
        else:
            # æ™®é€šz-scoreæ ‡å‡†åŒ–
            combined_df = self._apply_zscore_normalization(combined_df)
            ridge_col, lambda_col = 'ridge_z', 'lambda_z'

        # å»ç›¸å…³å¤„ç†ï¼ˆä¿ç•™æœ‰ä»·å€¼çš„ç›¸å…³æ€§ï¼‰
        if self.use_decorrelation:
            pre_decorr_corr = combined_df[ridge_col].corr(combined_df[lambda_col])
            combined_df = self._apply_decorrelation(combined_df, ridge_col, lambda_col)
            lambda_col = lambda_col + '_ortho'  # ä½¿ç”¨æ­£äº¤åŒ–åçš„LambdaRankä¿¡å·
            post_decorr_corr = combined_df[ridge_col].corr(combined_df[lambda_col])
            logger.info(f"   æ ‡å‡†åŒ–åç›¸å…³æ€§: {pre_decorr_corr:.4f} â†’ {post_decorr_corr:.4f}")

        # æ™ºèƒ½èåˆåˆ†æ•°
        if hasattr(self, 'enable_advanced_blending') and self.enable_advanced_blending:
            # é«˜çº§èåˆï¼šåŠ¨æ€æƒé‡ + é—¨æ§
            blended_scores = self._apply_advanced_blending(
                combined_df, ridge_col, lambda_col, ridge_weight, lambda_weight
            )
            # ç¡®ä¿èµ‹å€¼æ­£ç¡® - å¤„ç† Series/DataFrame è¿”å›å€¼
            if isinstance(blended_scores, pd.DataFrame):
                # å–ç¬¬ä¸€åˆ—ä½œä¸ºblendedåˆ†æ•°ï¼Œå¹¶å‘å‡ºæç¤º
                try:
                    first_col = blended_scores.columns[0]
                    combined_df['blended_score'] = blended_scores[first_col].reindex(combined_df.index)
                    logger.warning(f"é«˜çº§èåˆè¿”å›å¤šåˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ— '{first_col}' ä½œä¸ºblended_score")
                except Exception:
                    # å›é€€ï¼šæŒ‰è¡Œå‡å€¼
                    combined_df['blended_score'] = blended_scores.mean(axis=1).reindex(combined_df.index)
                    logger.warning("é«˜çº§èåˆè¿”å›å¤šåˆ—ï¼Œä½¿ç”¨è¡Œå‡å€¼ä½œä¸ºblended_score")
            elif isinstance(blended_scores, pd.Series):
                combined_df['blended_score'] = blended_scores.reindex(combined_df.index)
            else:
                # épandaså¯¹è±¡ï¼Œå°è¯•è½¬æ¢ä¸ºSeries
                combined_df['blended_score'] = pd.Series(blended_scores, index=combined_df.index)
        else:
            # æ ‡å‡†é—¨æ§èåˆ
            blended_scores = self._apply_gated_blending(
                combined_df, ridge_col, lambda_col, ridge_weight, lambda_weight
            )
            # ç¡®ä¿èµ‹å€¼æ­£ç¡® - å¤„ç† Series/DataFrame è¿”å›å€¼
            if isinstance(blended_scores, pd.DataFrame):
                try:
                    first_col = blended_scores.columns[0]
                    combined_df['blended_score'] = blended_scores[first_col].reindex(combined_df.index)
                    logger.warning(f"æ ‡å‡†é—¨æ§èåˆè¿”å›å¤šåˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ— '{first_col}' ä½œä¸ºblended_score")
                except Exception:
                    combined_df['blended_score'] = blended_scores.mean(axis=1).reindex(combined_df.index)
                    logger.warning("æ ‡å‡†é—¨æ§èåˆè¿”å›å¤šåˆ—ï¼Œä½¿ç”¨è¡Œå‡å€¼ä½œä¸ºblended_score")
            elif isinstance(blended_scores, pd.Series):
                combined_df['blended_score'] = blended_scores.reindex(combined_df.index)
            else:
                combined_df['blended_score'] = pd.Series(blended_scores, index=combined_df.index)

        # è®¡ç®—æœ€ç»ˆæ’å
        def _rank_by_date(group):
            scores = group['blended_score']
            return scores.rank(method='average', ascending=False)

        combined_df['blended_rank'] = combined_df.groupby(level='date').apply(_rank_by_date).values

        # ä»…å¯¹æœ€ç»ˆèåˆé˜¶æ®µåšç¨³å¥åå¤„ç†ï¼ˆä¸æ”¹ä¸Šæ¸¸ï¼‰ï¼š
        # 1) æ—¥å†…winsorize(1%-99%) 2) tanhå‹ç¼© 3) æ—¥å†…å»å‡å€¼ä¸å®šå°º

        def _postprocess_final_by_date(group):
            s = group['blended_score'].astype(float)
            if len(s) <= 1:
                group['blended_score_pp'] = s
                group['blended_z'] = 0.0
                return group
            # winsorize
            lo, hi = np.percentile(s, [1, 99])
            s_w = s.clip(lower=lo, upper=hi)
            # tanh å‹ç¼©åˆ° [-1,1]
            s_c = np.tanh(s_w / 2.0)
            # æ—¥å†…å»å‡å€¼ä¸å®šå°ºï¼ˆç›®æ ‡stdâ‰ˆ1ï¼‰
            std = s_c.std()
            if std < 1e-8:
                z = s_c * 0.0
            else:
                z = (s_c - s_c.mean()) / std
            group['blended_score_pp'] = s_c
            group['blended_z'] = z
            return group

        combined_df = combined_df.groupby(level='date', group_keys=False).apply(_postprocess_final_by_date)

        # è®°å½•æƒé‡å†å²
        self.weight_history.append({
            'date': combined_df.index.get_level_values('date').max(),
            'ridge_weight': ridge_weight,
            'lambda_weight': lambda_weight
        })

        # èåˆç»Ÿè®¡
        blend_stats = self._calculate_blend_statistics(combined_df)

        # è¾“å‡ºæ·±åº¦æ€§èƒ½æ´å¯Ÿ
        self._log_performance_insights(combined_df, ridge_weight, lambda_weight)

        logger.info(f"âœ… èåˆå®Œæˆ: Ridgeæƒé‡={ridge_weight:.3f}, Lambdaæƒé‡={lambda_weight:.3f}")
        logger.info(f"   èåˆæ ·æœ¬: {len(combined_df)}")
        logger.info(f"   èåˆç»Ÿè®¡: mean={blend_stats['mean']:.6f}, std={blend_stats['std']:.6f}")
        logger.info(f"   ä¿¡å·å¯¹æ¯”: Ridgeä¸Lambdaæ­£ç›¸å…³ç‡={blend_stats['agreement_rate']:.1%}")

        # å¯¹å¤–ä»æš´éœ² blended_scoreï¼ˆå…¼å®¹ï¼‰ï¼Œå¹¶æä¾›å‹ç¼©åçš„ blended_score_pp
        return combined_df[['ridge_score', 'lambda_score', 'blended_score', 'blended_score_pp', 'blended_rank', 'blended_z']]

    def _log_performance_insights(self, df: pd.DataFrame, ridge_weight: float, lambda_weight: float):
        """è¾“å‡ºæ·±åº¦æ€§èƒ½æ´å¯Ÿ"""
        if not self.enable_insightful_metrics:
            return

        logger.info("ğŸ“Š æ·±åº¦æ€§èƒ½åˆ†æ:")

        # æƒé‡åˆ†æ
        weight_ratio = lambda_weight / (ridge_weight + 1e-8)
        if weight_ratio > 1.5:
            insight = "LambdaRankä¸»å¯¼ (æ’åºä¼˜å…ˆ)"
        elif weight_ratio < 0.67:
            insight = "Ridgeä¸»å¯¼ (å¹…åº¦ä¼˜å…ˆ)"
        else:
            insight = "å‡è¡¡èåˆ (ååŒä¼˜åŒ–)"

        logger.info(f"   æƒé‡ç­–ç•¥: {insight}")
        logger.info(f"   æƒé‡æ¯”: {weight_ratio:.2f}")

        # ä¿¡å·åˆ†æ
        if 'ridge_score' in df.columns and 'lambda_score' in df.columns:
            correlation = df['ridge_score'].corr(df['lambda_score'])
            if correlation > 0.7:
                signal_insight = "é«˜åº¦ä¸€è‡´ (å¢å¼ºä¿¡å¿ƒ)"
            elif correlation > 0.3:
                signal_insight = "ä¸­åº¦ä¸€è‡´ (äº’è¡¥å¢ç›Š)"
            elif correlation > 0:
                signal_insight = "ä½åº¦ä¸€è‡´ (åˆ†æ•£é£é™©)"
            else:
                signal_insight = "è´Ÿç›¸å…³ (å¯¹å†²ä¿¡å·)"

            logger.info(f"   ä¿¡å·å…³ç³»: {signal_insight}")
            logger.info(f"   ç›¸å…³ç³»æ•°: {correlation:.3f}")

        # é£é™©åˆ†æ
        if 'blended_score' in df.columns:
            vol = df['blended_score'].std()
            skew = df['blended_score'].skew()
            if abs(skew) < 0.5:
                risk_insight = "å¯¹ç§°åˆ†å¸ƒ (é£é™©å‡è¡¡)"
            elif skew > 0.5:
                risk_insight = "å³ååˆ†å¸ƒ (ä¸Šè¡Œå€¾å‘)"
            else:
                risk_insight = "å·¦ååˆ†å¸ƒ (ä¸‹è¡Œé£é™©)"

            logger.info(f"   åˆ†å¸ƒç‰¹å¾: {risk_insight}")
            logger.info(f"   æ³¢åŠ¨ç‡: {vol:.3f}, ååº¦: {skew:.3f}")

        logger.info("ğŸ¯ èåˆæ•ˆæœè¯„ä¼°å®Œæˆ")

    def blend_with_gate(self,
                       ridge_predictions: pd.DataFrame,
                       lambda_predictions: pd.DataFrame,
                       targets: Optional[pd.DataFrame] = None,
                       cfg: Optional[RankGateConfig] = None) -> pd.DataFrame:
        """
        é—¨æ§å¢ç›Šèåˆ - LTRä¸“æ³¨æ’åé—¨æ§ï¼ŒRidgeä¸“æ³¨å¹…åº¦åˆ»åº¦

        æ ¸å¿ƒç­–ç•¥ï¼š
        - åªç”¨LTRçš„å½“æ—¥ç»„å†…ç™¾åˆ†ä½lambda_pctåšå‡†å…¥/åˆ†æ¡£ï¼Œç»ä¸ä¸Ridgeåˆ†æ•°çº¿æ€§åŠ æƒ
        - æœ€ç»ˆä¿¡å·åªæ¥è‡ªRidgeå¹…åº¦ï¼Œç»LTRçš„"é—¨æ§+å¢ç›Š"è°ƒåˆ¶
        - score_final = score_ridge_z Ã— gain(lambda_pct) Ã— gate(lambda_pct)

        Args:
            ridge_predictions: Ridgeé¢„æµ‹ç»“æœï¼ŒåŒ…å«'score'åˆ—
            lambda_predictions: LambdaRanké¢„æµ‹ç»“æœï¼ŒåŒ…å«'lambda_score'æˆ–'lambda_pct'åˆ—
            targets: å†å²ç›®æ ‡å˜é‡ï¼ˆç”¨äºç›‘æ§ï¼‰ï¼Œå¯é€‰
            cfg: é—¨æ§é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®å¦‚æœä¸ºNone

        Returns:
            é—¨æ§èåˆåçš„é¢„æµ‹ç»“æœ
        """
        if cfg is None:
            cfg = RankGateConfig()

        logger.info("ğŸšª å¼€å§‹é—¨æ§å¢ç›Šèåˆ...")
        logger.info(f"   é—¨æ§é˜ˆå€¼: é•¿â‰¥{cfg.tau_long}, çŸ­â‰¤{cfg.tau_short}")
        logger.info(f"   å¢ç›Šç³»æ•°: Î±_long={cfg.alpha_long}, Î±_short={cfg.alpha_short}")

        # éªŒè¯è¾“å…¥
        if not isinstance(ridge_predictions.index, pd.MultiIndex):
            raise ValueError("é¢„æµ‹æ•°æ®å¿…é¡»æœ‰MultiIndex(date, ticker)")

        # å¯¹é½ä¸¤ä¸ªé¢„æµ‹ç»“æœ
        combined_df = pd.DataFrame(index=ridge_predictions.index)
        # å¤„ç†Ridgeé¢„æµ‹çš„å¤šåˆ—è¾“å‡º - åªæå–'score'åˆ—ï¼ˆå®‰å…¨å¤„ç†Serieså’ŒDataFrameï¼‰
        if hasattr(ridge_predictions, 'columns') and 'score' in ridge_predictions.columns:
            combined_df['ridge_score'] = ridge_predictions['score']
        elif hasattr(ridge_predictions, 'columns') and 'score_z' in ridge_predictions.columns:
            combined_df['ridge_score'] = ridge_predictions['score_z']
        else:
            # å¦‚æœridge_predictionsæœ¬èº«æ˜¯Seriesæˆ–å•åˆ—DataFrameï¼Œç›´æ¥ä½¿ç”¨
            if isinstance(ridge_predictions, pd.Series):
                combined_df['ridge_score'] = ridge_predictions
            elif len(ridge_predictions.columns) == 1:
                combined_df['ridge_score'] = ridge_predictions.iloc[:, 0]
            else:
                combined_df['ridge_score'] = ridge_predictions.get('score', np.nan)

        # è·å–LambdaRankç™¾åˆ†ä½ï¼ˆå·²ç”±LambdaRankStacker.predictäº§å‡ºï¼‰
        if hasattr(lambda_predictions, 'columns') and 'lambda_pct' in lambda_predictions.columns:
            combined_df['lambda_pct'] = lambda_predictions['lambda_pct']
        elif hasattr(lambda_predictions, 'columns') and 'lambda_score' in lambda_predictions.columns:
            # å¦‚æœåªæœ‰lambda_scoreï¼Œéœ€è¦è®¡ç®—å½“æ—¥ç»„å†…ç™¾åˆ†ä½
            logger.info("   ä»lambda_scoreè®¡ç®—ç»„å†…ç™¾åˆ†ä½...")
            combined_df['lambda_score'] = lambda_predictions['lambda_score']
            combined_df = self._calculate_daily_percentiles(combined_df)
        elif isinstance(lambda_predictions, pd.Series):
            # å¦‚æœlambda_predictionsæ˜¯Seriesï¼Œå°†å…¶ä½œä¸ºlambda_scoreå¤„ç†
            combined_df['lambda_score'] = lambda_predictions
            combined_df = self._calculate_daily_percentiles(combined_df)
        else:
            raise ValueError("LambdaRanké¢„æµ‹å¿…é¡»åŒ…å«'lambda_pct'æˆ–'lambda_score'åˆ—ï¼Œæˆ–è€…æ˜¯åŒ…å«é¢„æµ‹å€¼çš„Series")

        # åˆ é™¤ä»»ä¸€æ¨¡å‹ç¼ºå¤±çš„æ ·æœ¬
        valid_mask = combined_df['ridge_score'].notna() & combined_df['lambda_pct'].notna()
        total_samples = len(combined_df)
        ridge_valid = combined_df['ridge_score'].notna().sum()
        lambda_valid = combined_df['lambda_pct'].notna().sum()
        both_valid = valid_mask.sum()

        logger.info(f"   é¢„æµ‹æ ·æœ¬ç»Ÿè®¡: æ€»æ•°={total_samples}, Ridgeæœ‰æ•ˆ={ridge_valid}, Lambdaæœ‰æ•ˆ={lambda_valid}, åŒæ–¹æœ‰æ•ˆ={both_valid}")

        combined_df = combined_df[valid_mask]

        if len(combined_df) == 0:
            # ğŸ”§ FIX: ä¼˜é›…å¤„ç†å•æ¨¡å‹æƒ…å†µ - ä½¿ç”¨æœ‰æ•ˆçš„å•ä¸€æ¨¡å‹
            if ridge_valid > 0 and lambda_valid == 0:
                logger.warning("LambdaRanké¢„æµ‹å…¨ä¸ºNaNï¼Œé—¨æ§é€€åŒ–ä¸ºçº¯Ridgeé¢„æµ‹")
                result_df = pd.DataFrame(index=ridge_predictions.index)
                if 'score' in ridge_predictions.columns:
                    result_df['blended_score'] = ridge_predictions['score']
                else:
                    result_df['blended_score'] = ridge_predictions.iloc[:, 0]
                result_df['blended_rank'] = result_df['blended_score'].rank(ascending=False)
                # é˜²æ­¢é™¤ä»¥0ï¼šstd() è¿”å›æ ‡é‡ï¼Œä½¿ç”¨maxè¿›è¡Œä¸‹é™è£å‰ª
                result_df['blended_z'] = (result_df['blended_score'] - result_df['blended_score'].mean()) / max(result_df['blended_score'].std(), 1e-8)
                result_df['gate'] = 1.0  # å…¨éƒ¨é€šè¿‡é—¨æ§
                result_df['gain'] = 1.0  # æ— å¢ç›Š
                return result_df
            elif lambda_valid > 0 and ridge_valid == 0:
                logger.warning("Ridgeé¢„æµ‹å…¨ä¸ºNaNï¼Œé—¨æ§é€€åŒ–ä¸ºçº¯LambdaRanké¢„æµ‹")
                result_df = pd.DataFrame(index=lambda_predictions.index)
                if 'lambda_score' in lambda_predictions.columns:
                    result_df['blended_score'] = lambda_predictions['lambda_score']
                elif 'lambda_pct' in lambda_predictions.columns:
                    # å°†ç™¾åˆ†ä½è½¬æ¢ä¸ºåˆ†æ•°
                    result_df['blended_score'] = lambda_predictions['lambda_pct'] - 0.5
                else:
                    result_df['blended_score'] = lambda_predictions.iloc[:, 0]
                result_df['blended_rank'] = result_df['blended_score'].rank(ascending=False)
                # é˜²æ­¢é™¤ä»¥0ï¼šstd() è¿”å›æ ‡é‡ï¼Œä½¿ç”¨maxè¿›è¡Œä¸‹é™è£å‰ª
                result_df['blended_z'] = (result_df['blended_score'] - result_df['blended_score'].mean()) / max(result_df['blended_score'].std(), 1e-8)
                result_df['gate'] = 1.0  # å…¨éƒ¨é€šè¿‡é—¨æ§
                result_df['gain'] = 1.0  # æ— å¢ç›Š
                return result_df
            else:
                error_msg = f"ä¸¤ä¸ªæ¨¡å‹é¢„æµ‹éƒ½æ— æ•ˆ (æ€»æ•°={total_samples}, Ridgeæœ‰æ•ˆ={ridge_valid}, Lambdaæœ‰æ•ˆ={lambda_valid})"
                logger.error(error_msg)
                raise ValueError(error_msg)

        logger.info(f"   æœ‰æ•ˆæ ·æœ¬: {len(combined_df)}")

        # æŒ‰äº¤æ˜“æ—¥ç»„å†…æ ‡å‡†åŒ–Ridgeåˆ†æ•°
        combined_df = self._standardize_ridge_scores(combined_df)

        # è®¡ç®—é—¨æ§ä¸å¢ç›Š
        combined_df = self._apply_rank_gate_and_gain(combined_df, cfg)

        # æœ€ç»ˆé—¨æ§èåˆ: score_final = ridge_z Ã— gain Ã— gate
        combined_df['gated_score'] = (
            combined_df['ridge_z'] *
            combined_df['gain'] *
            combined_df['gate']
        )

        # è®¡ç®—æœ€ç»ˆæ’åå’Œæ ‡å‡†åŒ–åˆ†æ•°
        combined_df = self._finalize_gated_results(combined_df)

        # ç›‘æ§ç»Ÿè®¡
        self._log_gate_monitoring(combined_df, cfg)

        # è¦†ç›–ç‡å…œåº•æ£€æŸ¥
        coverage = self._check_coverage_fallback(combined_df, cfg)

        logger.info(f"âœ… é—¨æ§èåˆå®Œæˆ: è¦†ç›–ç‡={coverage:.1%}")

        # ğŸ”§ FIX: ä¸ºäº†APIä¸€è‡´æ€§ï¼Œæ·»åŠ blended_scoreåˆ—ä½œä¸ºgated_scoreçš„åˆ«å
        combined_df['blended_score'] = combined_df['gated_score']
        combined_df['blended_rank'] = combined_df['gated_rank']
        combined_df['blended_z'] = combined_df['gated_z']

        return combined_df[['ridge_score', 'ridge_z', 'lambda_pct', 'gate', 'gain',
                           'gated_score', 'gated_rank', 'gated_z',
                           'blended_score', 'blended_rank', 'blended_z']]

    def _calculate_daily_percentiles(self, df: pd.DataFrame) -> pd.DataFrame:
        """æŒ‰æ—¥ç»„å†…è®¡ç®—LambdaRankç™¾åˆ†ä½"""

        def _pct_by_date(group):
            lambda_scores = group['lambda_score']
            # ä½¿ç”¨rankæ–¹æ³•è®¡ç®—ç™¾åˆ†ä½ï¼ˆ0-1èŒƒå›´ï¼‰
            percentiles = lambda_scores.rank(method='average') / len(lambda_scores)
            group['lambda_pct'] = percentiles
            return group

        return df.groupby(level='date', group_keys=False).apply(_pct_by_date)

    def _standardize_ridge_scores(self, df: pd.DataFrame) -> pd.DataFrame:
        """æŒ‰æ—¥ç»„å†…æ ‡å‡†åŒ–Ridgeåˆ†æ•°"""

        def _zscore_ridge_by_date(group):
            ridge_scores = group['ridge_score']
            if len(ridge_scores) > 1:
                z_scores = (ridge_scores - ridge_scores.mean()) / (ridge_scores.std() + 1e-8)
                group['ridge_z'] = z_scores
            else:
                group['ridge_z'] = 0.0
            return group

        return df.groupby(level='date', group_keys=False).apply(_zscore_ridge_by_date)

    def _apply_rank_gate_and_gain(self, df: pd.DataFrame, cfg: RankGateConfig) -> pd.DataFrame:
        """åº”ç”¨æ’åé—¨æ§ä¸åˆ†æ¡£å¢ç›Š"""

        # åˆå§‹åŒ–é—¨æ§å’Œå¢ç›Š
        df['gate'] = 0.0  # é»˜è®¤ä¸é€šè¿‡é—¨æ§
        df['gain'] = 1.0  # é»˜è®¤æ— å¢ç›Š

        # é•¿ä¾§é—¨æ§ä¸å¢ç›Š
        long_mask = df['lambda_pct'] >= cfg.tau_long
        if long_mask.any():
            # é•¿ä¾§é—¨æ§é€šè¿‡
            df.loc[long_mask, 'gate'] = 1.0
            # é•¿ä¾§åˆ†æ¡£å¢ç›Š: gain = 1 + Î±_long Ã— ((lambda_pct - Ï„_long)/(1-Ï„_long))
            long_gain_factor = (df.loc[long_mask, 'lambda_pct'] - cfg.tau_long) / (1 - cfg.tau_long)
            df.loc[long_mask, 'gain'] = 1.0 + cfg.alpha_long * long_gain_factor
            df.loc[long_mask, 'gain'] = np.clip(df.loc[long_mask, 'gain'], 1.0, cfg.max_gain)

        # çŸ­ä¾§é—¨æ§ä¸å¢ç›Š
        short_mask = df['lambda_pct'] <= cfg.tau_short
        if short_mask.any():
            # çŸ­ä¾§é—¨æ§é€šè¿‡
            df.loc[short_mask, 'gate'] = 1.0
            # çŸ­ä¾§åˆ†æ¡£å¢ç›Š: gain = 1 + Î±_short Ã— ((Ï„_short - lambda_pct)/Ï„_short)
            short_gain_factor = (cfg.tau_short - df.loc[short_mask, 'lambda_pct']) / cfg.tau_short
            df.loc[short_mask, 'gain'] = 1.0 + cfg.alpha_short * short_gain_factor
            df.loc[short_mask, 'gain'] = np.clip(df.loc[short_mask, 'gain'], 1.0, cfg.max_gain)

        # ä¸­æ€§å¸¦å¤„ç†ï¼ˆå¯é€‰ç½®é›¶ï¼‰
        if cfg.neutral_band:
            neutral_mask = (df['lambda_pct'] > cfg.tau_short) & (df['lambda_pct'] < cfg.tau_long)
            # ä¸­æ€§å¸¦æ—¢ä¸é€šè¿‡é—¨æ§ï¼Œä¹Ÿæ— å¢ç›Šï¼ˆå·²åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®ï¼‰
            pass

        return df

    def _finalize_gated_results(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æœ€ç»ˆæ’åå’Œæ ‡å‡†åŒ–åˆ†æ•°"""

        # æŒ‰æ—¥è®¡ç®—æœ€ç»ˆæ’å
        def _rank_by_date(group):
            gated_scores = group['gated_score']
            # æ’åï¼šåˆ†æ•°è¶Šé«˜æ’åè¶Šé å‰
            group['gated_rank'] = gated_scores.rank(method='average', ascending=False)
            return group

        df = df.groupby(level='date', group_keys=False).apply(_rank_by_date)

        # æŒ‰æ—¥æ ‡å‡†åŒ–æœ€ç»ˆåˆ†æ•°
        def _zscore_final_by_date(group):
            gated_scores = group['gated_score']
            if len(gated_scores) > 1 and gated_scores.std() > 1e-8:
                z_scores = (gated_scores - gated_scores.mean()) / gated_scores.std()
                group['gated_z'] = z_scores
            else:
                group['gated_z'] = 0.0
            return group

        df = df.groupby(level='date', group_keys=False).apply(_zscore_final_by_date)

        return df

    def _log_gate_monitoring(self, df: pd.DataFrame, cfg: RankGateConfig):
        """è®°å½•é—¨æ§ç›‘æ§ç»Ÿè®¡"""

        total_samples = len(df)
        if total_samples == 0:
            return

        # è¦†ç›–ç‡ç»Ÿè®¡
        gated_samples = (df['gate'] > 0).sum()
        coverage = gated_samples / total_samples

        # é•¿çŸ­ä¾§åˆ†å¸ƒ
        long_samples = (df['lambda_pct'] >= cfg.tau_long).sum()
        short_samples = (df['lambda_pct'] <= cfg.tau_short).sum()
        neutral_samples = total_samples - long_samples - short_samples

        # å¢ç›Šç»Ÿè®¡
        gain_mean = df['gain'].mean()
        gain_max = df['gain'].max()
        gain_top_rate = (df['gain'] >= cfg.max_gain * 0.95).sum() / total_samples

        logger.info(f"ğŸ“Š é—¨æ§ç»Ÿè®¡:")
        logger.info(f"   è¦†ç›–ç‡: {coverage:.1%} ({gated_samples}/{total_samples})")
        logger.info(f"   åˆ†å¸ƒ: é•¿ä¾§{long_samples}, çŸ­ä¾§{short_samples}, ä¸­æ€§{neutral_samples}")
        logger.info(f"   å¢ç›Š: å‡å€¼{gain_mean:.3f}, æœ€å¤§{gain_max:.3f}, è§¦é¡¶ç‡{gain_top_rate:.1%}")

        # æ›´æ–°å†å²è®°å½•
        cfg.coverage_history.append(coverage)
        cfg.gain_stats_history.append({
            'mean': gain_mean,
            'max': gain_max,
            'top_rate': gain_top_rate
        })

    def _check_coverage_fallback(self, df: pd.DataFrame, cfg: RankGateConfig) -> float:
        """æ£€æŸ¥è¦†ç›–ç‡å¹¶åœ¨å¿…è¦æ—¶å›é€€åˆ°Ridge"""

        coverage = (df['gate'] > 0).sum() / len(df) if len(df) > 0 else 0.0

        if coverage < cfg.min_coverage:
            logger.warning(f"âš ï¸ è¦†ç›–ç‡è¿‡ä½ ({coverage:.1%} < {cfg.min_coverage:.1%})ï¼Œå›é€€åˆ°Ridgeåˆ†æ•°")
            # å›é€€ç­–ç•¥ï¼šä½¿ç”¨Ridgeåˆ†æ•°ï¼Œæ— é—¨æ§å¢ç›Š
            df['gated_score'] = df['ridge_z']
            df['gate'] = 1.0  # å…¨éƒ¨é€šè¿‡
            df['gain'] = 1.0  # æ— å¢ç›Š
            # é‡æ–°è®¡ç®—æ’åå’Œæ ‡å‡†åŒ–
            df = self._finalize_gated_results(df)
            coverage = 1.0

        return coverage

    def _calculate_adaptive_weight(self, combined_df: pd.DataFrame, targets: pd.DataFrame) -> float:
        """
        åŸºäºå¤šç»´åº¦æ€§èƒ½è®¡ç®—æ™ºèƒ½è‡ªé€‚åº”æƒé‡

        æ ¸å¿ƒæ”¹è¿›ï¼š
        1. ä½¿ç”¨å¤šä¸ªæ€§èƒ½æŒ‡æ ‡ç»¼åˆè¯„ä¼°
        2. åŠ¨æ€è°ƒæ•´æƒé‡å¯¹æ¯”åŸºå‡†
        3. è€ƒè™‘è¿‘æœŸ vs è¿œæœŸæ€§èƒ½è¶‹åŠ¿
        4. åŠ å…¥é£é™©è°ƒæ•´å› å­

        Args:
            combined_df: å½“å‰é¢„æµ‹ç»“æœ
            targets: å†å²ç›®æ ‡å˜é‡

        Returns:
            LambdaRankæƒé‡
        """
        logger.info("ğŸ“Š è®¡ç®—æ™ºèƒ½è‡ªé€‚åº”æƒé‡...")

        target_aligned = targets.reindex(combined_df.index)
        ridge_metrics = {'ic': [], 'rankic': [], 'top_return': [], 'volatility': []}
        lambda_metrics = {'ic': [], 'rankic': [], 'ndcg': [], 'precision': []}

        # å¤šå°ºåº¦è¯„ä¼°å‚æ•°
        eval_k_list = self.top_k_list[:2] if len(self.top_k_list) > 1 else [5, 10]
        recent_window = min(10, self.lookback_window // 3)  # è¿‘æœŸçª—å£

        try:
            # åˆå¹¶æ•°æ®ç”¨äºè¯„ä¼°
            target_values = target_aligned.iloc[:, 0] if len(target_aligned.columns) > 0 else target_aligned
            eval_df = pd.DataFrame({
                'ridge_score': combined_df['ridge_score'],
                'lambda_score': combined_df['lambda_score'],
                'target': target_values
            }, index=combined_df.index).dropna()

            if len(eval_df) == 0:
                logger.warning("è¯„ä¼°æ•°æ®ä¸ºç©ºï¼Œä½¿ç”¨å½“å‰æƒé‡")
                return self.current_lambda_weight

            # æŒ‰æ—¥æœŸåˆ†ç»„è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            def _calculate_group_rankic(group_data, score_col):
                """è®¡ç®—ç»„å†…RankIC"""
                min_samples = eval_k_list[0] if eval_k_list else 5  # ä½¿ç”¨æœ€å°çš„kå€¼ä½œä¸ºæœ€å°æ ·æœ¬æ•°
                if len(group_data) < min_samples:
                    return 0.0
                try:
                    score_ranks = group_data[score_col].rank(ascending=False)
                    target_ranks = group_data['target'].rank(ascending=False)
                    correlation = score_ranks.corr(target_ranks, method='spearman')
                    return correlation if not np.isnan(correlation) else 0.0
                except:
                    return 0.0

            def _calculate_group_ndcg_at_k(group_data, score_col, k):
                """è®¡ç®—ç»„å†…NDCG@K"""
                if len(group_data) < k:
                    return 0.0
                try:
                    # æŒ‰åˆ†æ•°é™åºæ’åºï¼Œå–Top-K
                    sorted_group = group_data.sort_values(score_col, ascending=False).head(k)

                    # ç®€åŒ–NDCGè®¡ç®—ï¼šä½¿ç”¨ç›®æ ‡å€¼ä½œä¸ºç›¸å…³æ€§
                    relevance = sorted_group['target'].values

                    # DCGè®¡ç®—
                    dcg = 0.0
                    for i, rel in enumerate(relevance):
                        dcg += rel / np.log2(i + 2)  # i+2 because log2(1)=0

                    # IDCGè®¡ç®—ï¼ˆç†æƒ³æ’åºï¼‰
                    ideal_relevance = sorted(group_data['target'].values, reverse=True)[:k]
                    idcg = 0.0
                    for i, rel in enumerate(ideal_relevance):
                        idcg += rel / np.log2(i + 2)

                    # NDCGè®¡ç®—
                    if idcg == 0:
                        return 0.0
                    return dcg / idcg
                except:
                    return 0.0

            # æŒ‰æ—¥æœŸåˆ†ç»„è¯„ä¼°
            date_list = eval_df.index.get_level_values('date').unique()
            for idx, date in enumerate(date_list):
                group_data = eval_df.loc[date]
                is_recent = idx >= len(date_list) - recent_window
                weight_mult = 1.5 if is_recent else 1.0  # è¿‘æœŸæ•°æ®æ›´é‡è¦

                # Ridgeå¤šç»´åº¦è¯„ä¼°
                ridge_ic = group_data['ridge_score'].corr(group_data['target'])
                ridge_rankic = _calculate_group_rankic(group_data, 'ridge_score')

                # è®¡ç®—Top-Kå¹³å‡æ”¶ç›Š
                if len(group_data) >= eval_k_list[0]:
                    top_k_indices = group_data['ridge_score'].nlargest(eval_k_list[0]).index
                    ridge_top_return = group_data.loc[top_k_indices, 'target'].mean()
                    ridge_volatility = group_data.loc[top_k_indices, 'target'].std()
                else:
                    ridge_top_return = 0.0
                    ridge_volatility = 1.0

                ridge_metrics['ic'].append(ridge_ic * weight_mult if not np.isnan(ridge_ic) else 0)
                ridge_metrics['rankic'].append(max(0.0, ridge_rankic) * weight_mult)
                ridge_metrics['top_return'].append(ridge_top_return * weight_mult)
                ridge_metrics['volatility'].append(ridge_volatility)

                # Lambdaå¤šç»´åº¦è¯„ä¼°
                lambda_ic = group_data['lambda_score'].corr(group_data['target'])
                lambda_rankic = _calculate_group_rankic(group_data, 'lambda_score')

                # NDCGå’ŒPrecision@K
                for k in eval_k_list:
                    if len(group_data) >= k:
                        lambda_ndcg = _calculate_group_ndcg_at_k(group_data, 'lambda_score', k)
                        # Precision@K: Top-Kä¸­æ­£æ”¶ç›Šçš„æ¯”ä¾‹
                        top_k_indices = group_data['lambda_score'].nlargest(k).index
                        lambda_precision = (group_data.loc[top_k_indices, 'target'] > 0).mean()
                        break
                else:
                    lambda_ndcg = 0.0
                    lambda_precision = 0.5

                lambda_metrics['ic'].append(lambda_ic * weight_mult if not np.isnan(lambda_ic) else 0)
                lambda_metrics['rankic'].append(max(0.0, lambda_rankic) * weight_mult)
                lambda_metrics['ndcg'].append(lambda_ndcg * weight_mult)
                lambda_metrics['precision'].append(lambda_precision * weight_mult)

        except Exception as e:
            logger.warning(f"æƒé‡è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æƒé‡: {e}")
            return self.current_lambda_weight

        # ç»¼åˆæ€§èƒ½è®¡ç®—
        if all(ridge_metrics[k] for k in ridge_metrics) and all(lambda_metrics[k] for k in lambda_metrics):
            # Ridgeç»¼åˆå¾—åˆ†
            ridge_ic_score = np.mean(ridge_metrics['ic'])
            ridge_rankic_score = np.mean(ridge_metrics['rankic'])
            ridge_return_score = np.mean(ridge_metrics['top_return'])
            ridge_vol_penalty = 1.0 / (1.0 + np.mean(ridge_metrics['volatility']))  # æ³¢åŠ¨ç‡æƒ©ç½š

            # Ridgeç»¼åˆæ€§èƒ½ï¼ˆåŠ æƒå¹³å‡ï¼‰
            ridge_performance = (
                0.3 * (ridge_ic_score + 1) / 2 +  # ICè´¡çŒ®30%
                0.3 * (ridge_rankic_score + 1) / 2 +  # RankICè´¡çŒ®30%
                0.2 * np.tanh(ridge_return_score * 10) +  # æ”¶ç›Šè´¡çŒ®20%
                0.2 * ridge_vol_penalty  # ç¨³å®šæ€§è´¡çŒ®20%
            )

            # Lambdaç»¼åˆå¾—åˆ†
            lambda_ic_score = np.mean(lambda_metrics['ic'])
            lambda_rankic_score = np.mean(lambda_metrics['rankic'])
            lambda_ndcg_score = np.mean(lambda_metrics['ndcg'])
            lambda_precision_score = np.mean(lambda_metrics['precision'])

            # Lambdaç»¼åˆæ€§èƒ½ï¼ˆæ›´é‡è§†æ’åºæŒ‡æ ‡ï¼‰
            lambda_performance = (
                0.2 * (lambda_ic_score + 1) / 2 +  # ICè´¡çŒ®20%
                0.2 * (lambda_rankic_score + 1) / 2 +  # RankICè´¡çŒ®20%
                0.4 * lambda_ndcg_score +  # NDCGè´¡çŒ®40%ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ï¼‰
                0.2 * lambda_precision_score  # Precisionè´¡çŒ®20%
            )

            # åŠ¨æ€è°ƒæ•´åŸºå‡†
            performance_ratio = lambda_performance / (ridge_performance + 1e-8)

            # éçº¿æ€§æ˜ å°„ï¼šä½¿æƒé‡æ›´æ•æ„Ÿäºæ€§èƒ½å·®å¼‚
            if performance_ratio > 1.2:  # Lambdaæ˜æ˜¾æ›´å¥½
                raw_lambda_weight = 0.6 + 0.1 * min((performance_ratio - 1.2), 1.0)
            elif performance_ratio < 0.8:  # Ridgeæ˜æ˜¾æ›´å¥½
                raw_lambda_weight = 0.4 - 0.1 * min((0.8 - performance_ratio), 0.3)
            else:  # æ€§èƒ½æ¥è¿‘
                raw_lambda_weight = 0.45 + 0.1 * (performance_ratio - 0.95)

            # åŠ å…¥è¶‹åŠ¿è°ƒæ•´
            if len(ridge_metrics['ic']) >= recent_window:
                recent_ridge_trend = np.mean(ridge_metrics['ic'][-recent_window:]) - np.mean(ridge_metrics['ic'][:-recent_window])
                recent_lambda_trend = np.mean(lambda_metrics['ndcg'][-recent_window:]) - np.mean(lambda_metrics['ndcg'][:-recent_window])

                # è¶‹åŠ¿è°ƒæ•´æƒé‡
                if recent_lambda_trend > recent_ridge_trend + 0.02:  # Lambdaè¶‹åŠ¿æ›´å¥½
                    raw_lambda_weight = min(raw_lambda_weight + 0.05, 0.75)
                elif recent_ridge_trend > recent_lambda_trend + 0.02:  # Ridgeè¶‹åŠ¿æ›´å¥½
                    raw_lambda_weight = max(raw_lambda_weight - 0.05, 0.25)

            # åº”ç”¨çº¦æŸå’Œæ™ºèƒ½å¹³æ»‘
            constrained_lambda_weight = np.clip(raw_lambda_weight, self.min_weight, self.max_weight)

            # åŠ¨æ€å¹³æ»‘ç³»æ•°ï¼šæƒé‡å˜åŒ–å¤§æ—¶åŠ å¼ºå¹³æ»‘
            weight_change = abs(constrained_lambda_weight - self.current_lambda_weight)
            dynamic_smoothing = self.weight_smoothing * (1 + weight_change)  # å˜åŒ–å¤§æ—¶æ›´å¹³æ»‘
            dynamic_smoothing = min(dynamic_smoothing, 0.7)  # ä¸Šé™ä¸º0.7

            # EWMAå¹³æ»‘
            smoothed_lambda_weight = (
                (1 - dynamic_smoothing) * constrained_lambda_weight +
                dynamic_smoothing * self.current_lambda_weight
            )

            logger.info(f"   Ridgeç»¼åˆæ€§èƒ½: {ridge_performance:.4f}")
            logger.info(f"     - IC: {ridge_ic_score:.4f}, RankIC: {ridge_rankic_score:.4f}")
            logger.info(f"     - Topæ”¶ç›Š: {ridge_return_score:.4f}, æ³¢åŠ¨æƒ©ç½š: {ridge_vol_penalty:.4f}")
            logger.info(f"   Lambdaç»¼åˆæ€§èƒ½: {lambda_performance:.4f}")
            logger.info(f"     - IC: {lambda_ic_score:.4f}, RankIC: {lambda_rankic_score:.4f}")
            logger.info(f"     - NDCG: {lambda_ndcg_score:.4f}, Precision: {lambda_precision_score:.4f}")
            logger.info(f"   æ€§èƒ½æ¯”: {performance_ratio:.3f}")
            logger.info(f"   åŸå§‹æƒé‡: Î»={raw_lambda_weight:.3f}")
            logger.info(f"   çº¦æŸæƒé‡: Î»={constrained_lambda_weight:.3f}")
            logger.info(f"   å¹³æ»‘æƒé‡(ç³»æ•°{dynamic_smoothing:.2f}): Î»={smoothed_lambda_weight:.3f}")

            return smoothed_lambda_weight

        else:
            logger.warning("æ— æ³•è®¡ç®—æ€§èƒ½æŒ‡æ ‡ï¼Œä½¿ç”¨å½“å‰æƒé‡")
            return self.current_lambda_weight

    def _apply_copula_normalization(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åº”ç”¨Copulaæ­£æ€åŒ–ï¼šç§©ç™¾åˆ†ä½ â†’ æ­£æ€åˆ†ä½æ•°

        Args:
            df: åŒ…å«é¢„æµ‹åˆ†æ•°çš„DataFrame

        Returns:
            æ·»åŠ æ­£æ€åŒ–åˆ—çš„DataFrame
        """
        logger.info("ğŸ”„ åº”ç”¨Copulaæ­£æ€åŒ–...")

        df_norm = df.copy()

        def _copula_transform_by_date(group):
            """æŒ‰æ—¥æœŸç»„å†…è¿›è¡ŒCopulaæ­£æ€åŒ–"""
            for col in ['ridge_score', 'lambda_score']:
                if col in group.columns:
                    scores = group[col].dropna()
                    if len(scores) > 1:
                        # è®¡ç®—ç§©ç™¾åˆ†ä½
                        ranks_pct = scores.rank(pct=True)
                        # é¿å…æå€¼ï¼ˆ0å’Œ1ï¼‰
                        ranks_pct = np.clip(ranks_pct, 1e-6, 1-1e-6)
                        # æ­£æ€é€†å˜æ¢
                        norm_scores = ndtri(ranks_pct)
                        # æ˜ å°„å›åŸç´¢å¼•
                        full_norm = pd.Series(0.0, index=group.index)
                        full_norm.loc[scores.index] = norm_scores
                        group[col.replace('score', 'norm')] = full_norm
                    else:
                        group[col.replace('score', 'norm')] = 0.0
            return group

        df_norm = df_norm.groupby(level='date', group_keys=False).apply(_copula_transform_by_date)

        logger.info("âœ… Copulaæ­£æ€åŒ–å®Œæˆ")
        return df_norm

    def _apply_zscore_normalization(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åº”ç”¨æ™®é€šz-scoreæ ‡å‡†åŒ–

        Args:
            df: åŒ…å«é¢„æµ‹åˆ†æ•°çš„DataFrame

        Returns:
            æ·»åŠ æ ‡å‡†åŒ–åˆ—çš„DataFrame
        """
        logger.info("ğŸ”„ åº”ç”¨Z-scoreæ ‡å‡†åŒ–...")

        df_norm = df.copy()

        def _zscore_by_date(group):
            """æŒ‰æ—¥æœŸç»„å†…è¿›è¡Œz-scoreæ ‡å‡†åŒ–"""
            for col in ['ridge_score', 'lambda_score']:
                if col in group.columns:
                    scores = group[col]
                    if len(scores) > 1:
                        z_scores = (scores - scores.mean()) / (scores.std() + 1e-8)
                        group[col.replace('score', 'z')] = z_scores
                    else:
                        group[col.replace('score', 'z')] = 0.0
            return group

        df_norm = df_norm.groupby(level='date', group_keys=False).apply(_zscore_by_date)

        logger.info("âœ… Z-scoreæ ‡å‡†åŒ–å®Œæˆ")
        return df_norm

    def _apply_decorrelation(self, df: pd.DataFrame, ridge_col: str, lambda_col: str) -> pd.DataFrame:
        """
        åº”ç”¨æ™ºèƒ½å»ç›¸å…³èåˆ - ä¿ç•™æœ‰ä»·å€¼çš„ç›¸å…³æ€§ï¼Œç§»é™¤å†—ä½™å…±æŒ¯

        æ ¸å¿ƒæ”¹è¿›ï¼š
        1. éƒ¨åˆ†å»ç›¸å…³ï¼šä¿ç•™30-40%æœ‰ç›Šç›¸å…³æ€§
        2. è‡ªé€‚åº”è°ƒæ•´å»ç›¸å…³å¼ºåº¦
        3. ä½¿ç”¨è½¯é˜ˆå€¼è€Œéç¡¬æ¶ˆé™¤
        4. ä¿æŠ¤æå€¼ä¿¡å·ä¸è¢«è¿‡åº¦ä¿®æ­£

        Args:
            df: åŒ…å«æ ‡å‡†åŒ–åˆ†æ•°çš„DataFrame
            ridge_col: Ridgeåˆ†æ•°åˆ—å
            lambda_col: LambdaRankåˆ†æ•°åˆ—å

        Returns:
            æ·»åŠ æ­£äº¤åŒ–åˆ—çš„DataFrame
        """
        logger.info("ğŸ”§ åº”ç”¨æ™ºèƒ½å»ç›¸å…³èåˆ...")

        df_ortho = df.copy()

        # ç›®æ ‡ç›¸å…³æ€§èŒƒå›´ï¼ˆä¿ç•™é€‚åº¦ç›¸å…³æ€§ï¼‰
        target_corr_range = (0.15, 0.35)  # ä¿ç•™15%-35%çš„ç›¸å…³æ€§
        decorr_strength = 0.7  # å»ç›¸å…³å¼ºåº¦ï¼ˆ0=ä¸å»ç›¸å…³, 1=å®Œå…¨å»ç›¸å…³ï¼‰

        def _decorrelate_by_date(group):
            """æŒ‰æ—¥æœŸç»„å†…æ™ºèƒ½å»ç›¸å…³"""
            z_ridge = group[ridge_col]
            z_lambda = group[lambda_col]

            # æ£€æŸ¥æœ‰æ•ˆæ•°æ®
            valid_mask = z_ridge.notna() & z_lambda.notna()
            if valid_mask.sum() < 10:  # éœ€è¦è¶³å¤Ÿæ ·æœ¬æ‰å»ç›¸å…³
                # æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨åŸå§‹ä¿¡å·
                group[lambda_col + '_ortho'] = z_lambda
                return group

            z_ridge_valid = z_ridge[valid_mask]
            z_lambda_valid = z_lambda[valid_mask]

            try:
                # è®¡ç®—åŸå§‹ç›¸å…³æ€§
                original_corr = z_ridge_valid.corr(z_lambda_valid)

                # åˆ¤æ–­æ˜¯å¦éœ€è¦å»ç›¸å…³
                if abs(original_corr) < 0.5:  # ç›¸å…³æ€§ä¸é«˜æ—¶ä¸éœ€è¦å¤„ç†
                    group[lambda_col + '_ortho'] = z_lambda
                    return group

                # OLSå›å½’ï¼šz_L = Î² * z_R + Îµ
                cov_lr = (z_ridge_valid * z_lambda_valid).mean() - z_ridge_valid.mean() * z_lambda_valid.mean()
                var_r = ((z_ridge_valid - z_ridge_valid.mean()) ** 2).mean()

                if var_r > 1e-8:
                    beta = cov_lr / var_r

                    # è‡ªé€‚åº”è°ƒæ•´å»ç›¸å…³å¼ºåº¦
                    if abs(original_corr) > 0.8:  # é«˜åº¦ç›¸å…³æ—¶å¢å¼ºå»ç›¸å…³
                        adjusted_strength = min(1.0, decorr_strength * 1.2)
                    elif abs(original_corr) > 0.6:
                        adjusted_strength = decorr_strength
                    else:
                        adjusted_strength = decorr_strength * 0.6  # ä¸­åº¦ç›¸å…³æ—¶å‡å¼±å»ç›¸å…³

                    # éƒ¨åˆ†å»ç›¸å…³ï¼šä¿ç•™ä¸€å®šæ¯”ä¾‹çš„å…±åŒä¿¡å·
                    beta_adjusted = beta * adjusted_strength
                else:
                    beta_adjusted = 0.0

                # è®¡ç®—è½¯å»ç›¸å…³ä¿¡å·
                z_lambda_ortho = z_lambda - beta_adjusted * z_ridge

                # ä¿æŠ¤æå€¼ä¿¡å·ï¼ˆtop/bottom 5%ä¸è¿‡åº¦ä¿®æ­£ï¼‰
                extreme_mask = (abs(z_lambda) > np.percentile(abs(z_lambda_valid), 95))
                if extreme_mask.any():
                    # æå€¼ä½ç½®ä½¿ç”¨è¾ƒå¼±çš„å»ç›¸å…³
                    z_lambda_ortho[extreme_mask] = z_lambda[extreme_mask] - 0.3 * beta_adjusted * z_ridge[extreme_mask]

                group[lambda_col + '_ortho'] = z_lambda_ortho

                # è®°å½•å»ç›¸å…³æ•ˆæœ
                if len(z_ridge_valid) > 1:
                    z_lambda_ortho_valid = z_lambda_ortho[valid_mask]
                    new_corr = z_ridge_valid.corr(z_lambda_ortho_valid) if z_ridge_valid.std() > 0 and z_lambda_ortho_valid.std() > 0 else 0.0

                    # ç¡®ä¿ç›¸å…³æ€§åœ¨ç›®æ ‡èŒƒå›´å†…
                    if abs(new_corr) < target_corr_range[0] and abs(original_corr) > 0.3:
                        # ç›¸å…³æ€§è¿‡ä½ï¼Œå‡å¼±å»ç›¸å…³
                        correction_factor = 0.5
                        z_lambda_ortho = z_lambda - beta_adjusted * correction_factor * z_ridge
                        group[lambda_col + '_ortho'] = z_lambda_ortho

                    # å­˜å‚¨è°ƒè¯•ä¿¡æ¯
                    group._decorr_info = {
                        'beta': beta_adjusted,
                        'original_corr': original_corr,
                        'ortho_corr': new_corr,
                        'strength': adjusted_strength,
                        'n_valid': len(z_ridge_valid)
                    }

            except Exception as e:
                # å›å½’å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ä¿¡å·
                logger.debug(f"å»ç›¸å…³å¤„ç†è·³è¿‡: {e}")
                group[lambda_col + '_ortho'] = z_lambda

            return group

        # æŒ‰æ—¥æœŸåˆ†ç»„æ‰§è¡Œå»ç›¸å…³
        df_ortho = df_ortho.groupby(level='date', group_keys=False).apply(_decorrelate_by_date)

        # ç»Ÿè®¡å»ç›¸å…³æ•ˆæœ
        try:
            # è®¡ç®—å…¨å±€ç›¸å…³æ€§å˜åŒ–
            z_ridge_all = df_ortho[ridge_col].dropna()
            z_lambda_all = df_ortho[lambda_col].dropna()
            z_lambda_ortho_all = df_ortho[lambda_col + '_ortho'].dropna()

            if len(z_ridge_all) > 1 and len(z_lambda_all) > 1:
                # è®¡ç®—å¯¹é½çš„æ•°æ®
                common_idx = z_ridge_all.index.intersection(z_lambda_all.index).intersection(z_lambda_ortho_all.index)
                if len(common_idx) > 1:
                    z_r = z_ridge_all.reindex(common_idx)
                    z_l = z_lambda_all.reindex(common_idx)
                    z_l_ortho = z_lambda_ortho_all.reindex(common_idx)

                    original_corr = z_r.corr(z_l) if z_r.std() > 0 and z_l.std() > 0 else 0.0
                    ortho_corr = z_r.corr(z_l_ortho) if z_r.std() > 0 and z_l_ortho.std() > 0 else 0.0

                    logger.info(f"   åŸå§‹ç›¸å…³æ€§: {original_corr:.4f}")
                    logger.info(f"   è°ƒæ•´åç›¸å…³æ€§: {ortho_corr:.4f} (ç›®æ ‡èŒƒå›´: 0.15-0.35)")
                    logger.info(f"   å»ç›¸å…³é™å¹…: {abs(original_corr - ortho_corr):.4f}")

                    # éªŒè¯æ˜¯å¦è¾¾åˆ°ç†æƒ³æ•ˆæœ
                    if abs(ortho_corr) < 0.05:
                        logger.warning("   âš ï¸ ç›¸å…³æ€§è¿‡ä½ï¼Œå¯èƒ½æŸå¤±ååŒä¿¡å·")
                    elif abs(ortho_corr) > 0.5:
                        logger.warning("   âš ï¸ ç›¸å…³æ€§ä»è¾ƒé«˜ï¼Œå»ç›¸å…³æ•ˆæœæœ‰é™")

        except Exception as e:
            logger.warning(f"å»ç›¸å…³ç»Ÿè®¡å¤±è´¥: {e}")

        logger.info("âœ… æ™ºèƒ½å»ç›¸å…³å®Œæˆ")
        return df_ortho

    def _apply_gated_blending(self, df: pd.DataFrame, ridge_col: str, lambda_col: str,
                             ridge_weight: float, lambda_weight: float) -> pd.Series:
        """é—¨æ§+æ®‹å·®å¾®èåˆï¼šä¿ç•™ååŒçº¢åˆ©ï¼Œé¿å…æ’åº-å¹…åº¦é”™é…"""

        def _gate_with_residual_fusion(group):
            ridge_scores = group[ridge_col]
            lambda_scores = group[lambda_col]
            n_samples = len(ridge_scores)

            if n_samples < 10:  # æ ·æœ¬å¤ªå°‘æ—¶å›é€€åˆ°ç®€å•ç»„åˆ
                return ridge_weight * ridge_scores + lambda_weight * lambda_scores

            # Step 1: æŒ‰æ—¥æ ‡å‡†åŒ–å¾—åˆ° z_ridge, z_lambda
            z_ridge = (ridge_scores - ridge_scores.mean()) / (ridge_scores.std() + 1e-8)
            z_lambda = (lambda_scores - lambda_scores.mean()) / (lambda_scores.std() + 1e-8)

            # Step 2: è®¡ç®— lambda_pct (LambdaRank çš„ç™¾åˆ†ä½æ’å)
            lambda_pct = lambda_scores.rank(method='average') / n_samples

            # Step 3: é—¨æ§åˆ¤æ–­ - åŸºäº lambda_pct é˜ˆå€¼
            tau_long = getattr(self, 'tau_long', 0.65)
            tau_short = getattr(self, 'tau_short', 0.35)
            alpha_long = getattr(self, 'alpha_long', 0.15)
            alpha_short = getattr(self, 'alpha_short', 0.15)
            max_gain = getattr(self, 'max_gain', 1.25)

            # è®¡ç®—é—¨æ§å’Œå¢ç›Š
            gate = np.zeros(n_samples, dtype=float)
            gain = np.ones(n_samples, dtype=float)

            # é•¿ä¾§é—¨æ§ä¸å¢ç›Š
            long_mask = lambda_pct >= tau_long
            if long_mask.any():
                gate[long_mask] = 1.0
                long_gain_factor = (lambda_pct[long_mask] - tau_long) / (1 - tau_long)
                gain[long_mask] = 1.0 + alpha_long * long_gain_factor
                gain[long_mask] = np.clip(gain[long_mask], 1.0, max_gain)

            # çŸ­ä¾§é—¨æ§ä¸å¢ç›Š
            short_mask = lambda_pct <= tau_short
            if short_mask.any():
                gate[short_mask] = 1.0
                short_gain_factor = (tau_short - lambda_pct[short_mask]) / tau_short
                gain[short_mask] = 1.0 + alpha_short * short_gain_factor
                gain[short_mask] = np.clip(gain[short_mask], 1.0, max_gain)

            # Step 4: è®¡ç®—è¦†ç›–ç‡å¹¶æ£€æŸ¥è§¦å‘æ¡ä»¶
            coverage = gate.sum() / n_samples
            min_coverage = getattr(self, 'min_coverage', 0.30)

            # Step 5: å»ç›¸å…³æ®‹å·®è®¡ç®—ï¼ˆä»…åœ¨é—¨å†…æ ·æœ¬ä¸Šï¼‰
            gated_mask = gate > 0
            if coverage >= min_coverage and gated_mask.any():
                # ä½¿ç”¨é—¨å†…æ ·æœ¬è®¡ç®—å»ç›¸å…³å›å½’
                z_ridge_gated = z_ridge[gated_mask]
                z_lambda_gated = z_lambda[gated_mask]

                if len(z_ridge_gated) > 5 and z_ridge_gated.std() > 1e-6:
                    # è®¡ç®—å›å½’ç³»æ•° Î²_reg = Cov(z_Î», z_r) / Var(z_r)
                    cov_lr = np.cov(z_lambda_gated, z_ridge_gated)[0, 1]
                    var_r = np.var(z_ridge_gated)
                    beta_reg = cov_lr / (var_r + 1e-8)

                    # è®¡ç®—å»ç›¸å…³æ®‹å·®ï¼šz_Î»âŠ¥ = z_lambda - Î²_reg * z_ridge
                    z_lambda_ortho = z_lambda - beta_reg * z_ridge
                else:
                    z_lambda_ortho = z_lambda.copy()
            else:
                z_lambda_ortho = z_lambda.copy()

            # Step 6: æ®‹å·®å¾®èåˆå‚æ•° Î² ç®¡ç†
            current_beta = getattr(self, 'current_beta', 0.08)
            beta_range = getattr(self, 'beta_range', [0.0, 0.15])

            # è§¦å‘æ¡ä»¶æ£€æŸ¥
            enable_micro_fusion = self._check_micro_fusion_trigger(coverage)

            if enable_micro_fusion:
                # å¯ç”¨å¾®èåˆæ—¶ï¼Œæ ¹æ®æ€§èƒ½è°ƒæ•´Î²
                new_beta = self._update_beta_with_ewma(current_beta)
            else:
                # é™çº§åˆ°çº¯é—¨æ§ï¼ŒÎ² æŒ‡æ•°è¡°å‡è‡³0
                ewma_alpha = getattr(self, 'beta_ewma_alpha', 0.3)
                new_beta = current_beta * (1 - ewma_alpha)  # EWMAè¡°å‡
                new_beta = max(0.0, new_beta)

            # ç¡®ä¿Î²åœ¨å…è®¸èŒƒå›´å†…
            new_beta = np.clip(new_beta, beta_range[0], beta_range[1])
            setattr(self, 'current_beta', new_beta)

            # è®°å½•Î²å†å²ï¼ˆç”¨äºç›‘æ§ï¼‰
            if not hasattr(self, 'beta_history'):
                setattr(self, 'beta_history', [])
            self.beta_history.append(new_beta)
            if len(self.beta_history) > 100:
                self.beta_history = self.beta_history[-100:]

            current_beta = new_beta

            # Step 7: è®¡ç®—æœ€ç»ˆåˆ†æ•°
            # score = z_ridge Ã— gain(lambda_pct) Ã— gate(lambda_pct) Ã— (1 + Î² Ã— clip(z_Î»âŠ¥, p2, p98))

            # è®¡ç®—æ®‹å·®è£å‰ªé˜ˆå€¼ (p2, p98)
            p2, p98 = np.percentile(z_lambda_ortho[gated_mask], [2, 98]) if gated_mask.any() else [-2.0, 2.0]
            z_lambda_ortho_clipped = np.clip(z_lambda_ortho, p2, p98)

            # æ®‹å·®å¾®èåˆé¡¹
            residual_tilt = 1.0 + current_beta * z_lambda_ortho_clipped

            # åŸºç¡€é—¨æ§åˆ†æ•°
            base_score = z_ridge * gain * gate

            # ä»…å¯¹é—¨å†…æ ·æœ¬åº”ç”¨æ®‹å·®å¾®èåˆ
            final_score = base_score.copy()
            if gated_mask.any():
                final_score[gated_mask] = base_score[gated_mask] * residual_tilt[gated_mask]

            # Step 8: æ–¹å‘çº¦æŸ - ç¡®ä¿ä¸ç¿»æ–¹å‘
            sign_flips = (np.sign(final_score) != np.sign(z_ridge)) & (np.abs(z_ridge) > 1e-6)
            if sign_flips.any():
                # å‘ç”Ÿç¿»æ–¹å‘æ—¶ï¼Œå›é€€åˆ°åŸå§‹é—¨æ§åˆ†æ•°
                final_score[sign_flips] = base_score[sign_flips]

            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            flip_ratio = sign_flips.sum() / n_samples if n_samples > 0 else 0.0
            if not hasattr(self, '_flip_ratio_history'):
                self._flip_ratio_history = []
            self._flip_ratio_history.append(flip_ratio)
            if len(self._flip_ratio_history) > 100:
                self._flip_ratio_history = self._flip_ratio_history[-100:]

            return pd.Series(final_score, index=ridge_scores.index)

        # æŒ‰æ—¥æœŸåˆ†ç»„åº”ç”¨é—¨æ§+æ®‹å·®å¾®èåˆ
        return df.groupby(level='date', group_keys=False).apply(_gate_with_residual_fusion)

    def _check_micro_fusion_trigger(self, coverage: float) -> bool:
        """æ£€æŸ¥æ®‹å·®å¾®èåˆè§¦å‘æ¡ä»¶"""

        # æ¡ä»¶1ï¼šè¦†ç›–ç‡æ»¡è¶³æœ€å°è¦æ±‚
        min_coverage = getattr(self, 'min_coverage', 0.30)
        if coverage < min_coverage:
            return False

        # æ¡ä»¶2ï¼šæœ€è¿‘çª—å£NDCGæœ‰æå‡ï¼ˆç®€åŒ–ç‰ˆæœ¬æ£€æŸ¥ï¼‰
        ndcg_history = getattr(self, '_ndcg_history', [])
        if len(ndcg_history) >= 5:
            # è®¡ç®—æœ€è¿‘5æ¬¡çš„NDCGè¶‹åŠ¿
            recent_ndcg = ndcg_history[-5:]
            if len(recent_ndcg) >= 3:
                # ç®€å•è¶‹åŠ¿æ£€æŸ¥ï¼šæœ€è¿‘3æ¬¡çš„å¹³å‡æ˜¯å¦ä¼˜äºå‰é¢
                recent_avg = np.mean(recent_ndcg[-3:])
                earlier_avg = np.mean(recent_ndcg[:-3]) if len(recent_ndcg) > 3 else recent_avg
                performance_improved = recent_avg >= earlier_avg
            else:
                performance_improved = True  # æ•°æ®ä¸è¶³æ—¶ä¿å®ˆå¯ç”¨
        else:
            performance_improved = True  # åˆå§‹é˜¶æ®µé»˜è®¤å¯ç”¨

        # è®°å½•æ€§èƒ½æ”¹å–„çŠ¶æ€
        setattr(self, '_recent_performance_improved', performance_improved)

        return performance_improved

    def _update_beta_with_ewma(self, current_beta: float) -> float:
        """ä½¿ç”¨EWMAå¹³æ»‘æ›´æ–°Î²å‚æ•°"""

        ewma_alpha = getattr(self, 'beta_ewma_alpha', 0.3)
        beta_range = getattr(self, 'beta_range', [0.0, 0.15])

        # åŸºäºæ€§èƒ½æŒ‡æ ‡è°ƒæ•´Î²çš„ç›®æ ‡å€¼
        performance_improved = getattr(self, '_recent_performance_improved', True)
        flip_ratio_history = getattr(self, '_flip_ratio_history', [])

        # è®¡ç®—ç›®æ ‡Î²
        if performance_improved:
            # æ€§èƒ½æ”¹å–„æ—¶ï¼Œç•¥å¾®å¢åŠ Î²ä»¥è·å¾—æ›´å¤šLTRçº¢åˆ©
            recent_flip_ratio = np.mean(flip_ratio_history[-10:]) if len(flip_ratio_history) >= 10 else 0.0

            if recent_flip_ratio < 0.05:  # ç¿»æ–¹å‘æ¯”ä¾‹å¾ˆä½ï¼Œå¯ä»¥å¢åŠ Î²
                target_beta = min(beta_range[1], current_beta * 1.05)
            elif recent_flip_ratio < 0.1:  # ç¿»æ–¹å‘æ¯”ä¾‹å¯æ¥å—ï¼Œä¿æŒÎ²
                target_beta = current_beta
            else:  # ç¿»æ–¹å‘æ¯”ä¾‹è¿‡é«˜ï¼Œå‡å°‘Î²
                target_beta = max(beta_range[0], current_beta * 0.95)
        else:
            # æ€§èƒ½æœªæ”¹å–„æ—¶ï¼Œä¿å®ˆå‡å°‘Î²
            target_beta = max(beta_range[0], current_beta * 0.9)

        # EWMAå¹³æ»‘æ›´æ–°
        new_beta = (1 - ewma_alpha) * current_beta + ewma_alpha * target_beta

        # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
        new_beta = np.clip(new_beta, beta_range[0], beta_range[1])

        return new_beta

    def update_performance_metrics(self, ridge_predictions: pd.DataFrame,
                                 lambda_predictions: pd.DataFrame,
                                 actual_returns: pd.DataFrame) -> None:
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡ç”¨äºè§¦å‘æ¡ä»¶åˆ¤æ–­"""

        try:
            # è®¡ç®—NDCG@KæŒ‡æ ‡
            for k in self.top_k_list:
                if k <= len(actual_returns):
                    # ç®€åŒ–çš„NDCGè®¡ç®—ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å®Œæ•´çš„å®ç°ï¼‰
                    ndcg_score = self._calculate_ndcg_k(ridge_predictions, actual_returns, k)

                    if not hasattr(self, '_ndcg_history'):
                        self._ndcg_history = []
                    self._ndcg_history.append(ndcg_score)
                    if len(self._ndcg_history) > 50:
                        self._ndcg_history = self._ndcg_history[-50:]
                    break  # åªä½¿ç”¨ç¬¬ä¸€ä¸ªå¯è®¡ç®—çš„Kå€¼

        except Exception as e:
            logger.warning(f"æ›´æ–°æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")

    def _calculate_ndcg_k(self, predictions: pd.DataFrame,
                         actual_returns: pd.DataFrame, k: int) -> float:
        """è®¡ç®—NDCG@KæŒ‡æ ‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""

        try:
            # ç¡®ä¿æ•°æ®å¯¹é½
            common_index = predictions.index.intersection(actual_returns.index)
            if len(common_index) < k:
                return 0.0

            pred_aligned = predictions.loc[common_index].squeeze()
            ret_aligned = actual_returns.loc[common_index].squeeze()

            # æŒ‰é¢„æµ‹æ’åºï¼Œå–top-k
            sorted_indices = pred_aligned.argsort()[::-1][:k]
            top_k_returns = ret_aligned.iloc[sorted_indices]

            # è®¡ç®—DCG
            dcg = np.sum(top_k_returns.values / np.log2(np.arange(2, k + 2)))

            # è®¡ç®—IDCGï¼ˆç†æƒ³æƒ…å†µï¼‰
            ideal_returns = ret_aligned.sort_values(ascending=False)[:k]
            idcg = np.sum(ideal_returns.values / np.log2(np.arange(2, k + 2)))

            # è®¡ç®—NDCG
            ndcg = dcg / idcg if idcg > 0 else 0.0

            return ndcg

        except Exception as e:
            logger.warning(f"NDCGè®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def get_weight_history(self) -> pd.DataFrame:
        """è·å–æƒé‡å†å²"""
        if not self.weight_history:
            return pd.DataFrame()

        return pd.DataFrame(self.weight_history)

    def _calculate_blend_statistics(self, df: pd.DataFrame) -> Dict[str, float]:
        """è®¡ç®—èåˆç»Ÿè®¡ä¿¡æ¯"""
        stats = {}

        if 'blended_score' in df.columns:
            stats['mean'] = df['blended_score'].mean()
            stats['std'] = df['blended_score'].std()
            stats['skew'] = df['blended_score'].skew()
            stats['kurt'] = df['blended_score'].kurtosis()

        # è®¡ç®—Ridgeå’ŒLambdaä¿¡å·ä¸€è‡´æ€§
        if 'ridge_score' in df.columns and 'lambda_score' in df.columns:
            # æ­£ç›¸å…³ç‡ï¼šä¸¤ä¸ªä¿¡å·åŒå‘çš„æ¯”ä¾‹
            same_sign = (np.sign(df['ridge_score']) == np.sign(df['lambda_score']))
            stats['agreement_rate'] = same_sign.mean()

            # Top-Ké‡å ç‡
            k = 100
            if len(df) >= k:
                ridge_top_k = set(df.nlargest(k, 'ridge_score').index)
                lambda_top_k = set(df.nlargest(k, 'lambda_score').index)
                overlap = len(ridge_top_k.intersection(lambda_top_k))
                stats['top_k_overlap'] = overlap / k

        return stats

    def _apply_advanced_blending(self, df: pd.DataFrame, ridge_col: str, lambda_col: str,
                                ridge_weight: float, lambda_weight: float) -> pd.Series:
        """é«˜çº§èåˆç­–ç•¥ï¼šç»“åˆåŠ¨æ€æƒé‡ã€é—¨æ§å’Œéçº¿æ€§èåˆ"""

        def _advanced_blend_by_date(group):
            ridge_scores = group[ridge_col]
            lambda_scores = group[lambda_col]
            n_samples = len(ridge_scores)

            if n_samples < 10:
                # æ ·æœ¬å¤ªå°‘ï¼Œä½¿ç”¨ç®€å•çº¿æ€§ç»„åˆ
                return ridge_weight * ridge_scores + lambda_weight * lambda_scores

            # 1. è®¡ç®—åˆ†ä½æ•°
            ridge_pct = ridge_scores.rank(pct=True)
            lambda_pct = lambda_scores.rank(pct=True)

            # 2. åŠ¨æ€æƒé‡è°ƒæ•´ï¼šæç«¯ä½ç½®ä½¿ç”¨ä¸åŒæƒé‡
            dynamic_weights = np.ones(n_samples)

            # æç«¯å¤šå¤´ï¼ˆtop 5%ï¼‰ï¼šLambdaæƒé‡æ›´é«˜ï¼ˆæ’åºæ›´å‡†ï¼‰
            extreme_long = (ridge_pct > 0.95) | (lambda_pct > 0.95)
            dynamic_weights[extreme_long] = lambda_weight + 0.1

            # æç«¯ç©ºå¤´ï¼ˆbottom 5%ï¼‰ï¼šRidgeæƒé‡æ›´é«˜ï¼ˆå¹…åº¦æ›´å‡†ï¼‰
            extreme_short = (ridge_pct < 0.05) | (lambda_pct < 0.05)
            dynamic_weights[extreme_short] = ridge_weight + 0.1

            # ä¸­é—´åŒºåŸŸï¼šæ ‡å‡†æƒé‡
            middle_zone = ~(extreme_long | extreme_short)
            dynamic_weights[middle_zone] = lambda_weight

            # å½’ä¸€åŒ–æƒé‡
            dynamic_weights = np.clip(dynamic_weights, 0, 1)
            ridge_dynamic = 1 - dynamic_weights

            # 3. éçº¿æ€§èåˆï¼šè€ƒè™‘ä¿¡å·ä¸€è‡´æ€§
            signal_agreement = np.sign(ridge_scores) == np.sign(lambda_scores)

            # ä¿¡å·ä¸€è‡´æ—¶åŠ å¼ºï¼Œä¸ä¸€è‡´æ—¶å‡å¼±
            boost_factor = np.where(signal_agreement, 1.1, 0.9)

            # æœ€ç»ˆèåˆ
            blended = (
                ridge_dynamic * ridge_scores +
                dynamic_weights * lambda_scores
            ) * boost_factor

            return pd.Series(blended, index=ridge_scores.index)

        # æŒ‰æ—¥æœŸåˆ†ç»„åº”ç”¨é«˜çº§èåˆ
        return df.groupby(level='date', group_keys=False).apply(_advanced_blend_by_date)

    def get_blender_info(self) -> Dict[str, Any]:
        """è·å–èåˆå™¨ä¿¡æ¯"""
        return {
            'lookback_window': self.lookback_window,
            'current_lambda_weight': self.current_lambda_weight,
            'current_ridge_weight': 1.0 - self.current_lambda_weight,
            'weight_constraints': [self.min_weight, self.max_weight],
            'use_copula': self.use_copula,
            'weight_smoothing': self.weight_smoothing,
            'n_weight_records': len(self.weight_history),
            # é—¨æ§+æ®‹å·®å¾®èåˆå‚æ•°
            'gate_params': {
                'tau_long': getattr(self, 'tau_long', 0.65),
                'tau_short': getattr(self, 'tau_short', 0.35),
                'alpha_long': getattr(self, 'alpha_long', 0.15),
                'alpha_short': getattr(self, 'alpha_short', 0.15),
                'max_gain': getattr(self, 'max_gain', 1.25),
                'min_coverage': getattr(self, 'min_coverage', 0.30)
            },
            'residual_fusion': {
                'current_beta': getattr(self, 'current_beta', 0.08),
                'beta_range': getattr(self, 'beta_range', [0.0, 0.15]),
                'beta_ewma_alpha': getattr(self, 'beta_ewma_alpha', 0.3),
                'recent_performance_improved': getattr(self, '_recent_performance_improved', True)
            },
            'performance_stats': self._get_performance_stats()
        }

    def _get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""

        flip_ratio_history = getattr(self, '_flip_ratio_history', [])
        coverage_history = getattr(self, '_coverage_history', [])
        ndcg_history = getattr(self, '_ndcg_history', [])
        beta_history = getattr(self, 'beta_history', [])

        stats = {}

        # æ–¹å‘ç¿»è½¬ç»Ÿè®¡
        if flip_ratio_history:
            stats['flip_ratio'] = {
                'recent_mean': np.mean(flip_ratio_history[-10:]) if len(flip_ratio_history) >= 10 else np.mean(flip_ratio_history),
                'overall_mean': np.mean(flip_ratio_history),
                'max': np.max(flip_ratio_history),
                'count': len(flip_ratio_history)
            }

        # è¦†ç›–ç‡ç»Ÿè®¡
        if coverage_history:
            stats['coverage'] = {
                'recent_mean': np.mean(coverage_history[-10:]) if len(coverage_history) >= 10 else np.mean(coverage_history),
                'overall_mean': np.mean(coverage_history),
                'min': np.min(coverage_history),
                'count': len(coverage_history)
            }

        # NDCGç»Ÿè®¡
        if ndcg_history:
            stats['ndcg'] = {
                'recent_mean': np.mean(ndcg_history[-10:]) if len(ndcg_history) >= 10 else np.mean(ndcg_history),
                'overall_mean': np.mean(ndcg_history),
                'trend': 'improving' if len(ndcg_history) >= 5 and np.mean(ndcg_history[-3:]) > np.mean(ndcg_history[-5:-3]) else 'stable',
                'count': len(ndcg_history)
            }

        # Î²å‚æ•°ç»Ÿè®¡
        if beta_history:
            stats['beta'] = {
                'current': beta_history[-1] if beta_history else 0.0,
                'recent_mean': np.mean(beta_history[-10:]) if len(beta_history) >= 10 else np.mean(beta_history),
                'trend': 'increasing' if len(beta_history) >= 5 and beta_history[-1] > np.mean(beta_history[-5:-1]) else 'stable',
                'count': len(beta_history)
            }

        return stats

    def calculate_acceptance_metrics(self, oof_predictions: pd.DataFrame,
                                   online_ridge_predictions: pd.DataFrame,
                                   actual_returns: pd.DataFrame,
                                   top_k_list: list = None) -> Dict[str, float]:
        """è®¡ç®—éªŒæ”¶æŒ‡æ ‡"""

        if top_k_list is None:
            top_k_list = self.top_k_list

        metrics = {}

        try:
            # 1. Top-K å‘½ä¸­ç‡å’ŒNDCGæå‡
            for k in top_k_list:
                if k <= len(actual_returns):
                    # è®¡ç®—OOFé¢„æµ‹çš„NDCG@K
                    oof_ndcg = self._calculate_ndcg_k(oof_predictions, actual_returns, k)
                    # è®¡ç®—åœ¨çº¿Ridgeé¢„æµ‹çš„NDCG@K
                    ridge_ndcg = self._calculate_ndcg_k(online_ridge_predictions, actual_returns, k)

                    metrics[f'ndcg@{k}_oof'] = oof_ndcg
                    metrics[f'ndcg@{k}_ridge'] = ridge_ndcg
                    metrics[f'ndcg@{k}_improvement'] = oof_ndcg - ridge_ndcg

                    # è®¡ç®—Top-Kå‘½ä¸­ç‡
                    top_k_hit_rate = self._calculate_top_k_hit_rate(oof_predictions, actual_returns, k)
                    metrics[f'top{k}_hit_rate'] = top_k_hit_rate

            # 2. OOS Information Ratio (ç®€åŒ–ç‰ˆæœ¬)
            if len(oof_predictions) > 20:
                oos_ir = self._calculate_oos_ir(oof_predictions, actual_returns)
                metrics['oos_ir'] = oos_ir

            # 3. KSæ£€éªŒï¼ˆOOF vs çº¿ä¸ŠRidgeè¾“å…¥ï¼‰
            ks_stat = self._calculate_ks_test(oof_predictions, online_ridge_predictions)
            metrics['ks_statistic'] = ks_stat

            # 4. æ–¹å‘ç¿»è½¬æ¯”ä¾‹
            flip_ratio_history = getattr(self, '_flip_ratio_history', [])
            if flip_ratio_history:
                metrics['flip_ratio'] = np.mean(flip_ratio_history[-10:]) if len(flip_ratio_history) >= 10 else np.mean(flip_ratio_history)

            # 5. è¦†ç›–ç‡ç»Ÿè®¡
            coverage_history = getattr(self, '_coverage_history', [])
            if coverage_history:
                metrics['coverage'] = np.mean(coverage_history[-10:]) if len(coverage_history) >= 10 else np.mean(coverage_history)

        except Exception as e:
            logger.error(f"éªŒæ”¶æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")

        return metrics

    def _calculate_top_k_hit_rate(self, predictions: pd.DataFrame,
                                actual_returns: pd.DataFrame, k: int) -> float:
        """è®¡ç®—Top-Kå‘½ä¸­ç‡"""

        try:
            common_index = predictions.index.intersection(actual_returns.index)
            if len(common_index) < k:
                return 0.0

            pred_aligned = predictions.loc[common_index].squeeze()
            ret_aligned = actual_returns.loc[common_index].squeeze()

            # æŒ‰é¢„æµ‹æ’åºï¼Œå–top-k
            top_k_pred_indices = pred_aligned.argsort()[::-1][:k]
            # æŒ‰å®é™…æ”¶ç›Šæ’åºï¼Œå–top-k
            top_k_actual_indices = ret_aligned.argsort()[::-1][:k]

            # è®¡ç®—å‘½ä¸­æ•°é‡
            hits = len(set(top_k_pred_indices).intersection(set(top_k_actual_indices)))
            hit_rate = hits / k

            return hit_rate

        except Exception as e:
            logger.warning(f"Top-Kå‘½ä¸­ç‡è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def _calculate_oos_ir(self, predictions: pd.DataFrame,
                         actual_returns: pd.DataFrame) -> float:
        """è®¡ç®—æ ·æœ¬å¤–ä¿¡æ¯æ¯”ç‡"""

        try:
            common_index = predictions.index.intersection(actual_returns.index)
            if len(common_index) < 20:
                return 0.0

            pred_aligned = predictions.loc[common_index].squeeze()
            ret_aligned = actual_returns.loc[common_index].squeeze()

            # è®¡ç®—IC
            ic = pred_aligned.corr(ret_aligned)

            # è®¡ç®—ICçš„æ ‡å‡†å·®ï¼ˆæ»šåŠ¨çª—å£ï¼‰
            if len(pred_aligned) >= 50:
                rolling_ics = []
                window_size = min(20, len(pred_aligned) // 3)
                for i in range(window_size, len(pred_aligned)):
                    window_pred = pred_aligned.iloc[i-window_size:i]
                    window_ret = ret_aligned.iloc[i-window_size:i]
                    window_ic = window_pred.corr(window_ret)
                    if not np.isnan(window_ic):
                        rolling_ics.append(window_ic)

                if rolling_ics:
                    ic_std = np.std(rolling_ics)
                    ir = ic / (ic_std + 1e-8)
                else:
                    ir = ic / 0.1  # é»˜è®¤åˆ†æ¯
            else:
                ir = ic / 0.1  # æ ·æœ¬ä¸è¶³æ—¶ä½¿ç”¨é»˜è®¤åˆ†æ¯

            return ir

        except Exception as e:
            logger.warning(f"OOS IRè®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def _calculate_ks_test(self, oof_predictions: pd.DataFrame,
                          online_predictions: pd.DataFrame) -> float:
        """è®¡ç®—KSæ£€éªŒç»Ÿè®¡é‡"""

        try:
            from scipy.stats import ks_2samp

            oof_values = oof_predictions.dropna().squeeze().values
            online_values = online_predictions.dropna().squeeze().values

            if len(oof_values) < 10 or len(online_values) < 10:
                return 1.0  # æ•°æ®ä¸è¶³æ—¶è¿”å›æœ€å¤§å€¼

            # è¿›è¡ŒKSåŒæ ·æœ¬æ£€éªŒ
            ks_stat, p_value = ks_2samp(oof_values, online_values)

            return ks_stat

        except Exception as e:
            logger.warning(f"KSæ£€éªŒè®¡ç®—å¤±è´¥: {e}")
            return 1.0

    def get_acceptance_summary(self, metrics: Dict[str, float]) -> Dict[str, str]:
        """ç”ŸæˆéªŒæ”¶ç»“æœæ‘˜è¦"""

        summary = {}

        # NDCGæå‡æ£€æŸ¥
        ndcg_improvements = [v for k, v in metrics.items() if 'ndcg' in k and 'improvement' in k]
        if ndcg_improvements:
            avg_improvement = np.mean(ndcg_improvements)
            summary['ndcg_status'] = 'PASS' if avg_improvement > 0 else 'FAIL'
            summary['ndcg_avg_improvement'] = f"{avg_improvement:.4f}"

        # OOS IRæ£€æŸ¥
        if 'oos_ir' in metrics:
            summary['oos_ir_status'] = 'PASS' if metrics['oos_ir'] > 0.5 else 'FAIL'  # ç®€åŒ–é˜ˆå€¼
            summary['oos_ir_value'] = f"{metrics['oos_ir']:.4f}"

        # KSæ£€éªŒæ£€æŸ¥
        if 'ks_statistic' in metrics:
            summary['ks_status'] = 'PASS' if metrics['ks_statistic'] < 0.1 else 'FAIL'
            summary['ks_value'] = f"{metrics['ks_statistic']:.4f}"

        # æ–¹å‘ç¿»è½¬æ£€æŸ¥
        if 'flip_ratio' in metrics:
            summary['flip_ratio_status'] = 'PASS' if metrics['flip_ratio'] < 0.1 else 'FAIL'
            summary['flip_ratio_value'] = f"{metrics['flip_ratio']:.4f}"

        # è¦†ç›–ç‡æ£€æŸ¥
        if 'coverage' in metrics:
            min_coverage = getattr(self, 'min_coverage', 0.30)
            summary['coverage_status'] = 'PASS' if metrics['coverage'] >= min_coverage else 'FAIL'
            summary['coverage_value'] = f"{metrics['coverage']:.4f}"

        # æ€»ä½“éªŒæ”¶çŠ¶æ€
        all_checks = [v for k, v in summary.items() if k.endswith('_status')]
        summary['overall_status'] = 'PASS' if all([check == 'PASS' for check in all_checks]) else 'FAIL'

        return summary