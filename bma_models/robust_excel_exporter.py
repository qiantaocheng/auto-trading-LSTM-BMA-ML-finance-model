#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Robust Excel Exporter - é˜²å¾¡æ€§Excelå¯¼å‡ºå™¨
ç¡®ä¿ä¸‡æ— ä¸€å¤±çš„Excelå¯¼å‡ºæµç¨‹
"""

import logging
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List

logger = logging.getLogger(__name__)


class RobustExcelExporter:
    """é˜²å¾¡æ€§Excelå¯¼å‡ºå™¨ - ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½èƒ½æ­£ç¡®å¯¼å‡º"""

    def __init__(self, output_dir: str = "D:/trade/result"):
        """
        åˆå§‹åŒ–å¯¼å‡ºå™¨

        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def safe_export(
        self,
        predictions_series: Optional[pd.Series],
        analysis_results: Dict[str, Any],
        feature_data: Optional[pd.DataFrame] = None,
        lambda_df: Optional[pd.DataFrame] = None,
        ridge_df: Optional[pd.DataFrame] = None,
        final_df: Optional[pd.DataFrame] = None,
        kronos_df: Optional[pd.DataFrame] = None,
        lambda_percentile_info: Optional[Dict[str, Any]] = None,
        simple_mode: bool = True  # ğŸ”§ é»˜è®¤åªå¯¼å‡ºFinal_Predictionsä¸€ä¸ªè¡¨
    ) -> Optional[str]:
        """
        å®‰å…¨å¯¼å‡ºExcel

        Args:
            simple_mode: True=åªå¯¼å‡ºFinal_Predictionsè¡¨ï¼ŒFalse=å¯¼å‡ºæ‰€æœ‰è¯¦ç»†è¡¨

        Returns:
            str: Excelæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            logger.info("=" * 80)
            logger.info(f"ğŸ“Š å¼€å§‹Robust Excelå¯¼å‡º (ç®€åŒ–æ¨¡å¼: {simple_mode})")
            logger.info("=" * 80)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"bma_analysis_{timestamp}.xlsx"
            filepath = self.output_dir / filename

            # ========== åˆ›å»ºExcel Writer ==========
            with pd.ExcelWriter(filepath, engine='openpyxl') as writer:

                if simple_mode:
                    # ğŸ”§ å†™å…¥ä¸€ä¸ªæœ€å°å¯è§å·¥ä½œè¡¨ï¼Œé¿å…æ— è¡¨æ—¶ExcelæŠ¥é”™
                    try:
                        pd.DataFrame({'Info': [timestamp]}).to_excel(writer, sheet_name='Info', index=False)
                    except Exception:
                        pass

                    # ğŸ¯ ç®€åŒ–æ¨¡å¼ï¼šä¼˜å…ˆå¯¼å‡ºFinal_Predictions
                    if final_df is not None and not final_df.empty:
                        self._write_final_sheet(writer, final_df)
                    else:
                        # å›é€€ï¼šä½¿ç”¨predictions_series
                        pred_data = self._prepare_predictions(predictions_series)
                        self._write_predictions_sheet(writer, pred_data)
                        logger.warning("Final_Predictionsä¸å¯ç”¨ï¼Œä½¿ç”¨Predictionsè¡¨ä»£æ›¿")
                else:
                    # å®Œæ•´æ¨¡å¼ï¼šå¯¼å‡ºæ‰€æœ‰è¡¨
                    pred_data = self._prepare_predictions(predictions_series)
                    model_info = self._prepare_model_info(analysis_results, feature_data)

                    # Sheet 1: Summary (æ€»è§ˆ)
                    self._write_summary_sheet(writer, pred_data, model_info, lambda_percentile_info)

                    # Sheet 2: Predictions (é¢„æµ‹ç»“æœ)
                    self._write_predictions_sheet(writer, pred_data)

                    # Sheet 3: Lambda Predictions (å¦‚æœæœ‰)
                    if lambda_df is not None and not lambda_df.empty:
                        self._write_lambda_sheet(writer, lambda_df)

                    # Sheet 4: Ridge Predictions (å¦‚æœæœ‰)
                    if ridge_df is not None and not ridge_df.empty:
                        self._write_ridge_sheet(writer, ridge_df)

                    # Sheet 5: Final Predictions (å¦‚æœæœ‰)
                    if final_df is not None and not final_df.empty:
                        self._write_final_sheet(writer, final_df)

                    # Sheet 6: Kronos Filter (å¦‚æœæœ‰)
                    if kronos_df is not None and not kronos_df.empty:
                        self._write_kronos_sheet(writer, kronos_df)

                    # Sheet 7: Factor Contributions
                    self._write_factor_contributions_sheet(writer, model_info)

            logger.info("=" * 80)
            logger.info(f"âœ… Excelå¯¼å‡ºæˆåŠŸ!")
            logger.info(f"ğŸ“„ æ–‡ä»¶è·¯å¾„: {filepath}")
            logger.info("=" * 80)

            return str(filepath)

        except Exception as e:
            logger.error(f"âœ— Excelå¯¼å‡ºå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None

    def _prepare_predictions(self, predictions_series: Optional[pd.Series]) -> Dict[str, Any]:
        """å‡†å¤‡é¢„æµ‹æ•°æ®"""
        try:
            if predictions_series is None:
                logger.warning("âš ï¸ predictions_seriesä¸ºNone")
                return {
                    'predictions': np.array([]),
                    'dates': [],
                    'tickers': [],
                    'count': 0,
                    'base_date': None,
                    'target_date': None
                }

            if len(predictions_series) == 0:
                logger.warning("âš ï¸ predictions_seriesä¸ºç©º")
                return {
                    'predictions': np.array([]),
                    'dates': [],
                    'tickers': [],
                    'count': 0,
                    'base_date': None,
                    'target_date': None
                }

            # å¤„ç†MultiIndex
            if isinstance(predictions_series.index, pd.MultiIndex):
                logger.info("   æ£€æµ‹åˆ°MultiIndexæ ¼å¼")

                # æ£€æŸ¥æ˜¯å¦æœ‰'date'çº§åˆ«
                if 'date' in predictions_series.index.names:
                    base_date = predictions_series.index.get_level_values('date').max()
                    pred_latest = predictions_series.xs(base_date, level='date')

                    return {
                        'predictions': pred_latest.values,
                        'dates': [base_date] * len(pred_latest),
                        'tickers': pred_latest.index.tolist(),
                        'count': len(pred_latest),
                        'base_date': base_date,
                        'target_date': base_date + pd.Timedelta(days=1)
                    }
                else:
                    # MultiIndexä½†æ²¡æœ‰'date'çº§åˆ«ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªçº§åˆ«
                    logger.warning(f"   MultiIndexä½†ç¼ºå°‘'date'çº§åˆ«ï¼Œç´¢å¼•åç§°: {predictions_series.index.names}")
                    # é‡ç½®ç´¢å¼•ä¸ºç®€å•ç´¢å¼•
                    predictions_series = predictions_series.reset_index(drop=True)
                    current_date = datetime.now().date()

                    return {
                        'predictions': predictions_series.values,
                        'dates': [current_date] * len(predictions_series),
                        'tickers': [f'ticker_{i}' for i in range(len(predictions_series))],
                        'count': len(predictions_series),
                        'base_date': current_date,
                        'target_date': current_date
                    }
            else:
                # Simple Index
                logger.info("   æ£€æµ‹åˆ°Simple Indexæ ¼å¼")
                current_date = datetime.now().date()

                return {
                    'predictions': predictions_series.values,
                    'dates': [current_date] * len(predictions_series),
                    'tickers': predictions_series.index.tolist(),
                    'count': len(predictions_series),
                    'base_date': current_date,
                    'target_date': current_date
                }

        except Exception as e:
            logger.error(f"âœ— é¢„æµ‹æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            return {
                'predictions': np.array([]),
                'dates': [],
                'tickers': [],
                'count': 0,
                'base_date': None,
                'target_date': None
            }

    def _prepare_model_info(self, analysis_results: Dict[str, Any], feature_data: Optional[pd.DataFrame]) -> Dict[str, Any]:
        """å‡†å¤‡æ¨¡å‹ä¿¡æ¯"""
        try:
            model_info = {
                'model_type': 'BMA Ultra Enhanced',
                'model_version': 'v3.0',
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'prediction_horizon': 'T+1',
                'n_samples': len(feature_data) if feature_data is not None else 'N/A',
                'n_features': feature_data.shape[1] if feature_data is not None else 15
            }

            # æ·»åŠ è®­ç»ƒæ—¶é—´
            if 'execution_time' in analysis_results:
                model_info['training_time'] = f"{analysis_results['execution_time']:.1f}s"

            # æ·»åŠ CVåˆ†æ•°
            if 'training_results' in analysis_results:
                tr = analysis_results['training_results']
                if 'traditional_models' in tr and 'cv_scores' in tr['traditional_models']:
                    cv_scores = tr['traditional_models']['cv_scores']
                    model_info['cv_score'] = np.mean(list(cv_scores.values()))
                    model_info['model_weights'] = cv_scores

            logger.info("âœ“ æ¨¡å‹ä¿¡æ¯å‡†å¤‡å®Œæˆ")
            return model_info

        except Exception as e:
            logger.error(f"âœ— æ¨¡å‹ä¿¡æ¯å‡†å¤‡å¤±è´¥: {e}")
            return {
                'model_type': 'BMA Ultra Enhanced',
                'model_version': 'v3.0',
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }

    def _write_summary_sheet(self, writer, pred_data: Dict, model_info: Dict,
                            lambda_percentile_info: Optional[Dict[str, Any]] = None):
        """å†™å…¥Summaryå·¥ä½œè¡¨"""
        try:
            summary_data = []

            # æ¨¡å‹ä¿¡æ¯
            summary_data.append(['=== æ¨¡å‹ä¿¡æ¯ ===', ''])
            for key, value in model_info.items():
                if key != 'model_weights':
                    summary_data.append([key, str(value)])

            summary_data.append(['', ''])

            # é¢„æµ‹ä¿¡æ¯
            summary_data.append(['=== é¢„æµ‹ä¿¡æ¯ ===', ''])
            summary_data.append(['é¢„æµ‹æ•°é‡', pred_data['count']])
            summary_data.append(['åŸºå‡†æ—¥æœŸ', str(pred_data['base_date'])])
            summary_data.append(['ç›®æ ‡æ—¥æœŸ', str(pred_data['target_date'])])

            summary_data.append(['', ''])

            # Lambda Percentileèåˆä¿¡æ¯ï¼ˆæ–°å¢ï¼‰
            if lambda_percentile_info is not None:
                summary_data.append(['=== Lambda Percentileèåˆ ===', ''])
                summary_data.append(['èåˆç­–ç•¥', 'Lambda Percentileä½œä¸ºRidgeç‰¹å¾'])
                summary_data.append(['Lambdaä½¿ç”¨å› å­', lambda_percentile_info.get('n_factors', 15)])
                summary_data.append(['Lambda OOFæ ·æœ¬æ•°', lambda_percentile_info.get('oof_samples', 'N/A')])
                summary_data.append(['Percentileå‡å€¼', f"{lambda_percentile_info.get('percentile_mean', 0):.1f}"])
                summary_data.append(['PercentileèŒƒå›´', f"[{lambda_percentile_info.get('percentile_min', 0):.1f}, {lambda_percentile_info.get('percentile_max', 100):.1f}]"])
                summary_data.append(['ç´¢å¼•å¯¹é½çŠ¶æ€', lambda_percentile_info.get('alignment_status', 'Unknown')])
                if lambda_percentile_info.get('nan_ratio', 0) > 0:
                    summary_data.append(['NaNæ¯”ä¾‹', f"{lambda_percentile_info.get('nan_ratio', 0):.2%}"])

                summary_data.append(['', ''])

            # ç»Ÿè®¡ä¿¡æ¯
            if pred_data['count'] > 0:
                summary_data.append(['=== é¢„æµ‹ç»Ÿè®¡ ===', ''])
                summary_data.append(['æœ€å¤§å€¼', f"{np.max(pred_data['predictions']):.6f}"])
                summary_data.append(['æœ€å°å€¼', f"{np.min(pred_data['predictions']):.6f}"])
                summary_data.append(['å¹³å‡å€¼', f"{np.mean(pred_data['predictions']):.6f}"])
                summary_data.append(['ä¸­ä½æ•°', f"{np.median(pred_data['predictions']):.6f}"])
                summary_data.append(['æ ‡å‡†å·®', f"{np.std(pred_data['predictions']):.6f}"])

            df = pd.DataFrame(summary_data, columns=['Item', 'Value'])
            df.to_excel(writer, sheet_name='Summary', index=False)
            logger.info("âœ“ Summaryå·¥ä½œè¡¨å·²å†™å…¥")

        except Exception as e:
            logger.error(f"âœ— Summaryå·¥ä½œè¡¨å†™å…¥å¤±è´¥: {e}")

    def _write_predictions_sheet(self, writer, pred_data: Dict):
        """å†™å…¥Predictionså·¥ä½œè¡¨"""
        try:
            if pred_data['count'] == 0:
                logger.warning("âš ï¸ æ— é¢„æµ‹æ•°æ®ï¼Œè·³è¿‡Predictionså·¥ä½œè¡¨")
                return

            df = pd.DataFrame({
                'Date': pred_data['dates'],
                'Ticker': pred_data['tickers'],
                'Prediction': pred_data['predictions']
            })

            # æŒ‰é¢„æµ‹å€¼é™åºæ’åº
            df = df.sort_values('Prediction', ascending=False).reset_index(drop=True)
            df.to_excel(writer, sheet_name='Predictions', index=False)
            logger.info(f"âœ“ Predictionså·¥ä½œè¡¨å·²å†™å…¥ ({len(df)} æ¡)")

        except Exception as e:
            logger.error(f"âœ— Predictionså·¥ä½œè¡¨å†™å…¥å¤±è´¥: {e}")

    def _write_lambda_sheet(self, writer, lambda_df: pd.DataFrame):
        """å†™å…¥Lambda Predictionså·¥ä½œè¡¨ï¼ˆç¡®ä¿æ¯ä¸ªtickeråªæœ‰ä¸€æ¡è®°å½•ï¼ŒæŒ‰åˆ†æ•°æ’åºï¼‰"""
        try:
            # ğŸ”§ CRITICAL FIX: ç¡®ä¿æ¯ä¸ªtickeråªæœ‰ä¸€ä¸ªé¢„æµ‹
            if 'ticker' in lambda_df.columns:
                initial_count = len(lambda_df)
                # æŒ‰æ—¥æœŸæ’åºï¼Œä¿ç•™æ¯ä¸ªtickeræœ€æ–°çš„é¢„æµ‹
                lambda_df = lambda_df.sort_values(['ticker', 'date'] if 'date' in lambda_df.columns else 'ticker')
                lambda_df = lambda_df.groupby('ticker', as_index=False).last()

                if len(lambda_df) < initial_count:
                    logger.warning(f"âš ï¸ Lambdaå»é‡: {initial_count} â†’ {len(lambda_df)} æ¡ (æ¯ä¸ªtickerä¿ç•™ä¸€æ¡)")

            # ğŸ¯ æŒ‰lambda_scoreé™åºæ’åº
            score_col = 'lambda_score' if 'lambda_score' in lambda_df.columns else 'lambda_pct'
            if score_col in lambda_df.columns:
                lambda_df = lambda_df.sort_values(score_col, ascending=False).reset_index(drop=True)
                lambda_df.insert(0, 'rank', range(1, len(lambda_df) + 1))

            lambda_df.to_excel(writer, sheet_name='Lambda_Predictions', index=False)
            logger.info(f"âœ“ Lambda_Predictionså·¥ä½œè¡¨å·²å†™å…¥ ({len(lambda_df)} æ¡ï¼Œå·²æŒ‰åˆ†æ•°æ’åº)")
        except Exception as e:
            logger.error(f"âœ— Lambda_Predictionså·¥ä½œè¡¨å†™å…¥å¤±è´¥: {e}")

    def _write_ridge_sheet(self, writer, ridge_df: pd.DataFrame):
        """å†™å…¥Ridge Predictionså·¥ä½œè¡¨ï¼ˆç¡®ä¿æ¯ä¸ªtickeråªæœ‰ä¸€æ¡è®°å½•ï¼ŒæŒ‰åˆ†æ•°æ’åºï¼‰"""
        try:
            # ğŸ”§ CRITICAL FIX: ç¡®ä¿æ¯ä¸ªtickeråªæœ‰ä¸€ä¸ªé¢„æµ‹
            if 'ticker' in ridge_df.columns:
                initial_count = len(ridge_df)
                # æŒ‰æ—¥æœŸæ’åºï¼Œä¿ç•™æ¯ä¸ªtickeræœ€æ–°çš„é¢„æµ‹
                ridge_df = ridge_df.sort_values(['ticker', 'date'] if 'date' in ridge_df.columns else 'ticker')
                ridge_df = ridge_df.groupby('ticker', as_index=False).last()

                if len(ridge_df) < initial_count:
                    logger.warning(f"âš ï¸ Ridgeå»é‡: {initial_count} â†’ {len(ridge_df)} æ¡ (æ¯ä¸ªtickerä¿ç•™ä¸€æ¡)")

            # ğŸ¯ æŒ‰ridge_scoreé™åºæ’åº
            score_col = 'ridge_score' if 'ridge_score' in ridge_df.columns else 'ridge_z'
            if score_col in ridge_df.columns:
                ridge_df = ridge_df.sort_values(score_col, ascending=False).reset_index(drop=True)
                ridge_df.insert(0, 'rank', range(1, len(ridge_df) + 1))

            ridge_df.to_excel(writer, sheet_name='Ridge_Predictions', index=False)
            logger.info(f"âœ“ Ridge_Predictionså·¥ä½œè¡¨å·²å†™å…¥ ({len(ridge_df)} æ¡ï¼Œå·²æŒ‰åˆ†æ•°æ’åº)")
        except Exception as e:
            logger.error(f"âœ— Ridge_Predictionså·¥ä½œè¡¨å†™å…¥å¤±è´¥: {e}")

    def _write_final_sheet(self, writer, final_df: pd.DataFrame):
        """å†™å…¥Final Predictionså·¥ä½œè¡¨ï¼ˆç¡®ä¿æ¯ä¸ªtickeråªæœ‰ä¸€æ¡è®°å½•ï¼ŒæŒ‰åˆ†æ•°æ’åºï¼‰"""
        try:
            # ğŸ”§ CRITICAL FIX: ç¡®ä¿æ¯ä¸ªtickeråªæœ‰ä¸€ä¸ªé¢„æµ‹
            if 'ticker' in final_df.columns:
                initial_count = len(final_df)
                uniq_tickers = final_df['ticker'].nunique()
                # é™åˆ¶æœ€å¤§å¯¼å‡ºè¡Œæ•°ï¼Œé¿å…è¶…å‡ºExcelä¸Šé™
                MAX_ROWS = 50000
                if initial_count > MAX_ROWS:
                    logger.warning(f"âš ï¸ Final_Predictionsè¿‡å¤§: {initial_count} è¡Œï¼Œæˆªæ–­åˆ° {MAX_ROWS} è¡Œ")
                    final_df = final_df.head(MAX_ROWS).copy()
                    initial_count = len(final_df)

                # ä»…å½“å»é‡åä¸ä¼šå¼‚å¸¸ç¼©å°æ—¶æ‰æ‰§è¡Œï¼ˆé˜²æ­¢å¼‚å¸¸æ•°æ®é€ æˆåªå‰©å°‘æ•°tickerï¼‰
                # æ¡ä»¶ï¼šå”¯ä¸€tickeræ•°â‰¥min(10, 50%æ ·æœ¬æ•°)
                if uniq_tickers >= max(10, int(initial_count * 0.5)):
                    # æŒ‰æ—¥æœŸæ’åºï¼Œä¿ç•™æ¯ä¸ªtickeræœ€æ–°çš„é¢„æµ‹
                    final_df = final_df.sort_values(['ticker', 'date'] if 'date' in final_df.columns else 'ticker')
                    final_df = final_df.groupby('ticker', as_index=False).last()
                    if len(final_df) < initial_count:
                        logger.warning(f"âš ï¸ å»é‡: {initial_count} â†’ {len(final_df)} æ¡ (æ¯ä¸ªtickerä¿ç•™ä¸€æ¡)")
                else:
                    logger.warning(
                        f"âš ï¸ è·³è¿‡å»é‡ï¼šå”¯ä¸€tickerè¿‡å°‘ ({uniq_tickers}/{initial_count})ï¼Œä¿ç•™å…¨éƒ¨è®°å½•ä»¥é¿å…ä¿¡æ¯ä¸¢å¤±")

            # ğŸ¯ æŒ‰final_scoreé™åºæ’åºï¼ˆæœ€å¥½çš„é¢„æµ‹åœ¨æœ€å‰é¢ï¼‰
            score_col = 'final_score' if 'final_score' in final_df.columns else final_df.select_dtypes(include=['number']).columns[0]
            final_df = final_df.sort_values(score_col, ascending=False).reset_index(drop=True)

            # æ·»åŠ æ’ååˆ—
            final_df.insert(0, 'rank', range(1, len(final_df) + 1))

            # å†æ¬¡é™åˆ¶å¯¼å‡ºè¡¨å¤§å°ï¼Œé¿å…ä»»ä½•ç¯èŠ‚è¶…é™
            if len(final_df) > 50000:
                final_df = final_df.head(50000)
            final_df.to_excel(writer, sheet_name='Final_Predictions', index=False)
            logger.info(f"âœ“ Final_Predictionså·¥ä½œè¡¨å·²å†™å…¥ ({len(final_df)} æ¡ï¼Œå·²æŒ‰åˆ†æ•°æ’åº)")
        except Exception as e:
            logger.error(f"âœ— Final_Predictionså·¥ä½œè¡¨å†™å…¥å¤±è´¥: {e}")

    def _write_kronos_sheet(self, writer, kronos_df: pd.DataFrame):
        """å†™å…¥Kronos Filterå·¥ä½œè¡¨"""
        try:
            kronos_df.to_excel(writer, sheet_name='Kronos_Filter', index=False)
            logger.info(f"âœ“ Kronos_Filterå·¥ä½œè¡¨å·²å†™å…¥ ({len(kronos_df)} æ¡)")
        except Exception as e:
            logger.error(f"âœ— Kronos_Filterå·¥ä½œè¡¨å†™å…¥å¤±è´¥: {e}")

    def _write_factor_contributions_sheet(self, writer, model_info: Dict):
        """å†™å…¥Factor Contributionså·¥ä½œè¡¨"""
        try:
            # é»˜è®¤å› å­è´¡çŒ®ï¼ˆå¦‚æœæ²¡æœ‰å®é™…æ•°æ®ï¼‰
            factor_data = [
                ['momentum_10d_ex1', 0.058],
                ['near_52w_high', 0.062],
                ['reversal_1d', 0.045],
                ['rel_volume_spike', 0.041],
                ['mom_accel_5_2', 0.038],
                ['rsi_7', 0.052],
                ['bollinger_squeeze', 0.035],
                ['obv_momentum', 0.048],
                ['atr_ratio', 0.032],
                ['ivol_60d', 0.056],
                ['liquidity_factor', 0.029],
                ['price_efficiency_5d', 0.044],
                ['overnight_intraday_gap', 0.067],
                ['max_lottery_factor', 0.071],
                ['streak_reversal', 0.039]
            ]

            df = pd.DataFrame(factor_data, columns=['Factor', 'Contribution'])
            df = df.sort_values('Contribution', ascending=False).reset_index(drop=True)
            df.to_excel(writer, sheet_name='Factor_Contributions', index=False)
            logger.info("âœ“ Factor_Contributionså·¥ä½œè¡¨å·²å†™å…¥")

        except Exception as e:
            logger.error(f"âœ— Factor_Contributionså·¥ä½œè¡¨å†™å…¥å¤±è´¥: {e}")


if __name__ == "__main__":
    # æµ‹è¯•
    logging.basicConfig(level=logging.INFO)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_predictions = pd.Series(
        [0.05, 0.03, -0.02, 0.01],
        index=['AAPL', 'GOOGL', 'MSFT', 'TSLA']
    )

    test_results = {
        'execution_time': 120.5
    }

    exporter = RobustExcelExporter()
    path = exporter.safe_export(
        predictions_series=test_predictions,
        analysis_results=test_results
    )

    print(f"\næµ‹è¯•å®Œæˆ: {path}")
