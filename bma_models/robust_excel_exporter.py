#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Robust Excel Exporter - é˜²å¾¡æ€§Excelå¯¼å‡ºå™¨
ç¡®ä¿ä¸‡æ— ä¸€å¤±çš„Excelå¯¼å‡ºæµç¨‹
"""

import logging
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple

logger = logging.getLogger(__name__)


class RobustExcelExporter:
    """é˜²å¾¡æ€§Excelå¯¼å‡ºå™¨ - ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½èƒ½æ­£ç¡®å¯¼å‡º"""

    def __init__(self, output_dir: str = "D:/trade/result"):
        """
        åˆå§‹åŒ–å¯¼å‡ºå™¨

        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self._prediction_horizon_days = self._resolve_prediction_horizon()


    def _resolve_prediction_horizon(self) -> int:
        """Resolve prediction horizon days from configuration (falls back to 10)."""
        try:
            from bma_models.unified_config_loader import get_time_config

            time_config = get_time_config()
            horizon = getattr(time_config, 'prediction_horizon_days', None)
            if isinstance(horizon, (int, float)) and horizon > 0:
                return int(horizon)
        except Exception:
            logger.debug("Falling back to default prediction horizon=10 (unable to load temporal config)")
        return 10

    def _compute_target_date(self, base_date: Optional[pd.Timestamp]) -> pd.Timestamp:
        try:
            base_ts = pd.Timestamp(base_date)
        except Exception:
            base_ts = pd.Timestamp(datetime.now().date())
        horizon = getattr(self, '_prediction_horizon_days', 10) or 10
        return base_ts + pd.Timedelta(days=horizon)

    def safe_export(
        self,
        predictions_series: Optional[pd.Series],
        analysis_results: Dict[str, Any],
        feature_data: Optional[pd.DataFrame] = None,
        lambda_df: Optional[pd.DataFrame] = None,
        ridge_df: Optional[pd.DataFrame] = None,
        final_df: Optional[pd.DataFrame] = None,
        base_models_df: Optional[pd.DataFrame] = None,
        kronos_df: Optional[pd.DataFrame] = None,
        kronos_pass_df: Optional[pd.DataFrame] = None,
        lambda_percentile_info: Optional[Dict[str, Any]] = None,
        tradingagents_df: Optional[pd.DataFrame] = None,
        model_prediction_tables: Optional[Dict[str, pd.DataFrame]] = None,
        top30_summary: Optional[pd.DataFrame] = None,
        top30_details: Optional[Dict[str, pd.DataFrame]] = None,
        simple_mode: bool = True  # default: only keep Final_Predictions sheet
    ) -> Optional[str]:
        """
        å®‰å…¨å¯¼å‡ºExcel

        Args:
            simple_mode: True=åªå¯¼å‡ºFinal_Predictionsè¡¨ï¼ŒFalse=å¯¼å‡ºæ‰€æœ‰è¯¦ç»†è¡¨

        Returns:
            str: Excelæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            logger.info("=" * 80)
            logger.info(f"ğŸ“Š å¼€å§‹Robust Excelå¯¼å‡º (ç®€åŒ–æ¨¡å¼: {simple_mode})")
            logger.info("=" * 80)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"bma_analysis_{timestamp}.xlsx"
            filepath = self.output_dir / filename

            # ========== åˆ›å»ºExcel Writer ==========
            with pd.ExcelWriter(filepath, engine='openpyxl') as writer:

                if simple_mode:
                    # ğŸ”§ å†™å…¥ä¸€ä¸ªæœ€å°å¯è§å·¥ä½œè¡¨ï¼Œé¿å…æ— è¡¨æ—¶ExcelæŠ¥é”™
                    try:
                        pd.DataFrame({'Info': [timestamp]}).to_excel(writer, sheet_name='Info', index=False)
                    except Exception:
                        pass

                    # ğŸ¯ ç®€åŒ–æ¨¡å¼ï¼šä¼˜å…ˆå¯¼å‡ºFinal_Predictions
                    if final_df is not None and not final_df.empty:
                        self._write_final_sheet(writer, final_df)
                    elif base_models_df is not None and not base_models_df.empty:
                        self._write_base_models_sheet(writer, base_models_df)
                    else:
                        # å›é€€ï¼šä½¿ç”¨predictions_series
                        pred_data = self._prepare_predictions(predictions_series)
                        self._write_predictions_sheet(writer, pred_data)
                        logger.warning("Final_Predictionsä¸å¯ç”¨ï¼Œä½¿ç”¨Predictionsè¡¨ä»£æ›¿")

                    if kronos_df is not None and not kronos_df.empty:
                        self._write_kronos_sheet(writer, kronos_df)
                    if kronos_pass_df is not None and not kronos_pass_df.empty:
                        self._write_kronos_sheet(writer, kronos_pass_df, sheet_name='Kronos_Passed_Over10')

                    if model_prediction_tables:
                        self._write_model_prediction_tables(writer, model_prediction_tables)
                    if top30_summary is not None and not top30_summary.empty:
                        self._write_top30_summary(writer, top30_summary)
                    if top30_details:
                        self._write_top30_details(writer, top30_details)
                else:
                    # å®Œæ•´æ¨¡å¼ï¼šå¯¼å‡ºæ‰€æœ‰è¡¨
                    pred_data = self._prepare_predictions(predictions_series)
                    model_info = self._prepare_model_info(analysis_results, feature_data)

                    # Sheet 1: Summary (æ€»è§ˆ)
                    self._write_summary_sheet(writer, pred_data, model_info, lambda_percentile_info)

                    # Sheet 2: Predictions (é¢„æµ‹ç»“æœ)
                    self._write_predictions_sheet(writer, pred_data)

                    # Sheet 3: Lambda Predictions (å¦‚æœæœ‰)
                    if lambda_df is not None and not lambda_df.empty:
                        self._write_lambda_sheet(writer, lambda_df)

                    # Sheet 4: Base Model Predictions (å¦‚æœæœ‰)
                    if base_models_df is not None and not base_models_df.empty:
                        self._write_base_models_sheet(writer, base_models_df)

                    # Sheet 5: Ridge Predictions (å¦‚æœæœ‰)
                    if ridge_df is not None and not ridge_df.empty:
                        self._write_ridge_sheet(writer, ridge_df)

                    # Sheet 6: Final Predictions (å¦‚æœæœ‰)
                    if final_df is not None and not final_df.empty:
                        self._write_final_sheet(writer, final_df)

                    # Sheet 7: Kronos Filter (å¦‚æœæœ‰)
                    if kronos_df is not None and not kronos_df.empty:
                        self._write_kronos_sheet(writer, kronos_df)
                    if kronos_pass_df is not None and not kronos_pass_df.empty:
                        self._write_kronos_sheet(writer, kronos_pass_df, sheet_name='Kronos_Passed_Over10')

                    # Sheet 8: TradingAgents Analysis (å¦‚æœæœ‰)
                    if tradingagents_df is not None and not tradingagents_df.empty:
                        self._write_tradingagents_sheet(writer, tradingagents_df)

                    # Sheet 9: Factor Contributions
                    self._write_factor_contributions_sheet(writer, model_info)

                    if model_prediction_tables:
                        self._write_model_prediction_tables(writer, model_prediction_tables, include_models=('catboost', 'xgboost', 'elastic_net'), use_special_handlers=False)
                    if top30_summary is not None and not top30_summary.empty:
                        self._write_top30_summary(writer, top30_summary)
                    if top30_details:
                        self._write_top30_details(writer, top30_details)

            logger.info("=" * 80)
            logger.info(f"âœ… Excelå¯¼å‡ºæˆåŠŸ!")
            logger.info(f"ğŸ“„ æ–‡ä»¶è·¯å¾„: {filepath}")
            logger.info("=" * 80)

            return str(filepath)

        except Exception as e:
            logger.error(f"âœ— Excelå¯¼å‡ºå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None

    def _prepare_predictions(self, predictions_series: Optional[pd.Series]) -> Dict[str, Any]:
        """å‡†å¤‡é¢„æµ‹æ•°æ®"""
        try:
            if predictions_series is None:
                logger.warning("âš ï¸ predictions_seriesä¸ºNone")
                return {
                    'predictions': np.array([]),
                    'dates': [],
                    'tickers': [],
                    'count': 0,
                    'base_date': None,
                    'target_date': None
                }

            if len(predictions_series) == 0:
                logger.warning("âš ï¸ predictions_seriesä¸ºç©º")
                return {
                    'predictions': np.array([]),
                    'dates': [],
                    'tickers': [],
                    'count': 0,
                    'base_date': None,
                    'target_date': None
                }

            # å¤„ç†MultiIndex
            if isinstance(predictions_series.index, pd.MultiIndex):
                logger.info("   æ£€æµ‹åˆ°MultiIndexæ ¼å¼")

                # æ£€æŸ¥æ˜¯å¦æœ‰'date'çº§åˆ«
                if 'date' in predictions_series.index.names:
                    base_date = predictions_series.index.get_level_values('date').max()
                    target_date = self._compute_target_date(base_date)
                    pred_latest = predictions_series.xs(base_date, level='date')

                    return {
                        'predictions': pred_latest.values,
                        'dates': [target_date] * len(pred_latest),
                        'tickers': pred_latest.index.tolist(),
                        'count': len(pred_latest),
                        'base_date': pd.Timestamp(base_date),
                        'target_date': target_date
                    }
                else:
                    # MultiIndexä½†æ²¡æœ‰'date'çº§åˆ«ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªçº§åˆ«
                    logger.warning(f"   MultiIndexä½†ç¼ºå°‘'date'çº§åˆ«ï¼Œç´¢å¼•åç§°: {predictions_series.index.names}")
                    # é‡ç½®ç´¢å¼•ä¸ºç®€å•ç´¢å¼•
                    predictions_series = predictions_series.reset_index(drop=True)
                    base_date = datetime.now().date()
                    target_date = self._compute_target_date(base_date)

                    return {
                        'predictions': predictions_series.values,
                        'dates': [target_date] * len(predictions_series),
                        'tickers': [f'ticker_{i}' for i in range(len(predictions_series))],
                        'count': len(predictions_series),
                        'base_date': pd.Timestamp(base_date),
                        'target_date': target_date
                    }
            else:
                # Simple Index
                logger.info("   æ£€æµ‹åˆ°Simple Indexæ ¼å¼")
                base_date = datetime.now().date()
                target_date = self._compute_target_date(base_date)

                return {
                    'predictions': predictions_series.values,
                    'dates': [target_date] * len(predictions_series),
                    'tickers': predictions_series.index.tolist(),
                    'count': len(predictions_series),
                    'base_date': pd.Timestamp(base_date),
                    'target_date': target_date
                }

        except Exception as e:
            logger.error(f"âœ— é¢„æµ‹æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            return {
                'predictions': np.array([]),
                'dates': [],
                'tickers': [],
                'count': 0,
                'base_date': None,
                'target_date': None
            }

    def _prepare_model_info(self, analysis_results: Dict[str, Any], feature_data: Optional[pd.DataFrame]) -> Dict[str, Any]:
        """å‡†å¤‡æ¨¡å‹ä¿¡æ¯"""
        try:
            model_info = {
                'model_type': 'BMA Ultra Enhanced',
                'model_version': 'v3.0',
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'prediction_horizon': 'T+1',
                'n_samples': len(feature_data) if feature_data is not None else 'N/A',
                'n_features': feature_data.shape[1] if feature_data is not None else 15
            }

            # æ·»åŠ è®­ç»ƒæ—¶é—´
            if 'execution_time' in analysis_results:
                model_info['training_time'] = f"{analysis_results['execution_time']:.1f}s"

            # æ·»åŠ CVåˆ†æ•°
            if 'training_results' in analysis_results:
                tr = analysis_results['training_results']
                if 'traditional_models' in tr and 'cv_scores' in tr['traditional_models']:
                    cv_scores = tr['traditional_models']['cv_scores']
                    model_info['cv_score'] = np.mean(list(cv_scores.values()))
                    model_info['model_weights'] = cv_scores

            logger.info("âœ“ æ¨¡å‹ä¿¡æ¯å‡†å¤‡å®Œæˆ")
            return model_info

        except Exception as e:
            logger.error(f"âœ— æ¨¡å‹ä¿¡æ¯å‡†å¤‡å¤±è´¥: {e}")
            return {
                'model_type': 'BMA Ultra Enhanced',
                'model_version': 'v3.0',
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }

    def _write_summary_sheet(self, writer, pred_data: Dict, model_info: Dict,
                            lambda_percentile_info: Optional[Dict[str, Any]] = None):
        """å†™å…¥Summaryå·¥ä½œè¡¨"""
        try:
            summary_data = []

            # æ¨¡å‹ä¿¡æ¯
            summary_data.append(['=== æ¨¡å‹ä¿¡æ¯ ===', ''])
            for key, value in model_info.items():
                if key != 'model_weights':
                    summary_data.append([key, str(value)])

            summary_data.append(['', ''])

            # é¢„æµ‹ä¿¡æ¯
            summary_data.append(['=== é¢„æµ‹ä¿¡æ¯ ===', ''])
            summary_data.append(['é¢„æµ‹æ•°é‡', pred_data['count']])
            summary_data.append(['åŸºå‡†æ—¥æœŸ', str(pred_data['base_date'])])
            summary_data.append(['ç›®æ ‡æ—¥æœŸ', str(pred_data['target_date'])])

            summary_data.append(['', ''])

            # Lambda Percentileèåˆä¿¡æ¯ï¼ˆæ–°å¢ï¼‰
            if lambda_percentile_info is not None:
                summary_data.append(['=== Lambda Percentileèåˆ ===', ''])
                summary_data.append(['èåˆç­–ç•¥', 'Lambda Percentileä½œä¸ºRidgeç‰¹å¾'])
                summary_data.append(['Lambdaä½¿ç”¨å› å­', lambda_percentile_info.get('n_factors', 15)])
                summary_data.append(['Lambda OOFæ ·æœ¬æ•°', lambda_percentile_info.get('oof_samples', 'N/A')])
                summary_data.append(['Percentileå‡å€¼', f"{lambda_percentile_info.get('percentile_mean', 0):.1f}"])
                summary_data.append(['PercentileèŒƒå›´', f"[{lambda_percentile_info.get('percentile_min', 0):.1f}, {lambda_percentile_info.get('percentile_max', 100):.1f}]"])
                summary_data.append(['ç´¢å¼•å¯¹é½çŠ¶æ€', lambda_percentile_info.get('alignment_status', 'Unknown')])
                if lambda_percentile_info.get('nan_ratio', 0) > 0:
                    summary_data.append(['NaNæ¯”ä¾‹', f"{lambda_percentile_info.get('nan_ratio', 0):.2%}"])

                summary_data.append(['', ''])

            # ç»Ÿè®¡ä¿¡æ¯
            if pred_data['count'] > 0:
                summary_data.append(['=== é¢„æµ‹ç»Ÿè®¡ ===', ''])
                summary_data.append(['æœ€å¤§å€¼', f"{np.max(pred_data['predictions']):.6f}"])
                summary_data.append(['æœ€å°å€¼', f"{np.min(pred_data['predictions']):.6f}"])
                summary_data.append(['å¹³å‡å€¼', f"{np.mean(pred_data['predictions']):.6f}"])
                summary_data.append(['ä¸­ä½æ•°', f"{np.median(pred_data['predictions']):.6f}"])
                summary_data.append(['æ ‡å‡†å·®', f"{np.std(pred_data['predictions']):.6f}"])

            df = pd.DataFrame(summary_data, columns=['Item', 'Value'])
            df.to_excel(writer, sheet_name='Summary', index=False)
            logger.info("âœ“ Summaryå·¥ä½œè¡¨å·²å†™å…¥")

        except Exception as e:
            logger.error(f"âœ— Summaryå·¥ä½œè¡¨å†™å…¥å¤±è´¥: {e}")

    def _write_predictions_sheet(self, writer, pred_data: Dict):
        """å†™å…¥Predictionså·¥ä½œè¡¨"""
        try:
            if pred_data['count'] == 0:
                logger.warning("âš ï¸ æ— é¢„æµ‹æ•°æ®ï¼Œè·³è¿‡Predictionså·¥ä½œè¡¨")
                return

            df = pd.DataFrame({
                'Date': pred_data['dates'],
                'Ticker': pred_data['tickers'],
                'Prediction': pred_data['predictions']
            })

            # æŒ‰é¢„æµ‹å€¼é™åºæ’åº
            df = df.sort_values('Prediction', ascending=False).reset_index(drop=True)
            df.to_excel(writer, sheet_name='Predictions', index=False)
            logger.info(f"âœ“ Predictionså·¥ä½œè¡¨å·²å†™å…¥ ({len(df)} æ¡)")

        except Exception as e:
            logger.error(f"âœ— Predictionså·¥ä½œè¡¨å†™å…¥å¤±è´¥: {e}")

    def _write_lambda_sheet(self, writer, lambda_df: pd.DataFrame):
        """å†™å…¥Lambda Predictionså·¥ä½œè¡¨ï¼ˆç¡®ä¿æ¯ä¸ªtickeråªæœ‰ä¸€æ¡è®°å½•ï¼ŒæŒ‰åˆ†æ•°æ’åºï¼‰"""
        try:
            # ğŸ”§ CRITICAL FIX: ç¡®ä¿æ¯ä¸ªtickeråªæœ‰ä¸€ä¸ªé¢„æµ‹
            if 'ticker' in lambda_df.columns:
                initial_count = len(lambda_df)
                # æŒ‰æ—¥æœŸæ’åºï¼Œä¿ç•™æ¯ä¸ªtickeræœ€æ–°çš„é¢„æµ‹
                lambda_df = lambda_df.sort_values(['ticker', 'date'] if 'date' in lambda_df.columns else 'ticker')
                lambda_df = lambda_df.groupby('ticker', as_index=False).last()

                if len(lambda_df) < initial_count:
                    logger.warning(f"âš ï¸ Lambdaå»é‡: {initial_count} â†’ {len(lambda_df)} æ¡ (æ¯ä¸ªtickerä¿ç•™ä¸€æ¡)")

            # ğŸ¯ æŒ‰lambda_scoreé™åºæ’åº
            score_col = 'lambda_score' if 'lambda_score' in lambda_df.columns else 'lambda_pct'
            if score_col in lambda_df.columns:
                lambda_df = lambda_df.sort_values(score_col, ascending=False).reset_index(drop=True)
                lambda_df.insert(0, 'rank', range(1, len(lambda_df) + 1))

            lambda_df.to_excel(writer, sheet_name='Lambda_Predictions', index=False)
            logger.info(f"âœ“ Lambda_Predictionså·¥ä½œè¡¨å·²å†™å…¥ ({len(lambda_df)} æ¡ï¼Œå·²æŒ‰åˆ†æ•°æ’åº)")
        except Exception as e:
            logger.error(f"âœ— Lambda_Predictionså·¥ä½œè¡¨å†™å…¥å¤±è´¥: {e}")

    def _write_base_models_sheet(self, writer, base_models_df: pd.DataFrame):
        """å†™å…¥Base Model Predictionså·¥ä½œè¡¨ï¼ˆæŒ‰tickeræ’åºå¹¶ä¿ç•™åŸå§‹æ•°æ®åˆ—ï¼‰"""
        try:
            if not isinstance(base_models_df, pd.DataFrame):
                df = pd.DataFrame(base_models_df)
            else:
                df = base_models_df.copy()

            if df.empty:
                logger.warning("âš ï¸ Base Modelæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡Base_Model_Predictionså·¥ä½œè¡¨")
                return

            if 'ticker' in df.columns:
                sort_columns: List[str] = ['ticker']
                if 'date' in df.columns:
                    sort_columns.append('date')
                df = df.sort_values(sort_columns)

            df.to_excel(writer, sheet_name='Base_Model_Predictions', index=False)
            logger.info(f"âœ… Base_Model_Predictionså·¥ä½œè¡¨å·²å†™å…¥ ({len(df)} æ¡)")
        except Exception as e:
            logger.error(f"âŒ Base_Model_Predictionså·¥ä½œè¡¨å†™å…¥å¤±è´¥: {e}")

    def _write_ridge_sheet(self, writer, ridge_df: pd.DataFrame):
        """å†™å…¥Ridge Predictionså·¥ä½œè¡¨ï¼ˆç¡®ä¿æ¯ä¸ªtickeråªæœ‰ä¸€æ¡è®°å½•ï¼ŒæŒ‰åˆ†æ•°æ’åºï¼‰"""
        try:
            # ğŸ”§ CRITICAL FIX: ç¡®ä¿æ¯ä¸ªtickeråªæœ‰ä¸€ä¸ªé¢„æµ‹
            if 'ticker' in ridge_df.columns:
                initial_count = len(ridge_df)
                # æŒ‰æ—¥æœŸæ’åºï¼Œä¿ç•™æ¯ä¸ªtickeræœ€æ–°çš„é¢„æµ‹
                ridge_df = ridge_df.sort_values(['ticker', 'date'] if 'date' in ridge_df.columns else 'ticker')
                ridge_df = ridge_df.groupby('ticker', as_index=False).last()

                if len(ridge_df) < initial_count:
                    logger.warning(f"âš ï¸ Ridgeå»é‡: {initial_count} â†’ {len(ridge_df)} æ¡ (æ¯ä¸ªtickerä¿ç•™ä¸€æ¡)")

            # ğŸ¯ æŒ‰ridge_scoreé™åºæ’åº
            score_col = 'ridge_score' if 'ridge_score' in ridge_df.columns else 'ridge_z'
            if score_col in ridge_df.columns:
                ridge_df = ridge_df.sort_values(score_col, ascending=False).reset_index(drop=True)
                ridge_df.insert(0, 'rank', range(1, len(ridge_df) + 1))

            ridge_df.to_excel(writer, sheet_name='Ridge_Predictions', index=False)
            logger.info(f"âœ“ Ridge_Predictionså·¥ä½œè¡¨å·²å†™å…¥ ({len(ridge_df)} æ¡ï¼Œå·²æŒ‰åˆ†æ•°æ’åº)")
        except Exception as e:
            logger.error(f"âœ— Ridge_Predictionså·¥ä½œè¡¨å†™å…¥å¤±è´¥: {e}")

    def _write_final_sheet(self, writer, final_df: pd.DataFrame):
        """å†™å…¥Final Predictionså·¥ä½œè¡¨ï¼ˆç¡®ä¿æ¯ä¸ªtickeråªæœ‰ä¸€æ¡è®°å½•ï¼ŒæŒ‰åˆ†æ•°æ’åºï¼‰"""
        try:
            # ğŸ”§ CRITICAL FIX: ç¡®ä¿æ¯ä¸ªtickeråªæœ‰ä¸€ä¸ªé¢„æµ‹
            if 'ticker' in final_df.columns:
                initial_count = len(final_df)
                uniq_tickers = final_df['ticker'].nunique()
                # é™åˆ¶æœ€å¤§å¯¼å‡ºè¡Œæ•°ï¼Œé¿å…è¶…å‡ºExcelä¸Šé™
                MAX_ROWS = 50000
                if initial_count > MAX_ROWS:
                    logger.warning(f"âš ï¸ Final_Predictionsè¿‡å¤§: {initial_count} è¡Œï¼Œæˆªæ–­åˆ° {MAX_ROWS} è¡Œ")
                    final_df = final_df.head(MAX_ROWS).copy()
                    initial_count = len(final_df)

                # ä»…å½“å»é‡åä¸ä¼šå¼‚å¸¸ç¼©å°æ—¶æ‰æ‰§è¡Œï¼ˆé˜²æ­¢å¼‚å¸¸æ•°æ®é€ æˆåªå‰©å°‘æ•°tickerï¼‰
                # æ¡ä»¶ï¼šå”¯ä¸€tickeræ•°â‰¥min(10, 50%æ ·æœ¬æ•°)
                if uniq_tickers >= max(10, int(initial_count * 0.5)):
                    # æŒ‰æ—¥æœŸæ’åºï¼Œä¿ç•™æ¯ä¸ªtickeræœ€æ–°çš„é¢„æµ‹
                    final_df = final_df.sort_values(['ticker', 'date'] if 'date' in final_df.columns else 'ticker')
                    final_df = final_df.groupby('ticker', as_index=False).last()
                    if len(final_df) < initial_count:
                        logger.warning(f"âš ï¸ å»é‡: {initial_count} â†’ {len(final_df)} æ¡ (æ¯ä¸ªtickerä¿ç•™ä¸€æ¡)")
                else:
                    logger.warning(
                        f"âš ï¸ è·³è¿‡å»é‡ï¼šå”¯ä¸€tickerè¿‡å°‘ ({uniq_tickers}/{initial_count})ï¼Œä¿ç•™å…¨éƒ¨è®°å½•ä»¥é¿å…ä¿¡æ¯ä¸¢å¤±")

            # ğŸ¯ æŒ‰final_scoreé™åºæ’åºï¼ˆæœ€å¥½çš„é¢„æµ‹åœ¨æœ€å‰é¢ï¼‰
            score_col = 'final_score' if 'final_score' in final_df.columns else final_df.select_dtypes(include=['number']).columns[0]
            final_df = final_df.sort_values(score_col, ascending=False).reset_index(drop=True)

            # æ·»åŠ æ’ååˆ—
            final_df.insert(0, 'rank', range(1, len(final_df) + 1))

            # å†æ¬¡é™åˆ¶å¯¼å‡ºè¡¨å¤§å°ï¼Œé¿å…ä»»ä½•ç¯èŠ‚è¶…é™
            if len(final_df) > 50000:
                final_df = final_df.head(50000)
            final_df.to_excel(writer, sheet_name='Final_Predictions', index=False)
            logger.info(f"âœ“ Final_Predictionså·¥ä½œè¡¨å·²å†™å…¥ ({len(final_df)} æ¡ï¼Œå·²æŒ‰åˆ†æ•°æ’åº)")
        except Exception as e:
            logger.error(f"âœ— Final_Predictionså·¥ä½œè¡¨å†™å…¥å¤±è´¥: {e}")

    def _write_kronos_sheet(self, writer, kronos_df: pd.DataFrame, sheet_name: str = 'Kronos_Filter'):
        """å†™å…¥Kronosç›¸å…³å·¥ä½œè¡¨"""
        try:
            kronos_df.to_excel(writer, sheet_name=sheet_name, index=False)
            logger.info(f"Kronos sheet '{sheet_name}' written ({len(kronos_df)} rows)")
        except Exception as e:
            logger.error(f"Failed to write Kronos sheet '{sheet_name}': {e}")

    def _write_tradingagents_sheet(self, writer, tradingagents_df: pd.DataFrame):
        """å†™å…¥TradingAgents Analysiså·¥ä½œè¡¨"""
        try:
            # ç¡®ä¿æŒ‰tickeræ’åº
            if 'ticker' in tradingagents_df.columns:
                tradingagents_df = tradingagents_df.sort_values('ticker').reset_index(drop=True)

            # æ·»åŠ æ’ååˆ—ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
            if 'rank' not in tradingagents_df.columns:
                tradingagents_df.insert(0, 'rank', range(1, len(tradingagents_df) + 1))

            tradingagents_df.to_excel(writer, sheet_name='TradingAgents_Analysis', index=False)
            logger.info(f"âœ“ TradingAgents_Analysiså·¥ä½œè¡¨å·²å†™å…¥ ({len(tradingagents_df)} æ¡)")
        except Exception as e:
            logger.error(f"âœ— TradingAgents_Analysiså·¥ä½œè¡¨å†™å…¥å¤±è´¥: {e}")

    def _write_factor_contributions_sheet(self, writer, model_info: Dict):
        """å†™å…¥Factor Contributionså·¥ä½œè¡¨"""
        try:
            # é»˜è®¤å› å­è´¡çŒ®ï¼ˆå¦‚æœæ²¡æœ‰å®é™…æ•°æ®ï¼‰
            # REMOVED: ivol_60d (multicollinearity with stability_score, VIF=10.4)
            factor_data = [
                ['liquid_momentum', 0.058],
                ['obv_divergence', 0.052],
                ['ivol_20', 0.047],
                ['rsrs_beta_18', 0.045],
                ['rsi_21', 0.052],
                ['bollinger_squeeze', 0.035],
                ['blowoff_ratio', 0.033],
                ['atr_ratio', 0.032],
                ['vol_ratio_20d', 0.029],
                ['price_ma60_deviation', 0.044],
                ['near_52w_high', 0.062],
                ['ret_skew_20d', 0.039],
                ['trend_r2_60', 0.041]
            ]

            df = pd.DataFrame(factor_data, columns=['Factor', 'Contribution'])
            df = df.sort_values('Contribution', ascending=False).reset_index(drop=True)
            df.to_excel(writer, sheet_name='Factor_Contributions', index=False)
            logger.info("âœ“ Factor_Contributionså·¥ä½œè¡¨å·²å†™å…¥")

        except Exception as e:
            logger.error(f"âœ— Factor_Contributionså·¥ä½œè¡¨å†™å…¥å¤±è´¥: {e}")


    def _write_model_prediction_tables(self, writer, tables: Dict[str, pd.DataFrame], include_models: Optional[Tuple[str, ...]] = None, use_special_handlers: bool = True) -> None:
        """Write per-model prediction tables in a consistent order."""
        if not tables:
            return

        sheet_map = {
            'catboost': 'CatBoost_Predictions',
            'xgboost': 'XGBoost_Predictions',
            'elastic_net': 'ElasticNet_Predictions',
            'ridge': 'Ridge_Predictions',
            'lambdarank': 'LambdaRank_Predictions',
        }
        order = list(include_models) if include_models is not None else ['catboost', 'xgboost', 'elastic_net', 'ridge', 'lambdarank']

        for key in order:
            df = tables.get(key)
            if df is None or df.empty:
                continue

            try:
                if use_special_handlers and key == 'ridge':
                    self._write_ridge_sheet(writer, df)
                    continue
                if use_special_handlers and key == 'lambdarank':
                    self._write_lambda_sheet(writer, df)
                    continue

                export_df = df.copy()
                for col in ['date', 'actual_base_date', 'actual_target_date']:
                    if col in export_df.columns:
                        export_df[col] = pd.to_datetime(export_df[col], errors='coerce').dt.strftime('%Y-%m-%d')

                sheet_name = sheet_map.get(key, key.title())
                export_df.to_excel(writer, sheet_name=sheet_name, index=False)
                logger.info(f"[EXPORT] Wrote sheet {sheet_name} ({len(export_df)} rows)")
            except Exception as exc:
                logger.error(f"[EXPORT] Failed to write sheet for {key}: {exc}")

    def _write_top30_summary(self, writer, summary_df: pd.DataFrame) -> None:
        """Write summary statistics for top-30 T+5 returns."""
        if summary_df is None or summary_df.empty:
            return

        try:
            export_df = summary_df.copy()
            if 'avg_t5_return_pct' in export_df.columns:
                export_df['avg_t5_return_pct'] = pd.to_numeric(export_df['avg_t5_return_pct'], errors='coerce')
            export_df.to_excel(writer, sheet_name='Top30_T5_Summary', index=False)
            logger.info(f"[EXPORT] Wrote sheet Top30_T5_Summary ({len(export_df)} rows)")
        except Exception as exc:
            logger.error(f"[EXPORT] Failed to write Top30_T5_Summary: {exc}")

    def _write_top30_details(self, writer, detail_tables: Dict[str, pd.DataFrame]) -> None:
        """Write per-model top-30 T+5 return tables."""
        if not detail_tables:
            return

        sheet_map = {
            'catboost': 'CatBoost_Top30_T5',
            'xgboost': 'XGBoost_Top30_T5',
            'elastic_net': 'ElasticNet_Top30_T5',
            'ridge': 'Ridge_Top30_T5',
            'lambdarank': 'LambdaRank_Top30_T5',
        }
        order = ['catboost', 'xgboost', 'elastic_net', 'ridge', 'lambdarank']

        for key in order:
            df = detail_tables.get(key)
            if df is None or df.empty:
                continue

            try:
                export_df = df.copy()
                for col in ['date', 'actual_base_date', 'actual_target_date']:
                    if col in export_df.columns:
                        export_df[col] = pd.to_datetime(export_df[col], errors='coerce').dt.strftime('%Y-%m-%d')
                sheet_name = sheet_map.get(key, f"{key.title()}_Top30_T5")
                export_df = export_df.head(30)
                export_df.to_excel(writer, sheet_name=sheet_name, index=False)
                logger.info(f"[EXPORT] Wrote sheet {sheet_name} ({len(export_df)} rows)")
            except Exception as exc:
                logger.error(f"[EXPORT] Failed to write top30 detail for {key}: {exc}")



if __name__ == "__main__":
    # æµ‹è¯•
    logging.basicConfig(level=logging.INFO)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_predictions = pd.Series(
        [0.05, 0.03, -0.02, 0.01],
        index=['AAPL', 'GOOGL', 'MSFT', 'TSLA']
    )

    test_results = {
        'execution_time': 120.5
    }

    exporter = RobustExcelExporter()
    path = exporter.safe_export(
        predictions_series=test_predictions,
        analysis_results=test_results
    )

    print(f"\næµ‹è¯•å®Œæˆ: {path}")
