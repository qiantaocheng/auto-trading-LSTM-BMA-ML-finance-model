#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€å¹¶è¡Œè®­ç»ƒå¼•æ“ - æ­£ç¡®çš„æ•°æ®æµæ¶æ„

æ ¸å¿ƒæ”¹è¿›ï¼š
1. æ•°æ®æµåˆ†ç¦»ï¼šRidgeä½¿ç”¨ç¬¬ä¸€å±‚OOFï¼ŒLambdaRankä½¿ç”¨Alpha Factors
2. æ­£ç¡®çš„å¹¶è¡Œç­–ç•¥ï¼šé˜¶æ®µ1ç»Ÿä¸€è®­ç»ƒï¼Œé˜¶æ®µ2å¹¶è¡Œä¸åŒæ•°æ®æº
3. é›†æˆsimple25factorå¼•æ“å’Œpurged CV factory
4. ç¡®ä¿æ—¶é—´å®‰å…¨å’Œæ•°æ®è´¨é‡ä¸€è‡´æ€§
"""

import pandas as pd
import numpy as np
import logging
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Any, Optional, List

logger = logging.getLogger(__name__)


class UnifiedParallelTrainingEngine:
    """
    ç»Ÿä¸€å¹¶è¡Œè®­ç»ƒå¼•æ“ v3.0

    å…³é”®æ¶æ„æ”¹è¿›ï¼š
    1. æ•°æ®æµåˆ†ç¦»ï¼šRidgeä½¿ç”¨ç¬¬ä¸€å±‚OOFé¢„æµ‹ï¼ŒLambdaRankä½¿ç”¨Alpha FactorsåŸå§‹ç‰¹å¾
    2. æ­£ç¡®å¹¶è¡Œç­–ç•¥ï¼šå…ˆç»Ÿä¸€ç¬¬ä¸€å±‚ï¼Œå†å¹¶è¡ŒäºŒå±‚ï¼ˆä¸åŒæ•°æ®æºï¼‰
    3. æ—¶é—´å®‰å…¨ï¼šç»Ÿä¸€ä½¿ç”¨purged CV factory
    4. è´¨é‡ä¿è¯ï¼šé›†æˆsimple25factorå¼•æ“
    """

    def __init__(self, parent_model):
        """
        åˆå§‹åŒ–ç»Ÿä¸€å¹¶è¡Œè®­ç»ƒå¼•æ“

        Args:
            parent_model: ä¸»æ¨¡å‹å®ä¾‹ï¼ˆUltraEnhancedQuantitativeModelï¼‰
        """
        self.parent = parent_model
        self.timing_stats = {}
        self.quality_stats = {}

    def unified_parallel_train(self, X: pd.DataFrame, y: pd.Series,
                             dates: pd.Series, tickers: pd.Series,
                             alpha_factors: pd.DataFrame = None) -> Dict[str, Any]:
        """
        ç»Ÿä¸€å¹¶è¡Œè®­ç»ƒæ¶æ„

        æ­£ç¡®çš„å¹¶è¡Œç­–ç•¥ï¼š
        é˜¶æ®µ1: [ç»Ÿä¸€ç¬¬ä¸€å±‚è®­ç»ƒ] â†’ [ç»Ÿä¸€OOFé¢„æµ‹]
        é˜¶æ®µ2: åŸºäºç»Ÿä¸€OOFå¹¶è¡Œè®­ç»ƒ: [Ridge] || [LambdaRank]

        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        logger.info("="*70)
        logger.info("ğŸš€ ç»Ÿä¸€å¹¶è¡Œè®­ç»ƒå¼•æ“ v2.0")
        logger.info("   ä¿®å¤ï¼šæ•°æ®ä¸€è‡´æ€§ + æ­£ç¡®å¹¶è¡Œç­–ç•¥")
        logger.info("   é˜¶æ®µ1: ç»Ÿä¸€ç¬¬ä¸€å±‚è®­ç»ƒ")
        logger.info("   é˜¶æ®µ2: åŸºäºç›¸åŒOOFçš„å¹¶è¡ŒäºŒå±‚è®­ç»ƒ")
        logger.info("="*70)

        start_time = time.time()
        results = {
            'stage1_success': False,
            'ridge_success': False,
            'lambda_success': False,
            'unified_oof_predictions': None,
            'stacker_data': None,
            'timing': {},
            'quality_metrics': {}
        }

        try:
            # é˜¶æ®µ1ï¼šç»Ÿä¸€ç¬¬ä¸€å±‚è®­ç»ƒï¼ˆä½¿ç”¨simple25factorå¼•æ“ï¼‰
            stage1_start = time.time()
            logger.info("ğŸ“Š é˜¶æ®µ1: ç»Ÿä¸€ç¬¬ä¸€å±‚æ¨¡å‹è®­ç»ƒå¼€å§‹...")

            unified_first_layer_results = self._unified_first_layer_training(
                X, y, dates, tickers
            )

            if not unified_first_layer_results['success']:
                logger.error("âŒ é˜¶æ®µ1å¤±è´¥ï¼Œç»ˆæ­¢å¹¶è¡Œè®­ç»ƒ")
                return results

            results['stage1_success'] = True
            results['unified_oof_predictions'] = unified_first_layer_results['oof_predictions']
            results['timing']['stage1'] = time.time() - stage1_start

            logger.info(f"âœ… é˜¶æ®µ1å®Œæˆï¼Œè€—æ—¶: {results['timing']['stage1']:.2f}ç§’")
            self._log_oof_quality(unified_first_layer_results['oof_predictions'], y)

            # é˜¶æ®µ2ï¼šåŸºäºç»Ÿä¸€OOFçš„å¹¶è¡ŒäºŒå±‚è®­ç»ƒ
            stage2_start = time.time()
            logger.info("ğŸ”„ é˜¶æ®µ2: å¹¶è¡ŒäºŒå±‚è®­ç»ƒå¼€å§‹...")

            # æ˜¾å¼é€‰æ‹© DataFrame é¿å… pandas å¸ƒå°”æ­§ä¹‰
            af_df = alpha_factors if alpha_factors is not None else X
            parallel_results = self._parallel_second_layer_training(
                unified_first_layer_results['oof_predictions'],
                y, dates, tickers,
                alpha_factors=af_df
            )

            results.update({
                'ridge_success': parallel_results['ridge_success'],
                'lambda_success': parallel_results['lambda_success'],
                'stacker_data': parallel_results['stacker_data']
            })
            results['timing']['stage2'] = time.time() - stage2_start
            results['timing'].update(parallel_results['timing'])

            logger.info(f"âœ… é˜¶æ®µ2å®Œæˆï¼Œè€—æ—¶: {results['timing']['stage2']:.2f}ç§’")

        # å·²ç§»é™¤Rank-aware Blenderåˆå§‹åŒ–ï¼Œä¿ç•™ç©ºæ“ä½œ

        except Exception as e:
            logger.error(f"âŒ ç»Ÿä¸€å¹¶è¡Œè®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())

        # æ€§èƒ½ç»Ÿè®¡
        total_time = time.time() - start_time
        self._log_performance_summary(results, total_time)

        return results

    # Backward-compatible wrapper used by UltraEnhancedQuantitativeModel
    def train_unified_stackers(self, oof_predictions: Dict[str, pd.Series],
                               y: pd.Series, dates: pd.Series, tickers: pd.Series,
                               alpha_factors: pd.DataFrame = None) -> Dict[str, Any]:
        """Compatibility API: run only stage-2 training given precomputed OOF.

        Args:
            oof_predictions: dict of first-layer OOF prediction Series
            y: target Series aligned to dates/tickers
            dates: date Series for index construction
            tickers: ticker Series for index construction
            alpha_factors: optional alpha feature DataFrame for LambdaRank

        Returns:
            Dict with ridge_success, lambda_success, stacker_data, timing, lambda_percentile_info
        """
        return self._parallel_second_layer_training(
            unified_oof_predictions=oof_predictions,
            y=y,
            dates=dates,
            tickers=tickers,
            alpha_factors=alpha_factors
        )

    def _unified_first_layer_training(self, X: pd.DataFrame, y: pd.Series,
                                    dates: pd.Series, tickers: pd.Series) -> Dict[str, Any]:
        """
        ç»Ÿä¸€ç¬¬ä¸€å±‚è®­ç»ƒ - ç¡®ä¿æ•°æ®è´¨é‡ä¸€è‡´æ€§

        ä½¿ç”¨ç›¸åŒçš„å‚æ•°é…ç½®å’ŒCVç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆé«˜è´¨é‡çš„OOFé¢„æµ‹
        """
        logger.info("ğŸ—ï¸ ä½¿ç”¨ç»Ÿä¸€é…ç½®è¿›è¡Œç¬¬ä¸€å±‚è®­ç»ƒ...")

        try:
            # ä½¿ç”¨ä¸»æ¨¡å‹çš„ç»Ÿä¸€è®­ç»ƒæ–¹æ³•ï¼Œç¡®ä¿é…ç½®ä¸€è‡´æ€§
            first_layer_results = self.parent._unified_model_training(
                X, y, dates, tickers
            )

            if first_layer_results.get('success') and 'oof_predictions' in first_layer_results:
                # éªŒè¯OOFé¢„æµ‹è´¨é‡
                oof_preds = first_layer_results['oof_predictions']
                quality_metrics = self._validate_oof_quality(oof_preds, y)

                logger.info("âœ… ç»Ÿä¸€ç¬¬ä¸€å±‚è®­ç»ƒæˆåŠŸ")
                logger.info(f"   æ¨¡å‹æ•°é‡: {len(oof_preds)}")
                logger.info(f"   å¹³å‡IC: {quality_metrics['avg_ic']:.4f}")
                logger.info(f"   ICæ ‡å‡†å·®: {quality_metrics['ic_std']:.4f}")

                return {
                    'success': True,
                    'oof_predictions': oof_preds,
                    'models': first_layer_results.get('models', {}),
                    'cv_scores': first_layer_results.get('cv_scores', {}),
                    'quality_metrics': quality_metrics
                }
            else:
                logger.error("âŒ ç¬¬ä¸€å±‚è®­ç»ƒå¤±è´¥æˆ–ç¼ºå°‘OOFé¢„æµ‹")
                return {'success': False}

        except Exception as e:
            logger.error(f"âŒ ç»Ÿä¸€ç¬¬ä¸€å±‚è®­ç»ƒå¼‚å¸¸: {e}")
            return {'success': False}

    def _parallel_second_layer_training(self, unified_oof_predictions: Dict[str, pd.Series],
                                      y: pd.Series, dates: pd.Series, tickers: pd.Series,
                                      alpha_factors: pd.DataFrame = None) -> Dict[str, Any]:
        """
        å¹¶è¡ŒäºŒå±‚è®­ç»ƒ - åŸºäºç»Ÿä¸€çš„OOFé¢„æµ‹

        è¿™æ˜¯çœŸæ­£çš„å¹¶è¡Œï¼šä¸¤ä¸ªæ¨¡å‹ä½¿ç”¨ç›¸åŒçš„è¾“å…¥æ•°æ®
        """
        logger.info("âš¡ å¼€å§‹å¹¶è¡ŒäºŒå±‚è®­ç»ƒï¼ˆåŸºäºç»Ÿä¸€OOFï¼‰...")

        results = {
            'ridge_success': False,
            'lambda_success': False,
            'stacker_data': None,
            'timing': {}
        }

        # æ„å»ºä¸åŒçš„æ•°æ®æº
        # Ridgeä½¿ç”¨OOF
        ridge_data = self._build_unified_stacker_data(
            unified_oof_predictions, y, dates, tickers
        )

        # LambdaRankä½¿ç”¨Alpha Factors
        lambda_data = self._build_lambda_data(
            alpha_factors, y, dates, tickers
        )

        if ridge_data is None:
            logger.error("âŒ æ— æ³•æ„å»ºRidge stackeræ•°æ®")
            return results

        if lambda_data is None:
            logger.error("âŒ æ— æ³•æ„å»ºLambdaRankæ•°æ®")
            # å³ä½¿LambdaRankæ•°æ®å¤±è´¥ï¼Œä»å¯è®­ç»ƒRidge
            lambda_data = ridge_data  # é™çº§ä½¿ç”¨OOF

        logger.info(f"ğŸ“Š Ridgeæ•°æ®å½¢çŠ¶: {ridge_data.shape}")
        logger.info(f"ğŸ“Š LambdaRankæ•°æ®å½¢çŠ¶: {lambda_data.shape}")
        results['stacker_data'] = ridge_data  # ä¿ç•™å…¼å®¹æ€§

        # å¯ç”¨LambdaRankï¼šå¿…é¡»å¼€å¯ï¼ˆå¯ç”¨ä¸”æ ·æœ¬é‡è¶³å¤Ÿï¼‰ï¼Œå¦åˆ™ä¸­æ­¢æµç¨‹
        lambda_available = self._check_lambda_available()
        lambda_data_valid = (lambda_data is not None and len(lambda_data) > 0)
        sample_count_ok = len(ridge_data) >= 12  # æ”¾å®½ä»¥æ”¯æŒå°æ ·æœ¬

        logger.info(f"ğŸ“Š Lambdaå¯ç”¨æ£€æŸ¥:")
        logger.info(f"   Lambdaå¯å¯¼å…¥: {lambda_available}")
        logger.info(f"   Lambdaæ•°æ®æœ‰æ•ˆ: {lambda_data_valid}")
        logger.info(f"   æ ·æœ¬æ•°é‡: {len(ridge_data)} (éœ€è¦>=50: {sample_count_ok})")

        use_lambda = (lambda_available and sample_count_ok)

        if not use_lambda:
            logger.warning(
                f"âš ï¸ LambdaRankæœªå¯ç”¨æˆ–æ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡Lambdaè®­ç»ƒå¹¶ä»…è®­ç»ƒRidge"
            )
            # ç›´æ¥è®­ç»ƒRidgeå¹¶è¿”å›
            ridge_start = time.time()
            ridge_success = self.parent._train_ridge_stacker(
                unified_oof_predictions, y, dates, ridge_data=ridge_data
            )
            results['ridge_success'] = ridge_success
            results['timing']['ridge'] = time.time() - ridge_start
            logger.info(f"âœ… Ridgeè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {results['timing']['ridge']:.2f}ç§’")
            return results

        if lambda_data is None or len(lambda_data) == 0:
            logger.error("âŒ Lambdaæ•°æ®ä¸ºç©ºï¼Œè®­ç»ƒä¸­æ­¢")
            raise RuntimeError(f"Lambda data is empty or None")

        # é¡ºåºè®­ç»ƒï¼šå…ˆLambdaç”Ÿæˆpercentileï¼Œå†Ridgeä½¿ç”¨
        logger.info("ğŸ”„ æ–°èåˆç­–ç•¥ï¼šLambda Percentile â†’ Ridge Stacker")

        # æ­¥éª¤1ï¼šè®­ç»ƒLambdaRankï¼Œç”ŸæˆOOFé¢„æµ‹
        logger.info("="*60)
        logger.info("ğŸš€ æ­¥éª¤1: å¼€å§‹è®­ç»ƒLambdaRankæ¨¡å‹")
        logger.info(f"   Lambdaæ•°æ®å½¢çŠ¶: {lambda_data.shape}")
        logger.info(f"   Lambdaç‰¹å¾æ•°: {lambda_data.shape[1] - 1}")  # å‡å»targetåˆ—
        logger.info("="*60)

        lambda_start = time.time()
        lambda_result = self._train_lambda_unified(lambda_data)
        results['lambda_success'] = lambda_result['success']
        results['timing']['lambda'] = time.time() - lambda_start

        logger.info(f"ğŸ“Š Lambdaè®­ç»ƒç»“æœ: {'æˆåŠŸâœ…' if lambda_result['success'] else 'å¤±è´¥âŒ'}")
        logger.info(f"   è€—æ—¶: {results['timing']['lambda']:.2f}ç§’")

        if lambda_result['success']:
            self.parent.lambda_rank_stacker = lambda_result['model']
            logger.info(f"âœ… LambdaRankè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {results['timing']['lambda']:.2f}ç§’")

            # æ­¥éª¤2ï¼šè®¡ç®—Lambda OOF percentile
            logger.info("="*60)
            logger.info("ğŸ”§ æ­¥éª¤2: ç”ŸæˆLambda Percentileç‰¹å¾")
            logger.info("="*60)

            try:
                # è·å–Lambdaæ¨¡å‹çš„çœŸæ­£OOFé¢„æµ‹ï¼ˆé˜²æ•°æ®æ³„æ¼ï¼‰
                lambda_model = lambda_result['model']

                # ğŸ”§ ç¡®ä¿lambda_dataå¯¹é½åˆ°ridge_dataçš„ç´¢å¼•
                if not lambda_data.index.equals(ridge_data.index):
                    logger.info(f"ğŸ”§ å¯¹é½Lambdaæ•°æ®åˆ°Ridgeç´¢å¼•")
                    logger.info(f"   LambdaåŸå§‹: {len(lambda_data)} æ ·æœ¬")
                    logger.info(f"   Ridgeç›®æ ‡: {len(ridge_data)} æ ·æœ¬")
                    lambda_data_aligned = lambda_data.reindex(ridge_data.index)
                else:
                    logger.info(f"âœ… Lambdaå’ŒRidgeç´¢å¼•å·²å¯¹é½")
                    lambda_data_aligned = lambda_data

                lambda_oof = lambda_model.get_oof_predictions(lambda_data_aligned)
                logger.info(f"âœ… Lambda OOFé¢„æµ‹è·å–å®Œæˆ: {len(lambda_oof)} æ ·æœ¬")

                # ğŸ”§ Critical Fix: ä½¿ç”¨ä¸€è‡´æ€§è½¬æ¢å™¨è®¡ç®—percentile
                from bma_models.lambda_percentile_transformer import LambdaPercentileTransformer

                # åˆ›å»ºå¹¶æ‹Ÿåˆè½¬æ¢å™¨
                lambda_percentile_transformer = LambdaPercentileTransformer(method='quantile')
                lambda_percentile_series = lambda_percentile_transformer.fit_transform(lambda_oof)

                # ä¿å­˜è½¬æ¢å™¨ä¾›é¢„æµ‹æ—¶ä½¿ç”¨
                self.parent.lambda_percentile_transformer = lambda_percentile_transformer

                logger.info(f"âœ… Lambda Percentileè½¬æ¢å™¨å·²åˆ›å»ºå¹¶ä¿å­˜")
                logger.info(f"   Percentileç»Ÿè®¡: å‡å€¼={lambda_percentile_series.mean():.1f}, èŒƒå›´=[{lambda_percentile_series.min():.1f}, {lambda_percentile_series.max():.1f}]")

                # ğŸ“Š è¯¦ç»†ç´¢å¼•å¯¹é½è¯Šæ–­
                logger.info(f"ğŸ“Š ç´¢å¼•å¯¹é½è¯Šæ–­:")
                logger.info(f"   Ridgeå½¢çŠ¶: {ridge_data.shape}")
                logger.info(f"   Lambda Percentileå½¢çŠ¶: {lambda_percentile_series.shape}")
                logger.info(f"   ç´¢å¼•å®Œå…¨åŒ¹é…: {ridge_data.index.equals(lambda_percentile_series.index)}")

                # ğŸ”§ éªŒè¯ç´¢å¼•å¯¹é½
                if not lambda_percentile_series.index.equals(ridge_data.index):
                    logger.warning(f"âš ï¸ Lambda Percentileç´¢å¼•ä¸åŒ¹é…ï¼Œå¼ºåˆ¶å¯¹é½")
                    lambda_percentile_series = lambda_percentile_series.reindex(ridge_data.index)

                    # æ£€æŸ¥NaNæ¯”ä¾‹
                    nan_count = lambda_percentile_series.isna().sum()
                    nan_ratio = nan_count / len(ridge_data)
                    logger.warning(f"   å¯¹é½åNaN: {nan_count} ({nan_ratio:.2%})")

                    if nan_ratio > 0.05:
                        logger.error(f"âŒ Lambda Percentile NaNæ¯”ä¾‹è¿‡é«˜ ({nan_ratio:.2%})")
                        raise ValueError("Lambda Percentileå¯¹é½å¤±è´¥ï¼ŒNaNè¿‡å¤š")

                # æ­¥éª¤3ï¼šåŠ å…¥Ridgeæ•°æ®
                logger.info("="*60)
                logger.info("ğŸ”§ æ­¥éª¤3: å°†Lambda PercentileåŠ å…¥Ridgeç‰¹å¾")
                logger.info("="*60)
                logger.info(f"   RidgeåŸå§‹ç‰¹å¾: {list(ridge_data.columns)}")

                ridge_data['lambda_percentile'] = lambda_percentile_series

                logger.info(f"âœ… Lambda Percentileå·²åŠ å…¥Ridgeç‰¹å¾")
                logger.info(f"   Ridgeæ–°ç‰¹å¾: {list(ridge_data.columns)}")
                logger.info(f"   Lambda Percentileæ— NaN: {lambda_percentile_series.notna().all()}")
                logger.info(f"   Ridgeæ•°æ®æœ€ç»ˆå½¢çŠ¶: {ridge_data.shape}")

                # æ”¶é›†Lambda Percentileä¿¡æ¯ç”¨äºExcelå¯¼å‡º
                results['lambda_percentile_info'] = {
                    'n_factors': len(lambda_model._alpha_factor_cols) if hasattr(lambda_model, '_alpha_factor_cols') else 15,
                    'oof_samples': len(lambda_oof),
                    'percentile_mean': float(lambda_percentile_series.mean()),
                    'percentile_min': float(lambda_percentile_series.min()),
                    'percentile_max': float(lambda_percentile_series.max()),
                    'alignment_status': 'å®Œå…¨å¯¹é½' if ridge_data.index.equals(lambda_percentile_series.index) else 'å·²å¼ºåˆ¶å¯¹é½',
                    'nan_ratio': float(lambda_percentile_series.isna().sum() / len(lambda_percentile_series))
                }

            except Exception as e_perc:
                logger.warning(f"âš ï¸ è®¡ç®—Lambda Percentileå¤±è´¥: {e_perc}")
                logger.warning("   Ridgeå°†ä¸ä½¿ç”¨Lambdaç‰¹å¾")
        else:
            logger.warning("âš ï¸ LambdaRankè®­ç»ƒå¤±è´¥ï¼Œç»§ç»­ä»…ç”¨Ridgeæµç¨‹")

        # æ­¥éª¤4ï¼šè®­ç»ƒRidge Stackerï¼ˆä½¿ç”¨OOF + Lambda Percentileï¼‰
        ridge_start = time.time()
        ridge_success = self.parent._train_ridge_stacker(
            unified_oof_predictions, y, dates, ridge_data=ridge_data
        )
        results['ridge_success'] = ridge_success
        results['timing']['ridge'] = time.time() - ridge_start
        logger.info(f"âœ… Ridgeè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {results['timing']['ridge']:.2f}ç§’")

        return results

    def _build_unified_stacker_data(self, oof_predictions: Dict[str, pd.Series],
                                  y: pd.Series, dates: pd.Series, tickers: pd.Series) -> Optional[pd.DataFrame]:
        """
        æ„å»ºç»Ÿä¸€çš„stackeræ•°æ®é›†

        ç¡®ä¿Ridgeä¸LambdaRankä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ•°æ®
        """
        try:
            if not isinstance(y.index, pd.MultiIndex):
                multi_index = pd.MultiIndex.from_arrays(
                    [dates, tickers], names=['date', 'ticker']
                )
                y_indexed = pd.Series(y.values, index=multi_index)
            else:
                y_indexed = y

            stacker_dict: Dict[str, pd.Series] = {}
            for model_name, pred_series in oof_predictions.items():
                key = f'pred_{model_name}'
                if isinstance(pred_series.index, pd.MultiIndex):
                    stacker_dict[key] = pred_series
                else:
                    stacker_dict[key] = pd.Series(pred_series.values, index=y_indexed.index)

            parent_horizon = getattr(self.parent, 'horizon', 5)
            if parent_horizon != 5:
                raise ValueError(f'Unified stacker expects parent horizon=5, got {parent_horizon}.')

            target_col = 'ret_fwd_10d'
            stacker_dict[target_col] = y_indexed

            stacker_data = pd.DataFrame(stacker_dict)

            missing_data = stacker_data.isnull().sum()
            if missing_data.any():
                logger.warning(f"[Stacker] æ•°æ®ç¼ºå¤±: {missing_data.to_dict()}")

            feature_cols = [c for c in stacker_data.columns if c != target_col]
            stacker_data[feature_cols] = stacker_data[feature_cols].fillna(0.0)
            clean_data = stacker_data.dropna(subset=[target_col])

            if len(clean_data) < len(stacker_data) * 0.8:
                retention = len(clean_data) / len(stacker_data) * 100
                logger.warning(f"[Stacker] ç›®æ ‡åˆ—æ¸…ç†åå‰©ä½™ {len(clean_data)}/{len(stacker_data)} ({retention:.1f}%)")

            logger.info(f"[Stacker] æ„å»ºå®Œæˆ: {clean_data.shape}")
            return clean_data

        except Exception as e:
            logger.error(f"æ„å»ºstackeræ•°æ®å¤±è´¥: {e}")
            return None

    def _train_ridge_unified(self, oof_predictions: Dict[str, pd.Series],
                           y: pd.Series, dates: pd.Series) -> Dict[str, Any]:
        """
        è®­ç»ƒRidge Stackerï¼ˆç»Ÿä¸€æ•°æ®æºç‰ˆæœ¬ï¼‰
        """
        start_time = time.time()
        try:
            logger.info("[Ridge-Thread] å¼€å§‹è®­ç»ƒRidge Stacker...")
            success = self.parent._train_ridge_stacker(oof_predictions, y, dates)

            result = {
                'success': success,
                'elapsed_time': time.time() - start_time
            }

            if success:
                logger.info("[Ridge-Thread] âœ… Ridgeè®­ç»ƒæˆåŠŸ")
            else:
                logger.error("[Ridge-Thread] âŒ Ridgeè®­ç»ƒå¤±è´¥")

            return result

        except Exception as e:
            logger.error(f"[Ridge-Thread] è®­ç»ƒå¼‚å¸¸: {e}")
            return {
                'success': False,
                'elapsed_time': time.time() - start_time
            }


    def _train_lambda_unified(self, stacker_data: pd.DataFrame) -> Dict[str, Any]:
        """Train LambdaRank stacker using the unified stacker dataset."""
        start_time = time.time()
        try:
            from bma_models.lambda_rank_stacker import LambdaRankStacker
            from bma_models.unified_config_loader import get_time_config

            logger.info("[Lambda-Thread] ????LambdaRank...")

            time_config = get_time_config()
            time_horizon = int(getattr(time_config, 'prediction_horizon_days', getattr(self.parent, 'horizon', 10)))
            parent_horizon = int(getattr(self.parent, 'horizon', time_horizon))
            if parent_horizon != time_horizon:
                logger.warning(f"[Lambda-Thread] Parent horizon {parent_horizon}d != config horizon {time_horizon}d; using config value")

            cv_splits = int(getattr(time_config, 'cv_n_splits', 6))
            cv_gap = int(getattr(time_config, 'cv_gap_days', 5))
            cv_embargo = int(getattr(time_config, 'cv_embargo_days', 5))
            logger.info(f"[Lambda-Thread] CV config splits/gap/embargo = {(cv_splits, cv_gap, cv_embargo)}")

            required_target = f'ret_fwd_{time_horizon}d'
            if required_target not in stacker_data.columns:
                available_targets = [col for col in stacker_data.columns if col.startswith('ret_fwd_')]
                raise KeyError(f"Missing {required_target} in stacker data. Available targets: {available_targets}")

            prohibited_cols = {required_target, 'target'}
            feature_cols = [
                col for col in stacker_data.columns
                if col not in prohibited_cols and not col.startswith('ret_fwd_')
            ]
            if not feature_cols:
                raise ValueError('No usable feature columns for LambdaRank after enforcing target alignment.')

            lambda_training_df = stacker_data[feature_cols + [required_target]].copy()
            logger.info(f"[Lambda-Thread] ?????{required_target}")
            logger.info(f"[Lambda-Thread] ??????: {feature_cols}")

            lambda_config = {
                'base_cols': tuple(feature_cols),
                'n_quantiles': 128,
                'winsorize_quantiles': (0.01, 0.99),
                'label_gain_power': 1.5,
                'num_boost_round': 100,
                'early_stopping_rounds': 0,
                'use_purged_cv': True,
                'cv_n_splits': cv_splits,
                'cv_gap_days': cv_gap,
                'cv_embargo_days': cv_embargo,
                'random_state': 42
            }

            lambda_stacker = LambdaRankStacker(**lambda_config)
            lambda_stacker.fit(lambda_training_df, target_col=required_target)

            logger.info("[Lambda-Thread] ? LambdaRank????")

            return {
                'success': True,
                'model': lambda_stacker,
                'elapsed_time': time.time() - start_time
            }

        except Exception as e:
            logger.error(f"[Lambda-Thread] ????: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'success': False,
                'model': None,
                'elapsed_time': time.time() - start_time
            }
    def _check_lambda_available(self) -> bool:
        """æ£€æŸ¥LambdaRankæ˜¯å¦å¯ç”¨"""
        try:
            from bma_models.lambda_rank_stacker import LambdaRankStacker
            from bma_models.rank_aware_blender import RankAwareBlender
            return True
        except ImportError:
            return False

    def _init_unified_blender(self):
        """(Removed) Rank-aware blender deprecated; no-op for compatibility."""
        return None

    def _validate_oof_quality(self, oof_predictions: Dict[str, pd.Series], y: pd.Series) -> Dict[str, float]:
        """éªŒè¯OOFé¢„æµ‹è´¨é‡"""
        from scipy.stats import spearmanr

        quality_metrics = {
            'avg_ic': 0.0,
            'ic_std': 0.0,
            'min_ic': 0.0,
            'max_ic': 0.0
        }

        try:
            ics = []
            for model_name, pred_series in oof_predictions.items():
                # å¯¹é½é¢„æµ‹å’Œç›®æ ‡
                aligned_pred = pred_series.reindex(y.index)
                valid_mask = ~(aligned_pred.isna() | y.isna())

                if valid_mask.sum() > 10:  # è‡³å°‘10ä¸ªæœ‰æ•ˆæ ·æœ¬
                    ic, _ = spearmanr(
                        aligned_pred[valid_mask],
                        y[valid_mask]
                    )
                    if not np.isnan(ic):
                        ics.append(ic)

            if ics:
                quality_metrics['avg_ic'] = np.mean(ics)
                quality_metrics['ic_std'] = np.std(ics)
                quality_metrics['min_ic'] = np.min(ics)
                quality_metrics['max_ic'] = np.max(ics)

        except Exception as e:
            logger.warning(f"âš ï¸ è´¨é‡éªŒè¯å¤±è´¥: {e}")

        return quality_metrics

    def _log_oof_quality(self, oof_predictions: Dict[str, pd.Series], y: pd.Series):
        """è®°å½•OOFé¢„æµ‹è´¨é‡"""
        quality_metrics = self._validate_oof_quality(oof_predictions, y)

        logger.info("ğŸ“Š OOFé¢„æµ‹è´¨é‡æŠ¥å‘Š:")
        logger.info(f"   å¹³å‡IC: {quality_metrics['avg_ic']:.4f}")
        logger.info(f"   ICèŒƒå›´: [{quality_metrics['min_ic']:.4f}, {quality_metrics['max_ic']:.4f}]")
        logger.info(f"   ICæ ‡å‡†å·®: {quality_metrics['ic_std']:.4f}")

        # è´¨é‡è­¦å‘Š
        if quality_metrics['avg_ic'] < 0.01:
            logger.warning("âš ï¸ å¹³å‡ICè¿‡ä½ï¼Œå¯èƒ½å½±å“äºŒå±‚æ¨¡å‹è´¨é‡")
        if quality_metrics['ic_std'] > 0.1:
            logger.warning("âš ï¸ ICæ³¢åŠ¨è¿‡å¤§ï¼Œæ¨¡å‹ç¨³å®šæ€§å¯èƒ½å—å½±å“")

    def _log_performance_summary(self, results: Dict[str, Any], total_time: float):
        """è®°å½•æ€§èƒ½æ€»ç»“"""
        logger.info("="*70)
        logger.info("ğŸ“Š ç»Ÿä¸€å¹¶è¡Œè®­ç»ƒæ€§èƒ½æŠ¥å‘Š:")

        if 'stage1' in results['timing']:
            logger.info(f"   é˜¶æ®µ1ï¼ˆç»Ÿä¸€ç¬¬ä¸€å±‚ï¼‰: {results['timing']['stage1']:.2f}ç§’")

        if 'stage2' in results['timing']:
            logger.info(f"   é˜¶æ®µ2ï¼ˆå¹¶è¡ŒäºŒå±‚ï¼‰: {results['timing']['stage2']:.2f}ç§’")

        if 'ridge' in results['timing'] and 'lambda' in results['timing']:
            ridge_time = results['timing']['ridge']
            lambda_time = results['timing']['lambda']
            sequential_time = ridge_time + lambda_time
            parallel_time = max(ridge_time, lambda_time)
            time_saved = sequential_time - parallel_time

            logger.info(f"   Ridgeæ—¶é—´: {ridge_time:.2f}ç§’")
            logger.info(f"   LambdaRankæ—¶é—´: {lambda_time:.2f}ç§’")
            logger.info(f"   å¹¶è¡ŒèŠ‚çœæ—¶é—´: {time_saved:.2f}ç§’")
            if sequential_time > 0:
                logger.info(f"   äºŒå±‚åŠ é€Ÿæ¯”: {sequential_time/parallel_time:.2f}x")

        logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
        logger.info(f"   é˜¶æ®µ1æˆåŠŸ: {results['stage1_success']}")
        logger.info(f"   RidgeæˆåŠŸï¼ˆOOFï¼‰: {results['ridge_success']}")
        logger.info(f"   LambdaæˆåŠŸï¼ˆAlphaï¼‰: {results['lambda_success']}")
        logger.info("="*70)

    def _build_lambda_data(self, alpha_factors, y: pd.Series,
                          dates: pd.Series, tickers: pd.Series) -> Optional[pd.DataFrame]:
        """
        æ„å»ºLambdaRankçš„æ•°æ®ï¼ˆä½¿ç”¨Alpha Factorsæˆ–fallbackåˆ°OOFï¼‰
        """
        try:
            # åˆ›å»ºMultiIndex
            if not isinstance(y.index, pd.MultiIndex):
                multi_index = pd.MultiIndex.from_arrays(
                    [dates, tickers], names=['date', 'ticker']
                )
                y_indexed = pd.Series(y.values, index=multi_index)
            else:
                y_indexed = y

            # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
            if isinstance(alpha_factors, dict):
                raise ValueError("Unified lambda data builder requires alpha factor DataFrame; dict-based inputs are no longer supported.")
            else:
                # æ­£å¸¸çš„Alpha Factors DataFrame
                logger.info("ğŸ¯ ä½¿ç”¨Alpha Factorsæ„å»ºLambdaRankæ•°æ®")
                if isinstance(alpha_factors.index, pd.MultiIndex):
                    lambda_data = alpha_factors.copy()
                else:
                    # è®¾ç½®MultiIndex
                    lambda_data = alpha_factors.copy()
                    lambda_data.index = multi_index
                # å¦‚å­˜åœ¨ä¸ç´¢å¼•é‡å¤çš„è¾…åŠ©åˆ—ï¼Œå…ˆç§»é™¤ï¼Œé¿å…æ­§ä¹‰
                lambda_data = lambda_data.drop(columns=['date', 'ticker'], errors='ignore')

                # ç§»é™¤é¢„æµ‹åˆ—ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                pred_cols = [col for col in lambda_data.columns if 'pred_' in col.lower()]
                if pred_cols:
                    lambda_data = lambda_data.drop(columns=pred_cols)
                    logger.info(f"   ç§»é™¤{len(pred_cols)}ä¸ªé¢„æµ‹åˆ—")

            # æ·»åŠ ç›®æ ‡å˜é‡ï¼ˆåŠ¨æ€T+H from unified time configï¼‰
            # åŠ¨æ€ç›®æ ‡åˆ—ï¼šä¸¥æ ¼ä½¿ç”¨T+5
            from bma_models.unified_config_loader import get_time_config
            horizon_days = int(get_time_config().prediction_horizon_days)
            if horizon_days != 5:
                raise ValueError(f'Unified lambda data builder expects prediction_horizon_days=5, got {horizon_days}.')
            target_col = 'ret_fwd_10d'
            lambda_data[target_col] = y_indexed

            # éªŒè¯æ•°æ®
            feature_count = lambda_data.shape[1] - 1  # å‡å»targetåˆ—
            logger.info(f"ğŸ“Š LambdaRankæ•°æ®: {lambda_data.shape[0]}è¡Œ Ã— {feature_count}ä¸ªç‰¹å¾")

            # æ¸…ç†NaN
            clean_data = lambda_data.dropna()
            if len(clean_data) < len(lambda_data) * 0.8:
                logger.warning(f"âš ï¸ æ¸…ç†åå‰©ä½™ {len(clean_data)}/{len(lambda_data)} æ ·æœ¬")

            return clean_data

        except Exception as e:
            logger.error(f"âŒ æ„å»ºLambdaRankæ•°æ®å¤±è´¥: {e}")
            return None
