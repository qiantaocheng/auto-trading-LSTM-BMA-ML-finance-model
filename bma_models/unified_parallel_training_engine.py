#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€å¹¶è¡Œè®­ç»ƒå¼•æ“ - æ­£ç¡®çš„æ•°æ®æµæ¶æ„

æ ¸å¿ƒæ”¹è¿›ï¼š
1. æ•°æ®æµåˆ†ç¦»ï¼šRidgeä½¿ç”¨ç¬¬ä¸€å±‚OOFï¼ŒLambdaRankä½¿ç”¨Alpha Factors
2. æ­£ç¡®çš„å¹¶è¡Œç­–ç•¥ï¼šé˜¶æ®µ1ç»Ÿä¸€è®­ç»ƒï¼Œé˜¶æ®µ2å¹¶è¡Œä¸åŒæ•°æ®æº
3. é›†æˆsimple25factorå¼•æ“å’Œpurged CV factory
4. ç¡®ä¿æ—¶é—´å®‰å…¨å’Œæ•°æ®è´¨é‡ä¸€è‡´æ€§
"""

import pandas as pd
import numpy as np
import logging
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Any, Optional, List

logger = logging.getLogger(__name__)


class UnifiedParallelTrainingEngine:
    """
    ç»Ÿä¸€å¹¶è¡Œè®­ç»ƒå¼•æ“ v3.0

    å…³é”®æ¶æ„æ”¹è¿›ï¼š
    1. æ•°æ®æµåˆ†ç¦»ï¼šRidgeä½¿ç”¨ç¬¬ä¸€å±‚OOFé¢„æµ‹ï¼ŒLambdaRankä½¿ç”¨Alpha FactorsåŸå§‹ç‰¹å¾
    2. æ­£ç¡®å¹¶è¡Œç­–ç•¥ï¼šå…ˆç»Ÿä¸€ç¬¬ä¸€å±‚ï¼Œå†å¹¶è¡ŒäºŒå±‚ï¼ˆä¸åŒæ•°æ®æºï¼‰
    3. æ—¶é—´å®‰å…¨ï¼šç»Ÿä¸€ä½¿ç”¨purged CV factory
    4. è´¨é‡ä¿è¯ï¼šé›†æˆsimple25factorå¼•æ“
    """

    def __init__(self, parent_model):
        """
        åˆå§‹åŒ–ç»Ÿä¸€å¹¶è¡Œè®­ç»ƒå¼•æ“

        Args:
            parent_model: ä¸»æ¨¡å‹å®ä¾‹ï¼ˆUltraEnhancedQuantitativeModelï¼‰
        """
        self.parent = parent_model
        self.timing_stats = {}
        self.quality_stats = {}

    def unified_parallel_train(self, X: pd.DataFrame, y: pd.Series,
                             dates: pd.Series, tickers: pd.Series,
                             alpha_factors: pd.DataFrame = None) -> Dict[str, Any]:
        """
        ç»Ÿä¸€å¹¶è¡Œè®­ç»ƒæ¶æ„

        æ­£ç¡®çš„å¹¶è¡Œç­–ç•¥ï¼š
        é˜¶æ®µ1: [ç»Ÿä¸€ç¬¬ä¸€å±‚è®­ç»ƒ] â†’ [ç»Ÿä¸€OOFé¢„æµ‹]
        é˜¶æ®µ2: åŸºäºç»Ÿä¸€OOFå¹¶è¡Œè®­ç»ƒ: [Ridge] || [LambdaRank]

        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        logger.info("="*70)
        logger.info("ğŸš€ ç»Ÿä¸€å¹¶è¡Œè®­ç»ƒå¼•æ“ v2.0")
        logger.info("   ä¿®å¤ï¼šæ•°æ®ä¸€è‡´æ€§ + æ­£ç¡®å¹¶è¡Œç­–ç•¥")
        logger.info("   é˜¶æ®µ1: ç»Ÿä¸€ç¬¬ä¸€å±‚è®­ç»ƒ")
        logger.info("   é˜¶æ®µ2: åŸºäºç›¸åŒOOFçš„å¹¶è¡ŒäºŒå±‚è®­ç»ƒ")
        logger.info("="*70)

        start_time = time.time()
        results = {
            'stage1_success': False,
            'ridge_success': False,
            'lambda_success': False,
            'unified_oof_predictions': None,
            'stacker_data': None,
            'timing': {},
            'quality_metrics': {}
        }

        try:
            # é˜¶æ®µ1ï¼šç»Ÿä¸€ç¬¬ä¸€å±‚è®­ç»ƒï¼ˆä½¿ç”¨simple25factorå¼•æ“ï¼‰
            stage1_start = time.time()
            logger.info("ğŸ“Š é˜¶æ®µ1: ç»Ÿä¸€ç¬¬ä¸€å±‚æ¨¡å‹è®­ç»ƒå¼€å§‹...")

            unified_first_layer_results = self._unified_first_layer_training(
                X, y, dates, tickers
            )

            if not unified_first_layer_results['success']:
                logger.error("âŒ é˜¶æ®µ1å¤±è´¥ï¼Œç»ˆæ­¢å¹¶è¡Œè®­ç»ƒ")
                return results

            results['stage1_success'] = True
            results['unified_oof_predictions'] = unified_first_layer_results['oof_predictions']
            results['timing']['stage1'] = time.time() - stage1_start

            logger.info(f"âœ… é˜¶æ®µ1å®Œæˆï¼Œè€—æ—¶: {results['timing']['stage1']:.2f}ç§’")
            self._log_oof_quality(unified_first_layer_results['oof_predictions'], y)

            # é˜¶æ®µ2ï¼šåŸºäºç»Ÿä¸€OOFçš„å¹¶è¡ŒäºŒå±‚è®­ç»ƒ
            stage2_start = time.time()
            logger.info("ğŸ”„ é˜¶æ®µ2: å¹¶è¡ŒäºŒå±‚è®­ç»ƒå¼€å§‹...")

            parallel_results = self._parallel_second_layer_training(
                unified_first_layer_results['oof_predictions'],
                y, dates, tickers,
                alpha_factors=alpha_factors or X  # å¦‚æœæ²¡æœ‰æä¾›alpha_factorsï¼Œä½¿ç”¨X
            )

            results.update({
                'ridge_success': parallel_results['ridge_success'],
                'lambda_success': parallel_results['lambda_success'],
                'stacker_data': parallel_results['stacker_data']
            })
            results['timing']['stage2'] = time.time() - stage2_start
            results['timing'].update(parallel_results['timing'])

            logger.info(f"âœ… é˜¶æ®µ2å®Œæˆï¼Œè€—æ—¶: {results['timing']['stage2']:.2f}ç§’")

            # åˆå§‹åŒ–Blender (å¦‚æœä¸¤ä¸ªæ¨¡å‹éƒ½æˆåŠŸ)
            if results['ridge_success'] and results['lambda_success']:
                self._init_unified_blender()

        except Exception as e:
            logger.error(f"âŒ ç»Ÿä¸€å¹¶è¡Œè®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())

        # æ€§èƒ½ç»Ÿè®¡
        total_time = time.time() - start_time
        self._log_performance_summary(results, total_time)

        return results

    def _unified_first_layer_training(self, X: pd.DataFrame, y: pd.Series,
                                    dates: pd.Series, tickers: pd.Series) -> Dict[str, Any]:
        """
        ç»Ÿä¸€ç¬¬ä¸€å±‚è®­ç»ƒ - ç¡®ä¿æ•°æ®è´¨é‡ä¸€è‡´æ€§

        ä½¿ç”¨ç›¸åŒçš„å‚æ•°é…ç½®å’ŒCVç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆé«˜è´¨é‡çš„OOFé¢„æµ‹
        """
        logger.info("ğŸ—ï¸ ä½¿ç”¨ç»Ÿä¸€é…ç½®è¿›è¡Œç¬¬ä¸€å±‚è®­ç»ƒ...")

        try:
            # ä½¿ç”¨ä¸»æ¨¡å‹çš„ç»Ÿä¸€è®­ç»ƒæ–¹æ³•ï¼Œç¡®ä¿é…ç½®ä¸€è‡´æ€§
            first_layer_results = self.parent._unified_model_training(
                X, y, dates, tickers
            )

            if first_layer_results.get('success') and 'oof_predictions' in first_layer_results:
                # éªŒè¯OOFé¢„æµ‹è´¨é‡
                oof_preds = first_layer_results['oof_predictions']
                quality_metrics = self._validate_oof_quality(oof_preds, y)

                logger.info("âœ… ç»Ÿä¸€ç¬¬ä¸€å±‚è®­ç»ƒæˆåŠŸ")
                logger.info(f"   æ¨¡å‹æ•°é‡: {len(oof_preds)}")
                logger.info(f"   å¹³å‡IC: {quality_metrics['avg_ic']:.4f}")
                logger.info(f"   ICæ ‡å‡†å·®: {quality_metrics['ic_std']:.4f}")

                return {
                    'success': True,
                    'oof_predictions': oof_preds,
                    'models': first_layer_results.get('models', {}),
                    'cv_scores': first_layer_results.get('cv_scores', {}),
                    'quality_metrics': quality_metrics
                }
            else:
                logger.error("âŒ ç¬¬ä¸€å±‚è®­ç»ƒå¤±è´¥æˆ–ç¼ºå°‘OOFé¢„æµ‹")
                return {'success': False}

        except Exception as e:
            logger.error(f"âŒ ç»Ÿä¸€ç¬¬ä¸€å±‚è®­ç»ƒå¼‚å¸¸: {e}")
            return {'success': False}

    def _parallel_second_layer_training(self, unified_oof_predictions: Dict[str, pd.Series],
                                      y: pd.Series, dates: pd.Series, tickers: pd.Series,
                                      alpha_factors: pd.DataFrame = None) -> Dict[str, Any]:
        """
        å¹¶è¡ŒäºŒå±‚è®­ç»ƒ - åŸºäºç»Ÿä¸€çš„OOFé¢„æµ‹

        è¿™æ˜¯çœŸæ­£çš„å¹¶è¡Œï¼šä¸¤ä¸ªæ¨¡å‹ä½¿ç”¨ç›¸åŒçš„è¾“å…¥æ•°æ®
        """
        logger.info("âš¡ å¼€å§‹å¹¶è¡ŒäºŒå±‚è®­ç»ƒï¼ˆåŸºäºç»Ÿä¸€OOFï¼‰...")

        results = {
            'ridge_success': False,
            'lambda_success': False,
            'stacker_data': None,
            'timing': {}
        }

        # æ„å»ºä¸åŒçš„æ•°æ®æº
        # Ridgeä½¿ç”¨OOF
        ridge_data = self._build_unified_stacker_data(
            unified_oof_predictions, y, dates, tickers
        )

        # LambdaRankä½¿ç”¨Alpha Factors
        lambda_data = self._build_lambda_data(
            alpha_factors if alpha_factors is not None else X, y, dates, tickers
        )

        if ridge_data is None:
            logger.error("âŒ æ— æ³•æ„å»ºRidge stackeræ•°æ®")
            return results

        if lambda_data is None:
            logger.error("âŒ æ— æ³•æ„å»ºLambdaRankæ•°æ®")
            # å³ä½¿LambdaRankæ•°æ®å¤±è´¥ï¼Œä»å¯è®­ç»ƒRidge
            lambda_data = ridge_data  # é™çº§ä½¿ç”¨OOF

        logger.info(f"ğŸ“Š Ridgeæ•°æ®å½¢çŠ¶: {ridge_data.shape}")
        logger.info(f"ğŸ“Š LambdaRankæ•°æ®å½¢çŠ¶: {lambda_data.shape}")
        results['stacker_data'] = ridge_data  # ä¿ç•™å…¼å®¹æ€§

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¹¶è¡Œè®­ç»ƒ
        use_lambda = (hasattr(self.parent, 'use_rank_aware_blending') and
                     self.parent.use_rank_aware_blending and
                     self._check_lambda_available() and
                     len(ridge_data) >= 200)  # æ•°æ®é‡æ£€æŸ¥

        if not use_lambda or lambda_data is None:
            logger.info("ğŸ”„ åªè®­ç»ƒRidge Stackerï¼ˆLambdaRankä¸å¯ç”¨æˆ–æ•°æ®ä¸è¶³ï¼‰")
            ridge_start = time.time()
            ridge_success = self.parent._train_ridge_stacker(
                unified_oof_predictions, y, dates
            )
            results['ridge_success'] = ridge_success
            results['timing']['ridge_only'] = time.time() - ridge_start
            return results

        # å¹¶è¡Œè®­ç»ƒRidgeå’ŒLambdaRank
        with ThreadPoolExecutor(max_workers=2, thread_name_prefix="Unified-Parallel") as executor:
            futures = {}

            # ä»»åŠ¡1ï¼šRidge Stacker
            ridge_future = executor.submit(
                self._train_ridge_unified,
                unified_oof_predictions, y, dates
            )
            futures[ridge_future] = 'ridge'

            # ä»»åŠ¡2ï¼šLambdaRank Stackerï¼ˆä½¿ç”¨Alpha Factorsï¼‰
            lambda_future = executor.submit(
                self._train_lambda_unified,
                lambda_data  # Alpha Factorsæ•°æ®
            )
            futures[lambda_future] = 'lambda'

            # ç­‰å¾…å®Œæˆ
            for future in as_completed(futures):
                task_name = futures[future]
                try:
                    task_result = future.result(timeout=1800)  # 30åˆ†é’Ÿè¶…æ—¶

                    if task_name == 'ridge':
                        results['ridge_success'] = task_result['success']
                        results['timing']['ridge'] = task_result['elapsed_time']
                        logger.info(f"âœ… Ridgeå®Œæˆï¼Œè€—æ—¶: {task_result['elapsed_time']:.2f}ç§’")

                    elif task_name == 'lambda':
                        results['lambda_success'] = task_result['success']
                        results['timing']['lambda'] = task_result['elapsed_time']
                        if task_result['success']:
                            self.parent.lambda_rank_stacker = task_result['model']
                            logger.info(f"âœ… LambdaRankå®Œæˆï¼Œè€—æ—¶: {task_result['elapsed_time']:.2f}ç§’")

                except Exception as e:
                    logger.error(f"âŒ {task_name} è®­ç»ƒå¤±è´¥: {e}")

        return results

    def _build_unified_stacker_data(self, oof_predictions: Dict[str, pd.Series],
                                  y: pd.Series, dates: pd.Series, tickers: pd.Series) -> Optional[pd.DataFrame]:
        """
        æ„å»ºç»Ÿä¸€çš„stackerè¾“å…¥æ•°æ®

        ç¡®ä¿Ridgeå’ŒLambdaRankä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ•°æ®
        """
        try:
            # åˆ›å»ºMultiIndex
            if not isinstance(y.index, pd.MultiIndex):
                multi_index = pd.MultiIndex.from_arrays(
                    [dates, tickers], names=['date', 'ticker']
                )
                y_indexed = pd.Series(y.values, index=multi_index)
            else:
                y_indexed = y

            # æ„å»ºstacker DataFrame
            stacker_dict = {}
            for model_name, pred_series in oof_predictions.items():
                # ç¡®ä¿é¢„æµ‹seriesæœ‰æ­£ç¡®çš„ç´¢å¼•
                if isinstance(pred_series.index, pd.MultiIndex):
                    stacker_dict[f'pred_{model_name}'] = pred_series
                else:
                    # å¦‚æœæ²¡æœ‰MultiIndexï¼Œä½¿ç”¨yçš„ç´¢å¼•
                    stacker_dict[f'pred_{model_name}'] = pd.Series(
                        pred_series.values, index=y_indexed.index
                    )

            # æ·»åŠ ç›®æ ‡å˜é‡
            stacker_dict['ret_fwd_5d'] = y_indexed

            stacker_data = pd.DataFrame(stacker_dict)

            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            missing_data = stacker_data.isnull().sum()
            if missing_data.any():
                logger.warning(f"âš ï¸ Stackeræ•°æ®ç¼ºå¤±: {missing_data.to_dict()}")

            # ç§»é™¤åŒ…å«NaNçš„è¡Œ
            clean_data = stacker_data.dropna()
            if len(clean_data) < len(stacker_data) * 0.8:
                logger.warning(f"âš ï¸ æ•°æ®æ¸…ç†åå‰©ä½™ {len(clean_data)}/{len(stacker_data)} ({len(clean_data)/len(stacker_data)*100:.1f}%)")

            logger.info(f"ğŸ“Š ç»Ÿä¸€stackeræ•°æ®æ„å»ºå®Œæˆ: {clean_data.shape}")
            return clean_data

        except Exception as e:
            logger.error(f"âŒ æ„å»ºstackeræ•°æ®å¤±è´¥: {e}")
            return None

    def _train_ridge_unified(self, oof_predictions: Dict[str, pd.Series],
                           y: pd.Series, dates: pd.Series) -> Dict[str, Any]:
        """
        è®­ç»ƒRidge Stackerï¼ˆç»Ÿä¸€æ•°æ®æºç‰ˆæœ¬ï¼‰
        """
        start_time = time.time()
        try:
            logger.info("[Ridge-Thread] å¼€å§‹è®­ç»ƒRidge Stacker...")
            success = self.parent._train_ridge_stacker(oof_predictions, y, dates)

            result = {
                'success': success,
                'elapsed_time': time.time() - start_time
            }

            if success:
                logger.info("[Ridge-Thread] âœ… Ridgeè®­ç»ƒæˆåŠŸ")
            else:
                logger.error("[Ridge-Thread] âŒ Ridgeè®­ç»ƒå¤±è´¥")

            return result

        except Exception as e:
            logger.error(f"[Ridge-Thread] è®­ç»ƒå¼‚å¸¸: {e}")
            return {
                'success': False,
                'elapsed_time': time.time() - start_time
            }

    def _train_lambda_unified(self, stacker_data: pd.DataFrame) -> Dict[str, Any]:
        """
        è®­ç»ƒLambdaRank Stackerï¼ˆç»Ÿä¸€æ•°æ®æºç‰ˆæœ¬ï¼‰
        """
        start_time = time.time()
        try:
            from bma_models.lambda_rank_stacker import LambdaRankStacker

            logger.info("[Lambda-Thread] å¼€å§‹è®­ç»ƒLambdaRank...")

            # ä½¿ç”¨ç»Ÿä¸€çš„æ—¶é—´é…ç½®
            from bma_models.unified_config_loader import get_time_config
            time_config = get_time_config()

            # åŠ¨æ€ç¡®å®šç‰¹å¾åˆ—ï¼ˆæ ¹æ®å®é™…æ•°æ®ï¼‰
            feature_cols = [col for col in stacker_data.columns if col != 'ret_fwd_5d']
            logger.info(f"[Lambda-Thread] ä½¿ç”¨ç‰¹å¾åˆ—: {feature_cols}")

            # é…ç½®LambdaRankï¼ˆä½¿ç”¨purged CV factoryï¼‰
            lambda_config = {
                'base_cols': tuple(feature_cols),  # åŠ¨æ€ä½¿ç”¨å®é™…å¯ç”¨çš„ç‰¹å¾åˆ—
                'n_quantiles': 64,
                'winsorize_quantiles': (0.01, 0.99),
                'label_gain_power': 1.5,
                'num_boost_round': 100,
                'early_stopping_rounds': 0,
                'use_purged_cv': True,
                'cv_n_splits': time_config.cv_n_splits,
                'cv_gap_days': time_config.cv_gap_days,
                'cv_embargo_days': time_config.cv_embargo_days,
                'random_state': 42
            }

            lambda_stacker = LambdaRankStacker(**lambda_config)
            lambda_stacker.fit(stacker_data)

            logger.info("[Lambda-Thread] âœ… LambdaRankè®­ç»ƒæˆåŠŸ")

            return {
                'success': True,
                'model': lambda_stacker,
                'elapsed_time': time.time() - start_time
            }

        except Exception as e:
            logger.error(f"[Lambda-Thread] è®­ç»ƒå¼‚å¸¸: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'success': False,
                'model': None,
                'elapsed_time': time.time() - start_time
            }

    def _check_lambda_available(self) -> bool:
        """æ£€æŸ¥LambdaRankæ˜¯å¦å¯ç”¨"""
        try:
            from bma_models.lambda_rank_stacker import LambdaRankStacker
            from bma_models.rank_aware_blender import RankAwareBlender
            return True
        except ImportError:
            return False

    def _init_unified_blender(self):
        """åˆå§‹åŒ–ç»Ÿä¸€Blender"""
        try:
            from bma_models.rank_aware_blender import RankAwareBlender

            self.parent.rank_aware_blender = RankAwareBlender(
                lookback_window=60,
                min_weight=0.3,
                max_weight=0.7,
                weight_smoothing=0.3,
                use_copula=True,
                use_decorrelation=True,
                top_k_list=[5, 10, 20]
            )
            logger.info("âœ… ç»Ÿä¸€Rank-aware Blenderåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Blenderåˆå§‹åŒ–å¤±è´¥: {e}")

    def _validate_oof_quality(self, oof_predictions: Dict[str, pd.Series], y: pd.Series) -> Dict[str, float]:
        """éªŒè¯OOFé¢„æµ‹è´¨é‡"""
        from scipy.stats import spearmanr

        quality_metrics = {
            'avg_ic': 0.0,
            'ic_std': 0.0,
            'min_ic': 0.0,
            'max_ic': 0.0
        }

        try:
            ics = []
            for model_name, pred_series in oof_predictions.items():
                # å¯¹é½é¢„æµ‹å’Œç›®æ ‡
                aligned_pred = pred_series.reindex(y.index)
                valid_mask = ~(aligned_pred.isna() | y.isna())

                if valid_mask.sum() > 10:  # è‡³å°‘10ä¸ªæœ‰æ•ˆæ ·æœ¬
                    ic, _ = spearmanr(
                        aligned_pred[valid_mask],
                        y[valid_mask]
                    )
                    if not np.isnan(ic):
                        ics.append(ic)

            if ics:
                quality_metrics['avg_ic'] = np.mean(ics)
                quality_metrics['ic_std'] = np.std(ics)
                quality_metrics['min_ic'] = np.min(ics)
                quality_metrics['max_ic'] = np.max(ics)

        except Exception as e:
            logger.warning(f"âš ï¸ è´¨é‡éªŒè¯å¤±è´¥: {e}")

        return quality_metrics

    def _log_oof_quality(self, oof_predictions: Dict[str, pd.Series], y: pd.Series):
        """è®°å½•OOFé¢„æµ‹è´¨é‡"""
        quality_metrics = self._validate_oof_quality(oof_predictions, y)

        logger.info("ğŸ“Š OOFé¢„æµ‹è´¨é‡æŠ¥å‘Š:")
        logger.info(f"   å¹³å‡IC: {quality_metrics['avg_ic']:.4f}")
        logger.info(f"   ICèŒƒå›´: [{quality_metrics['min_ic']:.4f}, {quality_metrics['max_ic']:.4f}]")
        logger.info(f"   ICæ ‡å‡†å·®: {quality_metrics['ic_std']:.4f}")

        # è´¨é‡è­¦å‘Š
        if quality_metrics['avg_ic'] < 0.01:
            logger.warning("âš ï¸ å¹³å‡ICè¿‡ä½ï¼Œå¯èƒ½å½±å“äºŒå±‚æ¨¡å‹è´¨é‡")
        if quality_metrics['ic_std'] > 0.1:
            logger.warning("âš ï¸ ICæ³¢åŠ¨è¿‡å¤§ï¼Œæ¨¡å‹ç¨³å®šæ€§å¯èƒ½å—å½±å“")

    def _log_performance_summary(self, results: Dict[str, Any], total_time: float):
        """è®°å½•æ€§èƒ½æ€»ç»“"""
        logger.info("="*70)
        logger.info("ğŸ“Š ç»Ÿä¸€å¹¶è¡Œè®­ç»ƒæ€§èƒ½æŠ¥å‘Š:")

        if 'stage1' in results['timing']:
            logger.info(f"   é˜¶æ®µ1ï¼ˆç»Ÿä¸€ç¬¬ä¸€å±‚ï¼‰: {results['timing']['stage1']:.2f}ç§’")

        if 'stage2' in results['timing']:
            logger.info(f"   é˜¶æ®µ2ï¼ˆå¹¶è¡ŒäºŒå±‚ï¼‰: {results['timing']['stage2']:.2f}ç§’")

        if 'ridge' in results['timing'] and 'lambda' in results['timing']:
            ridge_time = results['timing']['ridge']
            lambda_time = results['timing']['lambda']
            sequential_time = ridge_time + lambda_time
            parallel_time = max(ridge_time, lambda_time)
            time_saved = sequential_time - parallel_time

            logger.info(f"   Ridgeæ—¶é—´: {ridge_time:.2f}ç§’")
            logger.info(f"   LambdaRankæ—¶é—´: {lambda_time:.2f}ç§’")
            logger.info(f"   å¹¶è¡ŒèŠ‚çœæ—¶é—´: {time_saved:.2f}ç§’")
            if sequential_time > 0:
                logger.info(f"   äºŒå±‚åŠ é€Ÿæ¯”: {sequential_time/parallel_time:.2f}x")

        logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
        logger.info(f"   é˜¶æ®µ1æˆåŠŸ: {results['stage1_success']}")
        logger.info(f"   RidgeæˆåŠŸï¼ˆOOFï¼‰: {results['ridge_success']}")
        logger.info(f"   LambdaæˆåŠŸï¼ˆAlphaï¼‰: {results['lambda_success']}")
        logger.info("="*70)

    def _build_lambda_data(self, alpha_factors, y: pd.Series,
                          dates: pd.Series, tickers: pd.Series) -> Optional[pd.DataFrame]:
        """
        æ„å»ºLambdaRankçš„æ•°æ®ï¼ˆä½¿ç”¨Alpha Factorsæˆ–fallbackåˆ°OOFï¼‰
        """
        try:
            # åˆ›å»ºMultiIndex
            if not isinstance(y.index, pd.MultiIndex):
                multi_index = pd.MultiIndex.from_arrays(
                    [dates, tickers], names=['date', 'ticker']
                )
                y_indexed = pd.Series(y.values, index=multi_index)
            else:
                y_indexed = y

            # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
            if isinstance(alpha_factors, dict):
                # å¦‚æœæ˜¯OOF predictions dictï¼Œè½¬æ¢ä¸ºDataFrame
                logger.info("ğŸ”„ ä½¿ç”¨OOFé¢„æµ‹æ„å»ºLambdaRankæ•°æ®ï¼ˆfallbackæ¨¡å¼ï¼‰")
                lambda_dict = {}
                for model_name, pred_series in alpha_factors.items():
                    if isinstance(pred_series.index, pd.MultiIndex):
                        lambda_dict[f'pred_{model_name}'] = pred_series
                    else:
                        lambda_dict[f'pred_{model_name}'] = pd.Series(
                            pred_series.values, index=y_indexed.index
                        )
                lambda_data = pd.DataFrame(lambda_dict)
            else:
                # æ­£å¸¸çš„Alpha Factors DataFrame
                logger.info("ğŸ¯ ä½¿ç”¨Alpha Factorsæ„å»ºLambdaRankæ•°æ®")
                if isinstance(alpha_factors.index, pd.MultiIndex):
                    lambda_data = alpha_factors.copy()
                else:
                    # è®¾ç½®MultiIndex
                    lambda_data = alpha_factors.copy()
                    lambda_data.index = multi_index
                # å¦‚å­˜åœ¨ä¸ç´¢å¼•é‡å¤çš„è¾…åŠ©åˆ—ï¼Œå…ˆç§»é™¤ï¼Œé¿å…æ­§ä¹‰
                lambda_data = lambda_data.drop(columns=['date', 'ticker'], errors='ignore')

                # ç§»é™¤é¢„æµ‹åˆ—ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                pred_cols = [col for col in lambda_data.columns if 'pred_' in col.lower()]
                if pred_cols:
                    lambda_data = lambda_data.drop(columns=pred_cols)
                    logger.info(f"   ç§»é™¤{len(pred_cols)}ä¸ªé¢„æµ‹åˆ—")

            # æ·»åŠ ç›®æ ‡å˜é‡
            lambda_data['ret_fwd_5d'] = y_indexed

            # éªŒè¯æ•°æ®
            feature_count = lambda_data.shape[1] - 1  # å‡å»targetåˆ—
            logger.info(f"ğŸ“Š LambdaRankæ•°æ®: {lambda_data.shape[0]}è¡Œ Ã— {feature_count}ä¸ªç‰¹å¾")

            # æ¸…ç†NaN
            clean_data = lambda_data.dropna()
            if len(clean_data) < len(lambda_data) * 0.8:
                logger.warning(f"âš ï¸ æ¸…ç†åå‰©ä½™ {len(clean_data)}/{len(lambda_data)} æ ·æœ¬")

            return clean_data

        except Exception as e:
            logger.error(f"âŒ æ„å»ºLambdaRankæ•°æ®å¤±è´¥: {e}")
            return None