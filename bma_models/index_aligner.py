#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IndexAligner - ç»Ÿä¸€ç´¢å¼•å¯¹é½å™¨
è§£å†³738 vs 748ç»´åº¦ä¸åŒ¹é…é—®é¢˜çš„æ ¸å¿ƒæ¨¡å—
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class AlignmentReport:
    """å¯¹é½æŠ¥å‘Š"""
    original_shapes: Dict[str, Tuple]
    final_shape: Tuple
    removed_samples: Dict[str, int]
    removal_reasons: Dict[str, int]
    coverage_rate: float
    effective_tickers: int
    effective_dates: int
    horizon_trimmed: int
    # [CRITICAL] æ–°å¢æ¨ªæˆªé¢ç»Ÿè®¡
    daily_tickers_stats: Dict[str, float] = None  # min/median/maxæ¯æ—¥è‚¡ç¥¨æ•°
    cross_section_ready: bool = True  # æ˜¯å¦æ»¡è¶³æ¨ªæˆªé¢è¦æ±‚
    
class IndexAligner:
    """ç»Ÿä¸€ç´¢å¼•å¯¹é½å™¨ - è§£å†³æ‰€æœ‰ç»´åº¦ä¸åŒ¹é…é—®é¢˜"""
    
    def __init__(self, horizon: int = 10, strict_mode: bool = True, mode: str = 'train'):
        """
        åˆå§‹åŒ–å¯¹é½å™¨
        
        Args:
            horizon: å‰ç»æœŸ(T+10)ï¼Œä»…ç”¨äºè®°å½•ï¼Œå®é™…å‰ªå°¾é€šè¿‡CVçš„gap/embargoå®ç°
            strict_mode: ä¸¥æ ¼æ¨¡å¼ï¼Œç»´åº¦ä¸åŒ¹é…æ—¶æŠ¥é”™
            mode: 'train' (è®­ç»ƒæ¨¡å¼) æˆ– 'predict' (é¢„æµ‹æ¨¡å¼) - ç°åœ¨éƒ½ä¸æ‰§è¡Œå‰ªå°¾
        """
        # ğŸ”§ CRITICAL FIX: ç»Ÿä¸€ä¸å‰ªå°¾ç­–ç•¥ï¼Œé˜²æ­¢ç»´åº¦ä¸åŒ¹é…
        # å‰è§†åå·®é˜²èŒƒé€šè¿‡CVçš„gap/embargoå®ç°ï¼Œè€Œä¸æ˜¯æ•°æ®å‰ªå°¾
        self.horizon = 0  # ç»Ÿä¸€è®¾ä¸º0ï¼Œä¸æ‰§è¡Œä»»ä½•å‰ªå°¾
        self.original_horizon = horizon  # ä¿ç•™åŸå§‹horizonè®¾ç½®ç”¨äºæ—¥å¿—
        self.strict_mode = strict_mode
        self.mode = mode
        self.alignment_history = []
        logger.info(f"IndexAligneråˆå§‹åŒ–: original_horizon={horizon}, actual_horizon=0, mode={mode}")
        
        # ğŸ¯ ç»Ÿä¸€å¤„ç†ç­–ç•¥è¯´æ˜
        logger.info(f"[DIMENSION_CONSISTENCY] è®­ç»ƒå’Œé¢„æµ‹å‡ä¸å‰ªå°¾ï¼Œç¡®ä¿ç»´åº¦ä¸€è‡´æ€§")
        logger.info(f"[TEMPORAL_SAFETY] å‰è§†åå·®é€šè¿‡CV gap={horizon-1}, embargo={horizon} é˜²èŒƒ")
    
    def align_all_data(self, **data_dict) -> Tuple[Dict[str, Any], AlignmentReport]:
        """
        ç»Ÿä¸€å¯¹é½æ‰€æœ‰æ•°æ®ï¼Œè§£å†³738 vs 748é—®é¢˜
        
        Args:
            **data_dict: å‘½åæ•°æ® å¦‚ X=features, y=labels, alpha=alpha_features, pred=predictions
            
        Returns:
            (aligned_data_dict, alignment_report)
        """
        logger.info("[TARGET] å¼€å§‹IndexAlignerç»Ÿä¸€å¯¹é½")
        
        # 1. è®°å½•åŸå§‹å½¢çŠ¶
        original_shapes = {}
        for name, data in data_dict.items():
            if hasattr(data, 'shape'):
                original_shapes[name] = data.shape
            elif hasattr(data, '__len__'):
                original_shapes[name] = (len(data),)
            else:
                original_shapes[name] = 'scalar'
        
        logger.info("[DATA] åŸå§‹æ•°æ®å½¢çŠ¶:")
        for name, shape in original_shapes.items():
            logger.info(f"  {name}: {shape}")
        
        # 2. ç»Ÿä¸€æ•°æ®é¢„å¤„ç† - ç»´åº¦ä¸€è‡´æ€§ä¼˜å…ˆï¼Œä¸æ‰§è¡Œå‰ªå°¾
        # ğŸ”§ CRITICAL FIX: ç§»é™¤æ‰€æœ‰å‰ªå°¾é€»è¾‘ï¼Œç¡®ä¿è®­ç»ƒ/é¢„æµ‹ç»´åº¦ä¸€è‡´
        processed_data = {}
        
        for name, data in data_dict.items():
            if data is None:
                processed_data[name] = None
                continue
            
            # ğŸ¯ ä¿æŒåŸå§‹æ•°æ®å®Œæ•´æ€§ï¼Œåªè¿›è¡Œå¿…è¦çš„éªŒè¯å’Œæ¸…ç†
            if hasattr(data, 'index') and isinstance(data.index, pd.MultiIndex):
                # MultiIndex (date, ticker) æ ¼å¼ - ä¿æŒå®Œæ•´ç»“æ„
                processed_data[name] = data.copy()
                logger.info(f"  {name}: ä¿æŒå®Œæ•´ MultiIndex ç»“æ„ {data.shape}")
            else:
                # æ™®é€šç´¢å¼•æˆ–æ•°ç»„ - ç›´æ¥ä¿æŒ
                processed_data[name] = data
                shape_info = getattr(data, 'shape', len(data) if hasattr(data, '__len__') else 'scalar')
                logger.info(f"  {name}: ä¿æŒåŸå§‹æ ¼å¼ {shape_info}")
        
        # æ—¥å¿—è¯´æ˜ï¼šä¸æ‰§è¡Œå‰ªå°¾çš„åŸå› 
        logger.info(f"[NO_TRIMMING] ä¸ºä¿è¯ç»´åº¦ä¸€è‡´æ€§ï¼Œæ‰€æœ‰æ•°æ®ä¿æŒå®Œæ•´")
        logger.info(f"[TEMPORAL_SAFETY] å‰è§†åå·®é€šè¿‡CV gap={self.original_horizon-1}, embargo={self.original_horizon} é˜²èŒƒ")
        horizon_trimmed = 0  # å®é™…æœªå‰ªå°¾ä»»ä½•æ•°æ®
        
        # 3. æ„å»ºé€šç”¨ç´¢å¼• - inner joinæ‰€æœ‰æ•°æ®
        common_index = None
        removal_reasons = {'horizon_trim': horizon_trimmed, 'nan_removal': 0, 'index_mismatch': 0}
        
        for name, data in processed_data.items():
            if data is None or (hasattr(data, 'empty') and data.empty):
                continue
                
            if hasattr(data, 'index'):
                current_index = data.index
                if common_index is None:
                    common_index = current_index
                else:
                    # Inner joinç´¢å¼•
                    before_len = len(common_index)
                    common_index = common_index.intersection(current_index)
                    after_len = len(common_index)
                    
                    if before_len > after_len:
                        removed = before_len - after_len
                        removal_reasons['index_mismatch'] += removed
                        logger.info(f"  {name}: ç´¢å¼•inner joinç§»é™¤ {removed} æ¡")
        
        if common_index is None or len(common_index) == 0:
            logger.error("[ERROR] æ‰€æœ‰æ•°æ®ç´¢å¼•äº¤é›†ä¸ºç©ºï¼Œæ— æ³•å¯¹é½")
            logger.warning("ç´¢å¼•å¯¹é½å¤±è´¥ï¼šæ‰€æœ‰æ•°æ®äº¤é›†ä¸ºç©º")
        
        logger.info(f"[TARGET] é€šç”¨ç´¢å¼•ç¡®å®š: {len(common_index)} æ¡")
        
        # 4. ä½¿ç”¨é€šç”¨ç´¢å¼•å¯¹é½æ‰€æœ‰æ•°æ®
        aligned_data = {}
        removed_samples = {}
        
        for name, data in processed_data.items():
            if data is None:
                aligned_data[name] = None
                removed_samples[name] = 0
                continue
            
            if hasattr(data, 'loc'):
                try:
                    # ä½¿ç”¨.locå¯¹é½åˆ°é€šç”¨ç´¢å¼•
                    original_len = len(data)
                    aligned_data[name] = data.loc[common_index]
                    final_len = len(aligned_data[name])
                    removed_samples[name] = original_len - final_len
                    
                    logger.info(f"  {name}: {original_len} â†’ {final_len} (-{removed_samples[name]})")
                except Exception as e:
                    logger.warning(f"  {name}: ç´¢å¼•å¯¹é½å¤±è´¥ï¼Œä½¿ç”¨åŸæ•°æ®: {e}")
                    aligned_data[name] = data
                    removed_samples[name] = 0
            else:
                # æ•°ç»„ç±»å‹ï¼ŒæŒ‰é•¿åº¦æˆªæ–­åˆ°common_indexé•¿åº¦
                if hasattr(data, '__len__') and len(data) > len(common_index):
                    aligned_data[name] = data[:len(common_index)]
                    removed_samples[name] = len(data) - len(common_index)
                    logger.info(f"  {name}: æ•°ç»„æˆªæ–­ {len(data)} â†’ {len(common_index)}")
                else:
                    aligned_data[name] = data
                    removed_samples[name] = 0
        
        # 5. éªŒè¯æœ€ç»ˆä¸€è‡´æ€§
        final_lengths = {}
        for name, data in aligned_data.items():
            if data is not None and hasattr(data, '__len__'):
                final_lengths[name] = len(data)
        
        if final_lengths:
            unique_lengths = set(final_lengths.values())
            if len(unique_lengths) > 1:
                error_msg = f"[ERROR] å¯¹é½åé•¿åº¦ä»ä¸ä¸€è‡´: {final_lengths}"
                logger.error(error_msg)
                if self.strict_mode:
                    logger.warning(error_msg)
        
        # 6. è®¡ç®—è¦†ç›–ç»Ÿè®¡å’Œæ¨ªæˆªé¢æ£€æŸ¥
        daily_tickers_stats = None
        cross_section_ready = True
        # åŠ¨æ€æ¨ªæˆªé¢è¦æ±‚ï¼šæ ¹æ®æ•°æ®è§„æ¨¡è°ƒæ•´
        # å¯¹äºç ”ç©¶å’Œå•è‚¡ç¥¨åˆ†æï¼Œå…è®¸è¾ƒä½çš„æ¨ªæˆªé¢è¦æ±‚
        if len(common_index) > 10000:  # å¤§æ•°æ®é›†
            MIN_CROSS_SECTION = 30
        elif len(common_index) > 1000:  # ä¸­ç­‰æ•°æ®é›†  
            MIN_CROSS_SECTION = 10
        else:  # å°æ•°æ®é›†æˆ–å•è‚¡ç¥¨ç ”ç©¶
            MIN_CROSS_SECTION = 1
        
        # [CRITICAL] DEBUG: æ£€æŸ¥common_indexçš„å®é™…çŠ¶æ€
        logger.info(f"[SEARCH] DEBUG common_indexæ£€æµ‹:")
        logger.info(f"  ç±»å‹: {type(common_index)}")
        logger.info(f"  é•¿åº¦: {len(common_index) if common_index is not None else 'None'}")
        logger.info(f"  hasattr get_level_values: {hasattr(common_index, 'get_level_values')}")
        logger.info(f"  hasattr nlevels: {hasattr(common_index, 'nlevels')}")
        if hasattr(common_index, 'nlevels'):
            logger.info(f"  nlevels: {common_index.nlevels}")
            logger.info(f"  nlevels >= 2: {common_index.nlevels >= 2}")
        logger.info(f"  isinstance MultiIndex: {isinstance(common_index, pd.MultiIndex)}")
        
        if hasattr(common_index, 'get_level_values') and common_index.nlevels >= 2:
            logger.info("[OK] è¿›å…¥MultiIndexåˆ†æ”¯")
            # MultiIndexæƒ…å†µ - è®¡ç®—æ¯æ—¥è‚¡ç¥¨æ•°åˆ†å¸ƒ
            effective_dates = len(common_index.get_level_values(0).unique())
            effective_tickers = len(common_index.get_level_values(1).unique())
            
            # [CRITICAL] CRITICAL: è®¡ç®—æ¯æ—¥è‚¡ç¥¨æ•°åˆ†å¸ƒ
            daily_tickers = pd.Series(common_index.get_level_values(1)).groupby(
                pd.Series(common_index.get_level_values(0))
            ).nunique()
            
            daily_tickers_stats = {
                'min': float(daily_tickers.min()),
                'median': float(daily_tickers.median()),
                'max': float(daily_tickers.max()),
                'mean': float(daily_tickers.mean())
            }
            
            # MultiIndexæƒ…å†µä¸‹çš„æ¨ªæˆªé¢æ£€æŸ¥
            if effective_tickers < MIN_CROSS_SECTION:
                cross_section_ready = False
                logger.error(f"[ERROR] MultiIndexè‚¡ç¥¨æ•°é‡ä¸è¶³ï¼š{effective_tickers} < {MIN_CROSS_SECTION}")
            else:
                cross_section_ready = True
                logger.info(f"[OK] MultiIndexè‚¡ç¥¨æ•°é‡å……è¶³ï¼š{effective_tickers} >= {MIN_CROSS_SECTION}")
                logger.info(f"[OK] æ¨ªæˆªé¢å……è¶³ï¼šæ¯æ—¥è‚¡ç¥¨æ•° min={daily_tickers_stats['min']}, median={daily_tickers_stats['median']}, max={daily_tickers_stats['max']}")
            
        else:
            logger.info("[ERROR] è¿›å…¥elseåˆ†æ”¯ï¼ˆéMultiIndexï¼‰")
            # [CRITICAL] CRITICAL FIX: éMultiIndexæƒ…å†µä¸‹æ­£ç¡®è®¡ç®—è‚¡ç¥¨æ•°é‡
            logger.warning("[WARNING] æ£€æµ‹åˆ°éMultiIndexæ ¼å¼ï¼Œå°è¯•ä»æ•°æ®ä¸­æ¨æ–­è‚¡ç¥¨æ•°é‡")
            
            effective_dates = len(common_index.unique()) if hasattr(common_index, 'unique') else len(common_index)
            effective_tickers = 1  # é»˜è®¤å€¼
            
            # [CRITICAL] PRIORITY 1: ä»å¤šç§å¯èƒ½çš„è‚¡ç¥¨å‚æ•°è·å–æ•°é‡ï¼ˆæ›´å¥å£®ï¼‰
            tickers_found = False
            for ticker_param_name in ['tickers', 'ticker', 'prediction_tickers', 'symbols', 'stocks']:
                if ticker_param_name in aligned_data and aligned_data[ticker_param_name] is not None:
                    tickers_data = aligned_data[ticker_param_name]
                    if hasattr(tickers_data, 'unique'):
                        try:
                            unique_tickers = tickers_data.unique()
                            effective_tickers = len(unique_tickers)
                            logger.info(f"[TARGET] ä»{ticker_param_name}å‚æ•°ç›´æ¥è·å–: {effective_tickers}åªè‚¡ç¥¨")
                            if effective_tickers > 1:
                                logger.info(f"[DATA] è‚¡ç¥¨åˆ—è¡¨: {list(unique_tickers)[:10]}...")
                            tickers_found = True
                            break
                        except Exception as e:
                            logger.warning(f"[WARNING] {ticker_param_name}.unique()å¤±è´¥: {e}")
                            continue
                    elif hasattr(tickers_data, '__len__'):
                        # å¦‚æœæ˜¯åˆ—è¡¨æˆ–æ•°ç»„
                        try:
                            unique_tickers = list(set(tickers_data)) if hasattr(tickers_data, '__iter__') else [tickers_data]
                            effective_tickers = len(unique_tickers)
                            logger.info(f"[TARGET] ä»{ticker_param_name}æ•°ç»„è·å–: {effective_tickers}åªè‚¡ç¥¨")
                            tickers_found = True
                            break
                        except Exception as e:
                            logger.warning(f"[WARNING] å¤„ç†{ticker_param_name}å‚æ•°æ—¶å‡ºé”™: {e}")
                            continue
            
            # [CRITICAL] FALLBACK: å¦‚æœæ‰€æœ‰è‚¡ç¥¨å‚æ•°éƒ½æ— æ•ˆï¼Œå°è¯•å…¶ä»–æ–¹æ³•æ¨æ–­
            if not tickers_found and effective_tickers == 1:
                logger.warning("[WARNING] tickerså‚æ•°æ— æ•ˆï¼Œå°è¯•ä»å…¶ä»–æ•°æ®æ¨æ–­...")
                
                # æ£€æŸ¥æ˜¯å¦èƒ½ä»å¯¹é½åçš„æ•°æ®æ¨æ–­è‚¡ç¥¨æ•°é‡
                for name, data in aligned_data.items():
                    if name in ['tickers', 'ticker', 'prediction_tickers', 'symbols', 'stocks']:  # è·³è¿‡å·²ç»å¤„ç†è¿‡çš„å‚æ•°
                        continue
                        
                    if data is not None and hasattr(data, 'columns'):
                        # æ£€æŸ¥DataFrameä¸­æ˜¯å¦æœ‰tickerç›¸å…³åˆ—
                        ticker_cols = [col for col in data.columns if 'ticker' in str(col).lower() or 'symbol' in str(col).lower()]
                        if ticker_cols:
                            ticker_col = ticker_cols[0]
                            unique_tickers = data[ticker_col].unique() if hasattr(data[ticker_col], 'unique') else []
                            inferred_tickers = len(unique_tickers)
                            if inferred_tickers > 1:
                                effective_tickers = inferred_tickers
                                logger.info(f"[DATA] ä»{name}.{ticker_col}åˆ—æ¨æ–­å‡º{effective_tickers}åªè‚¡ç¥¨: {list(unique_tickers)[:10]}...")
                                tickers_found = True
                                break
                    elif data is not None and hasattr(data, 'index'):
                        # æ£€æŸ¥ç´¢å¼•ä¸­æ˜¯å¦æœ‰è‚¡ç¥¨ä¿¡æ¯æ¨¡å¼
                        index_str = str(data.index)
                        if 'ticker' in index_str.lower() or 'symbol' in index_str.lower():
                            logger.info(f"[DATA] æ£€æµ‹åˆ°{name}æ•°æ®åŒ…å«è‚¡ç¥¨ç´¢å¼•ä¿¡æ¯")
                        
                        # å¦‚æœæ•°æ®é•¿åº¦è¿œå¤§äºæ—¥æœŸæ•°ï¼Œå¯èƒ½æ˜¯å¤šè‚¡ç¥¨
                        if effective_dates > 0 and len(data) > effective_dates * 2:  # è‡³å°‘æ˜¯æ—¥æœŸæ•°çš„2å€
                            inferred_tickers = len(data) // effective_dates
                            if 2 <= inferred_tickers <= 1000:  # åˆç†èŒƒå›´ï¼š2-1000åªè‚¡ç¥¨
                                effective_tickers = inferred_tickers
                                logger.info(f"[DATA] ä»{name}æ•°æ®é•¿åº¦æ¨æ–­å‡ºçº¦{effective_tickers}åªè‚¡ç¥¨ (æ•°æ®é•¿åº¦:{len(data)} / æ—¥æœŸæ•°:{effective_dates})")
                                tickers_found = True
                                break
                    elif data is not None and hasattr(data, '__len__'):
                        # å¯¹äºæ™®é€šæ•°ç»„/åˆ—è¡¨ï¼Œæ£€æŸ¥é•¿åº¦æ¨¡å¼
                        if effective_dates > 0 and len(data) > effective_dates * 2:
                            inferred_tickers = len(data) // effective_dates
                            if 2 <= inferred_tickers <= 1000:
                                effective_tickers = inferred_tickers  
                                logger.info(f"[DATA] ä»{name}æ•°ç»„é•¿åº¦æ¨æ–­å‡ºçº¦{effective_tickers}åªè‚¡ç¥¨")
                                tickers_found = True
                                break
            
            logger.info(f"[DATA] éMultiIndexæ•°æ®: {effective_tickers}åªè‚¡ç¥¨, {effective_dates}ä¸ªæ—¶é—´ç‚¹")
            
            # æ¨ªæˆªé¢æ£€æŸ¥
            if effective_tickers < MIN_CROSS_SECTION:
                cross_section_ready = False
                logger.error(f"[ERROR] è‚¡ç¥¨æ•°é‡ä¸è¶³ï¼š{effective_tickers} < {MIN_CROSS_SECTION}")
            else:
                cross_section_ready = True
                logger.info(f"[OK] è‚¡ç¥¨æ•°é‡å……è¶³ï¼š{effective_tickers} >= {MIN_CROSS_SECTION}")
        
        total_removed = sum(removed_samples.values()) 
        total_original = sum(shape[0] if isinstance(shape, tuple) else 1 for shape in original_shapes.values())
        coverage_rate = 1.0 - (total_removed / max(total_original, 1))
        
        # 7. ç”Ÿæˆå¯¹é½æŠ¥å‘Š
        final_shape = (len(common_index),) if common_index is not None else (0,)
        
        alignment_report = AlignmentReport(
            original_shapes=original_shapes,
            final_shape=final_shape,
            removed_samples=removed_samples,
            removal_reasons=removal_reasons,
            coverage_rate=coverage_rate,
            effective_tickers=effective_tickers,
            effective_dates=effective_dates,
            horizon_trimmed=horizon_trimmed,
            daily_tickers_stats=daily_tickers_stats,
            cross_section_ready=cross_section_ready
        )
        
        # 8. è®°å½•å¯¹é½å†å²
        self.alignment_history.append(alignment_report)
        
        # [HOT] CRITICAL FIX: Enhanced dimension validation
        self._validate_dimension_consistency(aligned_data, common_index)
        self._final_dimension_check(aligned_data)
        
        logger.info("[OK] IndexAlignerå¯¹é½å®Œæˆ")
        logger.info(f"[DATA] æœ€ç»ˆå½¢çŠ¶: {final_shape}")
        logger.info(f"[DATA] è¦†ç›–ç‡: {coverage_rate:.1%}")
        logger.info(f"[DATA] æœ‰æ•ˆè‚¡ç¥¨: {effective_tickers}, æœ‰æ•ˆæ—¥æœŸ: {effective_dates}")
        
        return aligned_data, alignment_report
    
    def align_data(self, **data_dict) -> Tuple[Dict[str, Any], AlignmentReport]:
        """åˆ«åæ–¹æ³• - è°ƒç”¨align_all_dataä»¥ä¿æŒå…¼å®¹æ€§"""
        return self.align_all_data(**data_dict)
    
    def print_alignment_report(self, report: AlignmentReport) -> None:
        """æ‰“å°è¯¦ç»†å¯¹é½æŠ¥å‘Š"""
        print("="*60)
        print("[DATA] IndexAlignerå¯¹é½æŠ¥å‘Š")
        print("="*60)
        
        print("\n[SEARCH] åŸå§‹æ•°æ®å½¢çŠ¶:")
        for name, shape in report.original_shapes.items():
            print(f"  {name:15s}: {shape}")
        
        print(f"\n[OK] æœ€ç»ˆç»Ÿä¸€å½¢çŠ¶: {report.final_shape}")
        print(f"[TREND] æ•°æ®è¦†ç›–ç‡: {report.coverage_rate:.1%}")
        print(f"[DATA] æœ‰æ•ˆè‚¡ç¥¨æ•°: {report.effective_tickers}")
        print(f"[DATA] æœ‰æ•ˆæ—¥æœŸæ•°: {report.effective_dates}")
        
        # [CRITICAL] æ–°å¢æ¨ªæˆªé¢ç»Ÿè®¡æ˜¾ç¤º
        if report.daily_tickers_stats:
            stats = report.daily_tickers_stats
            print(f"[TARGET] æ¯æ—¥è‚¡ç¥¨æ•°åˆ†å¸ƒ: min={stats['min']:.0f}, median={stats['median']:.0f}, max={stats['max']:.0f}")
            if not report.cross_section_ready:
                print("[ERROR] æ¨ªæˆªé¢ä¸è¶³ï¼šæ— æ³•è¿›è¡Œæœ‰æ•ˆçš„æ¨ªæˆªé¢åˆ†æ")
            else:
                print("[OK] æ¨ªæˆªé¢å……è¶³ï¼šå¯è¿›è¡Œæ¨ªæˆªé¢æ’åºåˆ†æ")
        
        print("\n[DELETE] æ•°æ®ç§»é™¤ç»Ÿè®¡:")
        for name, removed in report.removed_samples.items():
            if removed > 0:
                print(f"  {name:15s}: -{removed:,} æ¡")
        
        print("\n[LIST] ç§»é™¤åŸå› åˆ†æ:")
        for reason, count in report.removal_reasons.items():
            if count > 0:
                print(f"  {reason:15s}: {count:,} æ¡")
        
        print("="*60)

    def _validate_dimension_consistency(self, aligned_data: Dict[str, Any], common_index: pd.Index):
        """éªŒè¯ç»´åº¦ä¸€è‡´æ€§ï¼Œé˜²æ­¢738 vs 748ç­‰é—®é¢˜"""
        expected_len = len(common_index)
        
        for name, data in aligned_data.items():
            if data is None:
                continue
                
            actual_len = len(data) if hasattr(data, '__len__') else None
            
            if actual_len is not None and actual_len != expected_len:
                error_msg = f"CRITICAL DIMENSION MISMATCH: {name} has length {actual_len}, expected {expected_len}"
                logger.error(error_msg)
                
                # [HOT] FORCE ALIGNMENT: Truncate or pad to correct length
                if hasattr(data, 'iloc'):
                    if actual_len > expected_len:
                        aligned_data[name] = data.iloc[:expected_len]
                        logger.info(f"Force truncated {name} from {actual_len} to {expected_len}")
                    elif actual_len < expected_len:
                        # For MultiIndex, we can't easily pad, so we validate the common_index instead
                        if isinstance(data.index, pd.MultiIndex):
                            logger.warning(f"Cannot pad MultiIndex data {name}, using intersection")
                            intersection_index = common_index.intersection(data.index)
                            aligned_data[name] = data.loc[intersection_index]
                        else:
                            logger.warning(f"Cannot pad {name} from {actual_len} to {expected_len}")
    
    def _final_dimension_check(self, aligned_data: Dict[str, Any]):
        """æœ€ç»ˆç»´åº¦æ£€æŸ¥ï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®é•¿åº¦ä¸€è‡´"""
        lengths = {}
        for name, data in aligned_data.items():
            if data is not None and hasattr(data, '__len__'):
                lengths[name] = len(data)
        
        if not lengths:
            return
            
        unique_lengths = set(lengths.values())
        if len(unique_lengths) > 1:
            error_msg = f"FINAL DIMENSION CHECK FAILED: {dict(lengths)}"
            logger.error(error_msg)
            
            # Find the most common length and force align all to it
            from collections import Counter
            length_counts = Counter(lengths.values())
            target_length = length_counts.most_common(1)[0][0]
            
            logger.info(f"Force aligning all data to length {target_length}")
            
            for name, data in aligned_data.items():
                if data is not None and hasattr(data, '__len__') and len(data) != target_length:
                    if hasattr(data, 'iloc'):
                        aligned_data[name] = data.iloc[:target_length]
                    elif hasattr(data, '__getitem__'):
                        aligned_data[name] = data[:target_length]
                    logger.info(f"Force aligned {name} to length {target_length}")
        else:
            logger.info(f"[OK] Final dimension check passed: all data has length {list(unique_lengths)[0]}")


def create_index_aligner(horizon: int = 10, strict_mode: bool = True, mode: str = 'train') -> IndexAligner:
    """
    åˆ›å»ºç´¢å¼•å¯¹é½å™¨
    
    Args:
        horizon: å‰ç»æœŸï¼Œè®­ç»ƒæ¨¡å¼ä¸‹ç”¨äºå‰ªå°¾
        strict_mode: ä¸¥æ ¼æ¨¡å¼ 
        mode: 'train' (è®­ç»ƒæ¨¡å¼ï¼Œæ‰§è¡Œå‰ªå°¾) æˆ– 'predict' (é¢„æµ‹æ¨¡å¼ï¼Œä¸å‰ªå°¾)
    """
    return IndexAligner(horizon=horizon, strict_mode=strict_mode, mode=mode)


# å…¨å±€å¯¹é½å™¨å®ä¾‹
_global_aligner = None

def get_global_aligner() -> IndexAligner:
    """è·å–å…¨å±€å¯¹é½å™¨å®ä¾‹"""
    global _global_aligner
    if _global_aligner is None:
        _global_aligner = create_index_aligner()
    return _global_aligner


if __name__ == "__main__":
    # æµ‹è¯•å¯¹é½å™¨
    import numpy as np
    
    # æ¨¡æ‹Ÿ738 vs 748é—®é¢˜
    dates = pd.date_range('2020-01-01', periods=500, freq='D')
    tickers = ['AAPL', 'MSFT', 'GOOGL']
    
    # åˆ›å»ºå¤šçº§ç´¢å¼•
    multi_index = pd.MultiIndex.from_product([dates, tickers], names=['date', 'ticker'])
    
    # æ¨¡æ‹Ÿä¸åŒé•¿åº¦çš„æ•°æ®
    X = pd.DataFrame(np.random.randn(len(multi_index), 5), index=multi_index)  # 1500æ¡
    y = pd.Series(np.random.randn(1490), index=multi_index[:1490])  # 1490æ¡ (ç¼º10æ¡)
    alpha = pd.DataFrame(np.random.randn(1495, 3), index=multi_index[:1495])  # 1495æ¡
    pred = pd.Series(np.random.randn(1485), index=multi_index[:1485])  # 1485æ¡
    
    print("=== æµ‹è¯•IndexAligner ===")
    aligner = create_index_aligner(horizon=10)
    
    aligned_data, report = aligner.align_all_data(
        X=X, y=y, alpha=alpha, pred=pred
    )
    
    aligner.print_alignment_report(report)
    
    print("\n[OK] å¯¹é½åæ•°æ®é•¿åº¦:")
    for name, data in aligned_data.items():
        if data is not None:
            print(f"  {name}: {len(data)}")