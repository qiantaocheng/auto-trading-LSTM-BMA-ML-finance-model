#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IndexAligner - ç»Ÿä¸€ç´¢å¼•å¯¹é½å™¨
è§£å†³738 vs 748ç»´åº¦ä¸åŒ¹é…é—®é¢˜çš„æ ¸å¿ƒæ¨¡å—
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class AlignmentReport:
    """å¯¹é½æŠ¥å‘Š"""
    original_shapes: Dict[str, Tuple]
    final_shape: Tuple
    removed_samples: Dict[str, int]
    removal_reasons: Dict[str, int]
    coverage_rate: float
    effective_tickers: int
    effective_dates: int
    horizon_trimmed: int
    # ğŸ”¥ æ–°å¢æ¨ªæˆªé¢ç»Ÿè®¡
    daily_tickers_stats: Dict[str, float] = None  # min/median/maxæ¯æ—¥è‚¡ç¥¨æ•°
    cross_section_ready: bool = True  # æ˜¯å¦æ»¡è¶³æ¨ªæˆªé¢è¦æ±‚
    
class IndexAligner:
    """ç»Ÿä¸€ç´¢å¼•å¯¹é½å™¨ - è§£å†³æ‰€æœ‰ç»´åº¦ä¸åŒ¹é…é—®é¢˜"""
    
    def __init__(self, horizon: int = 10, strict_mode: bool = True):
        """
        åˆå§‹åŒ–å¯¹é½å™¨
        
        Args:
            horizon: å‰ç»æœŸ(T+10)ï¼Œç”¨äºç»Ÿä¸€å‰ªå°¾
            strict_mode: ä¸¥æ ¼æ¨¡å¼ï¼Œç»´åº¦ä¸åŒ¹é…æ—¶æŠ¥é”™
        """
        self.horizon = horizon
        self.strict_mode = strict_mode
        self.alignment_history = []
        logger.info(f"IndexAligneråˆå§‹åŒ–: horizon={horizon}, strict_mode={strict_mode}")
    
    def align_all_data(self, **data_dict) -> Tuple[Dict[str, Any], AlignmentReport]:
        """
        ç»Ÿä¸€å¯¹é½æ‰€æœ‰æ•°æ®ï¼Œè§£å†³738 vs 748é—®é¢˜
        
        Args:
            **data_dict: å‘½åæ•°æ® å¦‚ X=features, y=labels, alpha=alpha_features, pred=predictions
            
        Returns:
            (aligned_data_dict, alignment_report)
        """
        logger.info("ğŸ¯ å¼€å§‹IndexAlignerç»Ÿä¸€å¯¹é½")
        
        # 1. è®°å½•åŸå§‹å½¢çŠ¶
        original_shapes = {}
        for name, data in data_dict.items():
            if hasattr(data, 'shape'):
                original_shapes[name] = data.shape
            elif hasattr(data, '__len__'):
                original_shapes[name] = (len(data),)
            else:
                original_shapes[name] = 'scalar'
        
        logger.info("ğŸ“Š åŸå§‹æ•°æ®å½¢çŠ¶:")
        for name, shape in original_shapes.items():
            logger.info(f"  {name}: {shape}")
        
        # 2. ç»Ÿä¸€å‰ªå°¾å¤„ç† - è§£å†³T+10å‰ç»æœŸé—®é¢˜
        trimmed_data = {}
        horizon_trimmed = 0
        
        for name, data in data_dict.items():
            if data is None:
                trimmed_data[name] = None
                continue
                
            if hasattr(data, 'index') and isinstance(data.index, pd.MultiIndex):
                # MultiIndex (date, ticker) æ ¼å¼
                if self.horizon > 0:
                    # æŒ‰ç»„å‰ªå°¾ - æ¯ä¸ªtickeréƒ½å‰ªæ‰æœ€åhorizonæ¡
                    original_len = len(data)
                    trimmed_list = []
                    
                    for ticker in data.index.get_level_values(1).unique():
                        ticker_data = data.xs(ticker, level=1, drop_level=False)
                        if len(ticker_data) > self.horizon:
                            trimmed_ticker = ticker_data.iloc[:-self.horizon]
                            trimmed_list.append(trimmed_ticker)
                    
                    if trimmed_list:
                        trimmed_data[name] = pd.concat(trimmed_list).sort_index()
                        horizon_trimmed = original_len - len(trimmed_data[name])
                        logger.info(f"  {name}: å‰ªå°¾ {horizon_trimmed} æ¡ (T+{self.horizon})")
                    else:
                        trimmed_data[name] = data.iloc[:0]  # ç©ºDataFrame
                else:
                    trimmed_data[name] = data
            else:
                # æ™®é€šç´¢å¼•æˆ–æ•°ç»„
                if hasattr(data, 'iloc') and len(data) > self.horizon and self.horizon > 0:
                    trimmed_data[name] = data.iloc[:-self.horizon]
                    horizon_trimmed += len(data) - len(trimmed_data[name])
                    logger.info(f"  {name}: å‰ªå°¾ {len(data) - len(trimmed_data[name])} æ¡")
                else:
                    trimmed_data[name] = data
        
        # 3. æ„å»ºé€šç”¨ç´¢å¼• - inner joinæ‰€æœ‰æ•°æ®
        common_index = None
        removal_reasons = {'horizon_trim': horizon_trimmed, 'nan_removal': 0, 'index_mismatch': 0}
        
        for name, data in trimmed_data.items():
            if data is None or (hasattr(data, 'empty') and data.empty):
                continue
                
            if hasattr(data, 'index'):
                current_index = data.index
                if common_index is None:
                    common_index = current_index
                else:
                    # Inner joinç´¢å¼•
                    before_len = len(common_index)
                    common_index = common_index.intersection(current_index)
                    after_len = len(common_index)
                    
                    if before_len > after_len:
                        removed = before_len - after_len
                        removal_reasons['index_mismatch'] += removed
                        logger.info(f"  {name}: ç´¢å¼•inner joinç§»é™¤ {removed} æ¡")
        
        if common_index is None or len(common_index) == 0:
            logger.error("âŒ æ‰€æœ‰æ•°æ®ç´¢å¼•äº¤é›†ä¸ºç©ºï¼Œæ— æ³•å¯¹é½")
            raise ValueError("ç´¢å¼•å¯¹é½å¤±è´¥ï¼šæ‰€æœ‰æ•°æ®äº¤é›†ä¸ºç©º")
        
        logger.info(f"ğŸ¯ é€šç”¨ç´¢å¼•ç¡®å®š: {len(common_index)} æ¡")
        
        # 4. ä½¿ç”¨é€šç”¨ç´¢å¼•å¯¹é½æ‰€æœ‰æ•°æ®
        aligned_data = {}
        removed_samples = {}
        
        for name, data in trimmed_data.items():
            if data is None:
                aligned_data[name] = None
                removed_samples[name] = 0
                continue
            
            if hasattr(data, 'loc'):
                try:
                    # ä½¿ç”¨.locå¯¹é½åˆ°é€šç”¨ç´¢å¼•
                    original_len = len(data)
                    aligned_data[name] = data.loc[common_index]
                    final_len = len(aligned_data[name])
                    removed_samples[name] = original_len - final_len
                    
                    logger.info(f"  {name}: {original_len} â†’ {final_len} (-{removed_samples[name]})")
                except Exception as e:
                    logger.warning(f"  {name}: ç´¢å¼•å¯¹é½å¤±è´¥ï¼Œä½¿ç”¨åŸæ•°æ®: {e}")
                    aligned_data[name] = data
                    removed_samples[name] = 0
            else:
                # æ•°ç»„ç±»å‹ï¼ŒæŒ‰é•¿åº¦æˆªæ–­åˆ°common_indexé•¿åº¦
                if hasattr(data, '__len__') and len(data) > len(common_index):
                    aligned_data[name] = data[:len(common_index)]
                    removed_samples[name] = len(data) - len(common_index)
                    logger.info(f"  {name}: æ•°ç»„æˆªæ–­ {len(data)} â†’ {len(common_index)}")
                else:
                    aligned_data[name] = data
                    removed_samples[name] = 0
        
        # 5. éªŒè¯æœ€ç»ˆä¸€è‡´æ€§
        final_lengths = {}
        for name, data in aligned_data.items():
            if data is not None and hasattr(data, '__len__'):
                final_lengths[name] = len(data)
        
        if final_lengths:
            unique_lengths = set(final_lengths.values())
            if len(unique_lengths) > 1:
                error_msg = f"âŒ å¯¹é½åé•¿åº¦ä»ä¸ä¸€è‡´: {final_lengths}"
                logger.error(error_msg)
                if self.strict_mode:
                    raise ValueError(error_msg)
        
        # 6. è®¡ç®—è¦†ç›–ç»Ÿè®¡å’Œæ¨ªæˆªé¢æ£€æŸ¥
        daily_tickers_stats = None
        cross_section_ready = True
        MIN_CROSS_SECTION = 30  # æœ€å°æ¨ªæˆªé¢è‚¡ç¥¨æ•°è¦æ±‚
        
        # ğŸ”¥ DEBUG: æ£€æŸ¥common_indexçš„å®é™…çŠ¶æ€
        logger.info(f"ğŸ” DEBUG common_indexæ£€æµ‹:")
        logger.info(f"  ç±»å‹: {type(common_index)}")
        logger.info(f"  é•¿åº¦: {len(common_index) if common_index is not None else 'None'}")
        logger.info(f"  hasattr get_level_values: {hasattr(common_index, 'get_level_values')}")
        logger.info(f"  hasattr nlevels: {hasattr(common_index, 'nlevels')}")
        if hasattr(common_index, 'nlevels'):
            logger.info(f"  nlevels: {common_index.nlevels}")
            logger.info(f"  nlevels >= 2: {common_index.nlevels >= 2}")
        logger.info(f"  isinstance MultiIndex: {isinstance(common_index, pd.MultiIndex)}")
        
        if hasattr(common_index, 'get_level_values') and common_index.nlevels >= 2:
            logger.info("âœ… è¿›å…¥MultiIndexåˆ†æ”¯")
            # MultiIndexæƒ…å†µ - è®¡ç®—æ¯æ—¥è‚¡ç¥¨æ•°åˆ†å¸ƒ
            effective_dates = len(common_index.get_level_values(0).unique())
            effective_tickers = len(common_index.get_level_values(1).unique())
            
            # ğŸ”¥ CRITICAL: è®¡ç®—æ¯æ—¥è‚¡ç¥¨æ•°åˆ†å¸ƒ
            daily_tickers = pd.Series(common_index.get_level_values(1)).groupby(
                pd.Series(common_index.get_level_values(0))
            ).nunique()
            
            daily_tickers_stats = {
                'min': float(daily_tickers.min()),
                'median': float(daily_tickers.median()),
                'max': float(daily_tickers.max()),
                'mean': float(daily_tickers.mean())
            }
            
            # MultiIndexæƒ…å†µä¸‹çš„æ¨ªæˆªé¢æ£€æŸ¥
            if effective_tickers < MIN_CROSS_SECTION:
                cross_section_ready = False
                logger.error(f"âŒ MultiIndexè‚¡ç¥¨æ•°é‡ä¸è¶³ï¼š{effective_tickers} < {MIN_CROSS_SECTION}")
            else:
                cross_section_ready = True
                logger.info(f"âœ… MultiIndexè‚¡ç¥¨æ•°é‡å……è¶³ï¼š{effective_tickers} >= {MIN_CROSS_SECTION}")
                logger.info(f"âœ… æ¨ªæˆªé¢å……è¶³ï¼šæ¯æ—¥è‚¡ç¥¨æ•° min={daily_tickers_stats['min']}, median={daily_tickers_stats['median']}, max={daily_tickers_stats['max']}")
            
        else:
            logger.info("âŒ è¿›å…¥elseåˆ†æ”¯ï¼ˆéMultiIndexï¼‰")
            # ğŸ”¥ CRITICAL FIX: éMultiIndexæƒ…å†µä¸‹æ­£ç¡®è®¡ç®—è‚¡ç¥¨æ•°é‡
            logger.warning("âš ï¸ æ£€æµ‹åˆ°éMultiIndexæ ¼å¼ï¼Œå°è¯•ä»æ•°æ®ä¸­æ¨æ–­è‚¡ç¥¨æ•°é‡")
            
            effective_dates = len(common_index.unique()) if hasattr(common_index, 'unique') else len(common_index)
            effective_tickers = 1  # é»˜è®¤å€¼
            
            # ğŸ”¥ PRIORITY 1: ä»tickerså‚æ•°ç›´æ¥è·å–è‚¡ç¥¨æ•°é‡ï¼ˆæœ€å¯é ï¼‰
            if 'tickers' in aligned_data and aligned_data['tickers'] is not None:
                tickers_data = aligned_data['tickers']
                if hasattr(tickers_data, 'unique'):
                    unique_tickers = tickers_data.unique()
                    effective_tickers = len(unique_tickers)
                    logger.info(f"ğŸ¯ ä»tickerså‚æ•°ç›´æ¥è·å–: {effective_tickers}åªè‚¡ç¥¨")
                    if effective_tickers > 1:
                        logger.info(f"ğŸ“Š è‚¡ç¥¨åˆ—è¡¨: {list(unique_tickers)[:10]}...")
                elif hasattr(tickers_data, '__len__'):
                    # å¦‚æœæ˜¯åˆ—è¡¨æˆ–æ•°ç»„
                    unique_tickers = list(set(tickers_data)) if hasattr(tickers_data, '__iter__') else [tickers_data]
                    effective_tickers = len(unique_tickers)
                    logger.info(f"ğŸ¯ ä»tickersæ•°ç»„è·å–: {effective_tickers}åªè‚¡ç¥¨")
            
            # ğŸ”¥ FALLBACK: å¦‚æœtickerså‚æ•°æ— æ•ˆï¼Œå°è¯•å…¶ä»–æ–¹æ³•æ¨æ–­
            if effective_tickers == 1:
                logger.warning("âš ï¸ tickerså‚æ•°æ— æ•ˆï¼Œå°è¯•ä»å…¶ä»–æ•°æ®æ¨æ–­...")
                
                # æ£€æŸ¥æ˜¯å¦èƒ½ä»å¯¹é½åçš„æ•°æ®æ¨æ–­è‚¡ç¥¨æ•°é‡
                for name, data in aligned_data.items():
                    if name == 'tickers':  # å·²ç»å¤„ç†è¿‡äº†
                        continue
                        
                    if data is not None and hasattr(data, 'columns') and 'ticker' in str(data.columns):
                        # å¦‚æœæ•°æ®ä¸­æœ‰tickeråˆ—
                        if hasattr(data, 'ticker'):
                            unique_tickers = data['ticker'].unique() if hasattr(data['ticker'], 'unique') else []
                            inferred_tickers = len(unique_tickers)
                            if inferred_tickers > 1:
                                effective_tickers = inferred_tickers
                                logger.info(f"ğŸ“Š ä»{name}æ•°æ®æ¨æ–­å‡º{effective_tickers}åªè‚¡ç¥¨: {list(unique_tickers)[:5]}...")
                                break
                    elif data is not None and hasattr(data, 'index'):
                        # æ£€æŸ¥ç´¢å¼•ä¸­æ˜¯å¦æœ‰è‚¡ç¥¨ä¿¡æ¯æ¨¡å¼
                        index_str = str(data.index)
                        if 'ticker' in index_str.lower() or len(data) > effective_dates:
                            # å¦‚æœæ•°æ®é•¿åº¦è¿œå¤§äºæ—¥æœŸæ•°ï¼Œå¯èƒ½æ˜¯å¤šè‚¡ç¥¨
                            inferred_tickers = len(data) // effective_dates if effective_dates > 0 else 1
                            if inferred_tickers > 1 and inferred_tickers <= 1000:  # åˆç†èŒƒå›´
                                effective_tickers = inferred_tickers
                                logger.info(f"ğŸ“Š ä»{name}æ•°æ®é•¿åº¦æ¨æ–­å‡ºçº¦{effective_tickers}åªè‚¡ç¥¨")
                                break
            
            logger.info(f"ğŸ“Š éMultiIndexæ•°æ®: {effective_tickers}åªè‚¡ç¥¨, {effective_dates}ä¸ªæ—¶é—´ç‚¹")
            
            # æ¨ªæˆªé¢æ£€æŸ¥
            if effective_tickers < MIN_CROSS_SECTION:
                cross_section_ready = False
                logger.error(f"âŒ è‚¡ç¥¨æ•°é‡ä¸è¶³ï¼š{effective_tickers} < {MIN_CROSS_SECTION}")
            else:
                cross_section_ready = True
                logger.info(f"âœ… è‚¡ç¥¨æ•°é‡å……è¶³ï¼š{effective_tickers} >= {MIN_CROSS_SECTION}")
        
        total_removed = sum(removed_samples.values()) 
        total_original = sum(shape[0] if isinstance(shape, tuple) else 1 for shape in original_shapes.values())
        coverage_rate = 1.0 - (total_removed / max(total_original, 1))
        
        # 7. ç”Ÿæˆå¯¹é½æŠ¥å‘Š
        final_shape = (len(common_index),) if common_index is not None else (0,)
        
        alignment_report = AlignmentReport(
            original_shapes=original_shapes,
            final_shape=final_shape,
            removed_samples=removed_samples,
            removal_reasons=removal_reasons,
            coverage_rate=coverage_rate,
            effective_tickers=effective_tickers,
            effective_dates=effective_dates,
            horizon_trimmed=horizon_trimmed,
            daily_tickers_stats=daily_tickers_stats,
            cross_section_ready=cross_section_ready
        )
        
        # 8. è®°å½•å¯¹é½å†å²
        self.alignment_history.append(alignment_report)
        
        logger.info("âœ… IndexAlignerå¯¹é½å®Œæˆ")
        logger.info(f"ğŸ“Š æœ€ç»ˆå½¢çŠ¶: {final_shape}")
        logger.info(f"ğŸ“Š è¦†ç›–ç‡: {coverage_rate:.1%}")
        logger.info(f"ğŸ“Š æœ‰æ•ˆè‚¡ç¥¨: {effective_tickers}, æœ‰æ•ˆæ—¥æœŸ: {effective_dates}")
        
        return aligned_data, alignment_report
    
    def align_data(self, **data_dict) -> Tuple[Dict[str, Any], AlignmentReport]:
        """åˆ«åæ–¹æ³• - è°ƒç”¨align_all_dataä»¥ä¿æŒå…¼å®¹æ€§"""
        return self.align_all_data(**data_dict)
    
    def print_alignment_report(self, report: AlignmentReport) -> None:
        """æ‰“å°è¯¦ç»†å¯¹é½æŠ¥å‘Š"""
        print("="*60)
        print("ğŸ“Š IndexAlignerå¯¹é½æŠ¥å‘Š")
        print("="*60)
        
        print("\nğŸ” åŸå§‹æ•°æ®å½¢çŠ¶:")
        for name, shape in report.original_shapes.items():
            print(f"  {name:15s}: {shape}")
        
        print(f"\nâœ… æœ€ç»ˆç»Ÿä¸€å½¢çŠ¶: {report.final_shape}")
        print(f"ğŸ“ˆ æ•°æ®è¦†ç›–ç‡: {report.coverage_rate:.1%}")
        print(f"ğŸ“Š æœ‰æ•ˆè‚¡ç¥¨æ•°: {report.effective_tickers}")
        print(f"ğŸ“Š æœ‰æ•ˆæ—¥æœŸæ•°: {report.effective_dates}")
        
        # ğŸ”¥ æ–°å¢æ¨ªæˆªé¢ç»Ÿè®¡æ˜¾ç¤º
        if report.daily_tickers_stats:
            stats = report.daily_tickers_stats
            print(f"ğŸ¯ æ¯æ—¥è‚¡ç¥¨æ•°åˆ†å¸ƒ: min={stats['min']:.0f}, median={stats['median']:.0f}, max={stats['max']:.0f}")
            if not report.cross_section_ready:
                print("âŒ æ¨ªæˆªé¢ä¸è¶³ï¼šæ— æ³•è¿›è¡Œæœ‰æ•ˆçš„æ¨ªæˆªé¢åˆ†æ")
            else:
                print("âœ… æ¨ªæˆªé¢å……è¶³ï¼šå¯è¿›è¡Œæ¨ªæˆªé¢æ’åºåˆ†æ")
        
        print("\nğŸ—‘ï¸ æ•°æ®ç§»é™¤ç»Ÿè®¡:")
        for name, removed in report.removed_samples.items():
            if removed > 0:
                print(f"  {name:15s}: -{removed:,} æ¡")
        
        print("\nğŸ“‹ ç§»é™¤åŸå› åˆ†æ:")
        for reason, count in report.removal_reasons.items():
            if count > 0:
                print(f"  {reason:15s}: {count:,} æ¡")
        
        print("="*60)


def create_index_aligner(horizon: int = 10, strict_mode: bool = True) -> IndexAligner:
    """åˆ›å»ºç´¢å¼•å¯¹é½å™¨"""
    return IndexAligner(horizon=horizon, strict_mode=strict_mode)


# å…¨å±€å¯¹é½å™¨å®ä¾‹
_global_aligner = None

def get_global_aligner() -> IndexAligner:
    """è·å–å…¨å±€å¯¹é½å™¨å®ä¾‹"""
    global _global_aligner
    if _global_aligner is None:
        _global_aligner = create_index_aligner()
    return _global_aligner


if __name__ == "__main__":
    # æµ‹è¯•å¯¹é½å™¨
    import numpy as np
    
    # æ¨¡æ‹Ÿ738 vs 748é—®é¢˜
    dates = pd.date_range('2020-01-01', periods=500, freq='D')
    tickers = ['AAPL', 'MSFT', 'GOOGL']
    
    # åˆ›å»ºå¤šçº§ç´¢å¼•
    multi_index = pd.MultiIndex.from_product([dates, tickers], names=['date', 'ticker'])
    
    # æ¨¡æ‹Ÿä¸åŒé•¿åº¦çš„æ•°æ®
    X = pd.DataFrame(np.random.randn(len(multi_index), 5), index=multi_index)  # 1500æ¡
    y = pd.Series(np.random.randn(1490), index=multi_index[:1490])  # 1490æ¡ (ç¼º10æ¡)
    alpha = pd.DataFrame(np.random.randn(1495, 3), index=multi_index[:1495])  # 1495æ¡
    pred = pd.Series(np.random.randn(1485), index=multi_index[:1485])  # 1485æ¡
    
    print("=== æµ‹è¯•IndexAligner ===")
    aligner = create_index_aligner(horizon=10)
    
    aligned_data, report = aligner.align_all_data(
        X=X, y=y, alpha=alpha, pred=pred
    )
    
    aligner.print_alignment_report(report)
    
    print("\nâœ… å¯¹é½åæ•°æ®é•¿åº¦:")
    for name, data in aligned_data.items():
        if data is not None:
            print(f"  {name}: {len(data)}")