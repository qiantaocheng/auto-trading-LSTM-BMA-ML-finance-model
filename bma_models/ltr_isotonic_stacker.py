"""
LTR (LambdaRank) + Isotonic Regression äºŒå±‚ Stacking æ¨¡å‹
æ›¿æ¢åŸæœ‰çš„ EWA (æŒ‡æ•°åŠ æƒå¹³å‡) æ–¹æ¡ˆï¼Œæä¾›æ›´ä¼˜çš„æ’åºå’Œæ ¡å‡†èƒ½åŠ›

Author: BMA Trading System
Date: 2025-01-16
"""

import numpy as np
import pandas as pd
from sklearn.isotonic import IsotonicRegression
from scipy.stats import rankdata, spearmanr
import lightgbm as lgb
from typing import Dict, List, Tuple, Optional, Any
import logging
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)


def _ensure_sorted(df: pd.DataFrame) -> pd.DataFrame:
    """ç¡®ä¿æ•°æ®æŒ‰ (date, ticker) æ’åº"""
    if not isinstance(df.index, pd.MultiIndex):
        raise ValueError("df index must be MultiIndex[(date,ticker)]")
    return df.sort_index(level=['date','ticker'])


def _group_sizes_by_date(df: pd.DataFrame) -> List[int]:
    """
    ä»¥ date ä¸º query ç”Ÿæˆ LightGBM çš„ group
    ä¾èµ– df å·²æŒ‰ (date,ticker) æ’åºï¼
    """
    return [len(g) for _, g in df.groupby(level='date', sort=False)]


def _winsorize_by_date(s: pd.Series, limits=(0.01, 0.99)) -> pd.Series:
    """é€æ—¥åˆ†ä½è£å‰ªï¼ˆæ›´ç¨³å¥ï¼‰"""
    def _w(x):
        if len(x) < 2:
            return x
        lo, hi = x.quantile(limits[0]), x.quantile(limits[1])
        return x.clip(lo, hi)
    return s.groupby(level='date').apply(_w)


def _zscore_by_date(s: pd.Series) -> pd.Series:
    """é€æ—¥æ ‡å‡†åŒ–"""
    def _z(x):
        if len(x) < 2:
            return x
        mu, sd = x.mean(), x.std(ddof=0)
        return (x - mu) / (sd if sd > 1e-12 else 1.0)
    return s.groupby(level='date').apply(_z)


def _demean_by_group(df_feat: pd.DataFrame, group_col: str) -> pd.DataFrame:
    """æŒ‰è¡Œä¸šç­‰åˆ†ç±»åœ¨æˆªé¢å»å‡å€¼ï¼šX := X - group_mean(X)"""
    def _demean(group):
        return group - group.mean()
    return df_feat.groupby([df_feat.index.get_level_values('date'), df_feat[group_col]]).transform(_demean)


def _neutralize(df: pd.DataFrame, cols: List[str], cfg: Optional[Dict] = None) -> pd.DataFrame:
    """
    ç®€ç‰ˆä¸­æ€§åŒ–ï¼šä¼˜å…ˆæŒ‰ 'by' åˆ—å»å‡å€¼ï¼Œå¯é€‰å†å¯¹ beta åšçº¿æ€§å›å½’æ®‹å·®
    cfg ç¤ºä¾‹: {'by':['sector'], 'beta_col':'beta'}
    """
    out = df.copy()
    if cfg and 'by' in cfg:
        for gcol in cfg['by']:
            if gcol not in out.columns:
                continue
            # æŒ‰ç»„å»å‡å€¼
            for c in cols:
                temp_df = out[[c, gcol]].copy()
                temp_df.columns = ['_v', gcol]
                out[c] = _demean_by_group(temp_df, gcol)['_v']
    return out


def _spearman_ic_eval(preds: np.ndarray, dataset: lgb.Dataset):
    """è‡ªå®šä¹‰è¯„ä¼°ï¼šSpearman ICï¼ˆé€æ—¥è®¡ç®—åå–å‡å€¼ï¼‰"""
    y = dataset.get_label()
    groups = dataset.get_group()

    ic_list = []
    start = 0
    for g in groups:
        end = start + int(g)
        y_g = y[start:end]
        p_g = preds[start:end]

        if len(y_g) > 1:
            # Spearmanï¼šå¯¹ y ä¸ p åˆ†åˆ«å–ç§©ç›¸å…³
            r_y = rankdata(y_g, method='average')
            r_p = rankdata(p_g, method='average')
            # çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆå¯¹ç§©ï¼‰
            ic = np.corrcoef(r_y, r_p)[0,1] if len(r_y) > 1 else 0.0
        else:
            ic = 0.0

        ic_list.append(ic)
        start = end

    ic_mean = float(np.mean(ic_list)) if ic_list else 0.0
    # LightGBM éœ€è¦ (åç§°, å€¼, è¶Šå¤§è¶Šå¥½)
    return ('spearman_ic', ic_mean, True)


def make_purged_splits(dates_sorted: np.ndarray, n_splits=5, embargo=10) -> List[Tuple]:
    """
    æ—¶åºCVï¼ˆå¸¦ purge + embargoï¼‰çš„æŠ˜ç”Ÿæˆå™¨
    dates_sorted: å‡åºçš„ä¸é‡å¤äº¤æ˜“æ—¥æ•°ç»„
    è¿”å› [(train_date_idx, valid_date_idx), ...]ï¼Œå¹¶è‡ªåŠ¨åœ¨æ¯æŠ˜é—´éš” embargo å¤©
    """
    n = len(dates_sorted)
    fold_size = n // (n_splits + 1)  # ç•™å‡ºæœ«æ®µåšæµ‹è¯•/ç•™ç™½
    splits = []

    for k in range(n_splits):
        train_end = fold_size * (k + 1)
        valid_start = train_end + embargo
        valid_end = min(valid_start + fold_size, n)

        if valid_end <= valid_start:
            break

        train_idx = np.arange(0, train_end)   # [0, train_end)
        valid_idx = np.arange(valid_start, valid_end)
        splits.append((train_idx, valid_idx))

    return splits


class LtrIsotonicStacker:
    """
    LambdaRank + å…¨å±€ Isotonic æ ¡å‡†çš„äºŒå±‚ Stacking æ¨¡å‹
    ç”¨äºæ›¿æ¢åŸæœ‰çš„ EWA æ–¹æ¡ˆï¼Œæä¾›æ›´ä¼˜çš„ T+10 é¢„æµ‹èƒ½åŠ›
    """

    def __init__(self,
                 base_cols=('pred_catboost','pred_elastic','pred_xgb'),
                 horizon=10,
                 winsor_limits=(0.01, 0.99),
                 do_zscore=True,
                 neutralize_cfg=None,
                 lgbm_params=None,
                 n_splits=5,
                 embargo=10,
                 random_state=42):
        """
        åˆå§‹åŒ– LTR Stacker

        Args:
            base_cols: ä¸€å±‚æ¨¡å‹é¢„æµ‹åˆ—å
            horizon: é¢„æµ‹æœŸé™ï¼ˆå¤©ï¼‰
            winsor_limits: æå€¼å¤„ç†åˆ†ä½æ•°
            do_zscore: æ˜¯å¦åšZ-scoreæ ‡å‡†åŒ–
            neutralize_cfg: ä¸­æ€§åŒ–é…ç½®
            lgbm_params: LightGBMå‚æ•°
            n_splits: CVæŠ˜æ•°
            embargo: æ—¶é—´é—´éš”å¤©æ•°
            random_state: éšæœºç§å­
        """
        self.base_cols_ = list(base_cols)
        self.horizon_ = int(horizon)
        self.winsor_limits_ = winsor_limits
        self.do_zscore_ = do_zscore
        self.neutralize_cfg_ = neutralize_cfg or {}
        self.n_splits_ = n_splits
        self.embargo_ = embargo
        self.random_state_ = random_state

        # é»˜è®¤çš„ LambdaRank å‚æ•°ï¼ˆå¯å†è°ƒï¼‰
        self.lgbm_params_ = lgbm_params or dict(
            objective='lambdarank',
            boosting_type='gbdt',
            learning_rate=0.03,
            num_leaves=31,
            max_depth=-1,
            min_data_in_leaf=100,
            feature_fraction=0.9,
            bagging_fraction=0.9,
            bagging_freq=1,
            metric='ndcg',
            lambda_l1=0.1,
            lambda_l2=0.1,
            verbosity=-1,
            n_estimators=3000,
            importance_type='gain'
        )

        self.ranker_ = None
        self.calibrator_ = None
        self.fitted_ = False
        self._col_cache_ = None  # è®°å½•è®­ç»ƒæœŸçš„åˆ—é¡ºåº/åç§°
        self.feature_importance_ = None
        self.cv_scores_ = []
        self.oof_predictions_ = None
        self.oof_targets_ = None

    def _preprocess(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç»Ÿä¸€é¢„å¤„ç†ï¼šå¯¹è®­ç»ƒæˆ–æ¨ç†æ•°æ®éƒ½å¯ç”¨"""
        df = _ensure_sorted(df.copy())

        # æ£€æŸ¥å¿…è¦åˆ—
        use_cols = [c for c in self.base_cols_ if c in df.columns]
        if len(use_cols) != len(self.base_cols_):
            miss = set(self.base_cols_) - set(use_cols)
            logger.warning(f"ç¼ºå°‘ä¸€å±‚åˆ—ï¼š{miss}ï¼Œå°è¯•ä½¿ç”¨å¯ç”¨åˆ—")
            if len(use_cols) == 0:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä¸€å±‚é¢„æµ‹åˆ—")

        X = df[use_cols].copy()

        # é€æ—¥ winsorize
        for c in use_cols:
            X[c] = _winsorize_by_date(X[c], self.winsor_limits_)

        # ï¼ˆå¯é€‰ï¼‰é€æ—¥ zscore
        if self.do_zscore_:
            for c in use_cols:
                X[c] = _zscore_by_date(X[c])

        # ï¼ˆå¯é€‰ï¼‰ä¸­æ€§åŒ–
        if self.neutralize_cfg_:
            neutralize_cols = self.neutralize_cfg_.get('by', [])
            for col in neutralize_cols:
                if col in df.columns:
                    X[col] = df[col]
            X = _neutralize(X, cols=use_cols, cfg=self.neutralize_cfg_)
            X = X[use_cols]  # åªä¿ç•™ç‰¹å¾åˆ—

        # åˆå¹¶å¤„ç†åçš„ç‰¹å¾å›åŸæ•°æ®
        out = df.copy()
        for c in use_cols:
            out[c] = X[c]

        return out

    def fit(self, df: pd.DataFrame) -> "LtrIsotonicStacker":
        """
        è®­ç»ƒ LTR + Isotonic æ¨¡å‹

        Args:
            df: åŒ…å«ä¸€å±‚é¢„æµ‹å’Œæ ‡ç­¾çš„æ•°æ®ï¼ŒMultiIndex[(date,ticker)]

        Returns:
            self
        """
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ LTR + Isotonic Stacker")

        df = self._preprocess(df)

        # æ£€æŸ¥æ ‡ç­¾åˆ—
        label_col = None
        for col in ['ret_fwd_10d', 'target', 'returns_10d', 'label']:
            if col in df.columns:
                label_col = col
                break

        if label_col is None:
            raise ValueError("è®­ç»ƒæœŸéœ€è¦æ ‡ç­¾åˆ— (ret_fwd_10d/target/returns_10d/label)")

        logger.info(f"ä½¿ç”¨æ ‡ç­¾åˆ—: {label_col}")

        # æ ‡ç­¾ä¹Ÿè£å‰ªç¨³å¥äº›ï¼ˆé¿å…æç«¯æ”¶ç›Šä¸»å¯¼ NDCGï¼‰
        y = _winsorize_by_date(df[label_col], self.winsor_limits_)

        # ç”Ÿæˆæ—¶åºæŠ˜ï¼ˆæŒ‰æ—¥æœŸï¼‰
        unique_dates = df.index.get_level_values('date').unique().sort_values().values
        splits = make_purged_splits(unique_dates, n_splits=self.n_splits_, embargo=self.embargo_)

        logger.info(f"ç”Ÿæˆ {len(splits)} ä¸ªæ—¶åºCVæŠ˜")

        # æ”¶é›† OOF é¢„æµ‹ç”¨äºå…¨å±€ Isotonic æ ¡å‡†
        oof_preds = []
        oof_y = []
        self.cv_scores_ = []

        # ç¡®å®šå®é™…ä½¿ç”¨çš„ç‰¹å¾åˆ—
        actual_base_cols = [c for c in self.base_cols_ if c in df.columns]

        for fold_idx, (tr_idx, va_idx) in enumerate(splits):
            dates_tr = unique_dates[tr_idx]
            dates_va = unique_dates[va_idx]

            df_tr = df.loc[df.index.get_level_values('date').isin(dates_tr)]
            df_va = df.loc[df.index.get_level_values('date').isin(dates_va)]

            logger.info(f"Fold {fold_idx+1}/{len(splits)}: è®­ç»ƒ {len(df_tr)} æ ·æœ¬, éªŒè¯ {len(df_va)} æ ·æœ¬")

            X_tr = df_tr[actual_base_cols].values
            y_tr = y.loc[df_tr.index].values
            grp_tr = _group_sizes_by_date(df_tr)

            X_va = df_va[actual_base_cols].values
            y_va = y.loc[df_va.index].values
            grp_va = _group_sizes_by_date(df_va)

            # åˆ›å»º LightGBM æ•°æ®é›†
            dtrain = lgb.Dataset(X_tr, label=y_tr, group=grp_tr, free_raw_data=False)
            dvalid = lgb.Dataset(X_va, label=y_va, group=grp_va, free_raw_data=False)

            # è®­ç»ƒ ranker
            ranker = lgb.LGBMRanker(**self.lgbm_params_, random_state=self.random_state_)
            ranker.fit(
                X_tr, y_tr,
                group=grp_tr,
                eval_set=[(X_va, y_va)],
                eval_group=[grp_va],
                eval_metric=[_spearman_ic_eval, 'ndcg'],
                callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False),
                          lgb.log_evaluation(period=0)]
            )

            # éªŒè¯é›†é¢„æµ‹ï¼ˆOOFï¼‰
            va_pred = ranker.predict(X_va, num_iteration=ranker.best_iteration_)
            oof_preds.append(pd.Series(va_pred, index=df_va.index))
            oof_y.append(pd.Series(y_va, index=df_va.index))

            # è®¡ç®—éªŒè¯åˆ†æ•°
            ic_score = spearmanr(va_pred, y_va)[0]
            self.cv_scores_.append(ic_score)
            logger.info(f"Fold {fold_idx+1} IC: {ic_score:.4f}")

        # åˆå¹¶OOFé¢„æµ‹
        oof_preds = pd.concat(oof_preds).sort_index()
        oof_y = pd.concat(oof_y).sort_index()

        self.oof_predictions_ = oof_preds
        self.oof_targets_ = oof_y

        mean_ic = np.mean(self.cv_scores_)
        logger.info(f"ğŸ“Š CVå¹³å‡IC: {mean_ic:.4f} (std: {np.std(self.cv_scores_):.4f})")

        # è®­ç»ƒå…¨å±€ Isotonicï¼ˆä¿æŒå•è°ƒã€æ ¡æ­£åˆ»åº¦ï¼‰
        logger.info("è®­ç»ƒå…¨å±€ Isotonic æ ¡å‡†å™¨...")
        self.calibrator_ = IsotonicRegression(out_of_bounds='clip')
        self.calibrator_.fit(oof_preds.values, oof_y.values)

        # æœ€ç»ˆåœ¨å…¨æ ·æœ¬é‡è®­ ranker
        logger.info("åœ¨å…¨æ ·æœ¬é‡è®­æœ€ç»ˆ ranker...")
        X_all = df[actual_base_cols].values
        y_all = y.loc[df.index].values
        grp_all = _group_sizes_by_date(df)

        dtrain_all = lgb.Dataset(X_all, label=y_all, group=grp_all, free_raw_data=False)

        # ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°è¿›è¡Œæœ€ç»ˆè®­ç»ƒ
        final_params = self.lgbm_params_.copy()
        final_params['n_estimators'] = min(final_params.get('n_estimators', 3000),
                                          int(np.mean([r.best_iteration_ for r in [ranker]])) + 500)

        self.ranker_ = lgb.LGBMRanker(**final_params, random_state=self.random_state_)
        self.ranker_.fit(
            X_all, y_all,
            group=grp_all,
            eval_set=[(X_all, y_all)],
            eval_group=[grp_all],
            eval_metric=[_spearman_ic_eval, 'ndcg'],
            callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False),
                      lgb.log_evaluation(period=0)]
        )

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        self.feature_importance_ = pd.DataFrame({
            'feature': actual_base_cols,
            'importance': self.ranker_.feature_importances_
        }).sort_values('importance', ascending=False)

        logger.info("ç‰¹å¾é‡è¦æ€§:")
        for _, row in self.feature_importance_.iterrows():
            logger.info(f"  {row['feature']}: {row['importance']:.0f}")

        self._col_cache_ = list(actual_base_cols)
        self.fitted_ = True

        logger.info("âœ… LTR + Isotonic Stacker è®­ç»ƒå®Œæˆ")
        return self

    def predict(self, df_today: pd.DataFrame) -> pd.DataFrame:
        """
        å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹

        Args:
            df_today: åŒ…å«ä¸€å±‚é¢„æµ‹çš„æ•°æ®ï¼ŒMultiIndex[(date,ticker)]

        Returns:
            åŒ…å«é¢„æµ‹åˆ†æ•°çš„ DataFrame
        """
        if not self.fitted_:
            raise RuntimeError("è¯·å…ˆ fit() å† predict()")

        # å…è®¸æ‰¹é‡å¤šæ—¥æ¨ç†
        df_today = self._preprocess(df_today)

        # ä½¿ç”¨è®­ç»ƒæ—¶ç¡®å®šçš„ç‰¹å¾åˆ—
        X = df_today[self._col_cache_].values

        # LTR é¢„æµ‹
        raw = self.ranker_.predict(X, num_iteration=self.ranker_.best_iteration_)

        # å…¨å±€å•è°ƒæ ¡å‡†
        cal = self.calibrator_.transform(raw)

        out = df_today.copy()
        out['score_raw'] = raw
        out['score'] = cal

        # å¯é€‰ï¼šè¾“å‡ºæ—¥å†…rank / z
        def _rank(x):
            return pd.Series(rankdata(x, method='average'), index=x.index)

        out['score_rank'] = out.groupby(level='date')['score'].transform(_rank)
        out['score_z'] = out.groupby(level='date')['score'].transform(
            lambda x: (x-x.mean())/(x.std(ddof=0)+1e-12)
        )

        return out[['score_raw','score','score_rank','score_z']]

    def replace_ewa_in_pipeline(self, df_today: pd.DataFrame) -> pd.DataFrame:
        """
        ä½œä¸º"EWAæ›¿æ¢ä»¶"çš„è–„å°è£…ï¼šè¿”å›ä¸€åˆ— final scoreï¼ˆå·²æ ¡å‡†ï¼‰
        åŸæ¥æ‹¿ EWA åˆ†æ•°å–‚ç»„åˆ/å›æµ‹çš„åœ°æ–¹ï¼Œç›´æ¥æ¢è¿™åˆ—å³å¯

        Args:
            df_today: åŒ…å«ä¸€å±‚é¢„æµ‹çš„æ•°æ®

        Returns:
            å•åˆ— DataFrameï¼ŒåŒ…å«æœ€ç»ˆåˆ†æ•°
        """
        scores = self.predict(df_today)
        return scores[['score']]

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯ç”¨äºæŠ¥å‘Š"""
        if not self.fitted_:
            return {'fitted': False}

        return {
            'fitted': True,
            'model_type': 'LTR + Isotonic Calibration',
            'base_features': self._col_cache_,
            'cv_mean_ic': np.mean(self.cv_scores_) if self.cv_scores_ else None,
            'cv_std_ic': np.std(self.cv_scores_) if self.cv_scores_ else None,
            'n_iterations': self.ranker_.best_iteration_ if self.ranker_ else None,
            'feature_importance': self.feature_importance_.to_dict() if self.feature_importance_ is not None else None,
            'calibrator_fitted': self.calibrator_ is not None,
            'horizon': self.horizon_
        }