#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Walk-Forwardé‡è®­ç»ƒç³»ç»Ÿ
å®ç°æ»šåŠ¨çª—å£è®­ç»ƒã€run_idç”Ÿæˆã€çƒ­èº«æœŸå’Œå¼ºåˆ¶é‡è®­é—´éš”
"""

import numpy as np
import pandas as pd
import hashlib
import json
import pickle
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
import logging

logger = logging.getLogger(__name__)

@dataclass
class WalkForwardConfig:
    """Walk-Forwardé…ç½®"""
    train_window_months: int = 24  # è®­ç»ƒçª—å£(æœˆ) - ä¿®æ”¹ä¸º2å¹´
    step_size_days: int = 30  # æ­¥é•¿(å¤©)ï¼Œæœˆåº¦é‡è®­
    warmup_periods: int = 3  # çƒ­èº«æœŸæ•°é‡
    force_refit_days: int = 90  # å¼ºåˆ¶é‡è®­é—´éš”(å¤©)
    min_train_samples: int = 1000  # æœ€å°è®­ç»ƒæ ·æœ¬æ•°
    max_lookback_days: int = 730  # æœ€å¤§å›çœ‹å¤©æ•°(2å¹´) - ä¸è®­ç»ƒçª—å£åŒ¹é…
    window_type: str = 'rolling'  # 'rolling' æˆ– 'expanding'
    enable_version_control: bool = True
    cache_models: bool = True
    
@dataclass  
class RunConfig:
    """è¿è¡Œé…ç½®"""
    run_id: str
    code_hash: str
    config_hash: str
    data_slice_hash: str
    train_start: str
    train_end: str
    test_start: str
    test_end: str
    model_version: str
    creation_timestamp: str
    
class WalkForwardRetrainingSystem:
    """Walk-Forwardé‡è®­ç»ƒç³»ç»Ÿ"""
    
    def __init__(self, config: WalkForwardConfig):
        self.config = config
        self.cache_dir = Path("cache/walk_forward")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.model_cache = {}
        self.run_history = []
        
    def generate_run_id(self, 
                       code_content: str,
                       config_dict: Dict,
                       data_slice: pd.DataFrame,
                       train_start: str,
                       train_end: str) -> RunConfig:
        """
        ç”Ÿæˆrun_id = code_hash + config_hash + data_slice_hash
        """
        # 1. ä»£ç hash
        code_hash = hashlib.md5(code_content.encode('utf-8')).hexdigest()[:8]
        
        # 2. é…ç½®hash
        config_str = json.dumps(config_dict, sort_keys=True, default=str)
        config_hash = hashlib.md5(config_str.encode('utf-8')).hexdigest()[:8]
        
        # 3. æ•°æ®åˆ‡ç‰‡hash
        data_str = f"{len(data_slice)}_{train_start}_{train_end}"
        if not data_slice.empty:
            # æ·»åŠ æ•°æ®çš„ç»Ÿè®¡ç‰¹å¾
            numeric_cols = data_slice.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                data_stats = data_slice[numeric_cols].describe().values.flatten()
                data_str += f"_{hash(tuple(data_stats)) % 1000000}"
        
        data_slice_hash = hashlib.md5(data_str.encode('utf-8')).hexdigest()[:8]
        
        # 4. ç”Ÿæˆrun_id
        run_id = f"{code_hash}_{config_hash}_{data_slice_hash}"
        
        # 5. åˆ›å»ºè¿è¡Œé…ç½®
        run_config = RunConfig(
            run_id=run_id,
            code_hash=code_hash,
            config_hash=config_hash,
            data_slice_hash=data_slice_hash,
            train_start=train_start,
            train_end=train_end,
            test_start=train_end,  # æµ‹è¯•å¼€å§‹æ—¶é—´ç­‰äºè®­ç»ƒç»“æŸæ—¶é—´
            test_end=train_end,    # å°†ç”±è°ƒç”¨è€…æ›´æ–°
            model_version="1.0.0",
            creation_timestamp=datetime.now().isoformat()
        )
        
        logger.info(f"ç”Ÿæˆrun_id: {run_id}")
        logger.info(f"  - ä»£ç hash: {code_hash}")
        logger.info(f"  - é…ç½®hash: {config_hash}")  
        logger.info(f"  - æ•°æ®hash: {data_slice_hash}")
        
        return run_config
    
    def should_retrain(self, 
                      last_train_date: Optional[str],
                      current_date: str,
                      force_retrain: bool = False) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è®­ç»ƒ
        
        Returns:
            (should_retrain, reason)
        """
        if force_retrain:
            return True, "å¼ºåˆ¶é‡è®­ç»ƒ"
            
        if last_train_date is None:
            return True, "é¦–æ¬¡è®­ç»ƒ"
            
        # è®¡ç®—å¤©æ•°å·®å¼‚
        last_date = pd.to_datetime(last_train_date).date()
        curr_date = pd.to_datetime(current_date).date()
        days_diff = (curr_date - last_date).days
        
        # æ£€æŸ¥æ­¥é•¿
        if days_diff >= self.config.step_size_days:
            return True, f"è¾¾åˆ°æ­¥é•¿é˜ˆå€¼({days_diff} >= {self.config.step_size_days}å¤©)"
            
        # æ£€æŸ¥å¼ºåˆ¶é‡è®­é—´éš”
        if days_diff >= self.config.force_refit_days:
            return True, f"è¾¾åˆ°å¼ºåˆ¶é‡è®­é—´éš”({days_diff} >= {self.config.force_refit_days}å¤©)"
            
        return False, f"æ— éœ€é‡è®­ç»ƒ(è·ç¦»ä¸Šæ¬¡è®­ç»ƒ{days_diff}å¤©)"
    
    def create_training_windows(self, 
                              data: pd.DataFrame,
                              date_column: str = 'date',
                              current_date: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        åˆ›å»ºWalk-Forwardè®­ç»ƒçª—å£
        
        Returns:
            List of training windows with train/test splits
        """
        if current_date is None:
            current_date = datetime.now().strftime('%Y-%m-%d')
            
        data = data.sort_values(date_column)
        dates = pd.to_datetime(data[date_column])
        
        # è®¡ç®—çª—å£å‚æ•°
        train_window = timedelta(days=self.config.train_window_months * 30)
        step_size = timedelta(days=self.config.step_size_days)
        
        # èµ·å§‹æ—¥æœŸï¼šå½“å‰æ—¥æœŸå¾€å‰æ¨æœ€å¤§å›çœ‹å¤©æ•°
        start_date = pd.to_datetime(current_date) - timedelta(days=self.config.max_lookback_days)
        end_date = pd.to_datetime(current_date)
        
        windows = []
        current_window_end = end_date
        
        # ğŸ”’ å®‰å…¨æœºåˆ¶ï¼šé˜²æ­¢æ— é™å¾ªç¯
        max_iterations = 100  # æœ€å¤š100æ¬¡è¿­ä»£
        iteration_count = 0
        
        while current_window_end >= start_date + train_window and iteration_count < max_iterations:
            iteration_count += 1
            # è®¡ç®—è®­ç»ƒçª—å£
            if self.config.window_type == 'rolling':
                train_start = current_window_end - train_window
            else:  # expanding
                train_start = start_date
                
            train_end = current_window_end - step_size  # ç•™å‡ºgap
            
            # æµ‹è¯•çª—å£(ä¸‹ä¸€ä¸ªå‘¨æœŸ)
            test_start = current_window_end
            test_end = min(current_window_end + step_size, end_date)
            
            # è¿‡æ»¤æ•°æ®
            train_mask = (dates >= train_start) & (dates <= train_end)
            test_mask = (dates >= test_start) & (dates <= test_end)
            
            train_samples = train_mask.sum()
            test_samples = test_mask.sum()
            
            # æ£€æŸ¥æ ·æœ¬æ•°é‡
            if train_samples >= self.config.min_train_samples and test_samples > 0:
                window = {
                    'train_start': train_start.strftime('%Y-%m-%d'),
                    'train_end': train_end.strftime('%Y-%m-%d'), 
                    'test_start': test_start.strftime('%Y-%m-%d'),
                    'test_end': test_end.strftime('%Y-%m-%d'),
                    'train_samples': train_samples,
                    'test_samples': test_samples,
                    'train_mask': train_mask,
                    'test_mask': test_mask,
                    'window_id': f"wf_{train_start.strftime('%Y%m%d')}_{train_end.strftime('%Y%m%d')}"
                }
                windows.append(window)
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ›´æ–°å¾ªç¯å˜é‡ï¼Œé¿å…æ— é™å¾ªç¯
            current_window_end = current_window_end - step_size
        
        # ğŸ”’ å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè®°å½•è­¦å‘Š
        if iteration_count >= max_iterations:
            logger.warning(f"Walk-Forwardçª—å£åˆ›å»ºè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°({max_iterations})ï¼Œå¯èƒ½å­˜åœ¨é…ç½®é—®é¢˜")
                
        # åªä¿ç•™æœ€è¿‘çš„å‡ ä¸ªçª—å£(é¿å…è¿‡åº¦è®¡ç®—)
        max_windows = max(5, self.config.warmup_periods + 2)
        windows = sorted(windows, key=lambda x: x['train_end'])[-max_windows:]
        
        logger.info(f"åˆ›å»º{len(windows)}ä¸ªWalk-Forwardè®­ç»ƒçª—å£")
        for i, w in enumerate(windows):
            logger.info(f"  çª—å£{i+1}: {w['train_start']} â†’ {w['train_end']} "
                       f"(è®­ç»ƒ{w['train_samples']}, æµ‹è¯•{w['test_samples']})")
        
        return windows
    
    def is_warmup_complete(self, window_index: int) -> bool:
        """æ£€æŸ¥çƒ­èº«æœŸæ˜¯å¦å®Œæˆ"""
        return window_index >= self.config.warmup_periods
    
    def save_run_config(self, run_config: RunConfig, results: Optional[Dict] = None):
        """ä¿å­˜è¿è¡Œé…ç½®å’Œç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜é…ç½®
        config_file = self.cache_dir / f"run_config_{run_config.run_id}_{timestamp}.json"
        config_data = asdict(run_config)
        if results:
            config_data['results'] = results
            
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False, default=str)
            
        # æ›´æ–°å†å²è®°å½•
        self.run_history.append({
            'run_id': run_config.run_id,
            'timestamp': timestamp,
            'config_file': str(config_file)
        })
        
        logger.info(f"è¿è¡Œé…ç½®å·²ä¿å­˜: {config_file}")
        
    def load_cached_model(self, run_id: str) -> Optional[Any]:
        """åŠ è½½ç¼“å­˜çš„æ¨¡å‹"""
        if not self.config.cache_models:
            return None
            
        cache_file = self.cache_dir / f"model_{run_id}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    model = pickle.load(f)
                logger.info(f"åŠ è½½ç¼“å­˜æ¨¡å‹: {cache_file}")
                return model
            except Exception as e:
                logger.warning(f"åŠ è½½ç¼“å­˜æ¨¡å‹å¤±è´¥: {e}")
                
        return None
        
    def cache_model(self, run_id: str, model: Any):
        """ç¼“å­˜æ¨¡å‹"""
        if not self.config.cache_models:
            return
            
        cache_file = self.cache_dir / f"model_{run_id}.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(model, f)
            logger.info(f"æ¨¡å‹å·²ç¼“å­˜: {cache_file}")
        except Exception as e:
            logger.warning(f"ç¼“å­˜æ¨¡å‹å¤±è´¥: {e}")
    
    def cleanup_old_cache(self, keep_latest: int = 10):
        """æ¸…ç†æ—§ç¼“å­˜æ–‡ä»¶"""
        try:
            # æ¸…ç†æ¨¡å‹ç¼“å­˜
            model_files = list(self.cache_dir.glob("model_*.pkl"))
            if len(model_files) > keep_latest:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„
                model_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                for old_file in model_files[keep_latest:]:
                    old_file.unlink()
                    logger.info(f"æ¸…ç†æ—§æ¨¡å‹ç¼“å­˜: {old_file}")
                    
            # æ¸…ç†é…ç½®æ–‡ä»¶
            config_files = list(self.cache_dir.glob("run_config_*.json"))
            if len(config_files) > keep_latest * 2:
                config_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                for old_file in config_files[keep_latest * 2:]:
                    old_file.unlink()
                    
        except Exception as e:
            logger.warning(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")

def create_walk_forward_system(config: Optional[WalkForwardConfig] = None) -> WalkForwardRetrainingSystem:
    """åˆ›å»ºWalk-Forwardé‡è®­ç»ƒç³»ç»Ÿ"""
    if config is None:
        config = WalkForwardConfig()
    return WalkForwardRetrainingSystem(config)

# ğŸ”¥ é›†æˆåˆ°BMAç³»ç»Ÿçš„æ¥å£å‡½æ•°
def integrate_walk_forward_to_bma(data: pd.DataFrame,
                                 code_content: str,
                                 config_dict: Dict,
                                 current_date: Optional[str] = None) -> Dict[str, Any]:
    """
    é›†æˆWalk-Forwardç³»ç»Ÿåˆ°BMA
    
    Args:
        data: è®­ç»ƒæ•°æ®
        code_content: ä»£ç å†…å®¹(ç”¨äºç”Ÿæˆhash)
        config_dict: é…ç½®å­—å…¸
        current_date: å½“å‰æ—¥æœŸ
    
    Returns:
        Walk-Forwardç»“æœå­—å…¸
    """
    # åˆ›å»ºç³»ç»Ÿ
    wf_config = WalkForwardConfig(
        train_window_months=config_dict.get('train_window_months', 24),
        step_size_days=config_dict.get('step_size_days', 30),
        force_refit_days=config_dict.get('force_refit_days', 90)
    )
    
    wf_system = create_walk_forward_system(wf_config)
    
    try:
        # åˆ›å»ºè®­ç»ƒçª—å£
        windows = wf_system.create_training_windows(data, current_date=current_date)
        
        if not windows:
            logger.warning("æ— æ³•åˆ›å»ºæœ‰æ•ˆçš„è®­ç»ƒçª—å£")
            return {'success': False, 'error': 'æ— æœ‰æ•ˆçª—å£'}
        
        # è·å–æœ€æ–°çª—å£
        latest_window = windows[-1]
        
        # ç”Ÿæˆrun_id
        train_data = data[latest_window['train_mask']]
        run_config = wf_system.generate_run_id(
            code_content=code_content,
            config_dict=config_dict,
            data_slice=train_data,
            train_start=latest_window['train_start'],
            train_end=latest_window['train_end']
        )
        
        # æ›´æ–°æµ‹è¯•æ—¶é—´
        run_config.test_start = latest_window['test_start']
        run_config.test_end = latest_window['test_end']
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è®­ç»ƒ
        last_train_date = None  # ä»å†å²è®°å½•ä¸­è·å–
        should_retrain, reason = wf_system.should_retrain(
            last_train_date, latest_window['train_end']
        )
        
        # æ£€æŸ¥çƒ­èº«æœŸ
        warmup_complete = wf_system.is_warmup_complete(len(windows) - 1)
        
        return {
            'success': True,
            'run_config': asdict(run_config),
            'windows': windows,
            'latest_window': latest_window,
            'should_retrain': should_retrain,
            'retrain_reason': reason,
            'warmup_complete': warmup_complete,
            'total_windows': len(windows)
        }
        
    except Exception as e:
        logger.error(f"Walk-Forwardé›†æˆå¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}