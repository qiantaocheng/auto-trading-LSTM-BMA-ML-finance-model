#!/usr/bin/env python3
"""
Enhanced Alpha Strategy Module
Integrates advanced techniques: delay/decay, hump+rank, neutralization, winsorize
"""

import numpy as np
import pandas as pd
import yaml
import warnings
from typing import Dict, List, Optional, Tuple, Any, Callable
from scipy import stats
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import RobustScaler
from bma_models.unified_purged_cv_factory import create_unified_cv
from bma_models.enhanced_alpha_quality_monitor import EnhancedAlphaQualityMonitor, AlphaFactorQualityReport

# Configure logging first
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ ¸å¿ƒä¾èµ– - å¿…éœ€
try:
    from bma_models.unified_nan_handler import unified_nan_handler, clean_nan_predictive_safe
except ImportError:
    from unified_nan_handler import unified_nan_handler, clean_nan_predictive_safe

try:
    from cross_sectional_standardizer import CrossSectionalStandardizer, standardize_factors_cross_sectionally
    # Create alias for compatibility
    standardize_cross_sectional_predictive_safe = standardize_factors_cross_sectionally
except ImportError:
    CrossSectionalStandardizer = None
    standardize_cross_sectional_predictive_safe = None

# å¯é€‰ä¾èµ–
try:
    from bma_models.factor_orthogonalization import orthogonalize_factors_predictive_safe, FactorOrthogonalizer
except ImportError:
    try:
        from factor_orthogonalization import orthogonalize_factors_predictive_safe, FactorOrthogonalizer
    except ImportError:
        logger.warning("FactorOrthogonalizer not available, using simplified version")
        orthogonalize_factors_predictive_safe = None
        FactorOrthogonalizer = None

# Parameter optimization module removed - functionality integrated inline
TechnicalIndicatorOptimizer = None
ParameterConfig = None

# Dynamic factor weighting removed - using pure PCA approach

# ä¸ºäº†å…¼å®¹æ€§ï¼Œåˆ›å»ºåˆ«å
cross_sectional_standardize = standardize_cross_sectional_predictive_safe

# Removed external advanced factor dependencies, all factors integrated into this module

class AlphaStrategiesEngine:
    """Alpha Strategy Engine: Unified computation, neutralization, ranking, gating"""
    
    def __init__(self, config_path: str = None):
        """
        Initialize Alpha Strategy Engine
        
        Args:
            config_path: Configuration file path (auto-detect if None)
        """
        # Auto-detect data availability and choose appropriate config
        if config_path is None:
            self.data_availability = self._detect_data_availability()
            if self.data_availability['has_fundamental_data']:
                config_path = "alphas_config.yaml"
            else:
                config_path = "alphas_config_delayed_data.yaml"
                logger.info("ğŸŸ¡ æ£€æµ‹åˆ°æ— åŸºæœ¬é¢æ•°æ®è®¿é—®æƒé™ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å»¶è¿Ÿæ•°æ®é…ç½®")
        
        self.config = self._load_config(config_path)
        self.alpha_functions = self._register_alpha_functions()
            
        self.alpha_cache = {}  # Cache computation results
        
        # All factors integrated into this module, no external dependencies needed
        logger.info("All Alpha factors integrated into this module")
        
        # [OK] NEW: å¯¼å…¥å› å­æ»åé…ç½®
        try:
            from factor_lag_config import factor_lag_manager
            self.lag_manager = factor_lag_manager
            logger.info(f"å› å­æ»åé…ç½®åŠ è½½æˆåŠŸï¼Œæœ€å¤§æ»å: T-{self.lag_manager.get_max_lag()}")
        except ImportError:
            logger.warning("å› å­æ»åé…ç½®æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤å…¨å±€æ»å")
            self.lag_manager = None
        
        # [OK] PERFORMANCE FIX: Initialize parameter optimizer
        if TechnicalIndicatorOptimizer is not None:
            self.parameter_optimizer = TechnicalIndicatorOptimizer()
        else:
            self.parameter_optimizer = None
        self.optimized_parameters = {}
        
        # Dynamic factor weighting removed
        
        # Statistics
        self.stats = {
            'computation_times': {},
            'cache_hits': 0,
            'cache_misses': 0,
            'neutralization_stats': {}
        }
        
        # åˆå§‹åŒ–æ•°æ®è´¨é‡ç›‘æ§å™¨
        self.quality_monitor = EnhancedAlphaQualityMonitor(
            strict_mode=False,  # é»˜è®¤éä¸¥æ ¼æ¨¡å¼ï¼Œè®°å½•ä½†ä¸ä¸­æ–­
            log_dir="logs/alpha_quality"
        )
        self.quality_reports = {}  # å­˜å‚¨æ¯ä¸ªå› å­çš„è´¨é‡æŠ¥å‘Š
        
        # Initialize data providers for fundamental data
        self._init_data_providers()
        
        logger.info(f"Alpha Strategy Engine initialized, loaded {len(self.config['alphas'])} factors")
    
    def _safe_groupby_apply(self, df: pd.DataFrame, groupby_col: str, apply_func, *args, **kwargs) -> pd.Series:
        """
        ğŸ”§ CRITICAL FIX: MultiIndexå®‰å…¨çš„groupbyæ“ä½œ
        ç»Ÿä¸€å¤„ç†groupby.applyï¼Œé¿å…reset_indexç ´åMultiIndexç»“æ„
        """
        if isinstance(df.index, pd.MultiIndex) and groupby_col in df.index.names:
            # MultiIndexæƒ…å†µï¼šæŒ‰æŒ‡å®šlevelè¿›è¡Œgroupby
            result = df.groupby(level=groupby_col).apply(apply_func, *args, **kwargs)
            # æ¸…ç†å¤šä½™çš„ç´¢å¼•å±‚çº§
            if hasattr(result, 'index') and result.index.nlevels > df.index.nlevels:
                result = result.droplevel(0)
            return result
        elif groupby_col in df.columns:
            # æ™®é€šDataFrameæƒ…å†µï¼šæŒ‰åˆ—è¿›è¡Œgroupby
            if isinstance(df.index, pd.MultiIndex):
                # å¦‚æœåŸæ¥æ˜¯MultiIndexï¼Œå°½é‡ä¿æŒç»“æ„
                result = df.groupby(groupby_col).apply(apply_func, *args, **kwargs)
                return result  # ä¿æŒç»“æœçš„ç´¢å¼•ç»“æ„
            else:
                # å®Œå…¨æ™®é€šçš„æƒ…å†µ
                result = df.groupby(groupby_col).apply(apply_func, *args, **kwargs)
                if hasattr(result, 'reset_index'):
                    return result.reset_index(level=0, drop=True)
                return result
        else:
            # å…¼å®¹åŸæœ‰é€»è¾‘
            logger.warning(f"âš ï¸ groupbyåˆ— '{groupby_col}' ä¸åœ¨ç´¢å¼•æˆ–åˆ—ä¸­ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
            return apply_func(df, *args, **kwargs)
    
    def _detect_data_availability(self) -> Dict:
        """æ£€æµ‹å¯ç”¨çš„æ•°æ®ç±»å‹å’ŒAPIè®¿é—®æƒé™"""
        availability = {
            'has_fundamental_data': False,
            'has_options_data': False,
            'has_news_data': False,
            'has_realtime_data': False
        }
        
        try:
            # å°è¯•è·å–ä¸€ä¸ªæµ‹è¯•è‚¡ç¥¨çš„åŸºæœ¬é¢æ•°æ®
            from bma_models.polygon_client import polygon_client
            test_data = polygon_client.get_financials('AAPL', limit=1)
            
            # å¦‚æœæ²¡æœ‰é”™è¯¯ä¸”æœ‰æ•°æ®ï¼Œåˆ™æœ‰åŸºæœ¬é¢æ•°æ®è®¿é—®æƒé™
            if test_data and 'results' in test_data and test_data['results']:
                availability['has_fundamental_data'] = True
                logger.info("[OK] æ£€æµ‹åˆ°åŸºæœ¬é¢æ•°æ®è®¿é—®æƒé™")
            else:
                logger.info("[ERROR] æ— åŸºæœ¬é¢æ•°æ®è®¿é—®æƒé™ - ä½¿ç”¨æŠ€æœ¯å› å­æ¨¡å¼")
            
            # å¯ä»¥æ·»åŠ å…¶ä»–æ•°æ®ç±»å‹çš„æ£€æµ‹
            # TODO: æ£€æµ‹æœŸæƒæ•°æ®ã€æ–°é—»æ•°æ®ç­‰
            
        except Exception as e:
            logger.warning(f"æ•°æ®å¯ç”¨æ€§æ£€æµ‹å¤±è´¥: {e}")
        
        return availability
    
    def decay_linear(self, series: pd.Series, decay: int) -> pd.Series:
        """
        åº”ç”¨çº¿æ€§æ—¶é—´è¡°å‡æƒé‡
        
        Args:
            series: éœ€è¦è¡°å‡çš„åºåˆ—
            decay: è¡°å‡æœŸæ•°
            
        Returns:
            åº”ç”¨è¡°å‡æƒé‡åçš„åºåˆ—
        """
        if decay <= 1:
            return series
        
        try:
            # åˆ›å»ºçº¿æ€§è¡°å‡æƒé‡ï¼šæœ€è¿‘çš„æƒé‡æœ€å¤§ï¼Œå†å²æƒé‡é€’å‡
            weights = np.linspace(1, 1/decay, decay)
            weights = weights / weights.sum()  # å½’ä¸€åŒ–
            
            # å¯¹åºåˆ—åº”ç”¨è¡°å‡æƒé‡
            result = series.copy().astype(float)  # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat
            if len(series) >= decay:
                # ä½¿ç”¨æ»šåŠ¨çª—å£åº”ç”¨è¡°å‡æƒé‡
                for i in range(decay-1, len(series)):
                    window_data = series.iloc[i-decay+1:i+1]
                    if len(window_data) == decay:
                        result.iloc[i] = float((window_data.values * weights).sum())
            
            return self.safe_fillna(result, df)
            
        except Exception as e:
            logger.warning(f"çº¿æ€§è¡°å‡è®¡ç®—å¤±è´¥: {e}")
            return self.safe_fillna(series, df)
    
    def _load_config(self, config_path: str) -> Dict:
        """Load configuration file"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = yaml.safe_load(f)
            
            # Merge user config with defaults
            default_config = self._get_default_config()
            default_config.update(user_config)
            return default_config
        except FileNotFoundError:
            logger.warning(f"Config file {config_path} not found, using default config")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """Get default configuration with all 25 required factors enabled"""
        # Define the required factors (removed ivol_60d due to multicollinearity with stability_score)
        required_17_factors = [
            'momentum_10d_ex1',
            'rsi', 'bollinger_squeeze',
            'obv_momentum', 'atr_ratio', 'blowoff_ratio', 'stability_score',
            'liquidity_factor',
            'near_52w_high', 'reversal_1d', 'mom_accel_5_2'
        ]
        
        # Create alpha config for each required factor
        alphas_config = []
        for factor_name in required_17_factors:
            alpha_config = {
                'name': factor_name,
                'kind': factor_name,  # Use factor name as kind
                'enabled': True,
                'windows': [20],  # Default window
                'decay': 6,       # Default decay
                'delay': 1        # Default delay
            }
            alphas_config.append(alpha_config)
        
        return {
            'universe': 'TOPDIV3000',
            'region': 'GLB',
            'neutralization': ['COUNTRY'],
            'rebalance': 'WEEKLY',
            'hump_levels': [0.003, 0.008],
            'winsorize_std': 2.5,
            'truncation': 0.10,
            'temperature': 1.2,
            'alphas': alphas_config  # Now includes all 25 factors
        }
    
    def _register_alpha_functions(self) -> Dict[str, Callable]:
        """Register FOCUSED 25 Alpha computation functions - Only selected high-value factors"""
        return {
            # FOCUSED 25 FACTORS - All others commented out
            
            # Momentum factors (1/23) - REMOVED: momentum_20d, momentum_reversal_short
            'momentum_10d_ex1': self._compute_momentum_10d_ex1,

            # Mean reversion factors (2/17) - REMOVED: price_to_ma20
            'rsi': self._compute_rsi,
            'bollinger_squeeze': self._compute_bollinger_squeeze,

            # Volume factors (1/17)
            'obv_momentum': self._compute_obv_momentum,

            # Volatility factors (3/17)
            'atr_ratio': self._compute_atr_ratio,
            'blowoff_ratio': self._compute_blowoff_ratio,
            'stability_score': self._compute_stability_score,

            # REMOVED: ivol_60d (multicollinearity with stability_score, r=-0.95)

            # Fundamental factors (2/17) - REMOVED: growth_proxy, profitability_momentum, growth_acceleration, value_proxy, profitability_proxy, quality_proxy, mfi
            'liquidity_factor': self._compute_liquidity_factor,

            # High-alpha factors (4/17)
            'near_52w_high': self._compute_52w_new_high_proximity,
            'reversal_1d': self._compute_reversal_1d,
            'mom_accel_5_2': self._compute_mom_accel_5_2,
            
            # ===== ALL OTHER FACTORS COMMENTED OUT =====
            
            # OLD Technical factors - COMMENTED OUT
            # 'momentum': self._compute_momentum,
            # 'momentum_6_1': self._compute_momentum_6_1,
            # 'reversal': self._compute_reversal,
            # 'reversal_10': self._compute_reversal_10,
            # 'mean_reversion': self._compute_mean_reversion,
            # 'volume_ratio': self._compute_volume_ratio,
            # 'price_position': self._compute_price_position,
            # 'volatility': self._compute_volatility,
            # 'residual_momentum': self._compute_residual_momentum,
            # 'pead': self._compute_pead,
            
            # OLD Extended momentum factors - COMMENTED OUT
            # 'new_high_proximity': self._compute_52w_new_high_proximity,
            # 'low_beta': self._compute_low_beta_anomaly,
            # 'idiosyncratic_vol': self._compute_idiosyncratic_volatility,
            
            # OLD Fundamental factors - COMMENTED OUT
            # 'earnings_surprise': self._compute_earnings_surprise,
            # 'analyst_revision': self._compute_analyst_revision,
            # 'ebit_ev': self._compute_ebit_ev,
            # 'fcf_ev': self._compute_fcf_ev,
            # 'earnings_yield': self._compute_earnings_yield,
            # 'sales_yield': self._compute_sales_yield,
            # 'pb_ratio': self._compute_pb_ratio,
            
            # OLD Profitability factors - COMMENTED OUT
            # 'gross_margin': self._compute_gross_margin,
            # 'operating_profitability': self._compute_operating_profitability,
            # 'roe_neutralized': self._compute_roe_neutralized,
            # 'roic_neutralized': self._compute_roic_neutralized,
            # 'net_margin': self._compute_net_margin,
            # 'cash_yield': self._compute_cash_yield,
            # 'shareholder_yield': self._compute_shareholder_yield,
            
            # OLD Accrual factors - COMMENTED OUT
            # 'total_accruals': self._compute_total_accruals,
            # 'working_capital_accruals': self._compute_working_capital_accruals,
            # 'net_operating_assets': self._compute_net_operating_assets,
            # 'asset_growth': self._compute_asset_growth,
            # 'net_equity_issuance': self._compute_net_equity_issuance,
            # 'investment_factor': self._compute_investment_factor,
            
            # OLD Quality score factors - COMMENTED OUT
            # 'piotroski_score': self._compute_piotroski_score,
            # 'ohlson_score': self._compute_ohlson_score,
            # 'altman_score': self._compute_altman_score,
            # 'qmj_score': self._compute_qmj_score,
            # 'earnings_stability': self._compute_earnings_stability,
            
            # OLD Sentiment factors - COMMENTED OUT
            # 'news_sentiment': self._compute_news_sentiment,
            # 'market_sentiment_10d': self._compute_market_sentiment_10d,
            # 'fear_greed_sentiment': self._compute_fear_greed_sentiment,
            # 'sentiment_momentum_10d': self._compute_sentiment_momentum_10d,
            
            # OLD Technical indicators - COMMENTED OUT
            # 'technical_sma_10': self._compute_sma_10,
            # 'technical_sma_20': self._compute_sma_20,
            # 'technical_sma_50': self._compute_sma_50,
            # 'technical_rsi': self._compute_rsi,
            # 'technical_bb_position_10d': self._compute_bb_position_10d,
            # 'technical_macd_10d': self._compute_macd_10d,
            # 'technical_price_momentum_5d': self._compute_price_momentum_5d,
            # 'technical_volume_ratio': self._compute_volume_ratio,
            
            # OLD Missing alpha types - COMMENTED OUT
            # 'volume_trend': self._compute_volume_trend,
            # 'gap_momentum': self._compute_gap_momentum,
            # 'intraday_momentum': self._compute_intraday_momentum,
            
            # OLD Risk indicators - COMMENTED OUT
            # 'risk_max_drawdown': self._compute_max_drawdown,
            # 'risk_sharpe_ratio': self._compute_sharpe_ratio,
            # 'risk_var_95': self._compute_var_95,
            
            'hump': None,  # Special handling
        }
    
    # ========== Fundamental Data Provider ==========
    
    def _init_data_providers(self):
        """Initialize data providers for fundamental data"""
        try:
            # Try to import Polygon client
            from bma_models.polygon_client import polygon_client as pc
            self.polygon_client = pc
            logger.info("[OK] Polygonå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            logger.warning("[WARN] Polygonå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼ŒåŸºæœ¬é¢å› å­å°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            self.polygon_client = None
            
        # Initialize data provider
        self.fundamental_cache = {}
    
    def get_fundamental_data(self, ticker: str, as_of_date: str = None) -> Dict:
        """
        è·å–åŸºæœ¬é¢æ•°æ® - ç»Ÿä¸€æ•°æ®æº
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            as_of_date: æ•°æ®æˆªæ­¢æ—¥æœŸ
            
        Returns:
            Dict: åŒ…å«åŸºæœ¬é¢æ•°æ®çš„å­—å…¸
        """
        cache_key = f"{ticker}_{as_of_date}"
        if cache_key in self.fundamental_cache:
            return self.fundamental_cache[cache_key]
            
        fundamental_data = {}
        
        try:
            if self.polygon_client:
                # ä½¿ç”¨çœŸå®çš„Polygon APIè·å–æ•°æ®
                try:
                    # è·å–è´¢åŠ¡æ•°æ®
                    financials = self.polygon_client.get_financials(ticker)
                    if financials and 'results' in financials:
                        latest_financial = financials['results'][0]
                        
                        # æå–å…³é”®è´¢åŠ¡æŒ‡æ ‡
                        fundamental_data.update({
                            'market_cap': latest_financial.get('market_capitalization'),
                            'enterprise_value': latest_financial.get('enterprise_value'),
                            'pe_ratio': latest_financial.get('price_earnings_ratio'),
                            'pb_ratio': latest_financial.get('price_book_ratio'),
                            'debt_to_equity': latest_financial.get('debt_to_equity_ratio'),
                            'roe': latest_financial.get('return_on_equity'),
                            'roa': latest_financial.get('return_on_assets'),
                            'revenue': latest_financial.get('revenues'),
                            'net_income': latest_financial.get('net_income_loss'),
                            'total_assets': latest_financial.get('assets'),
                            'total_debt': latest_financial.get('liabilities'),
                            'book_value': latest_financial.get('equity'),
                            'free_cash_flow': latest_financial.get('net_cash_flow_operating_activities'),
                            'dividend_yield': latest_financial.get('dividend_yield')
                        })
                        
                except Exception as api_error:
                    logger.warning(f"Polygon APIè·å–{ticker}æ•°æ®å¤±è´¥: {api_error}")
            
            # å¦‚æœæ²¡æœ‰è·å–åˆ°æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            if not fundamental_data:
                fundamental_data = self._get_simulated_fundamental_data(ticker)
                
        except Exception as e:
            logger.error(f"è·å–{ticker}åŸºæœ¬é¢æ•°æ®å¤±è´¥: {e}")
            fundamental_data = self._get_simulated_fundamental_data(ticker)
        
        # ç¼“å­˜ç»“æœ
        self.fundamental_cache[cache_key] = fundamental_data
        return fundamental_data

    # ========== Basic Utility Functions ==========
    
    def winsorize_series(self, s: pd.Series, k: float = 2.5) -> pd.Series:
        """Winsorize series: Remove outliers"""
        if s.isna().all():
            return s
        mu, sd = s.mean(), s.std(ddof=0)
        if sd == 0:
            return s
        lo, hi = mu - k * sd, mu + k * sd
        return s.clip(lo, hi)
    
    def zscore_by_group(self, df: pd.DataFrame, col: str, group_cols: List[str]) -> pd.Series:
        """Group standardization"""
        return df.groupby(group_cols)[col].transform(
            lambda x: (x - x.mean()) / (x.std(ddof=0) + 1e-12)
        )
    
    def neutralize_factor(self, df: pd.DataFrame, target_col: str, 
                         group_cols: List[str]) -> pd.Series:
        """Time-safe linear regression neutralization - Prevents use of future data"""
        def _neutralize_cross_section_safe(block):
            if len(block) < 2 or target_col not in block.columns:
                return block[target_col] if target_col in block.columns else pd.Series(index=block.index)
            
            y = block[target_col]  # Keep NaN for now
            if len(y) < 2:
                return block[target_col]
            
            # KEY FIX: Use expanding window to ensure only historical data is used
            # In real-time trading, at time T should not know future performance of other stocks on same day
            result = pd.Series(index=block.index, dtype=float)
            
            # Use time-progressive approach to calculate neutralization parameters
            sorted_indices = block.index.tolist()
            
            for i, idx in enumerate(sorted_indices):
                if idx not in y.index:
                    result.loc[idx] = 0.0
                    continue
                
                # CRITICAL FIX: ä¸¥æ ¼ä½¿ç”¨å†å²æ•°æ®ï¼Œæ’é™¤å½“å‰æ—¶ç‚¹
                # åœ¨æ¨ªæˆªé¢ä¸­æ€§åŒ–ä¸­ï¼Œä¸åº”ä½¿ç”¨åŒæ—¥å…¶ä»–è‚¡ç¥¨ä¿¡æ¯
                if i == 0:
                    # ç¬¬ä¸€ä¸ªæ—¶ç‚¹æ²¡æœ‰å†å²æ•°æ®ï¼Œä½¿ç”¨åŸå€¼æˆ–ç®€å•å»å‡å€¼
                    result.loc[idx] = y.loc[idx] if not pd.isna(y.loc[idx]) else 0.0
                    continue
                    
                hist_indices = sorted_indices[:i]  # æ’é™¤å½“å‰æ—¶ç‚¹(i)
                hist_y = y.loc[y.index.intersection(hist_indices)]
                
                if len(hist_y) < 2:
                    # å¦‚æœå†å²æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨å†å²å‡å€¼è°ƒæ•´
                    hist_mean = hist_y.mean() if len(hist_y) > 0 else 0.0
                    result.loc[idx] = y.loc[idx] - hist_mean if not pd.isna(y.loc[idx]) else 0.0
                    continue
                
                # Build historical dummy variable matrix
                hist_block = block.loc[hist_indices]
                X_df = pd.get_dummies(hist_block[group_cols], drop_first=False)
                X_df = X_df.loc[hist_y.index]
                
                if X_df.shape[1] == 0 or X_df.var().sum() == 0:
                    # CRITICAL FIX: ä½¿ç”¨å†å²æ•°æ®è®¡ç®—åŸºå‡†
                    hist_mean = hist_y.mean() if len(hist_y) > 0 else 0.0
                    result.loc[idx] = y.loc[idx] - hist_mean if not pd.isna(y.loc[idx]) else 0.0
                    continue
                
                try:
                    # Use historical data to fit regression model
                    lr = LinearRegression(fit_intercept=True)
                    lr.fit(X_df.values, hist_y.values)
                    
                    # Neutralize current point
                    current_X = pd.get_dummies(block.loc[[idx]][group_cols], drop_first=False)
                    current_X = current_X.reindex(columns=X_df.columns, fill_value=0)
                    
                    predicted = lr.predict(current_X.values)[0]
                    result.loc[idx] = y.loc[idx] - predicted
                    
                except Exception as e:
                    logger.warning(f"Point {idx} neutralization failed: {e}")
                    result.loc[idx] = hist_y.loc[idx] - hist_y.mean()
            
            return self.safe_fillna(result, df)
        
        # ğŸ”§ CRITICAL FIX: ä¿æŒMultiIndexç»“æ„ï¼Œé¿å…ç´¢å¼•é”™ä½
        if isinstance(df.index, pd.MultiIndex) and 'date' in df.index.names:
            # MultiIndexæƒ…å†µï¼šæŒ‰date levelè¿›è¡Œgroupbyï¼Œä¿æŒç´¢å¼•ç»“æ„
            result = df.groupby(level='date').apply(_neutralize_cross_section_safe)
            # ç§»é™¤groupbyäº§ç”Ÿçš„é¢å¤–å±‚çº§ï¼Œä½†ä¿æŒåŸæœ‰MultiIndexç»“æ„
            if result.index.nlevels > df.index.nlevels:
                result = result.droplevel(0)
            return result
        else:
            # éMultiIndexæƒ…å†µï¼šä¿æŒåŸæœ‰é€»è¾‘
            return df.groupby('date').apply(_neutralize_cross_section_safe).reset_index(level=0, drop=True)
    
    def hump_transform(self, z: pd.Series, hump: float = 0.003) -> pd.Series:
        """Gating transformation: Set small signals to zero"""
        return z.where(z.abs() >= hump, 0.0)
    
    def rank_transform(self, z: pd.Series) -> pd.Series:
        """Ranking transformation"""
        return z.rank(pct=True) - 0.5
    
    def ema_decay(self, s: pd.Series, span: int) -> pd.Series:
        """Time-safe exponential moving average decay - Only use historical data"""
        # [OK] PERFORMANCE FIX: ç§»é™¤è¿‡åº¦ä¿å®ˆçš„shift(1)
        # å·®å¼‚åŒ–æ»åå·²åœ¨å› å­çº§åˆ«åº”ç”¨ï¼Œæ­¤å¤„ä¸éœ€è¦é¢å¤–æ»å
        # Use expanding window to ensure each time point only uses historical data
        result = s.ewm(span=span, adjust=False).mean()
        # [ERROR] REMOVED: ç§»é™¤é¢å¤–shift(1)ä»¥ä¿æŒä¿¡å·åŠæ—¶æ€§å’Œå¼ºåº¦
        # return result.shift(1)
        return result
    
    def safe_apply_fillna(self, series: pd.Series, df: pd.DataFrame = None) -> pd.Series:
        """Helper method to safely apply fillna without causing float object errors"""
        try:
            if isinstance(series, pd.Series):
                return series.fillna(0.0)
            else:
                # If it's not a Series, create one
                return pd.Series(series, index=df.index if df is not None else None).fillna(0.0)
        except Exception:
            return pd.Series(0.0, index=df.index if df is not None else None)
    
    def safe_fillna(self, data: pd.Series, df: pd.DataFrame = None, 
                   date_col: str = 'date') -> pd.Series:
        """
        CRITICAL FIX: ä½¿ç”¨å…¨å±€ç»Ÿä¸€NaNå¤„ç†ç­–ç•¥
        é‡å®šå‘åˆ°global_nan_config.unified_nan_handler
        """
        try:
            from global_nan_config import unified_nan_handler
            return unified_nan_handler(data, df, date_col, 'cross_sectional_median')
        except ImportError:
            # å¦‚æœglobal_nan_configä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ°é€»è¾‘
            logger.warning("global_nan_configä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ°NaNå¤„ç†")
            if df is not None and date_col in df.columns:
                # ä½¿ç”¨æ¨ªæˆªé¢ä¸­ä½æ•°å¡«å……
                temp_df = pd.DataFrame({
                    'data': data,
                    'date': df[date_col],
                    'original_index': data.index
                })
                
                def fill_cross_section(group):
                    daily_median = group['data'].median()
                    if pd.isna(daily_median):
                        daily_median = 0
                    group['data'] = group['data'].fillna(daily_median)
                    return group
            
            filled_df = temp_df.groupby('date').apply(fill_cross_section)
            result = pd.Series(index=data.index, dtype=float)
            for idx, row in filled_df.iterrows():
                original_idx = row['original_index']
                result.loc[original_idx] = row['data']
            return result
        else:
            # å¦‚æœæ²¡æœ‰æ—¥æœŸä¿¡æ¯ï¼Œä½¿ç”¨å…¨å±€ä¸­ä½æ•°
            return data.fillna(data.median() if not data.isna().all() else 0)
    
    def optimize_technical_parameters(self, df: pd.DataFrame, 
                                    target_col: str = 'future_return_10d',
                                    force_reoptimize: bool = False) -> Dict[str, int]:
        """
        [OK] PERFORMANCE FIX: ä¼˜åŒ–æŠ€æœ¯æŒ‡æ ‡å‚æ•°
        åŸºäºæ»šåŠ¨ICé€‰æ‹©æœ€ä¼˜çª—å£å‚æ•°ï¼Œæå‡é¢„æµ‹æ€§èƒ½
        
        Args:
            df: å†å²æ•°æ®
            target_col: ç›®æ ‡å˜é‡åˆ—å
            force_reoptimize: å¼ºåˆ¶é‡æ–°ä¼˜åŒ–
            
        Returns:
            ä¼˜åŒ–åçš„å‚æ•°å­—å…¸
        """
        if self.optimized_parameters and not force_reoptimize:
            logger.info("ä½¿ç”¨ç¼“å­˜çš„ä¼˜åŒ–å‚æ•°")
            return self.optimized_parameters
        
        if target_col not in df.columns:
            logger.warning(f"ç›®æ ‡åˆ—{target_col}ä¸å­˜åœ¨ï¼Œè·³è¿‡å‚æ•°ä¼˜åŒ–")
            return {}
        
        logger.info("å¼€å§‹ä¼˜åŒ–æŠ€æœ¯æŒ‡æ ‡å‚æ•°...")
        
        # å®šä¹‰éœ€è¦ä¼˜åŒ–çš„æŠ€æœ¯æŒ‡æ ‡å‡½æ•°
        optimization_targets = [
            {
                'name': 'sma_10',
                'func': self._compute_sma_10,
                'param_range': [5, 8, 10, 12, 15]
            },
            {
                'name': 'sma_20', 
                'func': self._compute_sma_20,
                'param_range': [15, 18, 20, 22, 25]
            },
            {
                'name': 'sma_50',
                'func': self._compute_sma_50,
                'param_range': [30, 40, 50, 60, 70]
            },
            {
                'name': 'rsi',
                'func': self._compute_rsi,
                'param_range': [10, 12, 14, 16, 18]
            }
        ]
        
        optimized_params = {}
        
        for target in optimization_targets:
            try:
                # åˆ›å»ºæŒ‡æ ‡å‡½æ•°åŒ…è£…å™¨
                def indicator_wrapper(data, window):
                    # ä¸´æ—¶ä¿®æ”¹é»˜è®¤å‚æ•°æ¥æµ‹è¯•ä¸åŒçª—å£
                    original_func = target['func']
                    return original_func(data, window=window)
                
                result = self.parameter_optimizer.optimize_parameter(
                    data=df,
                    target_col=target_col,
                    indicator_func=indicator_wrapper,
                    parameter_name='window',
                    parameter_range=target['param_range']
                )
                
                if result and 'best_parameter' in result:
                    optimized_params[target['name']] = result['best_parameter']
                    logger.info(f"[OK] {target['name']}æœ€ä¼˜å‚æ•°: {result['best_parameter']} "
                              f"(ICå‡å€¼: {result['optimization_summary'].get('best_ic_mean', 0):.4f})")
                
            except Exception as e:
                logger.warning(f"ä¼˜åŒ–{target['name']}å¤±è´¥: {e}")
                continue
        
        # ç¼“å­˜ç»“æœ
        self.optimized_parameters = optimized_params
        logger.info(f"[OK] æŠ€æœ¯æŒ‡æ ‡å‚æ•°ä¼˜åŒ–å®Œæˆï¼Œä¼˜åŒ–äº†{len(optimized_params)}ä¸ªæŒ‡æ ‡")
        
        return optimized_params
    
    def get_optimized_window(self, indicator_name: str, default: int) -> int:
        """
        è·å–ä¼˜åŒ–åçš„çª—å£å‚æ•°
        
        Args:
            indicator_name: æŒ‡æ ‡åç§°
            default: é»˜è®¤å€¼
            
        Returns:
            ä¼˜åŒ–åçš„çª—å£å¤§å°
        """
        return self.optimized_parameters.get(indicator_name, default)
    
    def calculate_dynamic_weights(self, df: pd.DataFrame, 
                                alpha_cols: List[str],
                                target_col: str = 'future_return_10d',
                                force_rebalance: bool = False) -> Dict[str, float]:
        # Method removed - using equal weights for PCA preprocessing
        return {col: 1.0/len(alpha_cols) for col in alpha_cols}
    
    def apply_dynamic_weights(self, df: pd.DataFrame, 
                            alpha_cols: List[str],
                            weights: Dict[str, float]) -> pd.Series:
        # Method removed - return simple average
        """
        åº”ç”¨åŠ¨æ€æƒé‡åˆæˆæœ€ç»ˆAlpha
        
        Args:
            df: æ•°æ®
            alpha_cols: Alphaå› å­åˆ—å
            weights: æƒé‡å­—å…¸
            
        Returns:
            åŠ æƒåçš„ç»¼åˆAlphaå› å­
        """
        if not alpha_cols or not weights:
            return pd.Series(0, index=df.index)
        
        weighted_alpha = pd.Series(0.0, index=df.index)
        
        for col in alpha_cols:
            if col in df.columns and col in weights:
                weight = weights[col]
                factor_values = df[col].fillna(0)
                weighted_alpha += weight * factor_values
        
        return weighted_alpha
    
    # ========== Alpha Factor Computation Functions ==========
    
    def _compute_momentum(self, df: pd.DataFrame, windows: List[int], 
                         decay: int = 6) -> pd.Series:
        """Lightweight momentum factor - optimized version avoiding duplication with Polygon"""
        # Keep this for Alpha engine, but use simpler calculation to avoid exact duplication
        try:
            results = []
            for window in windows:
                g = df.groupby('ticker')['Close']
                # Simple momentum: current/past - 1, shifted by 1 day to align with unified T-1 lag
                momentum = (g.shift(1) / g.shift(window + 1) - 1.0).fillna(0.0)
                
                # Apply decay
                result = momentum.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay))
                results.append(result)
            
            if results:
                combined = pd.concat(results, axis=1).mean(axis=1)
                return combined.fillna(0.0)
            else:
                return pd.Series(0.0, index=df.index)
                
        except Exception as e:
            logger.warning(f"Lightweight momentum computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_reversal(self, df: pd.DataFrame, windows: List[int], 
                         decay: int = 6) -> pd.Series:
        """Reversal factor: Short-term price reversal - æ•°å€¼ç¨³å®šæ€§å¢å¼º"""
        # [HOT] CRITICAL FIX: å¯¼å…¥æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤
        try:
            from numerical_stability import safe_log, safe_divide
        except ImportError:
            # ç®€åŒ–å®ç°
            def safe_log(x, epsilon=1e-10):
                return np.log(np.maximum(x, epsilon))
            def safe_divide(a, b, epsilon=1e-10):
                return a / np.maximum(np.abs(b), epsilon)
        
        results = []
        
        for window in windows:
            # ğŸ›¡ï¸ SAFETY FIX: ä½¿ç”¨æ•°å€¼å®‰å…¨çš„åè½¬è®¡ç®—
            def safe_reversal_calc(x):
                """å®‰å…¨çš„åè½¬è®¡ç®—å‡½æ•°"""
                if len(x) <= window + 1:
                    return pd.Series(index=x.index, dtype=float)
                
                current_price = x.shift(1)
                past_price = x.shift(window + 1)
                
                # ä½¿ç”¨å®‰å…¨é™¤æ³•å’Œå¯¹æ•°è®¡ç®—ï¼Œåè½¬ä¿¡å·å–è´Ÿå·
                price_ratio = safe_divide(current_price, past_price, fill_value=1.0)
                reversal_values = -safe_log(price_ratio)  # åè½¬å› å­å–è´Ÿå·
                
                return reversal_values
            
            reversal = df.groupby('ticker')['Close'].transform(safe_reversal_calc)

            # Exponential decay
            reversal_decayed = reversal.groupby(df['ticker']).apply(
                lambda s: s.ewm(span=decay, adjust=False).mean()
            ).reset_index(level=0, drop=True)

            results.append(reversal_decayed)
        
        return pd.concat(results, axis=1).mean(axis=1)
    
    def _compute_volatility(self, df: pd.DataFrame, windows: List[int], 
                           decay: int = 6) -> pd.Series:
        """Lightweight volatility factor - optimized version avoiding duplication with Polygon"""
        try:
            window = windows[0] if windows else 20
            g = df.groupby('ticker')['Close']
            
            # Simple volatility: rolling std of returns, shifted to avoid lookahead
            returns = g.pct_change()  # T-1æ»åç”±ç»Ÿä¸€é…ç½®æ§åˆ¶
            volatility = returns.rolling(window).std().fillna(0.0)
            
            # Invert volatility (low vol = high score)
            vol_factor = -volatility  
            
            # Apply decay
            result = vol_factor.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay))
            return result.fillna(0.0)
            
        except Exception as e:
            logger.warning(f"Lightweight volatility computation failed: {e}")
            return pd.Series(0.0, index=df.index)
        results = []
        
        for window in windows:
            # ğŸ›¡ï¸ SAFETY FIX: Calculate log returns with numerical stability
            try:
                from numerical_stability import safe_log, safe_divide
            except ImportError:
                # ç®€åŒ–å®ç°
                def safe_log(x, epsilon=1e-10):
                    return np.log(np.maximum(x, epsilon))
                def safe_divide(a, b, epsilon=1e-10):
                    return a / np.maximum(np.abs(b), epsilon)
            
            def safe_log_returns_calc(x):
                """å®‰å…¨çš„å¯¹æ•°æ”¶ç›Šç‡è®¡ç®—"""
                if len(x) <= 1:
                    return pd.Series(index=x.index, dtype=float)
                
                current_price = x
                past_price = x.shift(1)
                price_ratio = safe_divide(current_price, past_price, fill_value=1.0)
                return safe_log(price_ratio)
            
            returns = df.groupby('ticker')['Close'].transform(safe_log_returns_calc)

            # Rolling volatility (calculated independently for each ticker)
            volatility = returns.groupby(df['ticker']).apply(
                lambda s: s.rolling(window=window, min_periods=max(1, window//2)).std()
            ).reset_index(level=0, drop=True)

            # ğŸ›¡ï¸ SAFETY FIX: Volatility reciprocal (low volatility anomaly)
            try:
                from numerical_stability import safe_divide
            except ImportError:
                # ç®€åŒ–å®ç°
                def safe_divide(a, b, epsilon=1e-10):
                    return a / np.maximum(np.abs(b), epsilon)
            inv_volatility = safe_divide(1.0, volatility, fill_value=0.0)

            # Exponential decay
            inv_vol_decayed = inv_volatility.groupby(df['ticker']).apply(
                lambda s: s.ewm(span=decay, adjust=False).mean()
            ).reset_index(level=0, drop=True)

            results.append(inv_vol_decayed)
        
        return pd.concat(results, axis=1).mean(axis=1)
    
    def _compute_volume_turnover(self, df: pd.DataFrame, windows: List[int], 
                                decay: int = 6) -> pd.Series:
        """Volume turnover factor"""
        results = []
        
        for window in windows:
            # Volume relative strength
            if 'volume' in df.columns:
                volume_ma = df.groupby('ticker')['volume'].transform(
                    lambda x: x.rolling(window=window, min_periods=max(1, window//2)).mean()
                )
                volume_ratio = df['volume'] / (volume_ma + 1e-9)
            else:
                # If no volume data, try amount or create synthetic
                if 'amount' in df.columns:
                    volume_ratio = df.groupby('ticker')['amount'].transform(
                        lambda x: x / (x.rolling(window=window, min_periods=max(1, window//2)).mean() + 1e-9)
                    )
                else:
                    # Create synthetic volume using price * dynamic multiplier
                    # Use median volume when available, otherwise use conservative estimate
                    if 'Volume' in df.columns and not df['Volume'].isna().all():
                        median_vol = df['Volume'].median()
                        synthetic_volume = df['Close'] * (median_vol / df['Close'].median())
                    else:
                        synthetic_volume = df['Close'] * 100000  # Conservative estimate
                    volume_ma = synthetic_volume.groupby(df['ticker']).transform(
                        lambda x: x.rolling(window=window, min_periods=max(1, window//2)).mean()
                    )
                    volume_ratio = synthetic_volume / (volume_ma + 1e-9)
            
            # Exponential decay
            volume_decayed = volume_ratio.groupby(df['ticker']).apply(
                lambda s: s.ewm(span=decay, adjust=False).mean()
            ).reset_index(level=0, drop=True)
            
            results.append(volume_decayed)
        
        return pd.concat(results, axis=1).mean(axis=1)
    
    def _compute_amihud_illiquidity(self, df: pd.DataFrame, windows: List[int], 
                                   decay: int = 6) -> pd.Series:
        """Amihud liquidity indicator: Reciprocal of price impact"""
        results = []
        
        for window in windows:
            # ğŸ›¡ï¸ SAFETY FIX: Calculate daily returns with stability
            try:
                from numerical_stability import safe_log, safe_divide
            except ImportError:
                # ç®€åŒ–å®ç°
                def safe_log(x, epsilon=1e-10):
                    return np.log(np.maximum(x, epsilon))
                def safe_divide(a, b, epsilon=1e-10):
                    return a / np.maximum(np.abs(b), epsilon)
            
            def safe_abs_log_returns(x):
                """å®‰å…¨çš„ç»å¯¹å¯¹æ•°æ”¶ç›Šç‡è®¡ç®—"""
                if len(x) <= 1:
                    return pd.Series(index=x.index, dtype=float)
                
                current_price = x
                past_price = x.shift(1)
                price_ratio = safe_divide(current_price, past_price, fill_value=1.0)
                log_returns = safe_log(price_ratio)
                return np.abs(log_returns)
            
            returns = df.groupby('ticker')['Close'].transform(safe_abs_log_returns)
            
            # ğŸ›¡ï¸ SAFETY FIX: Amihud liquidity with safe division
            if 'amount' in df.columns:
                amihud = safe_divide(returns, df['amount'], fill_value=0.0)
            elif 'volume' in df.columns:
                # Alternative: use price * volume
                volume_value = df['Close'] * df['volume']
                amihud = safe_divide(returns, volume_value, fill_value=0.0)
            else:
                # Use synthetic volume with dynamic calculation
                if 'Volume' in df.columns and not df['Volume'].isna().all():
                    median_vol = df['Volume'].median()
                    synthetic_volume = df['Close'] * (median_vol / df['Close'].median())
                else:
                    synthetic_volume = df['Close'] * 100000  # Conservative estimate
                amihud = safe_divide(returns, synthetic_volume, fill_value=0.0)
            
            # Rolling average
            amihud_ma = amihud.groupby(df['ticker']).apply(
                lambda s: s.rolling(window=window, min_periods=max(1, window//2)).mean()
            ).reset_index(level=0, drop=True)
            
            # Liquidity = 1 / Amihud (higher liquidity is better)
            liquidity = 1.0 / (amihud_ma + 1e-9)
            
            # Exponential decay
            liquidity_decayed = liquidity.groupby(df['ticker']).apply(
                lambda s: s.ewm(span=decay, adjust=False).mean()
            ).reset_index(level=0, drop=True)
            
            results.append(liquidity_decayed)
        
        return pd.concat(results, axis=1).mean(axis=1)
    
    def _compute_bid_ask_spread(self, df: pd.DataFrame, windows: List[int], 
                               decay: int = 6) -> pd.Series:
        """Bid-ask spread factor (simulated)"""
        results = []
        
        for window in windows:
            # If high-low price data available, use (high-low)/close  as spread proxy
            if 'High' in df.columns and 'Low' in df.columns:
                spread_proxy = (df['High'] - df['Low']) / (df['Close'] + 1e-9)
            else:
                # Alternative: use price volatility as spread proxy
                price_vol = df.groupby('ticker')['Close'].transform(
                    lambda x: x.rolling(window=5, min_periods=1).std() / (x + 1e-9)
                )
                spread_proxy = price_vol
            
            # Rolling average spread
            spread_ma = spread_proxy.groupby(df['ticker']).apply(
                lambda s: s.rolling(window=window, min_periods=max(1, window//2)).mean()
            ).reset_index(level=0, drop=True)
            
            # Narrow spread factor (smaller spread is better)
            narrow_spread = 1.0 / (spread_ma + 1e-6)
            
            # Exponential decay
            spread_decayed = narrow_spread.groupby(df['ticker']).apply(
                lambda s: s.ewm(span=decay, adjust=False).mean()
            ).reset_index(level=0, drop=True)
            
            results.append(spread_decayed)
        
        return pd.concat(results, axis=1).mean(axis=1)
    
    def _compute_residual_momentum(self, df: pd.DataFrame, windows: List[int], 
                                  decay: int = 6) -> pd.Series:
        """Residual momentum: Idiosyncratic momentum after removing market beta"""
        results = []
        
        for window in windows:
            # Calculate individual stock returns
            stock_returns = df.groupby('ticker')['Close'].transform(
                lambda x: np.log(x / x.shift(1))
            )
            
            # Calculate market returns (equal weight or market cap weighted)
            market_returns = df.groupby('date')['Close'].transform(
                lambda x: np.log(x.mean() / x.shift(1).mean())
            )
            
            # Rolling regression to calculate beta and residuals
            def calculate_residual_momentum(group):
                # Slice from externally pre-computed Series by index to avoid .name dependency
                group_returns = stock_returns.loc[group.index]
                group_market = market_returns.loc[group.index]
                
                residuals = []
                for i in range(len(group_returns)):
                    if i < window:
                        residuals.append(np.nan)
                        continue
                    
                    y = group_returns.iloc[i-window:i]  # Keep NaN for now
                    x = group_market.iloc[i-window:i]  # Keep NaN for now
                    
                    if len(y) < max(1, window//2) or len(x) != len(y):
                        residuals.append(np.nan)
                        continue
                    
                    try:
                        # Simple linear regression: stock return = alpha + beta * market return + residual
                        slope, intercept, _, _, _ = stats.linregress(x.values, y.values)
                        predicted = intercept + slope * x.iloc[-1]
                        residual = y.iloc[-1] - predicted
                        residuals.append(residual)
                    except:
                        residuals.append(0.0)
                
                return pd.Series(residuals, index=group_returns.index)
            
            residual_momentum = df.groupby('ticker').apply(calculate_residual_momentum)
            residual_momentum = residual_momentum.reset_index(level=0, drop=True)

            # Exponential decay
            residual_decayed = residual_momentum.groupby(df['ticker']).apply(
                lambda s: s.ewm(span=decay, adjust=False).mean()
            ).reset_index(level=0, drop=True)
            
            results.append(residual_decayed)
        
        return pd.concat(results, axis=1).mean(axis=1)
    
    # ===== v2 New factors: Unified entry into class methods and registered =====
    def _compute_reversal_5(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Time-safe short-term reversal (1-5 days), with safety margin"""
        try:
            g = df.groupby('ticker')['Close']
            # Using T-1 to T-6 data, aligned with unified lag
            rev = -(g.shift(1) / g.shift(6) - 1.0)
            rev_ema = rev.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay))
            return rev_ema.fillna(0.0)
        except Exception as e:
            logger.warning(f"Short-term reversal computation failed: {e}")
            return pd.Series(0.0, index=df.index)

    def _compute_amihud_illiquidity_new(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Improved Amihud illiquidity: More robust rolling median with EMA decay"""
        try:
            window = windows[0] if windows else 22
            # ğŸ”§ CRITICAL FIX: ä½¿ç”¨å®‰å…¨çš„groupbyæ–¹æ³•ï¼Œä¿æŒMultiIndexç»“æ„
            returns_abs = self._safe_groupby_apply(df, 'ticker', lambda s: s['Close'].pct_change().abs())  # T-1æ»åç”±ç»Ÿä¸€é…ç½®æ§åˆ¶
            if 'amount' in df.columns:
                volume_dollar = df['amount'].replace(0, np.nan)
            elif 'volume' in df.columns:
                volume_dollar = (df['volume'] * df['Close']).replace(0, np.nan)
            else:
                volume_dollar = (1e6 * df['Close']).replace(0, np.nan)
            illiq = (returns_abs / volume_dollar).replace([np.inf, -np.inf], np.nan)
            illiq_rolling = illiq.groupby(df['ticker']).rolling(window, min_periods=max(1, window//2)).median().reset_index(level=0, drop=True)
            illiq_factor = -illiq_rolling
            illiq_ema = illiq_factor.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay))
            return illiq_ema.fillna(0.0)
        except Exception as e:
            logger.warning(f"Amihud illiquidity computation failed: {e}")
            return pd.Series(0.0, index=df.index)

    def _compute_pead(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """PEADï¼ˆè´¢æŠ¥åæ¼‚ç§»ï¼‰event-driven proxy"""
        try:
            window = windows[0] if windows else 21
            returns_21d = df.groupby('ticker')['Close'].pct_change(periods=window).reset_index(level=0, drop=True)
            if 'volume' in df.columns:
                vol_ma = df.groupby('ticker')['volume'].rolling(window*2).mean().reset_index(level=0, drop=True)
                # Ensure proper index alignment for division
                vol_ratio = pd.Series(df['volume'].values / vol_ma.values, index=df.index)
                vol_anomaly = vol_ratio.groupby(df['ticker']).transform(lambda x: (x - x.rolling(window).mean()) / (x.rolling(window).std() + 1e-8))
            else:
                vol_anomaly = pd.Series(0.0, index=df.index)
            
            # Ensure index alignment
            returns_aligned = pd.Series(returns_21d.values, index=df.index)
            pead_signal = returns_aligned * (1 + vol_anomaly * 0.3)
            
            # Fix threshold calculation with proper index handling
            threshold = pead_signal.groupby(df['ticker']).rolling(252).quantile(0.8).reset_index(level=0, drop=True)
            threshold_aligned = pd.Series(threshold.values, index=df.index)
            pead_filtered = pead_signal.where(pead_signal.abs() > threshold_aligned.abs(), 0)
            
            result = pead_filtered.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay))
            return pd.Series(result.values, index=df.index, name='pead').fillna(0.0)
        except Exception as e:
            logger.warning(f"PEAD computation failed: {e}")
            return pd.Series(0.0, index=df.index)

    # ===== New momentum factors =====
    
    def _compute_momentum_6_1(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """6-1momentumï¼š(t-126 to t-21)çš„pricemomentumï¼Œæ’é™¤æœ€è¿‘1ä¸ªmonth"""
        try:
            g = df.groupby('ticker')['Close']
            # 6 months ago to1 months ago returns
            momentum_6_1 = (g.shift(21) / g.shift(126) - 1.0)
            momentum_ema = momentum_6_1.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay))
            return momentum_ema.fillna(0.0)
        except Exception as e:
            logger.warning(f"6-1momentum computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_52w_new_high_proximity(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """52å‘¨proximity to new high: Current price as percentage of52å‘¨ high price"""
        try:
            window = 252  # 52å‘¨ â‰ˆ 252ä¸ªäº¤æ˜“æ—¥
            g = df.groupby('ticker')['Close']
            max_52w = g.rolling(window=window, min_periods=min(window//2, 60)).max().reset_index(level=0, drop=True)
            current_price = df['Close']
            # Ensure index alignment
            max_52w_aligned = pd.Series(max_52w.values, index=df.index)
            proximity = current_price / max_52w_aligned
            # Apply decay with proper index handling
            result = proximity.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay))
            return pd.Series(result.values, index=df.index, name='new_high_proximity').fillna(0.0)
        except Exception as e:
            logger.warning(f"52å‘¨æ–°é«˜æ¥è¿‘åº¦ computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_mean_reversion(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Lightweight mean reversion factor - optimized version avoiding duplication with Polygon"""
        try:
            window = windows[0] if windows else 20
            g = df.groupby('ticker')['Close']
            
            # Simple mean reversion: (mean - current) / mean, shifted to avoid lookahead
            close_prices = g.shift(1)  # T-1 to align with unified lag
            rolling_mean = close_prices.rolling(window).mean()
            
            # Mean reversion signal: (mean - current) / mean
            mean_reversion = ((rolling_mean - close_prices) / rolling_mean).fillna(0.0)
            
            # Apply decay
            result = mean_reversion.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay))
            return result.fillna(0.0)
            
        except Exception as e:
            logger.warning(f"Lightweight mean reversion computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_volume_ratio(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Volume ratio factor - current volume vs average"""
        try:
            try:
                from numerical_stability import safe_divide
            except ImportError:
                # ç®€åŒ–å®ç°
                def safe_divide(a, b, epsilon=1e-10):
                    return a / np.maximum(np.abs(b), epsilon)
            
            window = windows[0] if windows else 20
            volume = df['Volume']
            
            # Calculate rolling average volume
            avg_volume = volume.rolling(window=window, min_periods=1).mean()
            
            # Volume ratio signal
            volume_ratio = safe_divide(volume, avg_volume, fill_value=1.0)
            
            # Apply log transformation and decay
            log_ratio = np.log(volume_ratio)
            result = log_ratio.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay))
            return pd.Series(result.values, index=df.index, name='volume_ratio').fillna(0.0)
        except Exception as e:
            logger.warning(f"Volume ratio computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_rsi(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """RSI (Relative Strength Index) factor"""
        try:
            try:
                from numerical_stability import safe_divide
            except ImportError:
                # ç®€åŒ–å®ç°
                def safe_divide(a, b, epsilon=1e-10):
                    return a / np.maximum(np.abs(b), epsilon)
            
            window = windows[0] if windows else 14
            close_prices = df['Close']
            
            # Calculate price changes
            delta = close_prices.diff()
            
            # Separate gains and losses
            gains = delta.where(delta > 0, 0)
            losses = (-delta).where(delta < 0, 0)
            
            # Calculate rolling averages
            avg_gains = gains.rolling(window=window, min_periods=1).mean()
            avg_losses = losses.rolling(window=window, min_periods=1).mean()
            
            # Calculate RS and RSI
            rs = safe_divide(avg_gains, avg_losses, fill_value=1.0)
            rsi = 100 - safe_divide(100, 1 + rs, fill_value=50.0)
            
            # Normalize RSI to [-1, 1] range
            normalized_rsi = (rsi - 50) / 50
            
            # Apply decay
            result = normalized_rsi.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay))
            return pd.Series(result.values, index=df.index, name='rsi').fillna(0.0)
        except Exception as e:
            logger.warning(f"RSI computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_price_position(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Price position within recent range"""
        try:
            try:
                from numerical_stability import safe_divide
            except ImportError:
                # ç®€åŒ–å®ç°
                def safe_divide(a, b, epsilon=1e-10):
                    return a / np.maximum(np.abs(b), epsilon)
            
            window = windows[0] if windows else 20
            close_prices = df['Close']
            
            # Calculate rolling min/max
            rolling_min = close_prices.rolling(window=window, min_periods=1).min()
            rolling_max = close_prices.rolling(window=window, min_periods=1).max()
            
            # Price position: (current - min) / (max - min)
            price_range = rolling_max - rolling_min
            position = safe_divide(close_prices - rolling_min, price_range, fill_value=0.5)
            
            # Normalize to [-1, 1] range
            normalized_position = (position - 0.5) * 2
            
            # Apply decay
            result = normalized_position.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay))
            return pd.Series(result.values, index=df.index, name='price_position').fillna(0.0)
        except Exception as e:
            logger.warning(f"Price position computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_low_beta_anomaly(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """ä½Î²å¼‚è±¡ï¼šUsingrollingé—­å¼ä¼°è®¡æˆ– ewm.cov implementation O(N) approximation, take negative (low beta is better)"""
        try:
            window = windows[0] if windows else 60
            close = df['Close']
            ret = close.groupby(df['ticker']).pct_change()
            mkt = close.groupby(df['date']).transform('mean')
            mkt_ret = mkt.groupby(df['ticker']).pct_change()  # ä¸ä¸ªè‚¡ç´¢å¼•å¯¹é½

            # Using ewm.cov çš„å‘é‡åŒ–ä¼°è®¡ beta = Cov(r_i, r_m)/Var(r_m)
            cov_im = ret.ewm(span=window, min_periods=max(10, window//3)).cov(mkt_ret)
            var_m = mkt_ret.ewm(span=window, min_periods=max(10, window//3)).var()
            beta = cov_im / (var_m + 1e-12)
            low_beta = (-beta).groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay))
            return low_beta.fillna(0.0)
        except Exception as e:
            logger.warning(f"ä½Î²å¼‚è±¡ computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_idiosyncratic_volatility(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """
        IVOL ç‰¹è´¨æ³¢åŠ¨ç‡ (T+10 é€‚ç”¨):
        ä½¿ç”¨60æ—¥æ»šåŠ¨å›å½’è®¡ç®—ç›¸å¯¹äºSPYçš„ç‰¹å¼‚æ€§æ³¢åŠ¨ç‡

        æ­¥éª¤:
        1. è®¡ç®—å¯¹æ•°æ”¶ç›Š: r_i,t = ln(c_t/c_t-1), r_m,t ä½¿ç”¨SPY
        2. 60æ—¥æ»šåŠ¨å›å½’: r_i,t = Î± + Î²*r_m,t + Îµ_i,t
        3. IVOL_60d = sqrt(1/(N-1) * Î£(Îµ_i,t-k)^2) for k=1 to 60
        4. æ¯æ—¥æ¨ªæˆªé¢ winsorize â†’ z-score
        """
        try:
            window = windows[0] if windows else 60
            min_periods = max(30, window // 2)

            # è®¡ç®—å¯¹æ•°æ”¶ç›Š
            close = df['Close']
            log_returns = close.groupby(df['ticker']).pct_change().reset_index(level=0, drop=True)  # T-1æ»åç”±ç»Ÿä¸€é…ç½®æ§åˆ¶

            # è·å–å¸‚åœºåŸºå‡†æ”¶ç›Š (SPY proxy: ä½¿ç”¨å¸‚åœºå¹³å‡ä½œä¸ºåŸºå‡†)
            # æ³¨æ„: å¦‚æœæœ‰SPYæ•°æ®ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ï¼›è¿™é‡Œä½¿ç”¨å¸‚åœºå¹³å‡ä½œä¸ºproxy
            market_close = df.groupby('date')['Close'].mean()
            market_returns = pd.Series(index=df.index, dtype=float)

            # ä¸ºæ¯ä¸ªæ—¥æœŸåˆ†é…å¸‚åœºæ”¶ç›Š
            for date in df['date'].unique():
                mask = df['date'] == date
                if date in market_close.index:
                    market_returns.loc[mask] = market_close[date]

            # è®¡ç®—å¸‚åœºå¯¹æ•°æ”¶ç›Š
            market_log_returns = pd.Series(index=df.index, dtype=float)
            for ticker in df['ticker'].unique():
                ticker_mask = df['ticker'] == ticker
                ticker_dates = df[ticker_mask]['date'].values
                for i, date in enumerate(ticker_dates[1:], 1):
                    prev_date = ticker_dates[i-1]
                    if prev_date in market_close.index and date in market_close.index:
                        market_log_returns.loc[(df['ticker'] == ticker) & (df['date'] == date)] = \
                            np.log(market_close[date] / market_close[prev_date])

            # ä¸ºæ¯ä¸ªè‚¡ç¥¨è®¡ç®—60æ—¥æ»šåŠ¨å›å½’æ®‹å·®
            ivol_results = pd.Series(index=df.index, dtype=float)

            for ticker in df['ticker'].unique():
                ticker_mask = df['ticker'] == ticker
                ticker_data = df[ticker_mask].copy()
                ticker_returns = log_returns[ticker_mask]
                ticker_market_returns = market_log_returns[ticker_mask]

                # å»é™¤ NaN å€¼
                valid_mask = ~(ticker_returns.isna() | ticker_market_returns.isna())
                if valid_mask.sum() < min_periods:
                    continue

                ticker_returns_clean = ticker_returns[valid_mask]
                ticker_market_returns_clean = ticker_market_returns[valid_mask]

                # æ»šåŠ¨å›å½’è®¡ç®—æ®‹å·®
                residuals = pd.Series(index=ticker_returns_clean.index, dtype=float)

                for i in range(window - 1, len(ticker_returns_clean)):
                    start_idx = max(0, i - window + 1)

                    y = ticker_returns_clean.iloc[start_idx:i+1].values
                    x = ticker_market_returns_clean.iloc[start_idx:i+1].values

                    if len(y) >= min_periods and len(x) >= min_periods:
                        # ç®€åŒ–CAPMå›å½’: r_i = Î± + Î²*r_m + Îµ
                        x_with_intercept = np.column_stack([np.ones(len(x)), x])
                        try:
                            # ä½¿ç”¨æœ€å°äºŒä¹˜æ³•
                            beta_coef = np.linalg.lstsq(x_with_intercept, y, rcond=None)[0]
                            predicted = x_with_intercept @ beta_coef
                            residual = y[-1] - predicted[-1]  # å½“å‰æœŸæ®‹å·®
                            residuals.iloc[i] = residual
                        except:
                            residuals.iloc[i] = 0.0

                # è®¡ç®—60æ—¥æ®‹å·®æ ‡å‡†å·®ä½œä¸ºIVOL
                ivol_values = pd.Series(index=residuals.index, dtype=float)
                for i in range(window - 1, len(residuals)):
                    start_idx = max(0, i - window + 1)
                    window_residuals = residuals.iloc[start_idx:i+1]
                    valid_residuals = window_residuals.dropna()
                    if len(valid_residuals) >= min_periods:
                        ivol_values.iloc[i] = valid_residuals.std()

                # æ˜ å°„å›åŸå§‹ç´¢å¼•
                for idx, value in ivol_values.items():
                    if not pd.isna(value):
                        ivol_results.loc[idx] = value

            # åº”ç”¨EMAè¡°å‡
            ivol_ema = ivol_results.groupby(df['ticker']).transform(
                lambda x: self.ema_decay(x, span=decay) if hasattr(self, 'ema_decay') else x
            )

            # è´Ÿå·å¤„ç†ï¼šä½æ³¢åŠ¨ç‡æ›´å¥½
            result = -ivol_ema.fillna(0.0)

            return result

        except Exception as e:
            logger.warning(f"IVOLç‰¹è´¨æ³¢åŠ¨ç‡ computation failed: {e}")
            return pd.Series(0.0, index=df.index)

    # ===== Fundamental factorsï¼ˆUsing proxydataï¼‰ =====
    
    def _compute_earnings_surprise(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Earnings surpriseSUEï¼šStandardizeç›ˆä½™æƒŠå–œï¼ˆUsingpriceååº”ä½œä¸º proxyï¼‰"""
        try:
            window = windows[0] if windows else 63  # Quarter
            # Usingpriceåœ¨è´¢æŠ¥æœŸé—´çš„å¼‚å¸¸ååº”ä½œä¸ºSUE proxy
            returns = df.groupby('ticker')['Close'].pct_change()
            # Quarterè¶…é¢returnç‡ä½œä¸ºSUE proxy
            quarterly_returns = df.groupby('ticker')['Close'].pct_change(periods=window)
            market_returns = df.groupby('date')['Close'].transform('mean').pct_change(periods=window)
            excess_returns = quarterly_returns - market_returns
            # Standardize
            sue_proxy = excess_returns.groupby(df['ticker']).transform(
                lambda x: (x - x.rolling(252).mean()) / (x.rolling(252).std() + 1e-8)
            )
            sue_ema = sue_proxy.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay))
            return sue_ema.fillna(0.0)
        except Exception as e:
            logger.warning(f"Earnings surpriseSUE computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_analyst_revision(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """AnalystEPSä¸Šè°ƒä¿®æ­£ï¼ˆUsingmomentumå˜åŒ–ç‡ä½œä¸º proxyï¼‰"""
        try:
            # Usingmomentumå˜åŒ–ä½œä¸ºAnalysté¢„æœŸä¿®æ­£çš„ proxy
            short_momentum = df.groupby('ticker')['Close'].pct_change(21)  # 1month
            long_momentum = df.groupby('ticker')['Close'].pct_change(63)   # 3month
            revision_proxy = short_momentum - long_momentum
            revision_ema = revision_proxy.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay))
            return revision_ema.fillna(0.0)
        except Exception as e:
            logger.warning(f"Analystä¿®æ­£ computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_ebit_ev(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """EBIT/EVreturnç‡ï¼ˆUsingreturnç‡ proxyï¼‰"""
        try:
            # UsingåŸºäºpriceçš„returnç‡ proxyEBIT/EV
            if 'volume' in df.columns:
                enterprise_value = df['Close'] * df['volume']  # SimplifiedEV proxy
                ebit_proxy = df.groupby('ticker')['Close'].pct_change(252).abs()  # Annualized return asEBIT proxy
                ebit_ev = ebit_proxy / (enterprise_value / enterprise_value.rolling(252).mean())
                return self.safe_fillna(ebit_ev.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay)), df)
            else:
                return pd.Series(0.0, index=df.index)
        except Exception as e:
            logger.warning(f"EBIT/EV computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_fcf_ev(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Free cash flow yieldFCF/EVï¼ˆUsingç°é‡‘æµ proxyï¼‰"""
        try:
            # UsingåŸºäºvolumeå’Œpriceçš„ç°é‡‘æµ proxy
            if 'amount' in df.columns:
                fcf_proxy = df['amount'] / df['Close']  # amount/priceä½œä¸ºç°é‡‘æµ proxy
                if 'volume' in df.columns:
                    ev_proxy = df['Close'] * df['volume']
                else:
                    # Create enterprise value proxy with dynamic calculation
                    if 'Volume' in df.columns and not df['Volume'].isna().all():
                        median_vol = df['Volume'].median()
                        ev_proxy = df['Close'] * (median_vol / df['Close'].median())
                    else:
                        ev_proxy = df['Close'] * 100000  # Conservative estimate
                fcf_ev = fcf_proxy / (ev_proxy + 1e-9)
            elif 'volume' in df.columns:
                fcf_proxy = df['volume'] * df['Close'] / df['Close']  # volume as proxy
                ev_proxy = df['Close'] * df['volume']
                fcf_ev = fcf_proxy / (ev_proxy + 1e-9)
                fcf_ev = fcf_ev.groupby(df['ticker']).transform(
                    lambda x: (x - x.rolling(252).mean()) / (x.rolling(252).std() + 1e-8)
                )
                return self.safe_fillna(fcf_ev.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay)), df)
            else:
                return pd.Series(0.0, index=df.index)
        except Exception as e:
            logger.warning(f"FCF/EV computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_earnings_yield(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """çœŸå®çš„Earnings Yield (E/P) - ä½¿ç”¨åŸºæœ¬é¢æ•°æ®"""
        try:
            results = []
            for ticker in df['ticker'].unique():
                ticker_data = df[df['ticker'] == ticker].copy()
                
                # è·å–åŸºæœ¬é¢æ•°æ®
                fundamental_data = self.get_fundamental_data(ticker)
                
                if fundamental_data.get('pe_ratio') and fundamental_data['pe_ratio'] > 0:
                    # E/P = 1/PE
                    earnings_yield = 1.0 / fundamental_data['pe_ratio']
                else:
                    # å›é€€åˆ°ä»·æ ¼ä»£ç†æ–¹æ³•
                    close_col = 'Close' if 'Close' in ticker_data.columns else 'close'
                    annual_return = ticker_data[close_col].pct_change(252).iloc[-1]
                    earnings_yield = annual_return / ticker_data[close_col].iloc[-1] * 100 if not pd.isna(annual_return) else 0
                
                # ä¸ºè¯¥tickerçš„æ‰€æœ‰è¡Œè®¾ç½®ç›¸åŒçš„å€¼
                ticker_results = pd.Series(earnings_yield, index=ticker_data.index)
                results.append(ticker_results)
            
            combined_results = pd.concat(results) if results else pd.Series(0, index=df.index)
            return self.safe_fillna(combined_results.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay)), df)            
        except Exception as e:
            logger.warning(f"çœŸå®Earnings yieldè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ³•: {e}")
            try:
                # å›é€€åˆ°åŸå§‹æ–¹æ³• - ä¿®å¤ç±»å‹é”™è¯¯
                close_col = 'Close' if 'Close' in df.columns else 'close'
                annual_return = df.groupby('ticker')[close_col].pct_change(252)
                earnings_yield = (annual_return / df[close_col] * 100).fillna(0)
                return self.safe_fillna(earnings_yield.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay)), df)
            except Exception as backup_e:
                logger.warning(f"å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {backup_e}")
                return pd.Series(0.0, index=df.index)
    
    def _compute_pb_ratio(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """çœŸå®çš„Price-to-Bookæ¯”ç‡ - ä½¿ç”¨åŸºæœ¬é¢æ•°æ®"""
        try:
            results = []
            for ticker in df['ticker'].unique():
                ticker_data = df[df['ticker'] == ticker].copy()
                
                # è·å–åŸºæœ¬é¢æ•°æ®
                fundamental_data = self.get_fundamental_data(ticker)
                
                if fundamental_data.get('pb_ratio') and fundamental_data['pb_ratio'] > 0:
                    pb_ratio = fundamental_data['pb_ratio']
                else:
                    # å›é€€ï¼šä½¿ç”¨å¸‚å€¼/è´¦é¢ä»·å€¼ä¼°ç®—
                    close_col = 'Close' if 'Close' in ticker_data.columns else 'close'
                    volume_col = 'volume' if 'volume' in ticker_data.columns else 'Volume'
                    
                    if volume_col in ticker_data.columns:
                        market_cap = ticker_data[close_col].iloc[-1] * ticker_data[volume_col].iloc[-1]
                        book_value = fundamental_data.get('book_value', market_cap * 0.5)  # ä¼°ç®—è´¦é¢ä»·å€¼
                        pb_ratio = market_cap / book_value if book_value > 0 else 1.0
                    else:
                        pb_ratio = 1.0
                
                # ä¸ºè¯¥tickerçš„æ‰€æœ‰è¡Œè®¾ç½®ç›¸åŒçš„å€¼
                ticker_results = pd.Series(pb_ratio, index=ticker_data.index)
                results.append(ticker_results)
            
            combined_results = pd.concat(results) if results else pd.Series(1.0, index=df.index)
            return self.safe_fillna(combined_results.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay)), df)            
        except Exception as e:
            logger.warning(f"PBæ¯”ç‡è®¡ç®—å¤±è´¥: {e}")
            try:
                # å›é€€æ–¹æ³• - ä½¿ç”¨ç®€å•ä¼°ç®—
                close_col = 'Close' if 'Close' in df.columns else 'close'  
                pb_estimate = (df[close_col] / df[close_col].rolling(252).mean()).fillna(1.0)
                return self.safe_fillna(pb_estimate.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay)), df)
            except Exception as backup_e:
                logger.warning(f"PBæ¯”ç‡å¤‡ç”¨æ–¹æ³•å¤±è´¥: {backup_e}")
                return pd.Series(1.0, index=df.index)

    def _compute_sales_yield(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Sales yieldS/Pï¼ˆå¸‚é”€ç‡å€’æ•°çš„ proxyï¼‰"""
        try:
            # Usingvolumeä½œä¸ºé”€å”®é¢ proxy
            if 'volume' in df.columns:
                sales_proxy = df['volume']
                sales_yield = sales_proxy / (df['Close'] + 1e-9)
                sales_yield = sales_yield.groupby(df['ticker']).transform(
                    lambda x: (x - x.rolling(252).mean()) / (x.rolling(252).std() + 1e-8)
                )
                return self.safe_fillna(sales_yield.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay)), df)
            else:
                return pd.Series(0.0, index=df.index)
        except Exception as e:
            logger.warning(f"Sales yieldS/P computation failed: {e}")
            return pd.Series(0.0, index=df.index)
 
    # ===== é«˜çº§Alphafactorï¼ˆæš‚æ—¶ç§»é™¤å¤æ‚implementationï¼Œä¿æŒåŸºç¡€åŠŸèƒ½ï¼‰ =====
    
    def _compute_gross_margin(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Gross marginGP/Assetsï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            annual_return = df.groupby('ticker')['Close'].pct_change(252)
            return self.safe_fillna(annual_return, df)
        except Exception as e:
            logger.warning(f"Gross margin computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_operating_profitability(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Operating profitabilityï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            if 'volume' in df.columns:
                efficiency = df['volume'] / (df['Close'] + 1e-9)
                return self.safe_fillna(efficiency, df)
            else:
                return pd.Series(0.0, index=df.index)
        except Exception as e:
            logger.warning(f"Operating profitability computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    # ä¸ºæ‰€æœ‰å…¶ä»–é«˜çº§factoræ·»åŠ ç®€åŒ–implementation
    def _compute_roe_neutralized(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """ROEneutralizeï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            returns = df.groupby('ticker')['Close'].pct_change(252)
            return self.safe_fillna(returns, df)
        except:
            return pd.Series(0.0, index=df.index)
    
    def _compute_roic_neutralized(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """ROICneutralizeï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            returns = df.groupby('ticker')['Close'].pct_change(126)
            return self.safe_fillna(returns, df)
        except:
            return pd.Series(0.0, index=df.index)
    
    def _compute_net_margin(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Net marginï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            returns = df.groupby('ticker')['Close'].pct_change(63)
            return self.safe_fillna(returns, df)
        except:
            return pd.Series(0.0, index=df.index)
    
    def _compute_cash_yield(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Cash yieldï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            if 'amount' in df.columns:
                cash_yield = df['amount'] / (df['Close'] + 1e-9)
                return self.safe_fillna(cash_yield, df)
            elif 'volume' in df.columns:
                cash_yield = (df['volume'] * df['Close']) / (df['Close'] + 1e-9)
                return self.safe_fillna(cash_yield, df)
            else:
                return pd.Series(0.0, index=df.index)
        except:
            return pd.Series(0.0, index=df.index)
    
    def _compute_shareholder_yield(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Shareholder yieldï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            if 'volume' in df.columns:
                volume_ma = df.groupby('ticker')['volume'].rolling(22).mean()
                ratio = df['volume'] / (volume_ma + 1e-9)
                return self.safe_fillna(ratio, df)
            else:
                return pd.Series(0.0, index=df.index)
        except:
            return pd.Series(0.0, index=df.index)
    
    # Accrual factors
    def _compute_total_accruals(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Total accrualsï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            price_change = df.groupby('ticker')['Close'].pct_change()
            return -price_change.apply(lambda x: self.safe_fillna(x, df))  # Take negative
        except:
            return pd.Series(0.0, index=df.index)
    
    def _compute_working_capital_accruals(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Working capital accrualsï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            if 'volume' in df.columns:
                wc_proxy = df.groupby('ticker')['volume'].pct_change()
                return -wc_proxy.apply(lambda x: self.safe_fillna(x, df))  # Take negative
            else:
                return pd.Series(0.0, index=df.index)
        except:
            return pd.Series(0.0, index=df.index)
    
    def _compute_net_operating_assets(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Net operating assetsï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            if 'volume' in df.columns:
                noa_proxy = df['volume'] / (df['Close'] + 1e-9)
                return -noa_proxy.pct_change().apply(lambda x: self.safe_fillna(x, df))  # Take negative
            else:
                return pd.Series(0.0, index=df.index)
        except:
            return pd.Series(0.0, index=df.index)
    
    def _compute_asset_growth(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Asset growthï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            if 'volume' in df.columns:
                market_value = df['Close'] * df['volume']
                growth = market_value.groupby(df['ticker']).pct_change(252)
                return -growth.apply(lambda x: self.safe_fillna(x, df))  # Take negative
            else:
                return pd.Series(0.0, index=df.index)
        except:
            return pd.Series(0.0, index=df.index)
    
    def _compute_net_equity_issuance(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Net equity issuanceï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            if 'volume' in df.columns:
                volume_spike = df.groupby('ticker')['volume'].pct_change()
                return -volume_spike.apply(lambda x: self.safe_fillna(x, df))  # Take negative
            else:
                return pd.Series(0.0, index=df.index)
        except:
            return pd.Series(0.0, index=df.index)
    
    def _compute_investment_factor(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Investment factorï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            # Fix index alignment issue
            price_vol = df.groupby('ticker')['Close'].rolling(22).std().reset_index(level=0, drop=True)
            result = -price_vol.fillna(0.0)  # Take negative
            # Ensure proper index alignment
            return pd.Series(result.values, index=df.index, name='investment_factor').fillna(0.0)
        except Exception as e:
            logger.warning(f"Investment factor computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    # Quality score factors
    def _compute_piotroski_score(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """PiotroskiScoreï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            annual_return = df.groupby('ticker')['Close'].pct_change(252)
            score = (annual_return > 0).astype(float)
            return score.apply(lambda x: self.safe_fillna(x, df) if x.isna().any() else x.fillna(0.5))
        except:
            return pd.Series(0.5, index=df.index)
    
    def _compute_ohlson_score(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """OhlsonScoreï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            # [OK] FIX: å…¼å®¹'Close'å’Œ'close'åˆ—å
            close_col = 'Close' if 'Close' in df.columns else 'close'
            price_vol = df.groupby('ticker')[close_col].rolling(126).std() / df[close_col]
            return -price_vol.apply(lambda x: self.safe_fillna(x, df))  # Take negativeï¼Œlower risk is better
        except:
            return pd.Series(0.0, index=df.index)
    
    def _compute_altman_score(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """AltmanScoreï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            # [OK] FIX: å…¼å®¹'Close'å’Œ'close'åˆ—å
            close_col = 'Close' if 'Close' in df.columns else 'close'
            returns = df.groupby('ticker')[close_col].pct_change()
            stability = -returns.rolling(126).std()  # Stability
            return self.safe_fillna(stability, df)
        except:
            return pd.Series(0.0, index=df.index)
    
    def _compute_qmj_score(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """QMJè´¨é‡Scoreï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            # [OK] FIX: å…¼å®¹'Close'å’Œ'close'åˆ—å
            close_col = 'Close' if 'Close' in df.columns else 'close'
            returns = df.groupby('ticker')[close_col].pct_change()
            quality = returns.rolling(252).mean() / (returns.rolling(252).std() + 1e-8)
            return self.safe_fillna(quality, df)
        except:
            return pd.Series(0.0, index=df.index)
    
    def _compute_earnings_stability(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """ç›ˆåˆ©Stabilityï¼ˆç®€åŒ–implementationï¼‰"""
        try:
            # [OK] FIX: å…¼å®¹'Close'å’Œ'close'åˆ—å
            close_col = 'Close' if 'Close' in df.columns else 'close'
            returns = df.groupby('ticker')[close_col].pct_change()
            stability = -returns.rolling(252).std()  # lower volatility is better
            return self.safe_fillna(stability, df)
        except:
            return pd.Series(0.0, index=df.index)
 
    # ========== Main Computation Pipeline ==========
    
    def compute_all_alphas(self, df) -> pd.DataFrame:
        """
        Compute all Alpha factors
        
        Args:
            df: DataFrame or dict containing price data, must have columns: ['date', 'ticker', 'Close', 'amount', ...]
            
        Returns:
            DataFrame containing all Alpha factors
        """
        logger.info(f"Starting computation of{len(self.config['alphas'])} Alpha factors")
        
        # [TOOL] ä¿®å¤æ•°æ®æ ¼å¼é—®é¢˜ï¼šç¡®ä¿è¾“å…¥æ˜¯DataFrame
        if isinstance(df, dict):
            # å¦‚æœè¾“å…¥æ˜¯dictï¼Œå°è¯•è½¬æ¢ä¸ºDataFrame
            try:
                if 'data' in df and isinstance(df['data'], pd.DataFrame):
                    df_work = df['data'].copy()
                else:
                    # å°è¯•ç›´æ¥ä»dictæ„å»ºDataFrame
                    df_work = pd.DataFrame(df)
                logger.debug(f"Successfully converted dict input to DataFrame: {df_work.shape}")
            except Exception as e:
                logger.error(f"Failed to convert dict to DataFrame: {e}")
                raise ValueError(f"Cannot convert input dict to DataFrame: {e}")
        elif isinstance(df, pd.DataFrame):
            df_work = df.copy()
        else:
            raise ValueError(f"Input must be DataFrame or dict, got {type(df)}")
        
        # Ensure required columns exist
        required_cols = ['date', 'ticker', 'Close']
        missing_cols = [col for col in required_cols if col not in df_work.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")
        
        # Add metadata columns (if not exist) - Use copy to avoid modifying original data
        for col in ['COUNTRY', 'SECTOR', 'SUBINDUSTRY']:
            if col not in df_work.columns:
                df_work[col] = 'Unknown'
        
        alpha_results = {}
        computation_times = {}
        
        for alpha_config in self.config['alphas']:
            # Check if alpha is enabled
            if not alpha_config.get('enabled', True):
                continue
                
            alpha_name = alpha_config['name']
            alpha_kind = alpha_config.get('kind', alpha_config.get('function', 'momentum'))
            
            try:
                start_time = pd.Timestamp.now()
                
                # Get parameters
                windows = alpha_config.get('windows', [22])
                decay = alpha_config.get('decay', 6)
                delay = alpha_config.get('delay', 1)  # é…ç½®æ–‡ä»¶ä¸­çš„delayå‚æ•°
                
                # [OK] NEW: è·å–å› å­ç‰¹å®šçš„æ»åé…ç½®
                factor_specific_lag = 0
                if self.lag_manager:
                    factor_specific_lag = self.lag_manager.get_lag(alpha_name)
                    if factor_specific_lag != delay:
                        logger.debug(f"å› å­{alpha_name}: ä½¿ç”¨å·®å¼‚åŒ–æ»åT-{factor_specific_lag}ï¼ˆåŸdelay={delay}ï¼‰")
                
                if alpha_kind == 'hump':
                    # Special handlinghumpå˜æ¢
                    base_name = alpha_config['base']
                    if base_name not in alpha_results:
                        logger.warning(f"Hump factor{alpha_name}'s base factor{base_name} not found")
                        continue
                    
                    base_factor = alpha_results[base_name].copy()
                    hump_level = alpha_config['hump']
                    alpha_factor = self.hump_transform(base_factor, hump=hump_level)
                else:
                    # Regular factor computation - All factors integrated into this module
                    if alpha_kind not in self.alpha_functions:
                        logger.warning(f"Unknown Alpha type: {alpha_kind}")
                        continue
                    
                    alpha_func = self.alpha_functions[alpha_kind]
                    alpha_factor = alpha_func(
                        df=df_work,
                        windows=windows,
                        decay=decay
                    )
                
                # Data processing pipeline
                alpha_factor = self._process_alpha_pipeline(
                    df=df_work,
                    alpha_factor=alpha_factor,
                    alpha_config=alpha_config,
                    alpha_name=alpha_name
                )
                
                # ã€æ–°å¢ã€‘æ•°æ®è´¨é‡ç›‘æ§ - å¯¹æ¯ä¸ªè®¡ç®—çš„Alphaå› å­è¿›è¡Œè´¨é‡æ£€æŸ¥
                try:
                    quality_report = self.quality_monitor.monitor_alpha_calculation(
                        factor_name=alpha_name,
                        input_data=df_work,
                        output_data=alpha_factor,
                        calculation_func=alpha_func if alpha_kind != 'hump' else None
                    )
                    
                    # å­˜å‚¨è´¨é‡æŠ¥å‘Š
                    self.quality_reports[alpha_name] = quality_report
                    
                    # è®°å½•è´¨é‡é—®é¢˜
                    if quality_report.errors:
                        logger.error(f"[{alpha_name}] æ•°æ®è´¨é‡é”™è¯¯: {', '.join(quality_report.errors)}")
                    if quality_report.warnings:
                        logger.warning(f"[{alpha_name}] æ•°æ®è´¨é‡è­¦å‘Š: {', '.join(quality_report.warnings)}")
                    
                    # è¾“å‡ºå…³é”®è´¨é‡æŒ‡æ ‡
                    logger.info(f"[{alpha_name}] è´¨é‡æŒ‡æ ‡ - ç¼ºå¤±ç‡:{quality_report.output_quality.missing_ratio:.2%}, "
                               f"è¦†ç›–ç‡:{quality_report.output_quality.coverage_ratio:.2%}, "
                               f"å¼‚å¸¸å€¼ç‡:{quality_report.output_quality.outlier_ratio:.2%}")
                    
                except Exception as monitor_error:
                    logger.warning(f"[{alpha_name}] è´¨é‡ç›‘æ§å¤±è´¥: {monitor_error}")
                
                # [OK] NEW: åº”ç”¨å·®å¼‚åŒ–æ»åç­–ç•¥
                if self.lag_manager and factor_specific_lag > 0:
                    # ä½¿ç”¨å› å­ç‰¹å®šçš„æ»å
                    alpha_factor = alpha_factor.groupby(df_work['ticker']).shift(factor_specific_lag)
                    logger.debug(f"åº”ç”¨å·®å¼‚åŒ–æ»å T-{factor_specific_lag} äº {alpha_name}")
                elif delay and delay > 0:
                    # å›é€€åˆ°é…ç½®æ–‡ä»¶ä¸­çš„delay
                    alpha_factor = alpha_factor.groupby(df_work['ticker']).shift(delay)
                
                # [OK] REMOVED: ä¸å†ä½¿ç”¨å…¨å±€ç»Ÿä¸€çš„lagï¼Œæ”¹ä¸ºå·®å¼‚åŒ–æ»å
                # åŸæœ‰çš„global_lagé€»è¾‘å·²è¢«å·®å¼‚åŒ–æ»åæ›¿ä»£
                
                alpha_results[alpha_name] = alpha_factor
                computation_times[alpha_name] = (pd.Timestamp.now() - start_time).total_seconds()
                
                logger.info(f"SUCCESS {alpha_name}:  computation completed ({computation_times[alpha_name]:.2f}s)")
                
            except Exception as e:
                logger.error(f"FAILED {alpha_name}:  computation failed - {e}")
                continue
        
        # æ›´æ–°Statistics
        self.stats['computation_times'].update(computation_times)
        
        # Build result DataFrame, preserve original columns
        result_df = df_work.copy()
        for alpha_name, alpha_series in alpha_results.items():
            result_df[alpha_name] = alpha_series
        
        if alpha_results:
            logger.info(f"Alpha computation completedï¼Œå…±{len(alpha_results)} factors")
            
            # [OK] PERFORMANCE FIX: Apply factor orthogonalization to remove redundancy
            try:
                # Get all alpha factor columns
                alpha_cols = [col for col in result_df.columns if col.startswith('alpha_')]
                
                if len(alpha_cols) >= 2:
                    logger.info(f"å¼€å§‹æ­£äº¤åŒ–{len(alpha_cols)}ä¸ªAlphaå› å­ï¼Œæ¶ˆé™¤å†—ä½™")
                    
                    # Apply orthogonalization with correlation threshold 0.8
                    orthogonalizer = FactorOrthogonalizer(
                        method="correlation_filter",  # ä½¿ç”¨ç›¸å…³æ€§è¿‡æ»¤ï¼Œæ›´é€‚åˆAlphaå› å­
                        correlation_threshold=0.8     # ç§»é™¤ç›¸å…³æ€§>0.8çš„å†—ä½™å› å­
                    )
                    
                    # Create temporary DataFrame for orthogonalization
                    ortho_data = result_df[['date', 'ticker'] + alpha_cols].copy()
                    orthogonalized_data = orthogonalizer.fit_transform(ortho_data)
                    
                    # Update result with orthogonalized factors
                    for col in orthogonalizer.retained_factors or alpha_cols:
                        if col in orthogonalized_data.columns:
                            result_df[col] = orthogonalized_data[col]
                    
                    # Remove redundant factors that were filtered out
                    removed_factors = [col for col in alpha_cols if col not in (orthogonalizer.retained_factors or alpha_cols)]
                    for col in removed_factors:
                        if col in result_df.columns:
                            result_df = result_df.drop(columns=[col])
                    
                    retained_count = len(orthogonalizer.retained_factors or alpha_cols)
                    removed_count = len(alpha_cols) - retained_count
                    logger.info(f"[OK] å› å­æ­£äº¤åŒ–å®Œæˆ: ä¿ç•™{retained_count}ä¸ª, ç§»é™¤{removed_count}ä¸ªå†—ä½™å› å­")
                    
                    # Get factor importance if available
                    importance = orthogonalizer.get_factor_importance()
                    if importance:
                        logger.debug(f"å› å­é‡è¦æ€§æ’åº: {sorted(importance.items(), key=lambda x: x[1], reverse=True)[:5]}")
                
            except Exception as e:
                logger.warning(f"å› å­æ­£äº¤åŒ–å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸå§‹å› å­: {e}")
            
            # [OK] PERFORMANCE FIX: Apply dynamic factor weighting
            try:
                final_alpha_cols = [col for col in result_df.columns if col.startswith('alpha_')]
                
                if len(final_alpha_cols) >= 2 and 'future_return_10d' in result_df.columns:
                    logger.info(f"å¼€å§‹è®¡ç®—{len(final_alpha_cols)}ä¸ªAlphaå› å­çš„åŠ¨æ€æƒé‡")
                    
                    # Calculate dynamic weights based on IC performance
                    dynamic_weights = self.calculate_dynamic_weights(
                        df=result_df,
                        alpha_cols=final_alpha_cols,
                        target_col='future_return_10d'
                    )
                    
                    # Dynamic weighting removed - all alpha factors used equally
                
            except Exception as e:
                logger.warning(f"åŠ¨æ€æƒé‡åº”ç”¨å¤±è´¥: {e}")
                
        else:
            logger.error("æ‰€æœ‰Alphafactor computation failed")
        
        # ğŸ”§ CRITICAL FIX: å¼ºåŒ–MultiIndexå¤„ç†é€»è¾‘ï¼Œé˜²æ­¢ç´¢å¼•é”™ä½
        if not isinstance(result_df.index, pd.MultiIndex):
            logger.warning("âš ï¸ ç»“æœä¸æ˜¯MultiIndexæ ¼å¼ï¼Œå°è¯•é‡å»º...")
            
            # æ–¹æ³•1ï¼šå¦‚æœæœ‰dateå’Œtickeråˆ—ï¼Œå°è¯•é‡å»ºMultiIndex
            if 'date' in result_df.columns and 'ticker' in result_df.columns:
                try:
                    # ğŸ“Š æ•°æ®éªŒè¯ï¼šç¡®ä¿dateå’Œtickeræ•°é‡åŒ¹é…
                    if len(result_df) != len(result_df['date']) or len(result_df) != len(result_df['ticker']):
                        raise ValueError("æ—¥æœŸæˆ–è‚¡ç¥¨ä»£ç æ•°æ®ä¸å®Œæ•´")
                    
                    # ğŸ¯ å®‰å…¨çš„MultiIndexé‡å»º
                    dates = pd.to_datetime(result_df['date'])
                    tickers = result_df['ticker'].astype(str)
                    
                    # éªŒè¯æ•°æ®å®Œæ•´æ€§
                    if dates.isnull().any():
                        raise ValueError(f"å‘ç°{dates.isnull().sum()}ä¸ªç©ºæ—¥æœŸ")
                    if tickers.isnull().any():
                        raise ValueError(f"å‘ç°{tickers.isnull().sum()}ä¸ªç©ºè‚¡ç¥¨ä»£ç ")
                    
                    # åˆ›å»ºMultiIndex
                    multi_idx = pd.MultiIndex.from_arrays([dates, tickers], names=['date', 'ticker'])
                    
                    # é‡å»ºDataFrameï¼Œä¿ç•™æ‰€æœ‰alphaåˆ—
                    result_clean = result_df.copy()
                    result_clean.index = multi_idx
                    
                    # åªä¿ç•™Alphaå› å­ï¼ˆç§»é™¤äº†ivol_60dï¼‰ï¼Œç§»é™¤åŸå§‹å¸‚åœºæ•°æ®å’Œå…ƒæ•°æ®åˆ—
                    required_17_factors = [
                        'momentum_10d_ex1',
                        'rsi', 'bollinger_squeeze',
                        'obv_momentum', 'atr_ratio', 'blowoff_ratio', 'stability_score',
                        'liquidity_factor',
                        'near_52w_high', 'reversal_1d', 'mom_accel_5_2'
                    ]

                    # åªä¿ç•™å­˜åœ¨çš„17ä¸ªå› å­åˆ—
                    alpha_cols_available = [col for col in required_17_factors if col in result_clean.columns]
                    
                    if alpha_cols_available:
                        final_result = result_clean[alpha_cols_available]
                        logger.info(f"âœ… MultiIndexé‡å»ºæˆåŠŸ: {final_result.shape} åŒ…å«17ä¸ªé«˜è´¨é‡å› å­: {len(alpha_cols_available)}/17")
                        return final_result
                    else:
                        logger.error("âŒ é‡å»ºåæ²¡æœ‰å¯ç”¨çš„ç‰¹å¾åˆ—")
                        
                except Exception as rebuild_error:
                    logger.error(f"âŒ MultiIndexé‡å»ºå¤±è´¥: {rebuild_error}")
                    logger.info(f"åŸå§‹DataFrameä¿¡æ¯: shape={result_df.shape}, columns={list(result_df.columns)[:10]}...")
            
            # æ–¹æ³•2ï¼šå¦‚æœåŸå§‹è¾“å…¥æ˜¯MultiIndexï¼Œå°è¯•æ¢å¤
            logger.warning("âš ï¸ MultiIndexé‡å»ºå¤±è´¥ï¼Œè¿”å›åŸæ ¼å¼")
            logger.warning("âš ï¸ è¿™å¯èƒ½å¯¼è‡´åç»­ç‰¹å¾åˆå¹¶æ—¶çš„ç´¢å¼•å¯¹é½é—®é¢˜")
        else:
            # å¯¹äºå·²ç»æ˜¯MultiIndexçš„æƒ…å†µï¼Œä¹Ÿåªè¿”å›alphaå› å­åˆ—
            required_alpha_factors = [
                'momentum_10d_ex1',
                'rsi', 'bollinger_position', 'price_to_ma20',
                'obv_momentum', 'ad_line', 'atr_20d', 'atr_ratio', 'blowoff_ratio', 'stability_score',
                'macd_histogram', 'stoch_k', 'cci',
                'market_cap_proxy',
                'liquidity_factor', 'growth_proxy', 'profitability_momentum',
                'growth_acceleration', 'quality_consistency', 'financial_resilience'
            ]

            alpha_cols_available = [col for col in required_alpha_factors if col in result_df.columns]

            if alpha_cols_available:
                final_result = result_df[alpha_cols_available]
                logger.info(f"âœ… ç»“æœå·²æ˜¯MultiIndexæ ¼å¼: {final_result.shape} åŒ…å«Alphaå› å­: {len(alpha_cols_available)}ä¸ª")
                return final_result
            else:
                logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•Alphaå› å­åˆ—")
                return pd.DataFrame()  # è¿”å›ç©ºDataFrame
        
        # å¦‚æœä»¥ä¸Šéƒ½å¤±è´¥ï¼Œè¿”å›ç©ºDataFrame
        logger.error("âŒ æ‰€æœ‰è¿”å›è·¯å¾„éƒ½å¤±è´¥")
        return pd.DataFrame()
    
    def _process_alpha_pipeline(self, df: pd.DataFrame, alpha_factor: pd.Series, 
                               alpha_config: Dict, alpha_name: str) -> pd.Series:
        """Alpha factor processing pipelineï¼šwinsorize -> neutralize -> zscore -> transform"""
        
        # 1. Winsorizeremove outliers
        winsorize_std = self.config.get('winsorize_std', 2.5)
        alpha_factor = self.winsorize_series(alpha_factor, k=winsorize_std)
        
        # 2. æ„å»ºä¸´æ—¶DataFrameè¿›è¡Œneutralize
        base_cols = ['date', 'ticker']
        neutralization_cols = []
        for col in self.config['neutralization']:
            if col in df.columns:
                neutralization_cols.append(col)
        
        temp_df = df[base_cols + neutralization_cols].copy()
        temp_df[alpha_name] = alpha_factor
        
        # 3. neutralizeï¼ˆdefaultå…³é—­ï¼Œé¿å…ä¸å…¨å±€Pipelineé‡å¤ï¼›ä»…ç ”ç©¶Usingæ—¶æ‰“å¼€ï¼‰
        if self.config.get('enable_alpha_level_neutralization', False):
            for neutralize_level in self.config['neutralization']:
                if neutralize_level in temp_df.columns:
                    alpha_factor = self.neutralize_factor(
                        temp_df, alpha_name, [neutralize_level]
                    )
                    temp_df[alpha_name] = alpha_factor
        
        # 4. [OK] PERFORMANCE FIX: æ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼Œæ¶ˆé™¤å¸‚åœºé£æ ¼åç§»
        try:
            from cross_sectional_standardization import CrossSectionalStandardizer
            
            standardizer = CrossSectionalStandardizer(method="robust_zscore")
            standardized_df = standardizer.fit_transform(
                temp_df[['date', 'ticker', alpha_name]], 
                feature_cols=[alpha_name]
            )
            alpha_factor = standardized_df[alpha_name]
            
        except Exception as e:
            logger.warning(f"æ¨ªæˆªé¢æ ‡å‡†åŒ–å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿzscore: {e}")
            # å›é€€åˆ°ä¼ ç»Ÿzscoreæ–¹æ³•
            alpha_factor = self.zscore_by_group(
                temp_df, alpha_name, ['date']
            )
        
        # 5. Transform (rank or keep original)
        xform = alpha_config.get('xform', 'zscore')
        if xform == 'rank':
            alpha_factor = temp_df.groupby('date')[alpha_name].transform(
                lambda x: self.rank_transform(x)
            )
        
        return alpha_factor
    
    def compute_oof_scores(self, alpha_df: pd.DataFrame, target: pd.Series, 
                          dates: pd.Series, metric: str = 'ic') -> pd.Series:
        """
        computationOut-of-FoldScore
        
        Args:
            alpha_df: Alpha factor DataFrame
            target: Target variable
            dates: Date sequence
            metric: ScoreæŒ‡æ ‡ ('ic', 'sharpe', 'fitness')
            
        Returns:
            æ¯ä¸ªAlphaçš„OOFScore
        """
        logger.info(f"Starting computation ofOOFScoreï¼ŒæŒ‡æ ‡: {metric}")

        # Unify indices to avoid boolean index misalignment
        try:
            alpha_index = alpha_df.index
            common_index = alpha_index
            if isinstance(target, pd.Series):
                common_index = common_index.intersection(target.index)
            if isinstance(dates, pd.Series):
                common_index = common_index.intersection(dates.index)

            if len(common_index) == 0:
                logger.warning("OOFScoreè·³è¿‡ï¼šalpha/target/datesæ— å…±åŒç´¢å¼•")
                return pd.Series(dtype=float)

            alpha_df = alpha_df.loc[common_index]
            if isinstance(target, pd.Series):
                target = target.loc[common_index]
            else:
                target = pd.Series(target, index=common_index)
            if isinstance(dates, pd.Series):
                dates = dates.loc[common_index]
            else:
                dates = pd.Series(dates, index=common_index)
        except Exception as e:
            logger.warning(f"Index alignment failed, trying to continueï¼š{e}")

        # Only evaluate numerical factor columns, exclude ID/price/metadata columns
        exclude_cols = set(['date','ticker','COUNTRY','SECTOR','SUBINDUSTRY',
                            'Open','High','Low','Close','Adj Close',
                            'open','high','low','close','adj_close','volume','amount'])
        factor_cols = [c for c in alpha_df.columns
                       if c not in exclude_cols and pd.api.types.is_numeric_dtype(alpha_df[c])]

        # ğŸš« SSOTè¿è§„æ£€æµ‹ï¼šé˜»æ­¢å†…éƒ¨CVåˆ›å»º
        try:
            from ssot_violation_detector import block_internal_cv_creation
            block_internal_cv_creation("Alphaç­–ç•¥ä¸­çš„TimeSeriesSplit")
        except ImportError:
            # å¤‡ç”¨å¤„ç† - ä»…è®°å½•è­¦å‘Š
            logger.debug("SSOT violation detector not available - skipping check")
        unique_dates = sorted(dates.unique())
        
        scores = {}
        for col in factor_cols:
            col_scores = []
            
            for train_idx, test_idx in tscv.split(unique_dates):
                # Get test period data
                test_dates = [unique_dates[i] for i in test_idx]
                # Usingnumpyå¸ƒå°”æ•°ç»„ï¼Œé¿å…ç´¢å¼•ä¸ä¸€è‡´
                test_mask = dates.isin(test_dates).values
                
                if test_mask.sum() == 0:
                    continue
                
                # Usingilocé…åˆå¸ƒå°”æ•°ç»„ï¼Œç¡®ä¿ä½ç½®ç´¢å¼•å¯¹é½
                y_test = target.iloc[test_mask]
                x_test = alpha_df[col].iloc[test_mask]
                
                # Reset index to ensure alignment
                y_test = y_test.reset_index(drop=True)
                x_test = x_test.reset_index(drop=True)
                
                # Remove NaN values
                valid_mask = ~(x_test.isna() | y_test.isna())
                if valid_mask.sum() < 10:  # Minimum required10 valid samples
                    continue
                
                # ç›´æ¥Usingå¸ƒå°”ç´¢å¼•ï¼Œå› ä¸ºç´¢å¼•å·²é‡ç½®
                x_valid = x_test[valid_mask]
                y_valid = y_test[valid_mask]
                
                # computationScore
                if metric == 'ic':
                    score = np.corrcoef(x_valid.values, y_valid.values)[0, 1]
                elif metric == 'sharpe':
                    returns = x_valid.values * y_valid.values
                    score = returns.mean() / (returns.std(ddof=0) + 1e-12)
                elif metric == 'fitness':
                    # Information Coefficient * sqrt(sample size)
                    ic = np.corrcoef(x_valid.values, y_valid.values)[0, 1]
                    score = ic * np.sqrt(len(x_valid))
                else:
                    score = 0.0
                
                if not np.isnan(score):
                    col_scores.append(score)
            
            scores[col] = np.nanmean(col_scores) if col_scores else 0.0
        
        # IC stats removed - using direct scores
        result = pd.Series(scores, name=f'oof_{metric}')
        logger.info(f"OOFScore completed, average {metric}: {result.mean():.4f}")
        
        return result
    
    def compute_bma_weights(self, scores: pd.Series, temperature: float = None) -> pd.Series:
        """
        Pure ML-based BMA weights computation - NO hardcoded weights
        
        Args:
            scores: OOF scores from cross-validation
            temperature: Temperature coefficient, controls weight concentration
            
        Returns:
            BMA weights based purely on performance scores
        """
        if temperature is None:
            temperature = self.config.get('temperature', 1.2)
        
        # Standardize scores
        scores_std = (scores - scores.mean()) / (scores.std(ddof=0) + 1e-12)
        scores_scaled = scores_std / max(temperature, 1e-3)
        
        # Log-sum-exp softmax (numerically stable)
        max_score = scores_scaled.max()
        exp_scores = np.exp(scores_scaled - max_score)
        
        # Pure softmax - no hardcoded priors
        eps = 1e-6
        weights = (exp_scores + eps) / (exp_scores.sum() + eps * len(exp_scores))
        
        weights_series = pd.Series(weights, index=scores.index, name='bma_weights')
        
        logger.info(f"Pure ML BMA weights computed, distribution: max={weights.max():.3f}, min={weights.min():.3f}")
        logger.info(f"Top factor weights: {weights_series.nlargest(5).to_dict()}")
        
        return weights_series
    
    def combine_alphas(self, alpha_df: pd.DataFrame, weights: pd.Series) -> pd.Series:
        """
        UsingBMA weightsportfolioAlphafactor
        
        Args:
            alpha_df: Alpha factor DataFrame
            weights: BMA weights
            
        Returns:
            Combined Alpha signal
        """
        # ä»…Usingæ•°å€¼å‹factoråˆ—ï¼Œæ’é™¤å…ƒdata
        exclude_cols = set(['date','ticker','COUNTRY','SECTOR','SUBINDUSTRY',
                            'Open','High','Low','Close','Adj Close',
                            'open','high','low','close','adj_close','volume','amount'])
        factor_cols = [c for c in alpha_df.columns
                       if c not in exclude_cols and pd.api.types.is_numeric_dtype(alpha_df[c])]
        if not factor_cols:
            return pd.Series(index=alpha_df.index, dtype=float)

        # Ensure weight alignment (column direction)
        aligned_weights = weights.reindex(factor_cols, fill_value=0.0)
        total_w = aligned_weights.sum()
        if total_w <= 0:
            aligned_weights[:] = 1.0 / len(aligned_weights)
        else:
            aligned_weights = aligned_weights / total_w

        # Column-wise multiplication to avoid type errors from row index alignment
        combined_signal = alpha_df[factor_cols].mul(aligned_weights, axis=1).sum(axis=1)
        
        logger.info(f"Alpha combination completed, signal range: [{combined_signal.min():.4f}, {combined_signal.max():.4f}]")
        
        return combined_signal
    
    def apply_trading_filters(self, signal: pd.Series, df: pd.DataFrame) -> pd.Series:
        """
        Apply trading filtersï¼šhumpgating, truncation, position limits
        
        Args:
            signal: Raw signal
            df: DataFrame containing date information
            
        Returns:
            Filtered trading signal
        """
        logger.info("Apply trading filters")
        
        # 1. æˆªé¢Standardize
        temp_df = df[['date', 'ticker']].copy()
        temp_df['signal'] = signal
        
        filtered_signal = temp_df.groupby('date')['signal'].transform(
            lambda x: (x - x.mean()) / (x.std(ddof=0) + 1e-12)
        )
        
        # 2. Humpgating
        hump_levels = self.config.get('hump_levels', [0.003, 0.008])
        for hump_level in hump_levels:
            filtered_signal = self.hump_transform(filtered_signal, hump=hump_level)
        
        # 3. Truncation controls concentration
        truncation = self.config.get('truncation', 0.10)
        if truncation > 0:
            lower_q = filtered_signal.quantile(truncation)
            upper_q = filtered_signal.quantile(1 - truncation)
            filtered_signal = filtered_signal.clip(lower=lower_q, upper=upper_q)
        
        # 4. Only keep top and bottom signals
        top_frac = self.config.get('top_fraction', 0.10)
        if top_frac > 0:
            def mask_top_bottom(x):
                if len(x) < 10:
                    return x
                lo_threshold = x.quantile(top_frac)
                hi_threshold = x.quantile(1 - top_frac)
                return x.where((x <= lo_threshold) | (x >= hi_threshold), 0.0)
            
            temp_df['signal'] = filtered_signal
            filtered_signal = temp_df.groupby('date')['signal'].transform(mask_top_bottom)
        
        logger.info(f"Trading filter completed, non-zero signal ratio: {(filtered_signal != 0).mean():.2%}")
        
        return filtered_signal
    
    # ========== Sentiment Factor Functions ==========
    # å°†æƒ…ç»ªæ•°æ®ä½œä¸ºç‹¬ç«‹çš„æœºå™¨å­¦ä¹ ç‰¹å¾ï¼Œæ— ç¡¬ç¼–ç æƒé‡
    
    def _compute_news_sentiment(self, df: pd.DataFrame, windows: List[int] = [5, 22], 
                               decay: int = 6) -> pd.Series:
        """è®¡ç®—æ–°é—»æƒ…ç»ªAlphaå› å­"""
        try:
            # æŸ¥æ‰¾æ–°é—»æƒ…ç»ªç›¸å…³åˆ—
            news_cols = [col for col in df.columns if col.startswith('news_')]
            
            if not news_cols:
                logger.debug("æœªæ‰¾åˆ°æ–°é—»æƒ…ç»ªæ•°æ®åˆ—")
                return pd.Series(0, index=df.index)
            
            # ä½¿ç”¨æœ€é‡è¦çš„æ–°é—»æƒ…ç»ªæŒ‡æ ‡
            primary_cols = ['news_sentiment_mean', 'news_sentiment_momentum_1d', 'news_news_count']
            available_cols = [col for col in primary_cols if col in df.columns]
            
            if available_cols:
                # è®¡ç®—å¤åˆæ–°é—»æƒ…ç»ªå› å­ï¼ˆä¸ä½¿ç”¨ç¡¬ç¼–ç æƒé‡ï¼Œè®©æ¨¡å‹å­¦ä¹ ï¼‰
                sentiment_factor = pd.Series(0, index=df.index)
                for col in available_cols:
                    col_factor = df[col]                    # åº”ç”¨æ—¶é—´è¡°å‡
                    col_factor = self.decay_linear(col_factor, decay)
                    sentiment_factor += col_factor / len(available_cols)  # ç®€å•å¹³å‡è€Œéç¡¬ç¼–ç æƒé‡
                
                return sentiment_factor
            else:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ–°é—»æƒ…ç»ªåˆ—
                col = news_cols[0]
                sentiment_factor = df[col]
                return self.decay_linear(sentiment_factor, decay)
                
        except Exception as e:
            logger.warning(f"è®¡ç®—æ–°é—»æƒ…ç»ªå› å­å¤±è´¥: {e}")
            return pd.Series(0, index=df.index)
    
    def _compute_market_sentiment(self, df: pd.DataFrame, windows: List[int] = [5, 22], 
                                 decay: int = 6) -> pd.Series:
        """è®¡ç®—å¸‚åœºæƒ…ç»ªAlphaå› å­ï¼ˆåŸºäºSP500æ•°æ®ï¼‰"""
        try:
            # æŸ¥æ‰¾å¸‚åœºæƒ…ç»ªç›¸å…³åˆ—
            market_cols = [col for col in df.columns if col.startswith('market_') or 'sp500' in col]
            
            if not market_cols:
                logger.debug("æœªæ‰¾åˆ°å¸‚åœºæƒ…ç»ªæ•°æ®åˆ—")
                return pd.Series(0, index=df.index)
            
            # ä¼˜å…ˆä½¿ç”¨å…³é”®å¸‚åœºæƒ…ç»ªæŒ‡æ ‡
            priority_cols = [col for col in market_cols if any(keyword in col for keyword in 
                            ['momentum', 'volatility', 'fear', 'sentiment'])]
            
            if priority_cols:
                # è®¡ç®—å¤åˆå¸‚åœºæƒ…ç»ªå› å­
                sentiment_factor = pd.Series(0, index=df.index)
                for col in priority_cols[:3]:  # é™åˆ¶æœ€å¤š3ä¸ªå› å­é¿å…è¿‡åº¦æ‹Ÿåˆ
                    col_factor = df[col]
                    col_factor = self.decay_linear(col_factor, decay)
                    sentiment_factor += col_factor / min(3, len(priority_cols))
                
                return sentiment_factor
            else:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„å¸‚åœºæƒ…ç»ªåˆ—
                col = market_cols[0]
                sentiment_factor = df[col]
                return self.decay_linear(sentiment_factor, decay)
                
        except Exception as e:
            logger.warning(f"è®¡ç®—å¸‚åœºæƒ…ç»ªå› å­å¤±è´¥: {e}")
            return pd.Series(0, index=df.index)
    
    def _compute_fear_greed_sentiment(self, df: pd.DataFrame, windows: List[int] = [5, 22], 
                                     decay: int = 6) -> pd.Series:
        """è®¡ç®—ææƒ§è´ªå©ªæŒ‡æ•°Alphaå› å­"""
        try:
            # æŸ¥æ‰¾ææƒ§è´ªå©ªç›¸å…³åˆ—
            fg_cols = [col for col in df.columns if 'fear_greed' in col or 'fear' in col or 'greed' in col]
            
            if not fg_cols:
                logger.debug("æœªæ‰¾åˆ°ææƒ§è´ªå©ªæŒ‡æ•°æ•°æ®åˆ—")
                return pd.Series(0, index=df.index)
            
            # ä¼˜å…ˆä½¿ç”¨è§„èŒƒåŒ–çš„ææƒ§è´ªå©ªæŒ‡æ ‡
            priority_cols = ['fear_greed_normalized', 'market_fear_level', 'market_greed_level']
            available_cols = [col for col in priority_cols if col in df.columns]
            
            if available_cols:
                # è®¡ç®—å¤åˆææƒ§è´ªå©ªå› å­
                sentiment_factor = pd.Series(0, index=df.index)
                for col in available_cols:
                    col_factor = df[col]
                    col_factor = self.decay_linear(col_factor, decay)
                    sentiment_factor += col_factor / len(available_cols)
                
                return sentiment_factor
            else:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„ææƒ§è´ªå©ªåˆ—
                col = fg_cols[0]
                sentiment_factor = df[col]                # å¦‚æœæ˜¯åŸå§‹å€¼ï¼Œè¿›è¡Œå½’ä¸€åŒ–
                if 'value' in col.lower():
                    sentiment_factor = (sentiment_factor - 50) / 50  # å½’ä¸€åŒ–åˆ°[-1,1]
                return self.decay_linear(sentiment_factor, decay)
                
        except Exception as e:
            logger.warning(f"è®¡ç®—ææƒ§è´ªå©ªæƒ…ç»ªå› å­å¤±è´¥: {e}")
            return pd.Series(0, index=df.index)
    
    def _compute_sentiment_momentum(self, df: pd.DataFrame, windows: List[int] = [5, 22], 
                                   decay: int = 6) -> pd.Series:
        """è®¡ç®—æƒ…ç»ªåŠ¨é‡å› å­"""
        try:
            # æŸ¥æ‰¾æƒ…ç»ªåŠ¨é‡ç›¸å…³åˆ—
            momentum_cols = [col for col in df.columns if 'sentiment' in col and 'momentum' in col]
            
            if not momentum_cols:
                # å¦‚æœæ²¡æœ‰ç°æˆçš„æƒ…ç»ªåŠ¨é‡åˆ—ï¼Œä»åŸºç¡€æƒ…ç»ªå› å­è®¡ç®—
                sentiment_cols = [col for col in df.columns if any(prefix in col for prefix in 
                                 ['news_sentiment_mean', 'fear_greed_normalized'])]
                
                if sentiment_cols:
                    # è®¡ç®—çŸ­æœŸæƒ…ç»ªåŠ¨é‡
                    sentiment_factor = pd.Series(0, index=df.index)
                    for col in sentiment_cols[:2]:  # æœ€å¤šä½¿ç”¨2ä¸ªåŸºç¡€æƒ…ç»ªå› å­
                        col_data = df[col]                        # è®¡ç®—çŸ­æœŸåŠ¨é‡ï¼ˆ3å¤©ï¼‰
                        momentum = col_data.groupby(df['ticker']).diff(3)
                        sentiment_factor += momentum / len(sentiment_cols[:2])
                    
                    return self.decay_linear(sentiment_factor.apply(lambda x: self.safe_fillna(x, df)), decay)
                else:
                    return pd.Series(0, index=df.index)
            else:
                # ä½¿ç”¨ç°æˆçš„æƒ…ç»ªåŠ¨é‡åˆ—
                sentiment_factor = df[momentum_cols[0]]
                return self.decay_linear(sentiment_factor, decay)
                
        except Exception as e:
            logger.warning(f"è®¡ç®—æƒ…ç»ªåŠ¨é‡å› å­å¤±è´¥: {e}")
            return pd.Series(0, index=df.index)
    
    # REMOVED: å¤æ‚çš„æƒ…ç»ªæ³¢åŠ¨ç‡å› å­å®ç° - æ•°æ®è´¨é‡å·®ï¼Œè®¡ç®—å¼€é”€å¤§
    def _compute_sentiment_volatility(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """DEPRECATED: æƒ…ç»ªæ³¢åŠ¨ç‡å› å­å·²åˆ é™¤"""
        return pd.Series(0, index=df.index)
    
    # ========== End Sentiment Factors ==========
    
    # ========== Advanced Behavioral Factors ==========
    
    # REMOVED: è¶…å¤æ‚çš„æ•£æˆ·ç¾Šç¾¤æ•ˆåº”å› å­å®ç° - è®¡ç®—æˆæœ¬æœ€é«˜ï¼Œæ•ˆæœé€’å‡
    def _compute_retail_herding_effect(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """DEPRECATED: æ•£æˆ·ç¾Šç¾¤æ•ˆåº”å› å­å·²åˆ é™¤ - è®¡ç®—æˆæœ¬è¿‡é«˜"""
        return pd.Series(0, index=df.index)
    
    # REMOVED: APMåŠ¨é‡åè½¬å› å­ - è¿‡åº¦å·¥ç¨‹åŒ–ï¼Œç¼ºä¹æ—¥å†…æ•°æ®æ”¯æŒ
    def _compute_apm_momentum_reversal(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """DEPRECATED: APMåŠ¨é‡åè½¬å› å­å·²åˆ é™¤ - è¿‡åº¦å·¥ç¨‹åŒ–ï¼Œå®é™…æ•ˆæœæœ‰é™"""
        return pd.Series(0, index=df.index)
    
    # ========== [HOT] NEW: Real Polygon TrainingæŠ€æœ¯æŒ‡æ ‡é›†æˆ ==========
    
    def _compute_sma_10(self, df: pd.DataFrame, window: int = None, **kwargs) -> pd.Series:
        """ç®€å•ç§»åŠ¨å¹³å‡çº¿(å¯ä¼˜åŒ–å‚æ•°)"""
        # [OK] PERFORMANCE FIX: ä½¿ç”¨ä¼˜åŒ–åçš„çª—å£å‚æ•°
        optimal_window = window or self.get_optimized_window('sma_10', 10)
        
        if 'Close' not in df.columns or len(df) < optimal_window:
            return pd.Series(0, index=df.index)
        
        try:
            sma = df['Close'].rolling(optimal_window).mean()
            # è½¬æ¢ä¸ºç›¸å¯¹å¼ºåº¦ä¿¡å·ï¼šå½“å‰ä»·æ ¼ç›¸å¯¹å‡çº¿çš„åç¦»åº¦
            return self.safe_fillna(((df['Close'] / sma) - 1), df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_sma_20(self, df: pd.DataFrame, window: int = None, **kwargs) -> pd.Series:
        """ç®€å•ç§»åŠ¨å¹³å‡çº¿(å¯ä¼˜åŒ–å‚æ•°)"""
        # [OK] PERFORMANCE FIX: ä½¿ç”¨ä¼˜åŒ–åçš„çª—å£å‚æ•°
        optimal_window = window or self.get_optimized_window('sma_20', 20)
        
        if 'Close' not in df.columns or len(df) < optimal_window:
            return pd.Series(0, index=df.index)
        
        try:
            sma = df['Close'].rolling(optimal_window).mean()
            return self.safe_fillna(((df['Close'] / sma) - 1), df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_sma_50(self, df: pd.DataFrame, window: int = None, **kwargs) -> pd.Series:
        """ç®€å•ç§»åŠ¨å¹³å‡çº¿(å¯ä¼˜åŒ–å‚æ•°)"""
        # [OK] PERFORMANCE FIX: ä½¿ç”¨ä¼˜åŒ–åçš„çª—å£å‚æ•°
        optimal_window = window or self.get_optimized_window('sma_50', 50)
        
        if 'Close' not in df.columns or len(df) < optimal_window:
            # å¦‚æœæ•°æ®ä¸è¶³ï¼Œå›é€€åˆ°å¯ç”¨æ•°æ®çš„å‡çº¿
            available_days = min(20, len(df))
            if available_days >= 10:
                sma = df['Close'].rolling(available_days).mean()
                return ((df['Close'] / sma) - 1)
            else:
                return pd.Series(0, index=df.index)
        
        try:
            sma = df['Close'].rolling(optimal_window).mean()
            return self.safe_fillna(((df['Close'] / sma) - 1), df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_rsi(self, df: pd.DataFrame, window: int = None, **kwargs) -> pd.Series:
        """ç›¸å¯¹å¼ºå¼±æŒ‡æ•°(RSI,å¯ä¼˜åŒ–å‚æ•°)"""
        # [OK] PERFORMANCE FIX: ä½¿ç”¨ä¼˜åŒ–åçš„çª—å£å‚æ•°
        optimal_window = window or self.get_optimized_window('rsi', 14)
        
        if 'Close' not in df.columns or len(df) < optimal_window + 1:
            return pd.Series(0, index=df.index)
        
        try:
            delta = df['Close'].diff()
            gain = delta.where(delta > 0, 0).rolling(optimal_window).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(optimal_window).mean()
            
            rs = gain / loss.replace(0, np.nan)  # é¿å…é™¤é›¶
            rsi = 100 - (100 / (1 + rs))
            
            # è½¬æ¢ä¸ºæ ‡å‡†åŒ–ä¿¡å·ï¼š-1åˆ°1èŒƒå›´
            rsi_normalized = (rsi - 50) / 50
            return self.safe_fillna(rsi_normalized, df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_bb_position(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """å¸ƒæ—å¸¦ä½ç½®"""
        if 'Close' not in df.columns or len(df) < 20:
            return pd.Series(0, index=df.index)
        
        try:
            sma = df['Close'].rolling(20).mean()
            std = df['Close'].rolling(20).std()
            upper_band = sma + (std * 2)
            lower_band = sma - (std * 2)
            
            # è®¡ç®—ä»·æ ¼åœ¨å¸ƒæ—å¸¦ä¸­çš„ç›¸å¯¹ä½ç½® (0-1)
            bb_position = (df['Close'] - lower_band) / (upper_band - lower_band)
            
            # è½¬æ¢ä¸ºæ ‡å‡†åŒ–ä¿¡å·ï¼š-1åˆ°1èŒƒå›´ (0.5æ˜ å°„åˆ°0)
            return self.safe_fillna(((bb_position - 0.5) * 2), df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_macd(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """MACDæŒ‡æ ‡"""
        if 'Close' not in df.columns or len(df) < 26:
            return pd.Series(0, index=df.index)
        
        try:
            exp1 = df['Close'].ewm(span=12).mean()
            exp2 = df['Close'].ewm(span=26).mean()
            macd = exp1 - exp2
            
            # æ ‡å‡†åŒ–MACDå€¼
            return self.safe_fillna((macd / df['Close']), df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_macd_signal(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """MACDä¿¡å·çº¿"""
        if 'Close' not in df.columns or len(df) < 35:
            return pd.Series(0, index=df.index)
        
        try:
            exp1 = df['Close'].ewm(span=12).mean()
            exp2 = df['Close'].ewm(span=26).mean()
            macd = exp1 - exp2
            signal = macd.ewm(span=9).mean()
            
            return self.safe_fillna((signal / df['Close']), df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_macd_histogram(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """MACDæŸ±çŠ¶å›¾ (MACD - Signal)"""
        if 'Close' not in df.columns or len(df) < 35:
            return pd.Series(0, index=df.index)
        
        try:
            exp1 = df['Close'].ewm(span=12).mean()
            exp2 = df['Close'].ewm(span=26).mean()
            macd = exp1 - exp2
            signal = macd.ewm(span=9).mean()
            histogram = macd - signal
            
            return self.safe_fillna((histogram / df['Close']), df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_price_momentum_5d(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """5æ—¥ä»·æ ¼åŠ¨é‡"""
        if 'Close' not in df.columns or len(df) < 6:
            return pd.Series(0, index=df.index)
        
        try:
            momentum_5d = df['Close'].pct_change(5)
            return self.safe_fillna(momentum_5d, df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_price_momentum_20d(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """20æ—¥ä»·æ ¼åŠ¨é‡"""
        if 'Close' not in df.columns or len(df) < 21:
            # å¦‚æœæ•°æ®ä¸è¶³ï¼Œä½¿ç”¨5æ—¥åŠ¨é‡
            return self._compute_price_momentum_5d(df, **kwargs)
        
        try:
            momentum_20d = df['Close'].pct_change(20)
            return self.safe_fillna(momentum_20d, df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_volume_ratio(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """æˆäº¤é‡æ¯”ç‡"""
        if 'Volume' not in df.columns or len(df) < 20:
            return pd.Series(0, index=df.index)
        
        try:
            volume_ma = df['Volume'].rolling(20).mean()
            volume_ratio = df['Volume'] / volume_ma.replace(0, np.nan)
            
            # å¯¹æ•°å˜æ¢ä»¥æ ‡å‡†åŒ–æç«¯å€¼
            return self.safe_fillna(np.log1p(volume_ratio - 1), df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_volume_trend(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """æˆäº¤é‡è¶‹åŠ¿å› å­"""
        if 'Volume' not in df.columns or len(df) < 10:
            return pd.Series(0, index=df.index)
        
        try:
            # è®¡ç®—æˆäº¤é‡çš„ç§»åŠ¨å¹³å‡å’Œè¶‹åŠ¿
            period = kwargs.get('period', 10)
            volume_ma_short = df['Volume'].rolling(period//2).mean()
            volume_ma_long = df['Volume'].rolling(period).mean()
            
            # æˆäº¤é‡è¶‹åŠ¿ = çŸ­æœŸå‡é‡/é•¿æœŸå‡é‡ - 1
            volume_trend = (volume_ma_short / volume_ma_long.replace(0, np.nan)) - 1
            return self.safe_fillna(volume_trend, df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_gap_momentum(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """è·³ç©ºåŠ¨é‡å› å­"""
        if 'Open' not in df.columns or 'Close' not in df.columns or len(df) < 2:
            return pd.Series(0, index=df.index)
        
        try:
            # è®¡ç®—è·³ç©ºï¼šä»Šæ—¥å¼€ç›˜ä»· vs æ˜¨æ—¥æ”¶ç›˜ä»·
            prev_close = df['Close'].shift(1)
            gap = (df['Open'] - prev_close) / prev_close.replace(0, np.nan)
            
            # ç´¯è®¡è·³ç©ºåŠ¨é‡
            period = kwargs.get('period', 10)
            gap_momentum = gap.rolling(period).sum()
            return self.safe_fillna(gap_momentum, df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_intraday_momentum(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """æ—¥å†…åŠ¨é‡å› å­"""
        if 'Open' not in df.columns or 'Close' not in df.columns or len(df) < 1:
            return pd.Series(0, index=df.index)
        
        try:
            # æ—¥å†…åŠ¨é‡ = (æ”¶ç›˜ä»· - å¼€ç›˜ä»·) / å¼€ç›˜ä»·
            intraday_return = (df['Close'] - df['Open']) / df['Open'].replace(0, np.nan)
            
            # ç§»åŠ¨å¹³å‡å¹³æ»‘
            period = kwargs.get('period', 5)
            intraday_momentum = intraday_return.rolling(period).mean()
            return self.safe_fillna(intraday_momentum, df)
        except:
            return pd.Series(0, index=df.index)
    
    # ========== [HOT] NEW: Real Polygon Trainingé£é™©æŒ‡æ ‡é›†æˆ ==========
    
    def _compute_max_drawdown(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """æœ€å¤§å›æ’¤"""
        if 'Close' not in df.columns or len(df) < 10:
            return pd.Series(0, index=df.index)
        
        try:
            returns = df['Close'].pct_change()
            cumulative = (1 + returns).cumprod()
            rolling_max = cumulative.expanding().max()
            drawdown = (cumulative - rolling_max) / rolling_max
            
            # ä½¿ç”¨æ»šåŠ¨çª—å£è®¡ç®—æœ€å¤§å›æ’¤
            max_drawdown = drawdown.rolling(20, min_periods=5).min()
            
            # è¿”å›å›æ’¤çš„ç»å¯¹å€¼ä½œä¸ºé£é™©ä¿¡å·
            return self.safe_fillna(abs(max_drawdown), df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_sharpe_ratio(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """å¤æ™®æ¯”ç‡ï¼ˆæ»šåŠ¨è®¡ç®—ï¼‰"""
        if 'Close' not in df.columns or len(df) < 20:
            return pd.Series(0, index=df.index)
        
        try:
            returns = df['Close'].pct_change()            
            # æ»šåŠ¨è®¡ç®—å¤æ™®æ¯”ç‡ (å‡è®¾æ— é£é™©åˆ©ç‡ä¸ºå¹´åŒ–2%)
            risk_free_daily = 0.02 / 252
            excess_returns = returns - risk_free_daily
            
            rolling_mean = excess_returns.rolling(20, min_periods=10).mean()
            rolling_std = returns.rolling(20, min_periods=10).std()
            
            sharpe = rolling_mean / rolling_std.replace(0, np.nan) * np.sqrt(252)
            
            # æ ‡å‡†åŒ–å¤æ™®æ¯”ç‡åˆ°åˆç†èŒƒå›´
            return self.safe_fillna(np.tanh(sharpe / 2), df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_var_95(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """95%ç½®ä¿¡åº¦çš„é£é™©ä»·å€¼(VaR)"""
        if 'Close' not in df.columns or len(df) < 20:
            return pd.Series(0, index=df.index)
        
        try:
            returns = df['Close'].pct_change()            
            # æ»šåŠ¨è®¡ç®—95% VaR
            var_95 = returns.rolling(20, min_periods=10).quantile(0.05)
            
            # è¿”å›VaRçš„ç»å¯¹å€¼ä½œä¸ºé£é™©æŒ‡æ ‡
            return self.safe_fillna(abs(var_95), df)
        except:
            return pd.Series(0, index=df.index)
    
    # ========== T+10 Adapted Factors ==========
    
    def _compute_reversal_10(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """10-day reversal adapted for T+10 prediction"""
        try:
            g = df.groupby('ticker')['Close']
            # Using T-1 to T-11 data for 10-day reversal, adapted for T+10 prediction
            rev = -(g.shift(1) / g.shift(11) - 1.0)
            rev_ema = rev.groupby(df['ticker']).transform(lambda x: self.ema_decay(x, span=decay))
            return rev_ema.fillna(0.0)
        except Exception as e:
            logger.warning(f"10-day reversal computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_market_sentiment_10d(self, df: pd.DataFrame, windows: List[int] = [10, 22], 
                                     decay: int = 25) -> pd.Series:
        """Market sentiment adapted for T+10 prediction"""
        try:
            # Create synthetic sentiment based on price momentum and volatility
            # Longer window for T+10 prediction
            window = windows[0] if windows else 10
            
            g = df.groupby('ticker')['Close']
            returns = g.pct_change()  # T-1æ»åç”±ç»Ÿä¸€é…ç½®æ§åˆ¶
            
            # Sentiment = momentum (positive) - volatility (negative)  
            momentum = returns.rolling(window).mean()
            volatility = returns.rolling(window).std()
            
            sentiment = momentum - 0.5 * volatility
            sentiment_ema = sentiment.groupby(df['ticker']).transform(
                lambda x: self.ema_decay(x, span=decay)
            )
            
            return sentiment_ema.fillna(0.0)
        except Exception as e:
            logger.warning(f"Market sentiment 10d computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_sentiment_momentum_10d(self, df: pd.DataFrame, windows: List[int] = [10, 22],
                                       decay: int = 30) -> pd.Series:
        """Sentiment momentum adapted for T+10 prediction"""
        try:
            # Sentiment momentum = change in sentiment over 10-day period
            window = windows[0] if windows else 10
            
            g = df.groupby('ticker')['Close']
            returns = g.pct_change()  # T-1æ»åç”±ç»Ÿä¸€é…ç½®æ§åˆ¶
            
            # Calculate base sentiment
            momentum = returns.rolling(window).mean()
            volatility = returns.rolling(window).std()
            sentiment = momentum - 0.5 * volatility
            
            # Sentiment momentum = current sentiment - past sentiment
            sentiment_momentum = sentiment - sentiment.shift(window)
            sentiment_momentum_ema = sentiment_momentum.groupby(df['ticker']).transform(
                lambda x: self.ema_decay(x, span=decay)
            )
            
            return sentiment_momentum_ema.fillna(0.0)
        except Exception as e:
            logger.warning(f"Sentiment momentum 10d computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_macd_10d(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """MACD adapted for T+10 prediction with longer periods"""
        try:
            g = df.groupby('ticker')['Close']
            close_prices = g.shift(1)  # T-1 prices
            
            # Longer periods for T+10 prediction: 10 and 20 days instead of 12,26
            fast_period = 10
            slow_period = 20
            signal_period = 9
            
            # Calculate EMAs
            ema_fast = close_prices.ewm(span=fast_period).mean()
            ema_slow = close_prices.ewm(span=slow_period).mean()
            
            # MACD line
            macd_line = ema_fast - ema_slow
            
            # Signal line
            signal_line = macd_line.ewm(span=signal_period).mean()
            
            # MACD histogram (final signal)
            macd_histogram = macd_line - signal_line
            
            # Apply decay
            macd_result = macd_histogram.groupby(df['ticker']).transform(
                lambda x: self.ema_decay(x, span=decay)
            )
            
            return macd_result.fillna(0.0)
        except Exception as e:
            logger.warning(f"MACD 10d computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    def _compute_bb_position_10d(self, df: pd.DataFrame, windows: List[int], decay: int) -> pd.Series:
        """Bollinger Band position adapted for T+10 prediction"""
        try:
            window = windows[0] if windows else 10
            g = df.groupby('ticker')['Close']
            close_prices = g.shift(1)  # T-1 prices
            
            # Calculate Bollinger Bands with longer period
            sma = close_prices.rolling(window).mean()
            std = close_prices.rolling(window).std()
            
            upper_band = sma + (2.0 * std)
            lower_band = sma - (2.0 * std)
            
            # Position within bands: 0.5 = middle, 1.0 = upper, 0.0 = lower
            bb_position = (close_prices - lower_band) / (upper_band - lower_band)
            bb_position = bb_position.clip(0, 1)  # Clamp to [0,1] range
            
            # Center around 0 for factor signal: -0.5 to +0.5
            bb_signal = bb_position - 0.5
            
            # Apply decay
            bb_result = bb_signal.groupby(df['ticker']).transform(
                lambda x: self.ema_decay(x, span=decay)
            )
            
            return bb_result.fillna(0.0)
        except Exception as e:
            logger.warning(f"Bollinger Band position 10d computation failed: {e}")
            return pd.Series(0.0, index=df.index)
    
    # ========== FOCUSED 25 FACTORS COMPUTATION METHODS ==========
    
    # Momentum factors (3/25)
    def _compute_momentum_10d_ex1(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """10-day price momentum"""
        if 'Close' not in df.columns or len(df) < 11:
            return pd.Series(0, index=df.index)
        try:
            # 10-day momentum: (price_t - price_t-10) / price_t-10
            momentum_10d = df['Close'].pct_change(10)
            return self.safe_fillna(momentum_10d, df)
        except:
            return pd.Series(0, index=df.index)

    # Keep old function name for backward compatibility
    def _compute_momentum_10d(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """Deprecated: Use _compute_momentum_10d_ex1 instead"""
        return self._compute_momentum_10d_ex1(df, **kwargs)
    
    def _compute_momentum_20d(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """20-day price momentum"""
        if 'Close' not in df.columns or len(df) < 21:
            return pd.Series(0, index=df.index)
        try:
            momentum_20d = df['Close'].pct_change(20)
            return self.safe_fillna(momentum_20d, df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_momentum_reversal_short(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """Short-term momentum reversal signal"""
        if 'Close' not in df.columns or len(df) < 6:
            return pd.Series(0, index=df.index)
        try:
            momentum_1d = df['Close'].pct_change(1)
            momentum_5d = df['Close'].pct_change(5)
            reversal_signal = -momentum_1d * momentum_5d
            return self.safe_fillna(reversal_signal, df)
        except:
            return pd.Series(0, index=df.index)

    def _compute_reversal_1d(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """1-day reversal: negative of 1-day return"""
        if 'Close' not in df.columns or len(df) < 2:
            return pd.Series(0, index=df.index)
        try:
            reversal_1d = -df['Close'].pct_change(1)
            return self.safe_fillna(reversal_1d, df)
        except:
            return pd.Series(0, index=df.index)

    def _compute_mom_accel_5_2(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """Momentum acceleration: 5-day momentum - 2-day momentum"""
        if 'Close' not in df.columns or len(df) < 6:
            return pd.Series(0, index=df.index)
        try:
            mom_5d = df['Close'].pct_change(5)
            mom_2d = df['Close'].pct_change(2)
            mom_accel = mom_5d - mom_2d
            return self.safe_fillna(mom_accel, df)
        except:
            return pd.Series(0, index=df.index)

    # Mean reversion factors (4/25)
    def _compute_price_to_ma20(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """Price relative to 20-day moving average"""
        if 'Close' not in df.columns or len(df) < 20:
            return pd.Series(0, index=df.index)
        try:
            ma20 = df['Close'].rolling(20).mean()
            price_to_ma = (df['Close'] / ma20) - 1
            return self.safe_fillna(price_to_ma, df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_bollinger_squeeze(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """Bollinger Band volatility squeeze
        ğŸ”¥ FIX: Shift for pre-market prediction (use previous day's volatility ratio)
        """
        if 'Close' not in df.columns or len(df) < 20:
            return pd.Series(0, index=df.index)
        try:
            # ğŸ”¥ FIX: Shift for pre-market prediction
            std_20 = df['Close'].rolling(20).std().shift(1)
            std_5 = df['Close'].rolling(5).std().shift(1)
            squeeze = std_5 / (std_20 + 1e-8)
            return self.safe_fillna(squeeze, df)
        except:
            return pd.Series(0, index=df.index)

    # Volume factors (2/25)
    def _compute_obv_momentum(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """On-Balance Volume momentum"""
        if 'Close' not in df.columns or 'Volume' not in df.columns or len(df) < 10:
            return pd.Series(0, index=df.index)
        try:
            price_change = df['Close'].diff()
            obv = (price_change.apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0)) * df['Volume']).cumsum()
            obv_momentum = obv.pct_change(10)
            return self.safe_fillna(obv_momentum, df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_ad_line(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """Accumulation/Distribution Line"""
        if not all(col in df.columns for col in ['High', 'Low', 'Close', 'Volume']) or len(df) < 5:
            return pd.Series(0, index=df.index)
        try:
            clv = ((df['Close'] - df['Low']) - (df['High'] - df['Close'])) / (df['High'] - df['Low'] + 1e-8)
            ad_line = (clv * df['Volume']).cumsum()
            ad_momentum = ad_line.pct_change(5)
            return self.safe_fillna(ad_momentum, df)
        except:
            return pd.Series(0, index=df.index)
    
    # Volatility factors (2/25)
    def _compute_atr_20d(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """20-day Average True Range"""
        if not all(col in df.columns for col in ['High', 'Low', 'Close']) or len(df) < 20:
            return pd.Series(0, index=df.index)
        try:
            tr1 = df['High'] - df['Low']
            tr2 = abs(df['High'] - df['Close'].shift(1))
            tr3 = abs(df['Low'] - df['Close'].shift(1))
            true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
            atr = true_range.rolling(20).mean()
            atr_normalized = atr / (df['Close'] + 1e-8)
            return self.safe_fillna(atr_normalized, df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_atr_ratio(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """ATR ratio (5d/20d expansion/contraction)"""
        if not all(col in df.columns for col in ['High', 'Low', 'Close']) or len(df) < 20:
            return pd.Series(0, index=df.index)
        try:
            tr1 = df['High'] - df['Low']
            tr2 = abs(df['High'] - df['Close'].shift(1))
            tr3 = abs(df['Low'] - df['Close'].shift(1))
            true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
            atr_5 = true_range.rolling(5).mean()
            atr_20 = true_range.rolling(20).mean()
            atr_ratio = atr_5 / (atr_20 + 1e-8)
            return self.safe_fillna(atr_ratio, df)
        except:
            return pd.Series(0, index=df.index)
    
    # Technical factors (4/25) - stoch_k, cci, mfi already exist, just need to add them
    def _compute_stoch_k(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """Stochastic %K oscillator"""
        if not all(col in df.columns for col in ['High', 'Low', 'Close']) or len(df) < 14:
            return pd.Series(0, index=df.index)
        try:
            lowest_low = df['Low'].rolling(14).min()
            highest_high = df['High'].rolling(14).max()
            stoch_k = 100 * (df['Close'] - lowest_low) / (highest_high - lowest_low + 1e-8)
            stoch_k_normalized = (stoch_k - 50) / 50
            return self.safe_fillna(stoch_k_normalized, df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_cci(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """Commodity Channel Index"""
        if not all(col in df.columns for col in ['High', 'Low', 'Close']) or len(df) < 20:
            return pd.Series(0, index=df.index)
        try:
            typical_price = (df['High'] + df['Low'] + df['Close']) / 3
            sma_tp = typical_price.rolling(20).mean()
            mad = typical_price.rolling(20).apply(lambda x: abs(x - x.mean()).mean())
            cci = (typical_price - sma_tp) / (0.015 * mad + 1e-8)
            cci_normalized = cci / 100
            return self.safe_fillna(cci_normalized, df)
        except:
            return pd.Series(0, index=df.index)
    
    # Fundamental factors (10/25)
    def _compute_market_cap_proxy(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """Market capitalization proxy"""
        if 'Close' not in df.columns:
            return pd.Series(0, index=df.index)
        try:
            # Use price as proxy for market cap (relative sizing)
            market_cap_proxy = np.log(df['Close'] + 1)
            return self.safe_fillna(market_cap_proxy, df)
        except:
            return pd.Series(0, index=df.index)
    
    # REMOVED: _compute_ivol_60d (multicollinearity with stability_score, r=-0.95, VIF=10.4)

    def _compute_blowoff_ratio(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """Blowoff ratio: max 5D log-return / (Ïƒ14 + Îµ)"""
        if 'Close' not in df.columns or len(df) < 14:
            return pd.Series(0, index=df.index)
        try:
            # Calculate log returns
            log_returns = np.log(df['Close'] / df['Close'].shift(1))
            # Get maximum 5-day log return
            max_5d_log_return = log_returns.rolling(5, min_periods=1).max()
            # Calculate 14-day volatility (standard deviation of returns)
            returns = df['Close'].pct_change()
            vol_14d = returns.rolling(14, min_periods=7).std()
            # Compute blowoff ratio
            epsilon = 1e-8
            blowoff_ratio = max_5d_log_return / (vol_14d + epsilon)
            return self.safe_fillna(blowoff_ratio, df)
        except:
            return pd.Series(0, index=df.index)

    def _compute_stability_score(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """Stability score: 1 / (Ïƒ20 + Îµ)"""
        if 'Close' not in df.columns or len(df) < 20:
            return pd.Series(0, index=df.index)
        try:
            # Calculate returns
            returns = df['Close'].pct_change()
            # Calculate 20-day volatility (standard deviation)
            vol_20d = returns.rolling(20, min_periods=10).std()
            # Compute stability score
            epsilon = 1e-8
            stability_score = 1.0 / (vol_20d + epsilon)
            return self.safe_fillna(stability_score, df)
        except:
            return pd.Series(0, index=df.index)

    def _compute_liquidity_factor(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """Liquidity from volume patterns"""
        if 'Volume' not in df.columns or len(df) < 10:
            return pd.Series(0, index=df.index)
        try:
            volume_ma = df['Volume'].rolling(10).mean()
            volume_std = df['Volume'].rolling(10).std()
            liquidity_factor = np.log(volume_ma + 1) / (volume_std + 1e-8)
            return self.safe_fillna(liquidity_factor, df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_growth_proxy(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """Growth factor from momentum"""
        if 'Close' not in df.columns or len(df) < 20:
            return pd.Series(0, index=df.index)
        try:
            momentum_5d = df['Close'].pct_change(5)
            momentum_20d = df['Close'].pct_change(20)
            growth_proxy = momentum_5d + momentum_20d
            return self.safe_fillna(growth_proxy, df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_profitability_momentum(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """Profitability momentum"""
        if 'Close' not in df.columns or len(df) < 20:
            return pd.Series(0, index=df.index)
        try:
            returns = df['Close'].pct_change()
            cumulative_returns = (1 + returns).rolling(20).apply(lambda x: x.prod()) - 1
            profitability_momentum = cumulative_returns
            return self.safe_fillna(profitability_momentum, df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_growth_acceleration(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """Growth acceleration"""
        if 'Close' not in df.columns or len(df) < 10:
            return pd.Series(0, index=df.index)
        try:
            momentum_5d = df['Close'].pct_change(5)
            momentum_acceleration = momentum_5d.diff(5)
            return self.safe_fillna(momentum_acceleration, df)
        except:
            return pd.Series(0, index=df.index)
    
    def _compute_quality_consistency(self, df: pd.DataFrame, **kwargs) -> pd.Series:
        """Quality consistency measure"""
        if 'Close' not in df.columns or len(df) < 20:
            return pd.Series(0, index=df.index)
        try:
            returns = df['Close'].pct_change()
            rolling_sharpe = returns.rolling(20).mean() / (returns.rolling(20).std() + 1e-8)
            quality_consistency = rolling_sharpe
            return self.safe_fillna(quality_consistency, df)
        except:
            return pd.Series(0, index=df.index)
    
    # ========== æ•°æ®è´¨é‡æŠ¥å‘Šæ–¹æ³• ==========
    
    def get_quality_summary(self) -> Dict[str, Any]:
        """
        è·å–æ‰€æœ‰Alphaå› å­çš„è´¨é‡æ±‡æ€»
        
        Returns:
            è´¨é‡æ±‡æ€»å­—å…¸
        """
        if not self.quality_reports:
            return {"message": "æ²¡æœ‰å¯ç”¨çš„è´¨é‡æŠ¥å‘Š"}
        
        summary = {
            "total_factors": len(self.quality_reports),
            "factors_with_errors": 0,
            "factors_with_warnings": 0,
            "average_metrics": {},
            "factor_details": {}
        }
        
        # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
        all_missing_ratios = []
        all_coverage_ratios = []
        all_outlier_ratios = []
        all_distribution_scores = []
        
        for factor_name, report in self.quality_reports.items():
            # ç»Ÿè®¡é”™è¯¯å’Œè­¦å‘Š
            if report.errors:
                summary["factors_with_errors"] += 1
            if report.warnings:
                summary["factors_with_warnings"] += 1
            
            # æ”¶é›†æŒ‡æ ‡
            all_missing_ratios.append(report.output_quality.missing_ratio)
            all_coverage_ratios.append(report.output_quality.coverage_ratio)
            all_outlier_ratios.append(report.output_quality.outlier_ratio)
            all_distribution_scores.append(report.output_quality.distribution_score)
            
            # è®°å½•æ¯ä¸ªå› å­çš„è¯¦ç»†ä¿¡æ¯
            summary["factor_details"][factor_name] = {
                "missing_ratio": f"{report.output_quality.missing_ratio:.2%}",
                "coverage_ratio": f"{report.output_quality.coverage_ratio:.2%}",
                "outlier_ratio": f"{report.output_quality.outlier_ratio:.2%}",
                "distribution_score": f"{report.output_quality.distribution_score:.2f}",
                "errors": len(report.errors),
                "warnings": len(report.warnings)
            }
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        if all_missing_ratios:
            summary["average_metrics"] = {
                "avg_missing_ratio": f"{np.mean(all_missing_ratios):.2%}",
                "avg_coverage_ratio": f"{np.mean(all_coverage_ratios):.2%}",
                "avg_outlier_ratio": f"{np.mean(all_outlier_ratios):.2%}",
                "avg_distribution_score": f"{np.mean(all_distribution_scores):.2f}"
            }
        
        return summary
    
    def export_quality_report(self, output_file: str = None):
        """
        å¯¼å‡ºè´¨é‡æŠ¥å‘Šåˆ°æ–‡ä»¶
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºlogs/alpha_quality/quality_summary_{timestamp}.csv
        """
        if not self.quality_reports:
            logger.warning("æ²¡æœ‰è´¨é‡æŠ¥å‘Šå¯å¯¼å‡º")
            return
        
        # ç”Ÿæˆé»˜è®¤æ–‡ä»¶å
        if output_file is None:
            timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"logs/alpha_quality/quality_summary_{timestamp}.csv"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        output_path = pd.io.common.get_filepath_or_buffer(output_file, mode='w')[0]
        output_dir = os.path.dirname(output_path)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # æ„å»ºæ•°æ®æ¡†
        data = []
        for factor_name, report in self.quality_reports.items():
            data.append({
                'factor_name': factor_name,
                'timestamp': report.timestamp,
                'input_missing_ratio': report.input_quality.missing_ratio,
                'output_missing_ratio': report.output_quality.missing_ratio,
                'coverage_ratio': report.output_quality.coverage_ratio,
                'outlier_ratio': report.output_quality.outlier_ratio,
                'zero_ratio': report.output_quality.zero_ratio,
                'distribution_score': report.output_quality.distribution_score,
                'stability_score': report.output_quality.stability_score,
                'time_consistency': report.output_quality.time_consistency,
                'error_count': len(report.errors),
                'warning_count': len(report.warnings),
                'recommendations_count': len(report.recommendations)
            })
        
        df = pd.DataFrame(data)
        df.to_csv(output_file, index=False)
        logger.info(f"è´¨é‡æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {output_file}")
        
        return df
    
    def get_factor_recommendations(self, factor_name: str = None) -> Dict[str, List[str]]:
        """
        è·å–å› å­çš„ä¼˜åŒ–å»ºè®®
        
        Args:
            factor_name: å› å­åç§°ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰å› å­
            
        Returns:
            å»ºè®®å­—å…¸
        """
        recommendations = {}
        
        if factor_name:
            if factor_name in self.quality_reports:
                report = self.quality_reports[factor_name]
                recommendations[factor_name] = report.recommendations
        else:
            for fname, report in self.quality_reports.items():
                if report.recommendations:
                    recommendations[fname] = report.recommendations
        
        return recommendations
    
    def print_quality_dashboard(self):
        """
        æ‰“å°è´¨é‡ä»ªè¡¨æ¿
        """
        summary = self.get_quality_summary()
        
        print("\n" + "="*80)
        print("Alphaå› å­æ•°æ®è´¨é‡ä»ªè¡¨æ¿")
        print("="*80)
        
        print(f"\næ€»å› å­æ•°: {summary['total_factors']}")
        print(f"å­˜åœ¨é”™è¯¯çš„å› å­: {summary['factors_with_errors']}")
        print(f"å­˜åœ¨è­¦å‘Šçš„å› å­: {summary['factors_with_warnings']}")
        
        if summary.get('average_metrics'):
            print("\nå¹³å‡è´¨é‡æŒ‡æ ‡:")
            for metric, value in summary['average_metrics'].items():
                print(f"  {metric}: {value}")
        
        # æ‰¾å‡ºé—®é¢˜æœ€ä¸¥é‡çš„å› å­
        if summary.get('factor_details'):
            print("\néœ€è¦å…³æ³¨çš„å› å­:")
            problem_factors = []
            for fname, details in summary['factor_details'].items():
                if details['errors'] > 0 or details['warnings'] > 2:
                    problem_factors.append((fname, details['errors'], details['warnings']))
            
            if problem_factors:
                problem_factors.sort(key=lambda x: (x[1], x[2]), reverse=True)
                for fname, errors, warnings in problem_factors[:5]:
                    print(f"  {fname}: {errors}ä¸ªé”™è¯¯, {warnings}ä¸ªè­¦å‘Š")
            else:
                print("  æ‰€æœ‰å› å­è´¨é‡è‰¯å¥½")
        
        print("\n" + "="*80)
