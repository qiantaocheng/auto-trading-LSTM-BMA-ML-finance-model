#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BMA Ultra Enhanced é‡åŒ–åˆ†ææ¨¡å‹ V6 - ç”Ÿäº§å°±ç»ªå¢å¼ºç‰ˆ
ä¸“æ³¨äºé€‰è‚¡é¢„æµ‹çš„Alphaç­–ç•¥ã€Learning-to-Rankã€BMAæœºå™¨å­¦ä¹ ç³»ç»Ÿ

V6æ–°å¢åŠŸèƒ½ï¼ˆä¿®å¤æ‰€æœ‰å…³é”®é—®é¢˜ï¼‰:
- ä¿®å¤Purge/EmbargoåŒé‡éš”ç¦»é—®é¢˜ï¼ˆé€‰æ‹©å•ä¸€éš”ç¦»æ–¹æ³•ï¼‰
- é˜²æ³„æ¼Regimeæ£€æµ‹ï¼ˆä»…ä½¿ç”¨è¿‡æ»¤ï¼Œç¦ç”¨å¹³æ»‘ï¼‰
- T-5åˆ°T-0/T-1ç‰¹å¾æ»åä¼˜åŒ–ï¼ˆA/Bæµ‹è¯•é€‰æ‹©ï¼‰
- å› å­æ—ç‰¹å®šè¡°å‡åŠè¡°æœŸï¼ˆæ›¿ä»£ç»Ÿä¸€8å¤©è¡°å‡ï¼‰
- ä¼˜åŒ–æ—¶é—´è¡°å‡åŠè¡°æœŸï¼ˆ60-90å¤©è€Œé90-120å¤©ï¼‰
- ç”Ÿäº§å°±ç»ªé—¨ç¦ç³»ç»Ÿï¼ˆå…·ä½“IC/QLIKEé˜ˆå€¼ï¼‰
- åŒå‘¨å¢é‡è®­ç»ƒ+æœˆåº¦å…¨é‡é‡æ„
- çŸ¥è¯†ä¿ç•™ç³»ç»Ÿï¼ˆç‰¹å¾é‡è¦æ€§ç›‘æ§ï¼‰

æä¾›Açº§ç”Ÿäº§å°±ç»ªçš„é‡åŒ–äº¤æ˜“è§£å†³æ–¹æ¡ˆ
"""

# === STANDARD LIBRARY IMPORTS ===
import sys
import os
import json
import time
import logging
import warnings
import argparse
import tempfile
import importlib.util
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple, Union
from dataclasses import dataclass, field

# === THIRD-PARTY CORE LIBRARIES ===
import pandas as pd
import numpy as np
import yaml

# === PROJECT PATH SETUP ===
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# STRICT_IMPORTS removed - all imports are now strict by default

# === PRODUCTION-GRADE FIXES IMPORTS ===
PRODUCTION_FIXES_AVAILABLE = False
try:
    from unified_timing_registry import get_global_timing_registry, TimingEnforcer, TimingRegistry
    from enhanced_production_gate import create_enhanced_production_gate, EnhancedProductionGate
    from regime_smoothing_enforcer import RegimeSmoothingEnforcer, enforce_regime_no_smoothing_globally
    from sample_weight_unification import SampleWeightUnifier, unify_sample_weights_globally
    from cv_leakage_prevention import CVLeakagePreventer, prevent_cv_leakage_globally
    from unified_nan_handler import clean_nan_predictive_safe, UnifiedNaNHandler
    from factor_orthogonalization import orthogonalize_factors_predictive_safe, FactorOrthogonalizer
    from cross_sectional_standardization import standardize_cross_sectional_predictive_safe, CrossSectionalStandardizer
    PRODUCTION_FIXES_AVAILABLE = True
    print("[INFO] ç”Ÿäº§çº§ä¿®å¤ç³»ç»Ÿå¯¼å…¥æˆåŠŸï¼šæ—¶åºç»Ÿä¸€+é—¨ç¦å¢å¼º+æ³„éœ²é˜²æŠ¤")
except ImportError as e:
    print(f"[WARN] ç”Ÿäº§çº§ä¿®å¤ç³»ç»Ÿå¯¼å…¥å¤±è´¥: {e}")

# === ML ENHANCEMENT IMPORTS ===
ML_ENHANCEMENT_AVAILABLE = False
try:
    from ml_enhancement_integration import MLEnhancementSystem, MLEnhancementConfig
    # å…³é”®æ¨¡å—å¯¼å…¥
    from advanced_alpha_system_integrated import AdvancedAlphaSystem
    from alpha_config_enhanced import EnhancedAlphaConfig
    from alpha_ic_weighted_processor import ICWeightedAlphaProcessor, ICWeightedConfig
    from oof_ensemble_system import OOFEnsembleSystem, BMAWeightCalculator
    from unified_ic_calculator import UnifiedICCalculator, ICCalculationConfig
    from unified_oof_generator import UnifiedOOFGenerator, OOFConfig
    from professional_factor_library import ProfessionalFactorCalculator, FactorDecayConfig
    from realtime_performance_monitor import RealtimePerformanceMonitor, AlertThresholds
    from real_oos_manager import RealOOSManager, OOSConfig
    # Enhanced error handler removed per user request
    from daily_neutralization_pipeline import DailyNeutralizationPipeline, NeutralizationConfig
    from dynamic_factor_weighting import DynamicFactorWeighting, WeightingConfig
    from bma_dependency_management_fix import BMaDependencyManager, fix_dependencies
    from bma_exception_handling_fix import BMAExceptionHandler, handle_bma_exceptions
    # Import Learning to Rank module
    from learning_to_rank_bma import LearningToRankBMA
    ML_ENHANCEMENT_AVAILABLE = True
    print("[INFO] MLå¢å¼ºç³»ç»Ÿ+å…³é”®æ¨¡å—å¯¼å…¥æˆåŠŸï¼šç‰¹å¾é€‰æ‹©+è¶…å‚æ•°ä¼˜åŒ–+é›†æˆå­¦ä¹ +OOF+ICè®¡ç®—+ä¸“ä¸šå› å­åº“")
except ImportError as e:
    print(f"[WARN] MLå¢å¼ºç³»ç»Ÿ+å…³é”®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    # ğŸš¨ CRITICAL FIX: è®¾ç½®ç¼ºå¤±å˜é‡å¹¶æ·»åŠ ç”Ÿäº§å®‰å…¨æ£€æŸ¥
    AdvancedAlphaSystem = None
    EnhancedAlphaConfig = None
    ICWeightedAlphaProcessor = None
    OOFEnsembleSystem = None
    UnifiedICCalculator = None
    UnifiedOOFGenerator = None
    ProfessionalFactorCalculator = None
    RealtimePerformanceMonitor = None
    RealOOSManager = None
    EnhancedErrorHandler = None
    DailyNeutralizationPipeline = None
    DynamicFactorWeighting = None
    BMaDependencyManager = None
    BMAExceptionHandler = None
    LearningToRankBMA = None
    create_enhanced_config = None
    
    # ğŸ”¥ PRODUCTION SAFETY: è®°å½•ç¼ºå¤±çš„å…³é”®ä¾èµ–
    MISSING_CRITICAL_DEPENDENCIES = [
        'AdvancedAlphaSystem', 'ICWeightedAlphaProcessor', 'UnifiedICCalculator',
        'LearningToRankBMA', 'EnhancedErrorHandler'
    ]
    print(f"ğŸš¨ PRODUCTION WARNING: {len(MISSING_CRITICAL_DEPENDENCIES)} critical dependencies missing!")
    print("ç³»ç»Ÿå°†ä½¿ç”¨é™çº§æ¨¡å¼è¿è¡Œï¼Œé¢„æµ‹æ€§èƒ½å¯èƒ½ä¸‹é™")

# log_import_fallback function removed - no longer needed with strict imports

# === T+10 CONFIGURATION IMPORT ===
try:
    from bma_models.t10_config import T10_CONFIG, get_config
    T10_AVAILABLE = True
except ImportError as e:
    print(f"[WARN] T10 Configä¸å¯ç”¨: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    T10_AVAILABLE = False
    T10_CONFIG = None
    
    # åˆ›å»ºé»˜è®¤é…ç½®å‡½æ•°
    def get_config():
        return {
            'feature_lag': 5,
            'prediction_start': 10,
            'prediction_end': 10,
            'safety_gap': 2
        }

# === PROJECT SPECIFIC IMPORTS ===
try:
    from polygon_client import polygon_client as pc, download as polygon_download, Ticker as PolygonTicker
except ImportError as e:
    print(f"[WARN] Polygonå®¢æˆ·ç«¯ä¸å¯ç”¨: {e}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æº")
    pc = None
    polygon_download = None
    PolygonTicker = None
    POLYGON_AVAILABLE = False

# BMA Enhanced V6ç³»ç»Ÿå·²åˆ é™¤ - åŠŸèƒ½å®Œå…¨èå…¥ç»Ÿä¸€è·¯å¾„B

# å¯¼å…¥è‡ªé€‚åº”æƒé‡å­¦ä¹ ç³»ç»Ÿï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–ï¼‰
ADAPTIVE_WEIGHTS_AVAILABLE = False


# === SCIENTIFIC COMPUTING LIBRARIES ===
from scipy.stats import spearmanr, entropy
from scipy.optimize import minimize
from scipy import stats
import statsmodels.api as sm

# === MACHINE LEARNING LIBRARIES ===
from sklearn.model_selection import TimeSeriesSplit, GroupKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.isotonic import IsotonicRegression
from sklearn.covariance import LedoitWolf

# === UTILITY LIBRARIES ===
from dataclasses import dataclass, field
from collections import Counter
import gc
import psutil
import traceback
from functools import wraps
from contextlib import contextmanager

# === IMPORT OPTIMIZATION COMPLETE ===
# All imports have been organized into logical groups:
# - Standard library imports
# - Third-party core libraries  
# - Project-specific imports with fallbacks
# - Scientific computing libraries
# - Machine learning libraries
# - Utility libraries
# PCAå¤šé‡å…±çº¿æ€§æ¶ˆé™¤ç›¸å…³å¯¼å…¥
from sklearn.decomposition import PCA
from statsmodels.stats.outliers_influence import variance_inflation_factor
try:
    from fixed_purged_time_series_cv import FixedPurgedGroupTimeSeriesSplit as PurgedGroupTimeSeriesSplit, ValidationConfig, create_time_groups, validate_timesplit_integrity
    PURGED_CV_AVAILABLE = True
    PURGED_CV_VERSION = "FIXED"
except ImportError as e:
    print(f"[WARN] Purged Time Series CVä¸å¯ç”¨: {e}ï¼Œå›é€€åˆ°sklearn TimeSeriesSplit")
    PURGED_CV_AVAILABLE = False
    PURGED_CV_VERSION = "SKLEARN_FALLBACK"

# å¯è§†åŒ–
import matplotlib.pyplot as plt
try:
    import seaborn as sns
    plt.style.use('default')  # ä½¿ç”¨é»˜è®¤æ ·å¼è€Œä¸æ˜¯seaborn
except ImportError:
    sns = None
    plt.style.use('default')
except Exception as e:
    # å¤„ç†seabornæ ·å¼é—®é¢˜
    plt.style.use('default')
    warnings.filterwarnings('ignore', message='.*seaborn.*')

# å¯¼å…¥Alphaå¼•æ“ï¼ˆæ ¸å¿ƒç»„ä»¶ï¼‰
ALPHA_ENGINE_AVAILABLE = False
try:
    from enhanced_alpha_strategies import AlphaStrategiesEngine
    ALPHA_ENGINE_AVAILABLE = True
    print("[INFO] Alphaå¼•æ“æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"[WARN] Alphaå¼•æ“æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# LTRåŠŸèƒ½å·²æ•´åˆåˆ°BMA Enhancedç³»ç»Ÿä¸­
# LTRå¯ç”¨æ€§å°†åœ¨è¿è¡Œæ—¶æ£€æŸ¥LearningToRankBMAæ¨¡å—
LTR_AVAILABLE = ML_ENHANCEMENT_AVAILABLE  # ä¾èµ–äºMLå¢å¼ºç³»ç»Ÿ
if LTR_AVAILABLE:
    print("[INFO] LTRåŠŸèƒ½é€šè¿‡BMA Enhancedç³»ç»Ÿå¯ç”¨")
else:
    print("[WARN] LTRåŠŸèƒ½ä¸å¯ç”¨ï¼ŒMLå¢å¼ºç³»ç»ŸæœªåŠ è½½")

# æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨åŠŸèƒ½å·²ç§»é™¤ï¼ˆç”¨æˆ·è¦æ±‚åˆ é™¤ï¼‰
PORTFOLIO_OPTIMIZER_AVAILABLE = False

# è®¾ç½®å¢å¼ºæ¨¡å—å¯ç”¨æ€§ï¼ˆåªè¦æ ¸å¿ƒAlphaå¼•æ“å¯ç”¨å³ä¸ºå¯ç”¨ï¼‰
ENHANCED_MODULES_AVAILABLE = ALPHA_ENGINE_AVAILABLE

# å•ç‹¬å¯¼å…¥Regime Detectionæ¨¡å—ï¼ˆç‹¬ç«‹å¤„ç†ï¼‰
try:
    from market_regime_detector import MarketRegimeDetector, RegimeConfig
    from regime_aware_trainer import RegimeAwareTrainer, RegimeTrainingConfig
    from regime_aware_cv import RegimeAwareTimeSeriesCV, RegimeAwareCVConfig
    REGIME_DETECTION_AVAILABLE = True
    print("[INFO] Regime Detectionæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"[WARN] Regime Detectionæ¨¡å—ä¸å¯ç”¨: {e}ï¼Œç¦ç”¨regimeæ„ŸçŸ¥åŠŸèƒ½")
    REGIME_DETECTION_AVAILABLE = False
    MarketRegimeDetector = None
    RegimeAwareTrainer = None  
    RegimeAwareTimeSeriesCV = None

# ç»Ÿä¸€å¸‚åœºæ•°æ®ï¼ˆè¡Œä¸š/å¸‚å€¼/å›½å®¶ç­‰ï¼‰
try:
    from unified_market_data_manager import UnifiedMarketDataManager
    MARKET_MANAGER_AVAILABLE = True
except Exception:
    MARKET_MANAGER_AVAILABLE = False

# ä¸­æ€§åŒ–å·²ç»Ÿä¸€ç”±Alphaå¼•æ“å¤„ç†ï¼Œç§»é™¤é‡å¤ä¾èµ–

# å¯¼å…¥isotonicæ ¡å‡† (IsotonicRegressionå·²åœ¨ä¸Šæ–¹å¯¼å…¥ï¼Œæ­¤å¤„åªè®¾ç½®å¯ç”¨æ€§æ ‡å¿—)
if 'IsotonicRegression' in globals():
    ISOTONIC_AVAILABLE = True
else:
    print("[WARN] Isotonicå›å½’ä¸å¯ç”¨ï¼Œç¦ç”¨æ ¡å‡†åŠŸèƒ½")
    ISOTONIC_AVAILABLE = False

# è‡ªé€‚åº”åŠ æ ‘ä¼˜åŒ–å™¨å·²ç§»é™¤ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å‹è®­ç»ƒ
ADAPTIVE_OPTIMIZER_AVAILABLE = False

# é«˜çº§æ¨¡å‹
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError as e:
    print(f"[WARN] XGBoostä¸å¯ç”¨: {e}ï¼Œç¦ç”¨XGBooståŠŸèƒ½")
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError as e:
    print(f"[WARN] LightGBMä¸å¯ç”¨: {e}ï¼Œç¦ç”¨LightGBMåŠŸèƒ½")
    LIGHTGBM_AVAILABLE = False

# CatBoost removed due to compatibility issues
CATBOOST_AVAILABLE = False

# é…ç½®
warnings.filterwarnings('ignore')

# ä¿®å¤matplotlibç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
try:
    import matplotlib
    if hasattr(matplotlib, '__version__') and matplotlib.__version__ >= '3.4.0':
        try:
            plt.style.use('seaborn-v0_8')
        except OSError:
            # å¦‚æœseaborn-v0_8ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ ·å¼
            plt.style.use('default')
            print("[WARN] seaborn-v0_8æ ·å¼ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ ·å¼")
    else:
        plt.style.use('seaborn')
except Exception as e:
    print(f"[WARN] matplotlibæ ·å¼è®¾ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æ ·å¼")
    plt.style.use('default')

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
def setup_logger():
    """Setup logger with proper encoding to handle Unicode characters"""
    import sys
    
    # Configure logging with UTF-8 encoding
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    return logger

@dataclass
class BMAModelConfig:
    """BMAæ¨¡å‹é…ç½®ç±» - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰ç¡¬ç¼–ç å‚æ•°"""
    
    # æ•°æ®ä¸‹è½½é…ç½®
    max_risk_model_tickers: int = 50
    max_market_regime_tickers: int = 20
    max_alpha_data_tickers: int = 50
    
    # æ—¶é—´çª—å£é…ç½®
    risk_model_history_days: int = 300
    market_regime_history_days: int = 200
    alpha_data_history_days: int = 200
    
    # æŠ€æœ¯æŒ‡æ ‡é…ç½®
    beta_calculation_window: int = 60
    rsi_period: int = 14
    volatility_window: int = 20
    
    # æ‰¹å¤„ç†é…ç½®
    batch_size: int = 50
    api_delay: float = 0.12
    max_retries: int = 3
    
    # æ•°æ®è´¨é‡è¦æ±‚
    min_data_days: int = 20
    min_risk_model_days: int = 100
    
    # é»˜è®¤è‚¡ç¥¨æ± 
    default_tickers: List[str] = field(default_factory=lambda: [
        'AAPL', 'MSFT', 'AMZN', 'NVDA', 'GOOGL', 
        'TSLA', 'META', 'BRK-B', 'UNH', 'JNJ'
    ])
    
    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> 'BMAModelConfig':
        """ä»å­—å…¸åˆ›å»ºé…ç½®å¯¹è±¡"""
        return cls(**{k: v for k, v in config_dict.items() if k in cls.__dataclass_fields__})

logger = setup_logger()

# Record Purged CV version information
try:
    if PURGED_CV_AVAILABLE:
        logger.info(f"Purged Time Series CV version: {PURGED_CV_VERSION}")
    else:
        logger.warning("Using sklearn TimeSeriesSplit as fallback")
except Exception as e:
    logger.warning(f"Error logging CV version: {e}")

# å…¨å±€é…ç½®
# å†…å­˜ä¼˜åŒ–çš„æ ¸å¿ƒè‚¡ç¥¨æ± ï¼ˆæ›¿ä»£åŸæ¥æ•°åƒåªè‚¡ç¥¨çš„å†…å­˜æµªè´¹ï¼‰
DEFAULT_TICKER_LIST = [
    # FAANG + å¤§å‹ç§‘æŠ€è‚¡
    "AAPL", "MSFT", "GOOGL", "GOOG", "AMZN", "TSLA", "META", "NFLX", "NVDA", "ADBE",
    # é‡‘èè‚¡  
    "BRK-B", "JPM", "BAC", "WFC", "GS", "MS", "V", "MA", "AXP", "C",
    # åŒ»ç–—ä¿å¥
    "UNH", "JNJ", "PFE", "ABBV", "MRK", "TMO", "ABT", "ISRG", "DHR", "BMY",
    # æ¶ˆè´¹å“
    "HD", "WMT", "PG", "KO", "PEP", "MCD", "NKE", "SBUX", "TGT", "LOW",
    # å·¥ä¸š
    "BA", "CAT", "MMM", "GE", "HON", "RTX", "UPS", "DE", "UNP", "CSX",
    # èƒ½æº
    "XOM", "CVX", "COP", "EOG", "SLB", "PSX", "VLO", "MPC", "KMI", "OKE"
]

def _get_extended_ticker_list():
    """
    å»¶è¿ŸåŠ è½½æ‰©å±•è‚¡ç¥¨åˆ—è¡¨ï¼Œé¿å…å¯¼å…¥æ—¶å†…å­˜å¼€é”€
    
    Returns:
        list: æ‰©å±•çš„è‚¡ç¥¨åˆ—è¡¨ï¼ŒåŒ…å«æ›´å¤šè‚¡ç¥¨é€‰æ‹©
    """
    extended_list = DEFAULT_TICKER_LIST.copy()
    
    # ä¸­å‹è‚¡ç¥¨ï¼ˆæŒ‰éœ€æ·»åŠ ï¼‰
    mid_caps = [
        "ROKU", "ZM", "SNOW", "DDOG", "OKTA", "CRWD", "NET", "PLTR", "COIN",
        "RIVN", "LCID", "SOFI", "HOOD", "AFRM", "SQ", "PYPL", "SHOP", "UBER", "LYFT"
    ]
    
    # ä¼ ç»Ÿä»·å€¼è‚¡  
    value_stocks = [
        "BRK-A", "T", "VZ", "IBM", "INTC", "CSCO", "ORCL", "XOM", "CVX", "KO"
    ]
    
    extended_list.extend(mid_caps)
    extended_list.extend(value_stocks)
    
    return list(set(extended_list))  # å»é‡


# ğŸ”¥ å…¨å±€ç»Ÿä¸€æ—¶é—´é…ç½® - é˜²æ­¢æ•°æ®æ³„éœ²çš„å…³é”®é…ç½®
GLOBAL_UNIFIED_TEMPORAL_CONFIG = {
    'prediction_horizon_days': 10,  # T+10é¢„æµ‹
    'feature_lag_days': 5,           # T-5ç‰¹å¾
    'safety_gap_days': 1,            # å®‰å…¨é—´éš”
    'cv_gap_days': 11,               # CVé—´éš” = prediction_horizon + safety
    'cv_embargo_days': 11,           # CVç¦æ­¢æœŸ = cv_gap
    'min_total_gap_days': 15         # feature_lag + cv_gap = 5 + 11 = 16 > 15 âœ“
}

def validate_dependency_integrity() -> dict:
    """éªŒè¯å…³é”®ä¾èµ–å®Œæ•´æ€§ï¼Œé˜²æ­¢é™é»˜é™çº§"""
    missing_deps = []
    available_deps = []
    
    # æ£€æŸ¥å…³é”®æ¨¡å—
    critical_modules = {
        'AdvancedAlphaSystem': AdvancedAlphaSystem,
        'ICWeightedAlphaProcessor': ICWeightedAlphaProcessor,
        'UnifiedICCalculator': UnifiedICCalculator,
        'LearningToRankBMA': LearningToRankBMA,
        'EnhancedErrorHandler': EnhancedErrorHandler
    }
    
    for name, module in critical_modules.items():
        if module is None:
            missing_deps.append(name)
        else:
            available_deps.append(name)
    
    integrity_status = {
        'available_modules': available_deps,
        'missing_modules': missing_deps,
        'integrity_score': len(available_deps) / len(critical_modules),
        'production_ready': len(missing_deps) == 0,
        'degraded_mode': len(missing_deps) > 0 and len(available_deps) > 0,
        'critical_failure': len(available_deps) == 0
    }
    
    return integrity_status

def validate_temporal_configuration(config: dict = None) -> dict:
    """
    éªŒè¯å’Œæ ‡å‡†åŒ–æ—¶é—´é…ç½®ï¼Œé˜²æ­¢æ•°æ®æ³„éœ²
    
    Args:
        config: å¯é€‰çš„è‡ªå®šä¹‰é…ç½®
        
    Returns:
        éªŒè¯è¿‡çš„æ—¶é—´é…ç½®å­—å…¸
        
    Raises:
        ValueError: å¦‚æœé…ç½®ä¸å®‰å…¨
    """
    if config is None:
        config = GLOBAL_UNIFIED_TEMPORAL_CONFIG.copy()
    
    # éªŒè¯å¿…éœ€å­—æ®µ
    required_fields = ['prediction_horizon_days', 'feature_lag_days', 'cv_gap_days', 'cv_embargo_days']
    for field in required_fields:
        if field not in config:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„æ—¶é—´é…ç½®å­—æ®µ: {field}")
    
    # éªŒè¯æ—¶é—´å®‰å…¨æ€§
    total_gap = config['feature_lag_days'] + config['cv_gap_days']
    min_safe_gap = config.get('min_total_gap_days', 15)
    
    if total_gap < min_safe_gap:
        raise ValueError(f"æ—¶é—´é…ç½®ä¸å®‰å…¨: æ€»é—´éš”{total_gap}å¤© < æœ€å°è¦æ±‚{min_safe_gap}å¤©ï¼Œå­˜åœ¨æ•°æ®æ³„éœ²é£é™©")
    
    # éªŒè¯CVå‚æ•°ä¸€è‡´æ€§
    if config['cv_gap_days'] != config['cv_embargo_days']:
        logger.warning(f"âš ï¸ CVå‚æ•°ä¸ä¸€è‡´: gap={config['cv_gap_days']} != embargo={config['cv_embargo_days']}")
        # ä½¿ç”¨è¾ƒå¤§å€¼ç¡®ä¿å®‰å…¨
        safe_value = max(config['cv_gap_days'], config['cv_embargo_days'])
        config['cv_gap_days'] = safe_value
        config['cv_embargo_days'] = safe_value
        logger.info(f"âœ… å·²è°ƒæ•´CVå‚æ•°ä¸ºå®‰å…¨å€¼: {safe_value}å¤©")
    
    return config

@dataclass
class ModuleThresholds:
    """æ¨¡å—å¯ç”¨é˜ˆå€¼é…ç½®"""
    # ç¨³å¥ç‰¹å¾é€‰æ‹©ï¼ˆå¿…å¼€ï¼‰
    robust_feature_min_samples: int = 300
    robust_feature_target_count: Tuple[int, int] = (12, 20)
    
    # Isotonicæ ¡å‡†ï¼ˆå¿…å¼€ï¼‰
    isotonic_min_val_samples: int = 200
    isotonic_monotony_test: bool = True
    
    # ä¼ ç»ŸMLå¤´ï¼ˆå¿…å¼€ï¼‰
    traditional_min_oof_coverage: float = 0.30
    
    # LTRï¼ˆæ¡ä»¶å¯ç”¨ï¼‰
    ltr_min_daily_group_size: int = 20
    ltr_min_date_coverage: float = 0.60
    ltr_min_oof_coverage: float = 0.40
    
    # Regime-awareï¼ˆæ¡ä»¶å¯ç”¨ï¼‰
    regime_min_samples_per_regime: int = 300
    regime_required_features: List[str] = field(default_factory=lambda: ['close', 'volume'])
    
    # Stackingï¼ˆé»˜è®¤å…³é—­ï¼‰
    stacking_min_base_models: int = 3
    stacking_min_ic_ir: float = 0.0
    stacking_min_oof_samples_ratio: float = 0.10
    stacking_max_correlation: float = 0.85  # è°ƒæ•´ç›¸å…³æ€§é—¨æ§›ä¸º0.85
    
    # V5ç³»ç»Ÿå·²è¢«V6å®Œå…¨æ›¿ä»£ï¼Œç›¸å…³é…ç½®å·²åˆ é™¤

@dataclass
class ModuleStatus:
    """æ¨¡å—çŠ¶æ€è·Ÿè¸ª"""
    enabled: bool = False
    degraded: bool = False
    reason: str = ""
    threshold_check: Dict[str, Any] = field(default_factory=dict)

class ModuleManager:
    """BMAæ¨¡å—ç®¡ç†å™¨ - æ ¹æ®é˜ˆå€¼æ™ºèƒ½å¯ç”¨/é™çº§"""
    
    def __init__(self, thresholds: ModuleThresholds = None):
        self.thresholds = thresholds or ModuleThresholds()
        self.status = {
            'robust_feature_selection': ModuleStatus(enabled=True),  # å¿…å¼€
            'isotonic_calibration': ModuleStatus(enabled=True),      # å¿…å¼€  
            'traditional_ml': ModuleStatus(enabled=True),            # å¿…å¼€
            'ltr_ranking': ModuleStatus(enabled=True),              # æ¡ä»¶å¯ç”¨
            'regime_aware': ModuleStatus(enabled=False),             # æ¡ä»¶å¯ç”¨
            'stacking': ModuleStatus(enabled=False)                  # é»˜è®¤å…³é—­
            # V5å¢å¼ºå·²åˆ é™¤ï¼Œå®Œå…¨ç”±V6ç³»ç»Ÿæ›¿ä»£
        }
        self.logger = logging.getLogger(__name__)
    
    def evaluate_module_eligibility(self, module_name: str, data_info: Dict[str, Any]) -> ModuleStatus:
        """è¯„ä¼°æ¨¡å—å¯ç”¨èµ„æ ¼"""
        status = ModuleStatus()
        
        if module_name == 'robust_feature_selection':
            # å¿…å¼€æ¨¡å—ï¼Œæ£€æŸ¥é™çº§æ¡ä»¶
            n_samples = data_info.get('n_samples', 0)
            n_features = data_info.get('n_features', 0)
            
            if n_samples >= self.thresholds.robust_feature_min_samples:
                status.enabled = True
                status.reason = f"æ ·æœ¬æ•°{n_samples}æ»¡è¶³è¦æ±‚"
            else:
                status.enabled = True
                status.degraded = True
                status.reason = f"æ ·æœ¬æ•°{n_samples}<{self.thresholds.robust_feature_min_samples}ï¼Œå¯ç”¨é™çº§ç‰ˆæœ¬"
            
            status.threshold_check = {
                'n_samples': n_samples,
                'threshold': self.thresholds.robust_feature_min_samples,
                'target_features': self.thresholds.robust_feature_target_count
            }
        
        elif module_name == 'isotonic_calibration':
            # å¿…å¼€æ¨¡å—
            val_samples = data_info.get('validation_samples', 0)
            
            if val_samples >= self.thresholds.isotonic_min_val_samples:
                status.enabled = True
                status.reason = f"éªŒè¯æ ·æœ¬{val_samples}æ»¡è¶³è¦æ±‚"
            else:
                status.enabled = True
                status.degraded = True
                status.reason = f"éªŒè¯æ ·æœ¬{val_samples}<{self.thresholds.isotonic_min_val_samples}ï¼Œä½¿ç”¨åˆ†ä½æ ¡å‡†"
            
            status.threshold_check = {
                'val_samples': val_samples,
                'threshold': self.thresholds.isotonic_min_val_samples
            }
        
        elif module_name == 'traditional_ml':
            # å¿…å¼€æ¨¡å—
            oof_coverage = data_info.get('oof_coverage', 0.0)
            
            if oof_coverage >= self.thresholds.traditional_min_oof_coverage:
                status.enabled = True
                status.reason = f"OOFè¦†ç›–ç‡{oof_coverage:.1%}æ»¡è¶³è¦æ±‚"
            else:
                status.enabled = True
                status.degraded = True
                status.reason = f"OOFè¦†ç›–ç‡{oof_coverage:.1%}<{self.thresholds.traditional_min_oof_coverage:.1%}ï¼Œä»…è¾“å‡ºrank"
        
        elif module_name == 'ltr_ranking':
            # æ¡ä»¶å¯ç”¨æ¨¡å—
            daily_group_sizes = data_info.get('daily_group_sizes', [])
            date_coverage = data_info.get('date_coverage_ratio', 0.0)
            oof_coverage = data_info.get('oof_coverage', 0.0)
            
            # æ£€æŸ¥æ‰€æœ‰æ¡ä»¶
            min_group_ok = min(daily_group_sizes) >= self.thresholds.ltr_min_daily_group_size if daily_group_sizes else False
            date_coverage_ok = date_coverage >= self.thresholds.ltr_min_date_coverage
            oof_coverage_ok = oof_coverage >= self.thresholds.ltr_min_oof_coverage
            
            if min_group_ok and date_coverage_ok and oof_coverage_ok:
                status.enabled = True
                status.reason = "æ‰€æœ‰LTRæ¡ä»¶æ»¡è¶³"
            else:
                status.enabled = False
                status.reason = f"ä¸æ»¡è¶³LTRæ¡ä»¶: ç»„è§„æ¨¡={min_group_ok}, æ—¥æœŸè¦†ç›–={date_coverage_ok}, OOF={oof_coverage_ok}"
            
            status.threshold_check = {
                'min_group_size': min(daily_group_sizes) if daily_group_sizes else 0,
                'date_coverage': date_coverage,
                'oof_coverage': oof_coverage,
                'thresholds': {
                    'min_group': self.thresholds.ltr_min_daily_group_size,
                    'date_cov': self.thresholds.ltr_min_date_coverage,
                    'oof_cov': self.thresholds.ltr_min_oof_coverage
                }
            }
        
        elif module_name == 'regime_aware':
            # æ¡ä»¶å¯ç”¨æ¨¡å—
            regime_samples = data_info.get('regime_samples', {})
            has_required_features = data_info.get('has_price_volume', False)
            regime_stability = data_info.get('regime_stability', 0.0)
            
            min_samples_ok = all(count >= self.thresholds.regime_min_samples_per_regime 
                               for count in regime_samples.values()) if regime_samples else False
            
            if min_samples_ok and has_required_features and regime_stability > 0.5:
                status.enabled = True
                status.reason = "Regime-awareæ¡ä»¶æ»¡è¶³"
            else:
                status.enabled = False
                status.degraded = True
                status.reason = "ä½¿ç”¨æ ·æœ¬åŠ æƒæ¨¡å¼æ›¿ä»£å¤šæ¨¡å‹"
            
            status.threshold_check = {
                'regime_samples': regime_samples,
                'has_price_volume': has_required_features,
                'stability': regime_stability
            }
        
        elif module_name == 'stacking':
            # é»˜è®¤å…³é—­æ¨¡å—
            base_models = data_info.get('base_models_ic_ir', {})
            oof_samples = data_info.get('oof_valid_samples', 0)
            total_samples = data_info.get('n_samples', 1)
            correlations = data_info.get('model_correlations', [])
            
            good_models = sum(1 for ic_ir in base_models.values() if ic_ir > self.thresholds.stacking_min_ic_ir)
            oof_ratio = oof_samples / total_samples
            max_corr = max(correlations) if correlations else 0.0
            
            if (good_models >= self.thresholds.stacking_min_base_models and 
                oof_ratio >= self.thresholds.stacking_min_oof_samples_ratio and
                max_corr < self.thresholds.stacking_max_correlation):
                status.enabled = True
                status.reason = "Stackingæ¡ä»¶æ»¡è¶³"
            else:
                status.enabled = False
                status.reason = f"ä½¿ç”¨IC/IRæ— è®­ç»ƒåŠ æƒ: å¥½æ¨¡å‹{good_models}, OOFæ¯”ä¾‹{oof_ratio:.1%}, æœ€å¤§ç›¸å…³{max_corr:.2f}"
        
        # V5è¯„ä¼°é€»è¾‘å·²åˆ é™¤ - V5ç³»ç»Ÿå·²è¢«V6å®Œå…¨æ›¿ä»£
        
        return status
    
    def update_module_status(self, data_info: Dict[str, Any]):
        """æ›´æ–°æ‰€æœ‰æ¨¡å—çŠ¶æ€"""
        for module_name in self.status.keys():
            self.status[module_name] = self.evaluate_module_eligibility(module_name, data_info)
            
        self.logger.info("æ¨¡å—çŠ¶æ€æ›´æ–°å®Œæˆ:")
        for name, status in self.status.items():
            icon = "âœ…" if status.enabled and not status.degraded else "âš ï¸" if status.degraded else "âŒ"
            self.logger.info(f"  {icon} {name}: {status.reason}")
    
    def is_enabled(self, module_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å—æ˜¯å¦å¯ç”¨"""
        return self.status.get(module_name, ModuleStatus()).enabled
    
    def is_degraded(self, module_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å—æ˜¯å¦é™çº§"""
        return self.status.get(module_name, ModuleStatus()).degraded
    
    def get_status_summary(self) -> Dict[str, Any]:
        """è·å–çŠ¶æ€æ‘˜è¦"""
        return {
            name: {
                'enabled': status.enabled,
                'degraded': status.degraded,
                'reason': status.reason,
                'checks': status.threshold_check
            }
            for name, status in self.status.items()
        }

class MemoryManager:
    """å†…å­˜ç®¡ç†å™¨ - é¢„é˜²å†…å­˜æ³„æ¼"""
    
    def __init__(self, memory_threshold: float = 80.0, auto_cleanup: bool = True):
        self.memory_threshold = memory_threshold
        self.auto_cleanup = auto_cleanup
        self.logger = logging.getLogger(__name__)
        
    def get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨ç‡"""
        try:
            return psutil.virtual_memory().percent
        except:
            return 0.0
    
    def check_memory_pressure(self) -> bool:
        """æ£€æŸ¥å†…å­˜å‹åŠ›"""
        usage = self.get_memory_usage()
        return usage > self.memory_threshold
    
    def force_cleanup(self):
        """éé˜»å¡å†…å­˜æ¸…ç†"""
        import threading
        
        def _async_cleanup():
            try:
                gc.collect()
                current_usage = self.get_memory_usage()
                self.logger.debug(f"å¼‚æ­¥å†…å­˜æ¸…ç†å®Œæˆ, å½“å‰ä½¿ç”¨ç‡: {current_usage:.1f}%")
            except Exception as e:
                self.logger.warning(f"å¼‚æ­¥å†…å­˜æ¸…ç†å¤±è´¥: {e}")
        
        # å¯åŠ¨åå°çº¿ç¨‹è¿›è¡Œæ¸…ç†ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
        cleanup_thread = threading.Thread(target=_async_cleanup, daemon=True)
        cleanup_thread.start()
    
    def memory_safe_wrapper(self, func):
        """å†…å­˜å®‰å…¨è£…é¥°å™¨"""
        @wraps(func)
        def wrapper(*args, **kwargs):
            initial_memory = self.get_memory_usage()
            # åªåœ¨å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡90%æ—¶è¿›è¡Œæ¸…ç†
            if initial_memory > 90.0:
                self.logger.warning(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {initial_memory:.1f}%, æ‰§è¡Œå¼‚æ­¥æ¸…ç†")
                self.force_cleanup()
            
            try:
                result = func(*args, **kwargs)
                
                final_memory = self.get_memory_usage()
                memory_increase = final_memory - initial_memory
                
                # åªè®°å½•æ˜¾è‘—çš„å†…å­˜å¢é•¿
                if memory_increase > 30:
                    self.logger.warning(f"å†…å­˜å¢é•¿æ˜¾è‘—: +{memory_increase:.1f}%")
                    
                # åªåœ¨å†…å­˜è¶…è¿‡85%æ—¶è§¦å‘æ¸…ç†
                if self.auto_cleanup and final_memory > 85.0:
                    self.force_cleanup()
                
                return result
                
            except MemoryError as e:
                self.logger.error(f"å†…å­˜ä¸è¶³é”™è¯¯: {e}")
                self.force_cleanup()
                raise
            except Exception as e:
                # åªåœ¨å†…å­˜é”™è¯¯æ—¶æ¸…ç†ï¼Œä¸è¦åœ¨æ‰€æœ‰å¼‚å¸¸æ—¶éƒ½æ¸…ç†
                if isinstance(e, (MemoryError, OSError)) and self.auto_cleanup:
                    self.force_cleanup()
                raise
                        
        return wrapper

class DataValidator:
    """æ•°æ®éªŒè¯å™¨ - ç»Ÿä¸€æ•°æ®éªŒè¯é€»è¾‘"""
    
    def __init__(self, logger):
        self.logger = logger
        
    def validate_dataframe(self, data: pd.DataFrame, name: str = "data", 
                          min_rows: int = 10, min_cols: int = 1) -> dict:
        """å…¨é¢çš„DataFrameéªŒè¯"""
        validation_result = {
            'valid': True,
            'errors': [],
            'warnings': [],
            'stats': {}
        }
        
        try:
            # åŸºç¡€æ£€æŸ¥
            if data is None:
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} is None")
                return validation_result
                
            if not isinstance(data, pd.DataFrame):
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} is not a DataFrame, got {type(data)}")
                return validation_result
                
            # ç©ºæ£€æŸ¥
            if data.empty:
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} is empty")
                return validation_result
                
            # å°ºå¯¸æ£€æŸ¥
            if len(data) < min_rows:
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} has only {len(data)} rows, minimum {min_rows} required")
                
            if len(data.columns) < min_cols:
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} has only {len(data.columns)} columns, minimum {min_cols} required")
            
            # NaNæ£€æŸ¥
            nan_counts = data.isnull().sum()
            total_nans = nan_counts.sum()
            if total_nans > 0:
                nan_ratio = total_nans / (len(data) * len(data.columns))
                if nan_ratio > 0.5:
                    validation_result['valid'] = False
                    validation_result['errors'].append(f"{name} has {nan_ratio:.1%} NaN values (>50%)")
                elif nan_ratio > 0.2:
                    validation_result['warnings'].append(f"{name} has {nan_ratio:.1%} NaN values")
            
            # æ•°æ®ç±»å‹æ£€æŸ¥
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) == 0:
                validation_result['warnings'].append(f"{name} has no numeric columns")
            
            # ç»Ÿè®¡ä¿¡æ¯
            validation_result['stats'] = {
                'rows': len(data),
                'cols': len(data.columns),
                'numeric_cols': len(numeric_cols),
                'nan_count': int(total_nans),
                'nan_ratio': total_nans / (len(data) * len(data.columns)) if len(data) > 0 and len(data.columns) > 0 else 0
            }
            
        except Exception as e:
            validation_result['valid'] = False
            validation_result['errors'].append(f"Validation error: {str(e)}")
            
        return validation_result
    
    def validate_series(self, data: pd.Series, name: str = "series",
                       min_length: int = 10, allow_nan: bool = True) -> dict:
        """SerieséªŒè¯"""
        validation_result = {
            'valid': True,
            'errors': [],
            'warnings': [],
            'stats': {}
        }
        
        try:
            if data is None:
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} is None")
                return validation_result
                
            if not isinstance(data, pd.Series):
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} is not a Series, got {type(data)}")
                return validation_result
                
            if len(data) < min_length:
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} has only {len(data)} values, minimum {min_length} required")
            
            nan_count = data.isnull().sum()
            if not allow_nan and nan_count > 0:
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} contains {nan_count} NaN values, but NaN not allowed")
            elif nan_count > len(data) * 0.5:
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} has {nan_count/len(data):.1%} NaN values (>50%)")
                
            validation_result['stats'] = {
                'length': len(data),
                'nan_count': int(nan_count),
                'nan_ratio': nan_count / len(data) if len(data) > 0 else 0
            }
            
        except Exception as e:
            validation_result['valid'] = False
            validation_result['errors'].append(f"Series validation error: {str(e)}")
            
        return validation_result
    
    def safe_validate_and_fix(self, data: pd.DataFrame, name: str = "data") -> pd.DataFrame:
        """éªŒè¯å¹¶å°è¯•ä¿®å¤æ•°æ®"""
        if data is None or data.empty:
            self.logger.warning(f"{name} is None or empty, returning empty DataFrame")
            return pd.DataFrame()
        
        # åŸºç¡€ä¿®å¤
        original_shape = data.shape
        
        # åˆ é™¤å…¨ç©ºè¡Œ/åˆ—
        data = data.dropna(how='all', axis=0)  # åˆ é™¤å…¨ç©ºè¡Œ
        data = data.dropna(how='all', axis=1)  # åˆ é™¤å…¨ç©ºåˆ—
        
        if data.empty:
            self.logger.warning(f"{name} became empty after removing all-NaN rows/columns")
            return pd.DataFrame()
        
        # å¤„ç†æ— ç©·å€¼
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            data[numeric_cols] = data[numeric_cols].replace([np.inf, -np.inf], np.nan)
        
        if data.shape != original_shape:
            self.logger.info(f"{name} cleaned: {original_shape} -> {data.shape}")
        
        return data
    
    def clean_numeric_data(self, data: pd.DataFrame, name: str = "data", 
                          strategy: str = "smart") -> pd.DataFrame:
        """ç»Ÿä¸€çš„æ•°å€¼æ•°æ®æ¸…ç†ç­–ç•¥"""
        if data is None or data.empty:
            return pd.DataFrame()
        
        cleaned_data = data.copy()
        numeric_cols = cleaned_data.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) == 0:
            self.logger.debug(f"{name}: æ²¡æœ‰æ•°å€¼åˆ—éœ€è¦æ¸…ç†")
            return cleaned_data
        
        # å¤„ç†æ— ç©·å€¼
        inf_mask = np.isinf(cleaned_data[numeric_cols])
        inf_count = inf_mask.sum().sum()
        if inf_count > 0:
            self.logger.warning(f"{name}: å‘ç° {inf_count} ä¸ªæ— ç©·å€¼")
            cleaned_data[numeric_cols] = cleaned_data[numeric_cols].replace([np.inf, -np.inf], np.nan)
        
        # NaNå¤„ç†ç­–ç•¥
        nan_count_before = cleaned_data[numeric_cols].isnull().sum().sum()
        
        # âœ… PERFORMANCE FIX: ä½¿ç”¨ç»Ÿä¸€çš„NaNå¤„ç†ç­–ç•¥ï¼Œé¿å…è™šå‡ä¿¡å·
        if PRODUCTION_FIXES_AVAILABLE:
            try:
                # ä½¿ç”¨é¢„æµ‹æ€§èƒ½å®‰å…¨çš„NaNæ¸…ç†
                cleaned_data = clean_nan_predictive_safe(
                    cleaned_data, 
                    feature_cols=numeric_cols,
                    method="cross_sectional_median"
                )
                logger.debug(f"âœ… ç»Ÿä¸€NaNå¤„ç†å®Œæˆï¼Œé¿å…è™šå‡ä¿¡å·å¹²æ‰°")
            except Exception as e:
                logger.error(f"ç»Ÿä¸€NaNå¤„ç†å¤±è´¥: {e}")
                # ğŸš¨ ä¸å…è®¸å¤‡é€‰æ–¹æ¡ˆï¼Œç›´æ¥æŠ¥é”™
                raise ValueError(f"NaNå¤„ç†å¤±è´¥ï¼Œæ— æ³•ç»§ç»­: {str(e)}")
                if 'date' in cleaned_data.columns:
                    def cross_sectional_fill(group):
                        for col in numeric_cols:
                            if col in group.columns:
                                fill_value = group[col].median()
                                if not pd.isna(fill_value):
                                    group[col] = group[col].fillna(fill_value)
                        return group
                    cleaned_data = cleaned_data.groupby('date').apply(cross_sectional_fill).reset_index(level=0, drop=True)
                else:
                    cleaned_data[numeric_cols] = cleaned_data[numeric_cols].fillna(method='ffill').fillna(0)
        else:
            # ç”Ÿäº§ä¿®å¤ä¸å¯ç”¨æ—¶çš„ä¼ ç»Ÿæ–¹æ³•
            if strategy == "smart":
                # æ™ºèƒ½ç­–ç•¥ï¼šæ ¹æ®åˆ—çš„æ€§è´¨é€‰æ‹©ä¸åŒå¡«å……æ–¹æ³•
                for col in numeric_cols:
                    if cleaned_data[col].isnull().sum() == 0:
                        continue
                        
                    col_name_lower = col.lower()
                    if any(keyword in col_name_lower for keyword in ['return', 'pct', 'change', 'momentum']):
                        # æ”¶ç›Šç‡ç±»æŒ‡æ ‡ç”¨0å¡«å……
                        cleaned_data[col] = cleaned_data[col].fillna(0)
                    elif any(keyword in col_name_lower for keyword in ['volume', 'amount', 'size']):
                        # æˆäº¤é‡ç±»æŒ‡æ ‡ç”¨ä¸­ä½æ•°å¡«å……
                        cleaned_data[col] = cleaned_data[col].fillna(cleaned_data[col].median())
                    elif any(keyword in col_name_lower for keyword in ['price', 'close', 'open', 'high', 'low']):
                        # ä»·æ ¼ç±»æŒ‡æ ‡ç”¨å‰å‘å¡«å……
                        cleaned_data[col] = cleaned_data[col].fillna(method='ffill').fillna(method='bfill')
                    else:
                        # å…¶ä»–æŒ‡æ ‡ç”¨å‡å€¼å¡«å……
                        mean_val = cleaned_data[col].mean()
                        if pd.isna(mean_val):
                            cleaned_data[col] = cleaned_data[col].fillna(0)
                        else:
                            cleaned_data[col] = cleaned_data[col].fillna(mean_val)
                            
            elif strategy == "zero":
                # å…¨éƒ¨ç”¨0å¡«å……
                cleaned_data[numeric_cols] = cleaned_data[numeric_cols].fillna(0)
                
            elif strategy == "forward":
                # å‰å‘å¡«å……
                cleaned_data[numeric_cols] = cleaned_data[numeric_cols].fillna(method='ffill').fillna(0)
                
            elif strategy == "median":
                # ä¸­ä½æ•°å¡«å……
                for col in numeric_cols:
                    median_val = cleaned_data[col].median()
                    if pd.isna(median_val):
                        cleaned_data[col] = cleaned_data[col].fillna(0)
                    else:
                        cleaned_data[col] = cleaned_data[col].fillna(median_val)
        
        nan_count_after = cleaned_data[numeric_cols].isnull().sum().sum()
        if nan_count_before > 0:
            self.logger.info(f"{name}: NaNæ¸…ç†å®Œæˆ {nan_count_before} -> {nan_count_after}")
        
        return cleaned_data

class BMAExceptionHandler:
    """BMAå¼‚å¸¸å¤„ç†å™¨"""
    
    def __init__(self, logger, config=None):
        self.logger = logger
        self.config = config or {}
        self.error_counts = {}
        self.max_retries = self.config.get('error_handling', {}).get('max_retries', 3)
        
    @contextmanager
    def safe_execution(self, operation_name: str):
        """å®‰å…¨æ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨ - ä¸å…è®¸fallback"""
        try:
            self.logger.debug(f"å¼€å§‹æ‰§è¡Œ: {operation_name}")
            yield
            self.logger.debug(f"æˆåŠŸå®Œæˆ: {operation_name}")
            
        except Exception as e:
            self.logger.error(f"æ“ä½œå¤±è´¥: {operation_name} - {e}")
            self.logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            
            # è®°å½•é”™è¯¯ç»Ÿè®¡
            self.error_counts[operation_name] = self.error_counts.get(operation_name, 0) + 1
            
            # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸ä½¿ç”¨fallback
            raise

@dataclass
class MarketRegime:
    """å¸‚åœºçŠ¶æ€"""
    regime_id: int
    name: str
    probability: float
    characteristics: Dict[str, float]
    duration: int = 0

@dataclass 
class RiskFactorExposure:
    """é£é™©å› å­æš´éœ²"""
    market_beta: float
    size_exposure: float  
    value_exposure: float
    momentum_exposure: float
    volatility_exposure: float
    quality_exposure: float
    country_exposure: Dict[str, float] = field(default_factory=dict)
    sector_exposure: Dict[str, float] = field(default_factory=dict)

def sanitize_ticker(raw: Union[str, Any]) -> str:
    """æ¸…ç†è‚¡ç¥¨ä»£ç ä¸­çš„BOMã€å¼•å·ã€ç©ºç™½ç­‰æ‚è´¨ã€‚"""
    try:
        s = str(raw)
    except Exception:
        return ''
    # å»é™¤BOMä¸é›¶å®½å­—ç¬¦
    s = s.replace('\ufeff', '').replace('\u200b', '').replace('\u200c', '').replace('\u200d', '')
    # å»é™¤å¼•å·ä¸ç©ºç™½
    s = s.strip().strip("'\"")
    # ç»Ÿä¸€å¤§å†™
    s = s.upper()
    return s


def load_universe_from_file(file_path: str) -> Optional[List[str]]:
    try:
        if os.path.exists(file_path):
            # ä½¿ç”¨utf-8-sigä»¥è‡ªåŠ¨å»é™¤BOM
            with open(file_path, 'r', encoding='utf-8-sig') as f:
                tickers = []
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    # æ”¯æŒé€—å·æˆ–ç©ºæ ¼åˆ†éš”
                    parts = [p for token in line.split(',') for p in token.split()]
                    for p in parts:
                        t = sanitize_ticker(p)
                        if t:
                            tickers.append(t)
            # å»é‡å¹¶ä¿æŒé¡ºåº
            tickers = list(dict.fromkeys(tickers))
            return tickers if tickers else None
    except Exception as e:
        logger.error(f"ğŸš¨ CRITICAL: åŠ è½½è‚¡ç¥¨æ¸…å•æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        logger.error("è¿™å¯èƒ½å¯¼è‡´ä½¿ç”¨é”™è¯¯çš„è‚¡ç¥¨æ± ï¼Œå½±å“æ•´ä¸ªäº¤æ˜“ç³»ç»Ÿ")
        return None
    return None

def load_universe_fallback() -> List[str]:
    # ç»Ÿä¸€ä»é…ç½®æ–‡ä»¶è¯»å–è‚¡ç¥¨æ¸…å•ï¼Œç§»é™¤æ—§ç‰ˆä¾èµ–
    root_stocks = os.path.join(os.path.dirname(__file__), 'filtered_stocks_20250817_002928')
    tickers = load_universe_from_file(root_stocks)
    if tickers:
        return tickers
    
    logger.warning("æœªæ‰¾åˆ°stocks.txtæ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤è‚¡ç¥¨æ¸…å•")
    return DEFAULT_TICKER_LIST
# CRITICAL TIME ALIGNMENT FIX APPLIED:
# - Prediction horizon set to T+5 for medium-term signals
# - Features use T-4 data, targets predict T+5 (10-day gap prevents data leakage)
# - This configuration is validated for production trading


class UltraEnhancedQuantitativeModel:
    """Ultra Enhanced é‡åŒ–æ¨¡å‹ V6ï¼šé›†æˆæ‰€æœ‰é«˜çº§åŠŸèƒ½ + å†…å­˜ä¼˜åŒ– + ç”Ÿäº§çº§å¢å¼º"""
    
    def __init__(self, config_path: str = "bma_models/alphas_config.yaml", enable_optimization: bool = True, 
                 enable_v6_enhancements: bool = True):
        """
        åˆå§‹åŒ–Ultra Enhancedé‡åŒ–æ¨¡å‹ V6
        
        Args:
            config_path: Alphaç­–ç•¥é…ç½®æ–‡ä»¶è·¯å¾„
            enable_optimization: å¯ç”¨å†…å­˜ä¼˜åŒ–åŠŸèƒ½
            enable_v6_enhancements: å¯ç”¨V6å¢å¼ºåŠŸèƒ½ï¼ˆç”Ÿäº§çº§æ”¹è¿›ï¼‰
        """
        self.config_path = config_path
        self.config = self._load_config()
        self.enable_optimization = enable_optimization
        self.enable_v6_enhancements = enable_v6_enhancements
        
        # BMA Enhanced V6ç³»ç»Ÿå·²åˆ é™¤ - åŠŸèƒ½èå…¥ç»Ÿä¸€è·¯å¾„
        
# ğŸš€ é¦–å…ˆåˆå§‹åŒ–åŸºç¡€å±æ€§ï¼ˆé¿å…AttributeErrorï¼‰
        self.health_metrics = {
            'risk_model_failures': 0,
            'alpha_computation_failures': 0,
            'neutralization_failures': 0,
            'prediction_failures': 0,
            'total_exceptions': 0
        }
        
        # ğŸ”¥ CRITICAL FIX: å…±äº«çº¿ç¨‹æ± é˜²æ­¢èµ„æºæ³„éœ²
        from concurrent.futures import ThreadPoolExecutor
        self._shared_thread_pool = ThreadPoolExecutor(
            max_workers=min(32, (os.cpu_count() or 1) + 4),  # é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°
            thread_name_prefix="BMA-Shared-Pool"
        )
        logger.info(f"âœ… åˆå§‹åŒ–å…±äº«çº¿ç¨‹æ± ï¼Œæœ€å¤§å·¥ä½œçº¿ç¨‹: {self._shared_thread_pool._max_workers}")
        
    def __del__(self):
        """ææ„å‡½æ•°ï¼šç¡®ä¿å…±äº«çº¿ç¨‹æ± æ­£ç¡®å…³é—­ï¼Œé˜²æ­¢èµ„æºæ³„éœ²"""
        try:
            if hasattr(self, '_shared_thread_pool') and self._shared_thread_pool:
                logger.info("ğŸ§¹ æ­£åœ¨å…³é—­å…±äº«çº¿ç¨‹æ± ...")
                self._shared_thread_pool.shutdown(wait=True)
                logger.info("âœ… å…±äº«çº¿ç¨‹æ± å·²å®‰å…¨å…³é—­")
        except Exception as e:
            # ææ„å‡½æ•°ä¸­çš„å¼‚å¸¸åº”è¯¥è¢«è®°å½•ä½†ä¸æŠ›å‡º
            logger.error(f"âš ï¸ å…³é—­å…±äº«çº¿ç¨‹æ± æ—¶å‡ºé”™: {e}")
        
        # ğŸš€ åˆå§‹åŒ–ç”Ÿäº§çº§ä¿®å¤ç³»ç»Ÿï¼ˆæ–°å¢ï¼‰
        self.timing_registry = None
        self.production_gate = None
        self.regime_enforcer = None
        self.weight_unifier = None
        self.cv_preventer = None
        if PRODUCTION_FIXES_AVAILABLE:
            self._init_production_fixes()
        
        # ğŸ”¥ åˆå§‹åŒ–è‡ªé€‚åº”æƒé‡å­¦ä¹ ç³»ç»Ÿï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼‰
        self.adaptive_weights = None
        self._init_adaptive_weights()
        
        # â­ åˆå§‹åŒ–é«˜çº§Alphaç³»ç»Ÿï¼ˆä¸“ä¸šæœºæ„çº§åŠŸèƒ½ï¼‰
        self.advanced_alpha_system = None
        self._init_advanced_alpha_system()
        
        # [ENHANCED] å†…å­˜ä¼˜åŒ–åŠŸèƒ½é›†æˆ
        if enable_optimization:
            self._init_optimization_components()
        
        # ğŸ”¥ æ–°å¢åŠŸèƒ½ï¼šWalk-Forwardé‡è®­ç»ƒç³»ç»Ÿ
        self.walk_forward_system = None
        self._init_walk_forward_system()
        
        # ğŸ”¥ æ–°å¢åŠŸèƒ½ï¼šç”Ÿäº§å°±ç»ªéªŒè¯å™¨
        self.production_validator = None
        self._init_production_validator()
        
        # ğŸ”¥ æ–°å¢åŠŸèƒ½ï¼šå¢å¼ºCVæ—¥å¿—è®°å½•å™¨
        self.cv_logger = None
        self._init_enhanced_cv_logger()
        
        # ğŸ”¥ æ–°å¢åŠŸèƒ½ï¼šEnhanced OOS System
        self.enhanced_oos_system = None
        self._init_enhanced_oos_system()
        
        # ğŸ”¥ æ–°å¢åŠŸèƒ½ï¼šReal Fundamental Data Provider
        self.fundamental_provider = None
        self._init_fundamental_provider()
        
        # ğŸ”¥ CRITICAL: Initialize Alpha Engine FIRST - MUST NOT BE MISSING
        # This must be done before other systems that depend on it
        self._init_alpha_engine()
        
        # ğŸ”§ ç»Ÿä¸€ç‰¹å¾ç®¡é“ - è§£å†³è®­ç»ƒ-é¢„æµ‹ç‰¹å¾ç»´åº¦ä¸åŒ¹é…é—®é¢˜
        self._init_unified_feature_pipeline()
        
        # ğŸ”¥ NEW: Regime Detectionç³»ç»Ÿ (depends on alpha engine)
        self.regime_detector = None
        self.regime_trainer = None
        self._init_regime_detection_system()
        
        # V5åˆå§‹åŒ–å·²åˆ é™¤ - åŠŸèƒ½å·²å®Œå…¨é›†æˆåˆ°V6ç³»ç»Ÿ
        
        # ğŸ”§ æ–°å¢ï¼šæ¨¡å—ç®¡ç†å™¨å’Œä¿®å¤ç»„ä»¶
        self.module_manager = ModuleManager()
        self.memory_manager = MemoryManager(memory_threshold=75.0)
        self.data_validator = DataValidator(logger)
        self.exception_handler = BMAExceptionHandler(logger, self.config)
        
        # ğŸ”¥ NEW: åˆå§‹åŒ–çœŸå®æ•°æ®æºè¿æ¥
        self._init_real_data_sources()
        
        # ä¸¥æ ¼æ—¶é—´éªŒè¯æ ‡å¿—
        self.strict_temporal_validation_enabled = True
    
    def _init_production_fixes(self):
        """åˆå§‹åŒ–ç”Ÿäº§çº§ä¿®å¤ç³»ç»Ÿ"""
        try:
            logger.info("åˆå§‹åŒ–ç”Ÿäº§çº§ä¿®å¤ç³»ç»Ÿ...")
            
            # 1. ç»Ÿä¸€æ—¶åºæ³¨å†Œè¡¨
            self.timing_registry = get_global_timing_registry()
            logger.info("âœ… ç»Ÿä¸€æ—¶åºæ³¨å†Œè¡¨åˆå§‹åŒ–å®Œæˆ")
            
            # 2. å¢å¼ºç”Ÿäº§é—¨ç¦
            self.production_gate = create_enhanced_production_gate()
            logger.info("âœ… å¢å¼ºç”Ÿäº§é—¨ç¦åˆå§‹åŒ–å®Œæˆ")
            
            # 3. Regimeå¹³æ»‘å¼ºåˆ¶ç¦ç”¨å™¨
            self.regime_enforcer = RegimeSmoothingEnforcer()
            logger.info("âœ… Regimeå¹³æ»‘å¼ºåˆ¶ç¦ç”¨å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # 4. æ ·æœ¬æƒé‡ç»Ÿä¸€åŒ–å™¨
            self.weight_unifier = SampleWeightUnifier()
            logger.info("âœ… æ ·æœ¬æƒé‡ç»Ÿä¸€åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # 5. CVæ³„éœ²é˜²æŠ¤å™¨
            self.cv_preventer = CVLeakagePreventer()
            # åº”ç”¨å±é™©CVå¯¼å…¥çš„çŒ´å­è¡¥ä¸
            self.cv_preventer.patch_dangerous_cv_imports()
            logger.info("âœ… CVæ³„éœ²é˜²æŠ¤å™¨åˆå§‹åŒ–å®Œæˆ")
            
            logger.info("ğŸ‰ ç”Ÿäº§çº§ä¿®å¤ç³»ç»Ÿå…¨éƒ¨åˆå§‹åŒ–æˆåŠŸ")
            
            # è®°å½•ä¿®å¤ç³»ç»ŸçŠ¶æ€
            self._log_production_fixes_status()
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿäº§çº§ä¿®å¤ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ç³»ç»Ÿç»§ç»­è¿è¡Œï¼Œä½†è®°å½•é”™è¯¯
            self.timing_registry = None
            self.production_gate = None
            self.regime_enforcer = None
            self.weight_unifier = None
            self.cv_preventer = None
    
    def _log_production_fixes_status(self):
        """è®°å½•ç”Ÿäº§çº§ä¿®å¤ç³»ç»ŸçŠ¶æ€"""
        if not self.timing_registry:
            return
            
        logger.info("=== ç”Ÿäº§çº§ä¿®å¤ç³»ç»ŸçŠ¶æ€ ===")
        
        # æ—¶åºå‚æ•°çŠ¶æ€
        timing_params = self.timing_registry.get_purged_cv_params()
        logger.info(f"ç»Ÿä¸€CVå‚æ•°: gap={timing_params['gap_days']}å¤©, embargo={timing_params['embargo_days']}å¤©")
        
        # ç”Ÿäº§é—¨ç¦å‚æ•°
        gate_params = self.timing_registry.get_production_gate_params()
        logger.info(f"ç”Ÿäº§é—¨ç¦: RankICâ‰¥{gate_params['min_rank_ic']}, tâ‰¥{gate_params['min_t_stat']}")
        
        # Regimeé…ç½®çŠ¶æ€
        regime_params = self.timing_registry.get_regime_params()
        logger.info(f"Regimeå¹³æ»‘: {'ç¦ç”¨' if not regime_params['enable_smoothing'] else 'å¯ç”¨'}")
        
        # æ ·æœ¬æƒé‡é…ç½®
        weight_params = self.timing_registry.get_sample_weight_params()
        logger.info(f"æ ·æœ¬æƒé‡åŠè¡°æœŸ: {weight_params['half_life_days']}å¤©")
        
        logger.info("=== ç”Ÿäº§çº§ä¿®å¤ç³»ç»Ÿå°±ç»ª ===")
    
    def get_production_fixes_status(self) -> Dict[str, Any]:
        """è·å–ç”Ÿäº§çº§ä¿®å¤ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š"""
        if not PRODUCTION_FIXES_AVAILABLE:
            return {'available': False, 'reason': 'ç”Ÿäº§çº§ä¿®å¤ç³»ç»Ÿæœªå¯¼å…¥'}
        
        status = {
            'available': True,
            'systems': {
                'timing_registry': self.timing_registry is not None,
                'production_gate': self.production_gate is not None,
                'regime_enforcer': self.regime_enforcer is not None,
                'weight_unifier': self.weight_unifier is not None,
                'cv_preventer': self.cv_preventer is not None
            }
        }
        
        if self.timing_registry:
            status['timing_config'] = {
                'cv_gap_days': self.timing_registry.cv_gap_days,
                'cv_embargo_days': self.timing_registry.cv_embargo_days,
                'sample_weight_half_life': self.timing_registry.sample_weight_half_life,
                'regime_enable_smoothing': self.timing_registry.regime_enable_smoothing
            }
        
        return status
    
    def _init_adaptive_weights(self):
        """å»¶è¿Ÿåˆå§‹åŒ–è‡ªé€‚åº”æƒé‡ç³»ç»Ÿ"""
        try:
            # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            from autotrader.adaptive_factor_weights import AdaptiveFactorWeights, WeightLearningConfig
            
            weight_config = WeightLearningConfig(
                lookback_days=252,
                validation_days=63,
                min_confidence=0.6,
                rebalance_frequency=21,
                enable_regime_detection=True
            )
            self.adaptive_weights = AdaptiveFactorWeights(weight_config)
            global ADAPTIVE_WEIGHTS_AVAILABLE
            ADAPTIVE_WEIGHTS_AVAILABLE = True
            logger.info("BMAè‡ªé€‚åº”æƒé‡ç³»ç»Ÿå»¶è¿Ÿåˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.warning(f"è‡ªé€‚åº”æƒé‡ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ç¡¬ç¼–ç æƒé‡")
            self.adaptive_weights = None
    
    def _init_walk_forward_system(self):
        """åˆå§‹åŒ–Walk-Forwardé‡è®­ç»ƒç³»ç»Ÿ"""
        try:
            # ğŸ”§ ä¿®å¤ç›¸å¯¹å¯¼å…¥é—®é¢˜ - ä½¿ç”¨ç»å¯¹å¯¼å…¥
            from walk_forward_retraining import create_walk_forward_system, WalkForwardConfig
            
            wf_config = WalkForwardConfig(
                train_window_months=24,  # 2å¹´è®­ç»ƒçª—å£
                step_size_days=30,
                warmup_periods=3,
                force_refit_days=90,
                window_type='rolling',
                enable_version_control=True
            )
            self.walk_forward_system = create_walk_forward_system(wf_config)
            logger.info("Walk-Forwardé‡è®­ç»ƒç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ(ç»å¯¹å¯¼å…¥)")
            
        except Exception as e:
            logger.warning(f"Walk-Forwardé‡è®­ç»ƒç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            self.walk_forward_system = None
    
    def _init_production_validator(self):
        """åˆå§‹åŒ–ç”Ÿäº§å°±ç»ªéªŒè¯å™¨"""
        try:
            # ğŸ”§ ä¿®å¤ç›¸å¯¹å¯¼å…¥é—®é¢˜ - ä½¿ç”¨ç»å¯¹å¯¼å…¥
            from production_readiness_validator import ProductionReadinessValidator, ValidationThresholds, ValidationConfig
            
            thresholds = ValidationThresholds(
                min_rank_ic=0.01,    # å·²ä¼˜åŒ–çš„é˜ˆå€¼
                min_t_stat=1.0,      # å·²ä¼˜åŒ–çš„é˜ˆå€¼
                min_coverage_months=1, # å·²ä¼˜åŒ–çš„é˜ˆå€¼
                min_stability_ratio=0.5,
                min_calibration_r2=0.6,
                max_correlation_median=0.7
            )
            config = ValidationConfig()
            self.production_validator = ProductionReadinessValidator(config, thresholds)
            logger.info("ç”Ÿäº§å°±ç»ªéªŒè¯å™¨åˆå§‹åŒ–æˆåŠŸ(ç»å¯¹å¯¼å…¥)")
            
        except Exception as e:
            logger.warning(f"ç”Ÿäº§å°±ç»ªéªŒè¯å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.production_validator = None
    
    def _prepare_data_for_advanced_alpha(self, stock_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """å‡†å¤‡é«˜çº§Alphaç³»ç»Ÿæ‰€éœ€çš„æ•°æ®æ ¼å¼"""
        try:
            # æ”¶é›†æ‰€æœ‰è‚¡ç¥¨æ•°æ®
            all_data = []
            
            for ticker, data in stock_data.items():
                if data.empty:
                    continue
                    
                # å‡†å¤‡å¿…è¦çš„åˆ—
                prepared = pd.DataFrame()
                
                # ä»·æ ¼å’Œæˆäº¤é‡æ•°æ®
                if 'close' in data.columns:
                    prepared['close'] = data['close']
                    prepared['price'] = data['close']
                
                if 'high' in data.columns:
                    prepared['high'] = data['high']
                    
                if 'low' in data.columns:
                    prepared['low'] = data['low']
                    
                if 'volume' in data.columns:
                    prepared['volume'] = data['volume']
                
                # è®¡ç®—å¸‚å€¼ï¼ˆç®€åŒ–ï¼‰
                if 'close' in data.columns and 'volume' in data.columns:
                    prepared['market_cap'] = data['close'] * data['volume'] * 1000  # ç²—ç•¥ä¼°ç®—
                
                # ğŸ”¥ ENHANCED: ä½¿ç”¨Real Fundamental Data Providerè·å–å¢å¼ºåŸºæœ¬é¢æ•°æ®
                if 'close' in data.columns:
                    try:
                        if self.fundamental_provider:
                            # ä½¿ç”¨å¢å¼ºçš„åŸºæœ¬é¢æ•°æ®æä¾›å™¨
                            fund_data = self.fundamental_provider.get_fundamentals(ticker)
                            
                            # éªŒè¯æ•°æ®è´¨é‡
                            quality_metrics = self.fundamental_provider.validate_data_quality(fund_data)
                            
                            if fund_data.data_source.value != 'unavailable':
                                # ä½¿ç”¨å¢å¼ºçš„åŸºæœ¬é¢æ•°æ®ï¼ˆ8ä¸ªæŒ‡æ ‡ï¼‰
                                prepared['book_to_market'] = fund_data.book_to_market
                                prepared['roe'] = fund_data.roe
                                prepared['debt_to_equity'] = fund_data.debt_to_equity
                                prepared['earnings'] = fund_data.earnings_per_share
                                prepared['pe_ratio'] = fund_data.pe_ratio
                                
                                # æ–°å¢çš„å¢å¼ºæŒ‡æ ‡
                                prepared['market_cap'] = fund_data.market_cap
                                prepared['revenue_growth'] = fund_data.revenue_growth
                                prepared['profit_margin'] = fund_data.profit_margin
                                
                                logger.info(f"Enhanced fundamental data for {ticker} "
                                           f"(completeness: {quality_metrics['completeness']:.1%}, "
                                           f"source: {fund_data.data_source.value})")
                                
                                # è®°å½•æ•°æ®è´¨é‡è­¦å‘Š
                                for warning in quality_metrics.get('warnings', []):
                                    logger.warning(f"{ticker} fundamental data: {warning}")
                            else:
                                logger.warning(f"No fundamental data available for {ticker}")
                                self._set_fundamental_nan_values(prepared)
                        else:
                            # å›é€€åˆ°åŸå§‹æ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰
                            logger.info(f"Using fallback fundamental data method for {ticker}")
                            self._get_fundamental_data_fallback(prepared, ticker, data)
                            
                    except Exception as e:
                        logger.warning(f"Enhanced fundamental data failed for {ticker}: {e}")
                        # å›é€€åˆ°åŸå§‹æ–¹æ³•
                        self._get_fundamental_data_fallback(prepared, ticker, data)
                    
                    # è®¡ç®—èµ„äº§å¢é•¿ï¼ˆåŸºäºä»·æ ¼æ•°æ®ï¼‰
                    prepared['asset_growth'] = data['close'].pct_change().rolling(20).mean()
                    
                    # è®¡ç®—æ”¶ç›Šç‡
                    prepared['returns'] = data['close'].pct_change()
                    prepared['returns_1m'] = data['close'].pct_change(22)
                    prepared['returns_12m'] = data['close'].pct_change(252)
                    
                    # æˆé•¿æŒ‡æ ‡
                    prepared['earnings_growth'] = prepared['earnings'].pct_change(252)
                    prepared['sales_growth'] = prepared['volume'].pct_change(252) if 'volume' in data.columns else 0
                
                prepared['ticker'] = ticker
                prepared['date'] = data.index
                
                all_data.append(prepared)
            
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            if all_data:
                combined = pd.concat(all_data, axis=0, ignore_index=True)
                # æŒ‰æ—¥æœŸæ’åº
                combined = combined.sort_values('date')
                return combined
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.warning(f"å‡†å¤‡é«˜çº§Alphaæ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _init_advanced_alpha_system(self):
        """é«˜çº§Alphaç³»ç»ŸåŠŸèƒ½å·²ç§»é™¤ï¼ˆç”¨æˆ·è¦æ±‚åˆ é™¤ï¼‰"""
        logger.info("é«˜çº§Alphaç³»ç»ŸåŠŸèƒ½å·²ç§»é™¤ï¼Œä½¿ç”¨åŸºç¡€Alphaå¤„ç†")
        self.advanced_alpha_system = None
    
    # V5ç³»ç»Ÿåˆå§‹åŒ–å‡½æ•°å·²åˆ é™¤ï¼ŒåŠŸèƒ½å®Œå…¨é›†æˆåˆ°V6ç³»ç»Ÿ
        """ğŸ”¥ V5æ–°å¢ï¼šåˆå§‹åŒ–ç«‹ç«¿è§å½±å¢å¼ºåŠŸèƒ½"""
        logger.info("åˆå§‹åŒ–BMA V5ç«‹ç«¿è§å½±å¢å¼ºåŠŸèƒ½")
        
        # 1. æ’åºå¢å¼ºé…ç½®
        self.ranking_config = {
            'use_lightgbm_ranker': True,      # å¯ç”¨LightGBM Ranker
            'ranking_objective': 'lambdarank', # æ’åºç›®æ ‡
            'daily_grouping': True,           # æŒ‰æ—¥åˆ†ç»„
            'ndcg_k': 10,                     # NDCG@Kè¯„ä¼°
            'n_estimators': 200,              # æ ‘çš„æ•°é‡
            'learning_rate': 0.05,            # å­¦ä¹ ç‡
            'num_leaves': 63,                 # å¶å­æ•°
            'feature_fraction': 0.8,          # ç‰¹å¾é‡‡æ ·
            'bagging_fraction': 0.8,          # æ ·æœ¬é‡‡æ ·
            'early_stopping_rounds': 50      # æ—©åœ
        }
        
        # 2. ä¸¥æ ¼Purged CVé…ç½® - ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨T10_CONFIGå‚æ•°
        from t10_config import T10_CONFIG
        self.purged_cv_config = {
            'strict_embargo': True,           # ä¸¥æ ¼ç¦è¿
            'embargo_align_target': True,     # ç¦è¿ä¸ç›®æ ‡è·¨åº¦å¯¹é½ï¼ˆT+10ï¼‰
            'validate_integrity': True,      # éªŒè¯åˆ‡åˆ†å®Œæ•´æ€§
            'embargo_days': T10_CONFIG.EMBARGO_DAYS,  # âœ… FIXED: ä½¿ç”¨ç»Ÿä¸€é…ç½® (15å¤©)
            'gap_days': T10_CONFIG.CV_GAP,            # âœ… FIXED: ä½¿ç”¨ç»Ÿä¸€é…ç½® (21å¤©)
            'min_train_ratio': 0.6,          # æœ€å°è®­ç»ƒé›†æ¯”ä¾‹
            'enable_group_constraints': True  # å¯ç”¨ç»„çº¦æŸ
        }
        
        # 3. Isotonicæ ¡å‡†é…ç½®
        self.calibration_config = {
            'use_isotonic': True,            # å¯ç”¨Isotonicæ ¡å‡†
            'out_of_bounds': 'clip',         # è¾¹ç•Œå¤„ç†
            'calibration_window': 252,       # æ ¡å‡†çª—å£ï¼ˆ1å¹´ï¼‰
            'y_min': None,                   # è‡ªåŠ¨ç¡®å®š
            'y_max': None,                   # è‡ªåŠ¨ç¡®å®š
            'increasing': True               # å•è°ƒé€’å¢
        }
        
        # 4. æ—¶é—´è¡°å‡å’Œå”¯ä¸€åº¦æƒé‡é…ç½®
        self.weighting_config = {
            'time_decay_enabled': True,      # æ—¶é—´è¡°å‡æƒé‡
            'uniqueness_weighting': True,    # å”¯ä¸€åº¦æƒé‡
            'half_life_days': 120,          # åŠè¡°æœŸï¼ˆ4ä¸ªæœˆï¼Œé€‚é…T+10ï¼‰
            'holding_period': 10,           # æŒæœ‰æœŸï¼ˆå¯¹åº”T+10ï¼‰
            'max_weight_ratio': 5.0,        # æœ€å¤§æƒé‡æ¯”ç‡
            'min_weight_threshold': 0.1     # æœ€å°æƒé‡é˜ˆå€¼
        }
        
        # 5. æ€§èƒ½è¯„ä¼°é…ç½®
        self.evaluation_config = {
            'cross_sectional_metrics': ['ic', 'rank_ic', 'ndcg'],
            'temporal_metrics': ['alpha_decay', 'turnover'],
            'calibration_metrics': ['calibration_slope', 'hit_ratio'],
            'enable_bootstrap': True,
            'bootstrap_n': 1000,
            'confidence_level': 0.95
        }
        
        # åˆå§‹åŒ–æ ¡å‡†å™¨å­˜å‚¨
        self.isotonic_calibrators = {}
        
        # åˆå§‹åŒ–æ€§èƒ½è¿½è¸ª
        self.v5_performance_tracker = {
            'ranking_performance': [],
            'calibration_quality': [],
            'weight_effectiveness': [],
            'cv_integrity_checks': []
        }
        
        logger.info("âœ… BMA V5ç«‹ç«¿è§å½±å¢å¼ºåŠŸèƒ½åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   - LightGBM Ranker: {self.ranking_config['use_lightgbm_ranker']}")
        logger.info(f"   - ä¸¥æ ¼CV: gap={self.purged_cv_config['gap_days']}å¤©, embargo={self.purged_cv_config['embargo_days']}å¤©")
        logger.info(f"   - Isotonicæ ¡å‡†: {self.calibration_config['use_isotonic']}")
        logger.info(f"   - æ—¶é—´è¡°å‡æƒé‡: åŠè¡°æœŸ={self.weighting_config['half_life_days']}å¤©")
    
    def _init_enhanced_cv_logger(self):
        """åˆå§‹åŒ–å¢å¼ºCVæ—¥å¿—è®°å½•å™¨"""
        try:
            # ğŸ”§ ä¿®å¤ç›¸å¯¹å¯¼å…¥é—®é¢˜ - ä½¿ç”¨ç»å¯¹å¯¼å…¥
            from enhanced_cv_logging import EnhancedCVLogger
            self.cv_logger = EnhancedCVLogger()
            logger.info("å¢å¼ºCVæ—¥å¿—è®°å½•å™¨åˆå§‹åŒ–æˆåŠŸ(ç»å¯¹å¯¼å…¥)")
            
        except Exception as e:
            logger.warning(f"å¢å¼ºCVæ—¥å¿—è®°å½•å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.cv_logger = None
    
    def _init_enhanced_oos_system(self):
        """åˆå§‹åŒ–Enhanced OOS System"""
        try:
            from enhanced_oos_system import create_enhanced_oos_system, OOSConfig
            
            # åˆ›å»ºOOSé…ç½® - ä¸BMA Enhancedå‚æ•°å¯¹é½
            oos_config = {
                'cv_n_splits': 5,
                'cv_gap_days': 10,  # ä¸BMA Enhancedä¸€è‡´
                'embargo_days': 5,
                'rolling_window_months': 24,  # ä¸Walk-Forwardä¸€è‡´
                'step_size_days': 30,
                'min_train_samples': 1000,
                'min_oos_ic': 0.01,  # ä¸ç”Ÿäº§å°±ç»ªéªŒè¯å™¨ä¸€è‡´
                'stability_threshold': 0.5,
                'cache_dir': 'cache/oos_system',
                'enable_caching': True
            }
            
            self.enhanced_oos_system = create_enhanced_oos_system(oos_config)
            logger.info("Enhanced OOS Systemåˆå§‹åŒ–æˆåŠŸ - é›†æˆæ—¶é—´æ„ŸçŸ¥éªŒè¯")
            
        except ImportError as e:
            logger.warning(f"Enhanced OOS Systemå¯¼å…¥å¤±è´¥: {e}")
            self.enhanced_oos_system = None
        except Exception as e:
            logger.warning(f"Enhanced OOS Systemåˆå§‹åŒ–å¤±è´¥: {e}")
            self.enhanced_oos_system = None
    
    def _init_fundamental_provider(self):
        """åˆå§‹åŒ–Real Fundamental Data Provider"""
        try:
            from real_fundamental_data_provider import create_fundamental_provider
            
            # è·å–Polygon APIå¯†é’¥ï¼ˆä¼˜å…ˆç¯å¢ƒå˜é‡ï¼‰
            import os
            polygon_api_key = os.getenv('POLYGON_API_KEY', '')
            
            self.fundamental_provider = create_fundamental_provider(
                polygon_api_key=polygon_api_key or None
            )
            
            logger.info("Real Fundamental Data Provideråˆå§‹åŒ–æˆåŠŸ")
            if polygon_api_key:
                logger.info("  - ä½¿ç”¨Polygon APIå¯†é’¥è·å–çœŸå®åŸºæœ¬é¢æ•°æ®")
            else:
                logger.warning("  - æœªé…ç½®POLYGON_API_KEYï¼ŒåŸºæœ¬é¢æ•°æ®å¯èƒ½ä¸å¯ç”¨")
            
        except ImportError as e:
            logger.warning(f"Real Fundamental Data Providerå¯¼å…¥å¤±è´¥: {e}")
            self.fundamental_provider = None
        except Exception as e:
            logger.warning(f"Real Fundamental Data Provideråˆå§‹åŒ–å¤±è´¥: {e}")
            self.fundamental_provider = None
    
    def _init_regime_detection_system(self):
        """åˆå§‹åŒ–Regime Detectionç³»ç»Ÿ"""
        try:
            global REGIME_DETECTION_AVAILABLE
            if not REGIME_DETECTION_AVAILABLE:
                logger.warning("Regime Detectionæ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡åˆå§‹åŒ–")
                return
            
            # æ£€æŸ¥é…ç½®ä¸­æ˜¯å¦å¯ç”¨Regime Detection
            regime_enabled = self.config.get('model_config', {}).get('regime_detection', False)
            
            if regime_enabled:
                # åˆ›å»ºBæ–¹æ¡ˆGMMçŠ¶æ€æ£€æµ‹é…ç½®
                regime_config = RegimeConfig(
                    n_regimes=3,
                    lookback_window=252,            # 1å¹´è®­ç»ƒçª—å£
                    update_frequency=63,            # å­£åº¦æ›´æ–°
                    prob_smooth_window=7,           # 7æ—¥æ—¶é—´å¹³æ»‘
                    hard_threshold=0.6,             # ç¡¬è·¯ç”±é˜ˆå€¼
                    min_regime_samples=50,          # æœ€å°æ ·æœ¬æ•°
                    enable_pca=False,               # å…³é—­PCAç®€åŒ–
                    robust_window=252               # Robustæ ‡å‡†åŒ–çª—å£
                )
                
                # åˆ›å»ºè®­ç»ƒé…ç½®  
                training_config = RegimeTrainingConfig(
                    enable_regime_aware=True,
                    regime_config=regime_config,
                    regime_training_strategy='separate',
                    min_samples_per_regime=100,
                    regime_prediction_mode='adaptive',
                    parallel_regime_training=True,
                    regime_feature_selection=True
                )
                
                # ğŸ”¥ CRITICAL FIX: ä½¿ç”¨å…¨å±€ç»Ÿä¸€æ—¶é—´é…ç½®é˜²æ­¢æ•°æ®æ³„éœ²
                UNIFIED_TEMPORAL_CONFIG = validate_temporal_configuration()
                
                # åˆ›å»ºçŠ¶æ€æ„ŸçŸ¥CVé…ç½®ï¼ˆä½¿ç”¨ç»Ÿä¸€æ—¶é—´å‚æ•°ï¼‰
                regime_cv_config = RegimeAwareCVConfig(
                    n_splits=5,
                    test_size=63,
                    gap=UNIFIED_TEMPORAL_CONFIG['cv_gap_days'],      # ç»Ÿä¸€ä½¿ç”¨11å¤©
                    embargo=UNIFIED_TEMPORAL_CONFIG['cv_embargo_days'], # ç»Ÿä¸€ä½¿ç”¨11å¤©
                    min_train_size=252,
                    enable_regime_stratification=True,
                    min_regime_samples=50,
                    regime_balance_threshold=0.8,
                    cross_regime_validation=True,
                    strict_regime_temporal_order=True,
                    regime_transition_buffer=10
                )
                
                # åˆå§‹åŒ–ç»„ä»¶
                self.regime_detector = MarketRegimeDetector(regime_config)
                self.regime_trainer = RegimeAwareTrainer(training_config)
                self.regime_cv = RegimeAwareTimeSeriesCV(regime_cv_config, self.regime_detector)
                
                logger.info("âœ… Regime Detectionç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
                logger.info(f"   - çŠ¶æ€æ•°é‡: {regime_config.n_regimes}")
                logger.info(f"   - è®­ç»ƒç­–ç•¥: {training_config.regime_training_strategy}")
                logger.info(f"   - é¢„æµ‹æ¨¡å¼: {training_config.regime_prediction_mode}")
                logger.info(f"   - CVçŠ¶æ€åˆ†å±‚: {regime_cv_config.enable_regime_stratification}")
                
                # è®¾ç½®å…¨å±€æ ‡è®°
                self._regime_cv_enabled = True
                
            else:
                logger.info("Regime Detectionåœ¨é…ç½®ä¸­æœªå¯ç”¨")
                
        except Exception as e:
            logger.warning(f"Regime Detectionç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            self.regime_detector = None
            self.regime_trainer = None
    
    def _init_alpha_engine(self):
        """åˆå§‹åŒ–Alphaå¼•æ“ - æ ¸å¿ƒç»„ä»¶ï¼Œå¿…é¡»æˆåŠŸ"""
        # æ ¸å¿ƒå¼•æ“ - ä¸¥æ ¼è¦æ±‚ï¼Œä¸å…è®¸ä½¿ç”¨Mock
        if ENHANCED_MODULES_AVAILABLE:
            try:
                # ğŸ”¥ åˆå§‹åŒ–Alphaå¼•æ“
                # è§£æé…ç½®æ–‡ä»¶è·¯å¾„ï¼Œç¡®ä¿ä»æ­£ç¡®ä½ç½®åŠ è½½
                if not os.path.isabs(self.config_path):
                    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œé¦–å…ˆå°è¯•ä»é¡¹ç›®æ ¹ç›®å½•æŸ¥æ‰¾
                    root_config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), self.config_path)
                    if os.path.exists(root_config_path):
                        resolved_config_path = root_config_path
                    else:
                        # å›é€€åˆ°å½“å‰ç›®å½•
                        resolved_config_path = self.config_path
                else:
                    resolved_config_path = self.config_path
                
                logger.info(f"å°è¯•åŠ è½½Alphaé…ç½®æ–‡ä»¶: {resolved_config_path}")
                self.alpha_engine = AlphaStrategiesEngine(resolved_config_path)
                
                # Alphaå¼•æ“å·²æˆåŠŸåˆå§‹åŒ–
                
                # éªŒè¯Alphaå¼•æ“çš„åŠŸèƒ½å®Œæ•´æ€§
                required_methods = ['compute_all_alphas', 'alpha_functions']
                missing_methods = [method for method in required_methods 
                                 if not hasattr(self.alpha_engine, method)]
                if missing_methods:
                    raise ValueError(f"âŒ Alphaå¼•æ“ç¼ºå°‘å¿…è¦æ–¹æ³•: {missing_methods}")
                
                logger.info(f"âœ… Alphaå¼•æ“åˆå§‹åŒ–æˆåŠŸ: {len(self.alpha_engine.alpha_functions)} ä¸ªå› å­å‡½æ•°")
                
                # LTRåŠŸèƒ½å·²æ•´åˆåˆ°BMA Enhancedç³»ç»Ÿä¸­
                if LearningToRankBMA is not None:
                    self.ltr_bma = LearningToRankBMA()
                    logger.info("âœ… LTRåŠŸèƒ½é€šè¿‡BMA Enhancedç³»ç»Ÿå¯ç”¨")
                else:
                    self.ltr_bma = None
                    logger.warning("âš ï¸ LTRåŠŸèƒ½ä¸å¯ç”¨ï¼ŒLearningToRankBMAæ¨¡å—ç¼ºå¤±")
                    
                # ç³»ç»Ÿä¸“æ³¨äºé€‰è‚¡é¢„æµ‹ï¼Œä¸éœ€è¦æŠ•èµ„ç»„åˆä¼˜åŒ–
                logger.info("ç³»ç»Ÿä¸“æ³¨äºè‚¡ç¥¨é¢„æµ‹å’Œé€‰è‚¡åŠŸèƒ½")
                
            except Exception as e:
                error_msg = f"âŒ Alphaå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}"
                logger.error(error_msg)
                # ğŸš¨ æ ¹æ®ç”¨æˆ·è¦æ±‚ï¼šä¸å…è®¸å›é€€ï¼Œç›´æ¥æŠ¥é”™
                raise ValueError(f"Alphaå¼•æ“åˆå§‹åŒ–å¤±è´¥ï¼Œå¿…é¡»ä¿®å¤: {error_msg}") from e
                
        else:
            # ğŸš¨ å¢å¼ºæ¨¡å—ä¸å¯ç”¨æ˜¯ä¸¥é‡é”™è¯¯ï¼Œä¸å…è®¸é™çº§
            error_msg = (
                "âŒ å¢å¼ºæ¨¡å—ä¸å¯ç”¨ï¼è¿™ä¼šå¯¼è‡´Mockå¯¹è±¡è¢«ä½¿ç”¨\n"
                "è§£å†³æ–¹æ¡ˆï¼š\n"
                "1. æ£€æŸ¥enhanced_alpha_strategies.pyæ–‡ä»¶\n"
                "2. ç¡®ä¿æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…\n"
                "3. ä¿®å¤å¯¼å…¥é”™è¯¯\n"
                "4. ç¡®ä¿çœŸå®çš„Alphaå¼•æ“æ­£ç¡®åˆå§‹åŒ–"
            )
            # ä¸å…è®¸ä½¿ç”¨Mockï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
            raise ImportError(error_msg)
    
    def _init_real_data_sources(self):
        """åˆå§‹åŒ–çœŸå®æ•°æ®æºè¿æ¥ - æ¶ˆé™¤Mockå› å­å‡½æ•°ä¾èµ–"""
        try:
            import os
            
            # 1. åˆå§‹åŒ–Polygon APIå®¢æˆ·ç«¯
            # ä¼˜å…ˆä½¿ç”¨å·²é…ç½®çš„polygon_clientå®ä¾‹
            if pc is not None:
                try:
                    self.polygon_client = pc
                    logger.info("âœ… ä½¿ç”¨é¢„é…ç½®çš„Polygon APIå®¢æˆ·ç«¯ - çœŸå®æ•°æ®æºå·²è¿æ¥")
                except Exception as e:
                    logger.warning(f"âš ï¸ Polygonå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                    self.polygon_client = None
            else:
                # å›é€€åˆ°ç¯å¢ƒå˜é‡æ£€æŸ¥  
                polygon_api_key = os.getenv('POLYGON_API_KEY')
                if polygon_api_key:
                    logger.info("âœ… æ£€æµ‹åˆ°POLYGON_API_KEYç¯å¢ƒå˜é‡")
                    self.polygon_client = None  # éœ€è¦æ‰‹åŠ¨åˆ›å»ºå®¢æˆ·ç«¯
                else:
                    logger.warning("âš ï¸ æœªæ‰¾åˆ°polygon_clientæ¨¡å—ï¼Œä¸”POLYGON_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®")
                    self.polygon_client = None
            
            # 2. åˆå§‹åŒ–å…¶ä»–çœŸå®æ•°æ®æº (å¯æ‰©å±•)
            # TODO: æ·»åŠ Alpha Vantage, Quandl, FREDç­‰æ•°æ®æº
            
            # 3. æ•°æ®æºçŠ¶æ€æ£€æŸ¥
            if self.polygon_client is not None:
                logger.info("ğŸ‰ Polygon APIå®¢æˆ·ç«¯å·²è¿æ¥ - æ”¯æŒçœŸå®åŸºæœ¬é¢æ•°æ®è·å–")
            else:
                raise ValueError(
                    "âŒ æ²¡æœ‰å¯ç”¨çš„çœŸå®æ•°æ®æº\n"
                    "è¯·è®¾ç½®POLYGON_API_KEYç¯å¢ƒå˜é‡ä»¥è·å–çœŸå®æ•°æ®"
                )
                
        except Exception as e:
            logger.error(f"âŒ çœŸå®æ•°æ®æºåˆå§‹åŒ–å¤±è´¥: {e}")
            self.polygon_client = None
    
    def _init_unified_feature_pipeline(self):
        """åˆå§‹åŒ–ç»Ÿä¸€ç‰¹å¾ç®¡é“"""
        try:
            logger.info("å¼€å§‹åˆå§‹åŒ–ç»Ÿä¸€ç‰¹å¾ç®¡é“...")
            from unified_feature_pipeline import UnifiedFeaturePipeline, FeaturePipelineConfig
            logger.info("ç»Ÿä¸€ç‰¹å¾ç®¡é“æ¨¡å—å¯¼å…¥æˆåŠŸ")
            
            config = FeaturePipelineConfig(
                enable_alpha_summary=True,
                enable_pca=True,
                pca_variance_threshold=0.95,
                enable_scaling=True,
                scaler_type='robust'
            )
            logger.info("ç‰¹å¾ç®¡é“é…ç½®åˆ›å»ºæˆåŠŸ")
            
            self.feature_pipeline = UnifiedFeaturePipeline(config)
            logger.info("ç»Ÿä¸€ç‰¹å¾ç®¡é“å®ä¾‹åˆ›å»ºæˆåŠŸ")
            logger.info("ç»Ÿä¸€ç‰¹å¾ç®¡é“åˆå§‹åŒ–æˆåŠŸ - å°†ç¡®ä¿è®­ç»ƒ-é¢„æµ‹ç‰¹å¾ä¸€è‡´æ€§")
        except Exception as e:
            logger.error(f"ç»Ÿä¸€ç‰¹å¾ç®¡é“åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            self.feature_pipeline = None
    
    def _init_alpha_summary_processor(self):
        """åˆå§‹åŒ–Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨ï¼ˆRoute Aé›†æˆï¼‰"""
        try:
            from alpha_summary_features import create_alpha_summary_processor, AlphaSummaryConfig
            
            # åˆ›å»ºAlphaæ‘˜è¦ç‰¹å¾é…ç½®
            alpha_config = AlphaSummaryConfig(
                max_alpha_features=18,  # 11ä¸ªPCA + 6ä¸ªæ‘˜è¦ + 1ä¸ªç­–ç•¥ä¿¡å·
                include_alpha_strategy_signal=True,  # åŒ…å«Alphaç­–ç•¥ç»¼åˆä¿¡å·
                pca_variance_explained=0.85,
                pls_n_components=8
            )
            
            # åˆ›å»ºå¤„ç†å™¨
            self.alpha_summary_processor = create_alpha_summary_processor(alpha_config.__dict__)
            logger.info("Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸï¼ˆåŒ…å«18ä¸ªç‰¹å¾ï¼š11PCA+6æ‘˜è¦+1ç­–ç•¥ä¿¡å·ï¼‰")
            
        except ImportError as e:
            logger.warning(f"Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨å¯¼å…¥å¤±è´¥: {e}")
            self.alpha_summary_processor = None
        except Exception as e:
            logger.warning(f"Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.alpha_summary_processor = None
        
        # ğŸ”¥ ç”Ÿäº§çº§åŠŸèƒ½ï¼šæ¨¡å‹ç‰ˆæœ¬æ§åˆ¶
        try:
            from model_version_control import ModelVersionControl
            self.version_control = ModelVersionControl("ultra_models")
            logger.info("æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿå·²å¯ç”¨")
        except ImportError as e:
            logger.warning(f"ç‰ˆæœ¬æ§åˆ¶æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
            self.version_control = None
        
        
        
        # ä¼ ç»ŸMLæ¨¡å‹ï¼ˆä½œä¸ºå¯¹æ¯”ï¼‰
        self.traditional_models = {}
        self.model_weights = {}
        
        # Professionalå¼•æ“åŠŸèƒ½
        self.risk_model_results = {}
        self.current_regime = None
        self.regime_weights = {}
        self.market_data_manager = UnifiedMarketDataManager() if MARKET_MANAGER_AVAILABLE else None
        
        # æ•°æ®å’Œç»“æœå­˜å‚¨
        self.raw_data = {}
        self.feature_data = None
        self.alpha_signals = None
        self.final_predictions = None
        self.portfolio_weights = None
        
        # é…ç½®ç®¡ç† - ç»Ÿä¸€ç¡¬ç¼–ç å‚æ•°
        model_params = self.config.get('model_params', {}) if self.config else {}
        self.model_config = BMAModelConfig.from_dict(model_params) if model_params else BMAModelConfig()
        
        # æ€§èƒ½è·Ÿè¸ª
        self.performance_metrics = {}
        self.backtesting_results = {}
        
        # å¥åº·ç›‘æ§è®¡æ•°å™¨
        self.health_metrics = {
            'risk_model_failures': 0,
            'alpha_computation_failures': 0,
            'neutralization_failures': 0,
            'prediction_failures': 0,
            'total_exceptions': 0
        }
        
        logger.info("DEBUG: å·²åˆ°è¾¾__init__æ–¹æ³•çš„æœ€åéƒ¨åˆ†ï¼Œå‡†å¤‡åˆå§‹åŒ–Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨")
        
        # ğŸ¯ åˆå§‹åŒ–Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨ï¼ˆRoute Aé›†æˆï¼‰
        logger.info("æ­£åœ¨åˆå§‹åŒ–Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨...")
        self._init_alpha_summary_processor()
        logger.info(f"Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼ŒçŠ¶æ€: {hasattr(self, 'alpha_summary_processor') and self.alpha_summary_processor is not None}")
        
        logger.info("DEBUG: __init__æ–¹æ³•å³å°†å®Œæˆ")
    
    def _init_optimization_components(self):
        """åˆå§‹åŒ–å†…å­˜ä¼˜åŒ–ç»„ä»¶"""
        try:
            # ä½¿ç”¨ç®€åŒ–çš„å†…å­˜ç®¡ç†ï¼ˆç§»é™¤å¤–éƒ¨ä¾èµ–ï¼‰
            import gc
            optimization_available = True  # ä½¿ç”¨å†…ç½®çš„åŸºç¡€ä¼˜åŒ–åŠŸèƒ½
            
            # ä½¿ç”¨å†…ç½®çš„ç®€åŒ–ç»„ä»¶ï¼ˆç§»é™¤å¤–éƒ¨ä¾èµ–ï¼‰
            if optimization_available:
                # ä½¿ç”¨å†…ç½®å†…å­˜ç®¡ç†
                self.memory_manager = self._create_basic_memory_manager()
                logger.info("å†…å­˜ä¼˜åŒ–ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
                
                # ç®€åŒ–ç»„ä»¶åˆå§‹åŒ–
                self.streaming_loader = None  # ç›´æ¥åŠ è½½ï¼Œä¸ä½¿ç”¨æµå¼å¤„ç†
                self.progress_monitor = self._create_basic_progress_monitor()
                self.model_cache = None  # ä¸ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–
                self.batch_trainer = None  # ä½¿ç”¨æ ‡å‡†è®­ç»ƒ
            
            # ä½¿ç”¨å…¨å±€è®­ç»ƒæ¨¡å¼
            self.batch_size = 250  
            self.memory_optimized = False  # ç®€åŒ–ï¼šä¸ä½¿ç”¨å†…å­˜ä¼˜åŒ–
            
            logger.info("å†…å­˜ä¼˜åŒ–ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
            
        except ImportError as e:
            logger.warning(f"ä¼˜åŒ–ç»„ä»¶å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨å…¨å±€è®­ç»ƒæ¨¡å¼: {e}")
            self.memory_optimized = False  # ç®€åŒ–ï¼šä¸ä½¿ç”¨å†…å­˜ä¼˜åŒ–
            self.batch_size = 150  # é€‚ä¸­çš„æ‰¹æ¬¡å¤§å°ï¼Œå…¼é¡¾æ€§èƒ½å’Œå†…å­˜
            # åˆå§‹åŒ–åŸºç¡€ç»„ä»¶
            self.memory_manager = None
            self.streaming_loader = None
            self.progress_monitor = self._create_basic_progress_monitor()
            self.model_cache = None
            self.batch_trainer = None
    
    
    def _run_optimized_analysis(self, tickers: List[str], start_date: str, end_date: str, 
                               top_n: int, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå†…å­˜ä¼˜åŒ–ç‰ˆåˆ†æï¼ˆç¡®ä¿ç»“æœå‡†ç¡®æ€§ï¼‰"""
        logger.info(f"å¯åŠ¨å†…å­˜ä¼˜åŒ–åˆ†æ: {len(tickers)} è‚¡ç¥¨ï¼Œæ‰¹æ¬¡å¤§å° {self.batch_size}")
        
        try:
            # å¯åŠ¨è¿›åº¦ç›‘æ§
            self.progress_monitor.add_stage("å…¨å±€ç‰¹å¾åˆ†æ", 1)
            self.progress_monitor.add_stage("æ•°æ®ä¸‹è½½", len(tickers))
            self.progress_monitor.add_stage("ç‰¹å¾å·¥ç¨‹", len(tickers))
            self.progress_monitor.add_stage("æ¨¡å‹è®­ç»ƒ", len(tickers))
            self.progress_monitor.add_stage("å…¨å±€æ ¡å‡†", 1)
            self.progress_monitor.add_stage("ç»“æœæ±‡æ€»", 1)
            self.progress_monitor.start_training()
            
            # ğŸ¯ ç¬¬ä¸€æ­¥ï¼šå…¨å±€ç‰¹å¾åˆ†æï¼ˆç¡®ä¿ä¸€è‡´æ€§ï¼‰
            self.progress_monitor.start_stage("å…¨å±€ç‰¹å¾åˆ†æ")
            global_stats = self._compute_global_feature_stats(tickers, start_date, end_date)
            self.progress_monitor.complete_stage("å…¨å±€ç‰¹å¾åˆ†æ", success=True)
            
            # ğŸ¯ ç¬¬äºŒæ­¥ï¼šåˆ†æ‰¹è®­ç»ƒï¼ˆä½¿ç”¨å…¨å±€ç»Ÿè®¡ï¼‰
            def batch_analysis_func(batch_tickers):
                return self._analyze_batch_optimized(batch_tickers, start_date, end_date, global_stats)
            
            if self.batch_trainer:
                results = self.batch_trainer.train_universe(
                    universe=tickers,
                    model_trainer_func=batch_analysis_func
                )
            else:
                # ğŸš¨ ä¸å…è®¸å›é€€åˆ°åŸºç¡€æ‰¹æ¬¡å¤„ç†
                logger.error("æ‰¹æ¬¡è®­ç»ƒå™¨ä¸å¯ç”¨ï¼Œæ‹’ç»ä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                raise ValueError("æ‰¹æ¬¡è®­ç»ƒå™¨ä¸å¯ç”¨ï¼Œç³»ç»Ÿæ— æ³•è¿›è¡Œæ‰¹é‡åˆ†æ")
            
            # ğŸ¯ ç¬¬ä¸‰æ­¥ï¼šå…¨å±€æ ¡å‡†ï¼ˆæ¶ˆé™¤æ‰¹æ¬¡åå·®ï¼‰
            self.progress_monitor.start_stage("å…¨å±€æ ¡å‡†")
            calibrated_results = self._calibrate_batch_results(results, global_stats)
            self.progress_monitor.complete_stage("å…¨å±€æ ¡å‡†", success=True)
            
            # æ±‡æ€»ç»“æœ
            analysis_results.update({
                'success': True,
                'total_time': (datetime.now() - analysis_results['start_time']).total_seconds(),
                'predictions': calibrated_results.get('predictions', {}),
                'model_performance': calibrated_results.get('model_performance', {}),
                'feature_importance': calibrated_results.get('feature_importance', {}),
                'optimization_stats': {
                    'memory_usage': self.memory_manager.get_statistics() if self.memory_manager else {},
                    'cache_stats': self.model_cache.get_cache_statistics() if self.model_cache else {},
                    'training_stats': results.get('training_statistics', {})
                }
            })
            
            # ç”Ÿæˆæ¨èï¼ˆä½¿ç”¨æ ¡å‡†åçš„ç»“æœï¼‰
            recommendations = self._generate_recommendations_from_predictions(
                calibrated_results.get('predictions', {}), top_n
            )
            analysis_results['recommendations'] = recommendations
            
            # ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            result_file = f"result/bma_ultra_enhanced_optimized_{timestamp}.xlsx"
            self._save_optimized_results(analysis_results, result_file)
            analysis_results['result_file'] = result_file
            
            self.progress_monitor.complete_training(success=True)
            logger.info("ä¼˜åŒ–åˆ†æå®Œæˆ")
            
            return analysis_results
            
        except Exception as e:
            import traceback
            logger.error(f"ä¼˜åŒ–åˆ†æå¤±è´¥: {e}")
            logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            self.progress_monitor.complete_training(success=False)
            analysis_results.update({
                'success': False,
                'error': str(e),
                'total_time': (datetime.now() - analysis_results['start_time']).total_seconds()
            })
            return analysis_results
    
    def _compute_global_feature_stats(self, tickers: List[str], start_date: str, end_date: str) -> Dict[str, Any]:
        """è®¡ç®—å…¨å±€ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯ï¼ˆç¡®ä¿æ‰¹æ¬¡é—´ä¸€è‡´æ€§ï¼‰"""
        logger.info("è®¡ç®—å…¨å±€ç‰¹å¾ç»Ÿè®¡...")
        
        # é‡‡æ ·ç­–ç•¥ï¼šéšæœºé€‰æ‹©æ ·æœ¬è‚¡ç¥¨è®¡ç®—å…¨å±€ç»Ÿè®¡
        import random
        random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
        sample_size = min(150, len(tickers))  # é€‚ä¸­çš„é‡‡æ ·å¤§å°
        sample_tickers = random.sample(tickers, sample_size)
        
        all_features = []
        successful_samples = 0
        
        for ticker in sample_tickers:
            try:
                data = self._download_single_ticker(ticker, start_date, end_date)
                if data is not None and len(data) >= 30:
                    features = self._calculate_features_optimized(data, ticker)
                    if features is not None and not features.empty and len(features) > 10:
                        # åªä¿ç•™æ•°å€¼ç‰¹å¾
                        numeric_features = features.select_dtypes(include=[np.number])
                        if not numeric_features.empty:
                            all_features.append(numeric_features)
                            successful_samples += 1
                
                # é™åˆ¶é‡‡æ ·æ•°é‡ä»¥æ§åˆ¶å†…å­˜å’Œæ—¶é—´
                if successful_samples >= 80:
                    break
                    
            except Exception as e:
                logger.warning(f"å…¨å±€ç»Ÿè®¡é‡‡æ ·å¤±è´¥ {ticker}: {e}")
                continue
        
        if not all_features:
            logger.warning("å…¨å±€ç‰¹å¾ç»Ÿè®¡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            return {
                'feature_means': {'returns': 0.0, 'volatility': 0.02, 'rsi': 50.0, 'sma_ratio': 1.0},
                'feature_stds': {'returns': 0.02, 'volatility': 0.01, 'rsi': 15.0, 'sma_ratio': 0.1},
                'feature_names': ['returns', 'volatility', 'rsi', 'sma_ratio'],
                'sample_size': 0
            }
        
        try:
            # âœ… CRITICAL FIX: ä½¿ç”¨æ—¶é—´çª—å£ç»Ÿè®¡ä»£æ›¿å…¨æ ·æœ¬ç»Ÿè®¡
            # åˆå¹¶æ‰€æœ‰ç‰¹å¾æ•°æ®ä½†ä¿ç•™æ—¶é—´ä¿¡æ¯
            combined_features = pd.concat(all_features, ignore_index=False)
            
            # ä½¿ç”¨ç»Ÿä¸€çš„æ•°å€¼æ¸…ç†ç­–ç•¥
            combined_features = self.data_validator.clean_numeric_data(combined_features, "combined_features", strategy="smart")
            
            # âœ… FIXED: ä½¿ç”¨å±•å¼€çª—å£ç»Ÿè®¡ä»£æ›¿å…¨æ ·æœ¬ç»Ÿè®¡ï¼ˆé˜²æ­¢æ•°æ®æ³„éœ²ï¼‰
            # è®¡ç®—å‰80%æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯ä½œä¸ºæ ‡å‡†åŒ–åŸºå‡†
            n_samples = len(combined_features)
            train_end_idx = int(n_samples * 0.8)  # åªä½¿ç”¨å‰80%æ•°æ®è®¡ç®—ç»Ÿè®¡
            
            if train_end_idx > 0:
                train_features = combined_features.iloc[:train_end_idx]
                feature_means = train_features.mean()
                feature_means = feature_means.fillna(0).to_dict()
                
                feature_stds = train_features.std()
                feature_stds = feature_stds.fillna(1).where(feature_stds > 1e-8, 1).to_dict()
            else:
                # å›é€€åˆ°å…¨æ ·æœ¬ï¼ˆå°æ•°æ®é›†æƒ…å†µï¼‰
                feature_means = combined_features.mean().fillna(0).to_dict()
                feature_stds = combined_features.std().fillna(1).where(combined_features.std() > 1e-8, 1).to_dict()
            
            # ç¡®ä¿æ ‡å‡†å·®ä¸ä¸º0
            for col in feature_stds:
                if feature_stds[col] <= 0:
                    feature_stds[col] = 1.0
                    
        except Exception as e:
            logger.error(f"å…¨å±€ç»Ÿè®¡è®¡ç®—å¤±è´¥: {e}")
            return {
                'feature_means': {'returns': 0.0, 'volatility': 0.02, 'rsi': 50.0, 'sma_ratio': 1.0},
                'feature_stds': {'returns': 0.02, 'volatility': 0.01, 'rsi': 15.0, 'sma_ratio': 0.1},
                'feature_names': ['returns', 'volatility', 'rsi', 'sma_ratio'],
                'sample_size': 0
            }
        
        # æ¸…ç†å†…å­˜
        del all_features, combined_features
        if self.memory_manager:
            self.memory_manager.force_garbage_collection()
        
        global_stats = {
            'feature_means': feature_means,
            'feature_stds': feature_stds,
            'feature_names': list(feature_means.keys()),
            'sample_size': successful_samples
        }
        
        logger.info(f"å…¨å±€ç»Ÿè®¡å®Œæˆ: {successful_samples} æ ·æœ¬, {len(feature_means)} ç‰¹å¾")
        return global_stats
    
    def _calibrate_batch_results(self, batch_results: Dict[str, Any], global_stats: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼ºçš„é¢„æµ‹æ ¡å‡†ï¼ŒåŒ…å«ç½®ä¿¡åŒºé—´å’Œç¨³å®šæ€§æ£€æŸ¥"""
        logger.info("å¼€å§‹å¢å¼ºé¢„æµ‹æ ¡å‡†...")
        
        predictions = batch_results.get('predictions', {})
        model_performance = batch_results.get('model_performance', {})
        
        if not predictions:
            return batch_results
        
        # å°†é¢„æµ‹å€¼è½¬æ¢ä¸ºDataFrameè¿›è¡Œç»Ÿè®¡
        pred_data = []
        for ticker, pred in predictions.items():
            confidence = model_performance.get(ticker, 0.5)
            pred_data.append({'ticker': ticker, 'raw_prediction': pred, 'confidence': confidence})
        
        pred_df = pd.DataFrame(pred_data)
        
        if len(pred_df) < 10:
            logger.warning("é¢„æµ‹æ•°é‡å¤ªå°‘ï¼Œè·³è¿‡æ ¡å‡†")
            return batch_results
        
        # å¢å¼ºæ ¡å‡†æ­¥éª¤
        logger.info("æ‰§è¡Œå¤šå±‚é¢„æµ‹æ ¡å‡†...")
        
        # 1. å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†
        q1, q99 = pred_df['raw_prediction'].quantile([0.01, 0.99])
        outliers = (pred_df['raw_prediction'] < q1) | (pred_df['raw_prediction'] > q99)
        if outliers.sum() > 0:
            logger.warning(f"å‘ç°{outliers.sum()}ä¸ªé¢„æµ‹å¼‚å¸¸å€¼ï¼Œè¿›è¡Œæˆªæ–­å¤„ç†")
            pred_df['raw_prediction'] = pred_df['raw_prediction'].clip(lower=q1, upper=q99)
        
        # 2. åŸºäºç½®ä¿¡åº¦çš„åŠ æƒæ ¡å‡†
        pred_df['confidence_weight'] = pred_df['confidence'].clip(0.1, 1.0)  # æœ€å°æƒé‡0.1
        
        # 3. ç¨³å¥çš„æ’åè®¡ç®—ï¼ˆä½¿ç”¨åŠ æƒæ’åï¼‰
        pred_df['weighted_score'] = pred_df['raw_prediction'] * pred_df['confidence_weight']
        pred_df['percentile_rank'] = pred_df['weighted_score'].rank(pct=True)
        
        # 4. å¤šé‡æ ¡å‡†æ–¹æ³•
        from scipy.stats import norm, rankdata
        
        # æ–¹æ³•1: æ ‡å‡†æ­£æ€æ˜ å°„
        pred_df['calibrated_normal'] = norm.ppf(
            pred_df['percentile_rank'].clip(0.005, 0.995)  # æ›´ä¿å®ˆçš„æå€¼å¤„ç†
        )
        
        # æ–¹æ³•2: åˆ†ä½æ•°å‡åŒ€åŒ–
        pred_df['calibrated_uniform'] = pred_df['percentile_rank']
        
        # æ–¹æ³•3: åŸºäºç½®ä¿¡åº¦çš„æ··åˆæ ¡å‡†
        high_conf_mask = pred_df['confidence'] > 0.7
        pred_df['calibrated_mixed'] = pred_df['calibrated_normal']
        pred_df.loc[~high_conf_mask, 'calibrated_mixed'] = (
            0.5 * pred_df.loc[~high_conf_mask, 'calibrated_normal'] + 
            0.5 * pred_df.loc[~high_conf_mask, 'calibrated_uniform']
        )
        
        # 5. æœ€ç»ˆæ ¡å‡†ç»“æœï¼ˆé€‰æ‹©æ··åˆæ–¹æ³•ï¼‰
        min_pred = pred_df['calibrated_mixed'].min()
        max_pred = pred_df['calibrated_mixed'].max()
        
        if max_pred > min_pred:
            pred_df['final_prediction'] = (
                (pred_df['calibrated_mixed'] - min_pred) / (max_pred - min_pred)
            )
        else:
            pred_df['final_prediction'] = 0.5
        
        # 6. è®¡ç®—æ ¡å‡†è´¨é‡æŒ‡æ ‡
        original_std = pred_df['raw_prediction'].std()
        calibrated_std = pred_df['final_prediction'].std()
        
        # æ›´æ–°ç»“æœ
        calibrated_predictions = dict(zip(pred_df['ticker'], pred_df['final_prediction']))
        
        calibrated_results = batch_results.copy()
        calibrated_results['predictions'] = calibrated_predictions
        calibrated_results['calibration_info'] = {
            'original_count': len(predictions),
            'calibrated_count': len(calibrated_predictions),
            'calibration_method': 'enhanced_multi_stage',
            'outliers_detected': int(outliers.sum()) if 'outliers' in locals() else 0,
            'high_confidence_count': int((pred_df['confidence'] > 0.7).sum()),
            'prediction_spread': {
                'original_std': float(original_std),
                'calibrated_std': float(calibrated_std),
                'spread_ratio': float(calibrated_std / original_std) if original_std > 0 else 1.0
            },
            'global_features_used': len(global_stats.get('feature_names', []))
        }
        
        logger.info(f"æ ¡å‡†å®Œæˆ: {len(calibrated_predictions)} ä¸ªé¢„æµ‹å€¼")
        return calibrated_results

    def _basic_batch_processing(self, tickers: List[str], start_date: str, end_date: str, 
                               global_stats: Dict[str, Any] = None) -> Dict[str, Any]:
        """åŸºç¡€æ‰¹æ¬¡å¤„ç†æ¨¡å¼ - æ ¹æ®é…ç½®å¯ç”¨"""
        if not self.memory_optimized:
            logger.warning(f"å†…å­˜ä¼˜åŒ–æœªå¯ç”¨ï¼Œé‡å®šå‘åˆ°å…¨å±€è®­ç»ƒæ¨¡å¼: {len(tickers)} è‚¡ç¥¨")
            # é‡å®šå‘åˆ°å…¨å±€è®­ç»ƒ
            return self._analyze_batch_optimized(tickers, start_date, end_date, global_stats)
        
        logger.info(f"å¯ç”¨æ‰¹å¤„ç†æ¨¡å¼: {len(tickers)} è‚¡ç¥¨")
        
        # å®ç°ç®€åŒ–çš„æ‰¹å¤„ç†é€»è¾‘
        batch_size = self.batch_size if hasattr(self, 'batch_size') else 50
        results = {'predictions': {}, 'success_rate': 0.0}
        
        for i in range(0, len(tickers), batch_size):
            batch = tickers[i:i+batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} è‚¡ç¥¨")
            batch_result = self._analyze_batch_optimized(batch, start_date, end_date, global_stats)
            results['predictions'].update(batch_result.get('predictions', {}))
        
        results['success_rate'] = len(results['predictions']) / len(tickers) if tickers else 0.0
        return results

    def _analyze_batch_optimized(self, batch_tickers: List[str], start_date: str, end_date: str, 
                                global_stats: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        ğŸ¤– é©å‘½æ€§æ”¹è¿›ï¼šæ‰¹å¤„ç†ä¹Ÿä½¿ç”¨å®Œæ•´çš„æœºå™¨å­¦ä¹ æµç¨‹
        ä¸å†ä½¿ç”¨ç®€åŒ–é¢„æµ‹ï¼Œè€Œæ˜¯ä¸ºæ¯ä¸ªæ‰¹æ¬¡è®­ç»ƒå®Œæ•´çš„MLæ¨¡å‹
        """
        logger.info(f"ğŸš€ å¯åŠ¨MLé©±åŠ¨çš„æ‰¹æ¬¡åˆ†æ: {len(batch_tickers)} è‚¡ç¥¨")
        
        try:
            # ğŸ”¥ ç¬¬1æ­¥ï¼šä¸ºå½“å‰æ‰¹æ¬¡ä¸‹è½½å’Œå‡†å¤‡æ•°æ®
            logger.info("ç¬¬1æ­¥: æ‰¹æ¬¡æ•°æ®æ”¶é›†å’Œç‰¹å¾å·¥ç¨‹")
            stock_data = {}
            feature_data_list = []
            
            for ticker in batch_tickers:
                try:
                    data = self._load_ticker_data_optimized(ticker, start_date, end_date)
                    if data is not None and len(data) >= 30:
                        stock_data[ticker] = data
                        
                        # è®¡ç®—ç‰¹å¾
                        features = self._calculate_features_optimized(data, ticker, global_stats)
                        if features is not None and not features.empty:
                            # æ·»åŠ tickeræ ‡è¯†ç”¨äºåç»­é¢„æµ‹
                            features['ticker'] = ticker
                            feature_data_list.append(features)
                            
                except Exception as e:
                    logger.debug(f"æ‰¹æ¬¡æ•°æ®å¤„ç†å¤±è´¥ {ticker}: {e}")
                    continue
            
            if not feature_data_list:
                logger.warning("æ‰¹æ¬¡ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè¿”å›ç©ºç»“æœ")
                return {'predictions': {}, 'model_performance': {}, 'feature_importance': {}}
            
            # åˆå¹¶ç‰¹å¾æ•°æ®
            combined_features = pd.concat(feature_data_list, ignore_index=True)
            logger.info(f"æ‰¹æ¬¡ç‰¹å¾æ•°æ®: {combined_features.shape[0]} æ ·æœ¬, {combined_features.shape[1]} ç‰¹å¾")
            
            # ğŸ”¥ ç¬¬2æ­¥ï¼šä¸ºå½“å‰æ‰¹æ¬¡è®­ç»ƒå®Œæ•´çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼
            logger.info("ç¬¬2æ­¥: ä¸ºå½“å‰æ‰¹æ¬¡è®­ç»ƒMLæ¨¡å‹")
            batch_training_results = self.train_enhanced_models(combined_features)
            
            # ğŸ”¥ ç¬¬3æ­¥ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆé¢„æµ‹
            logger.info("ç¬¬3æ­¥: ä½¿ç”¨è®­ç»ƒå¥½çš„MLæ¨¡å‹ç”Ÿæˆé¢„æµ‹")
            batch_predictions = {}
            batch_performance = {}
            batch_importance = {}
            
            # å¯¹æ¯åªè‚¡ç¥¨ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
            for ticker in batch_tickers:
                if ticker not in stock_data:
                    continue
                    
                try:
                    # è·å–è¯¥è‚¡ç¥¨çš„ç‰¹å¾
                    ticker_features = combined_features[combined_features['ticker'] == ticker]
                    if ticker_features.empty:
                        continue
                    
                    # ç§»é™¤tickeråˆ—ï¼Œåªä¿ç•™æ•°å€¼ç‰¹å¾
                    ticker_features = ticker_features.drop('ticker', axis=1, errors='ignore')
                    
                    # ğŸ¯ ä½¿ç”¨æˆ‘ä»¬æ–°çš„MLé¢„æµ‹å‡½æ•°ï¼
                    # ä¸´æ—¶å­˜å‚¨è®­ç»ƒç»“æœåˆ°å®ä¾‹ï¼Œä»¥ä¾¿é¢„æµ‹å‡½æ•°è®¿é—®
                    self._current_batch_training_results = batch_training_results
                    
                    # ä½¿ç”¨BMA Enhancedç³»ç»Ÿè¿›è¡Œé¢„æµ‹ï¼ˆæ›¿ä»£å·²åˆ é™¤çš„æ‰¹é‡é¢„æµ‹æ–¹æ³•ï¼‰
                    prediction_result = self._generate_prediction_optimized(ticker, ticker_features)
                    
                    if prediction_result is not None and (isinstance(prediction_result, dict) or not hasattr(prediction_result, 'empty') or not prediction_result.empty):
                        batch_predictions[ticker] = prediction_result['prediction']
                        batch_performance[ticker] = prediction_result['confidence']
                        batch_importance[ticker] = prediction_result['importance']
                        
                        logger.debug(f"æ‰¹æ¬¡MLé¢„æµ‹ {ticker}: {prediction_result['prediction']:.6f} "
                                   f"(ç½®ä¿¡åº¦: {prediction_result['confidence']:.3f}, "
                                   f"æ¥æº: {prediction_result['model_details']['source']})")
                    
                except Exception as e:
                    logger.debug(f"æ‰¹æ¬¡é¢„æµ‹å¤±è´¥ {ticker}: {e}")
                    continue
            
            # æ¸…ç†ä¸´æ—¶å­˜å‚¨
            if hasattr(self, '_current_batch_training_results'):
                delattr(self, '_current_batch_training_results')
            
            logger.info(f"æ‰¹æ¬¡MLåˆ†æå®Œæˆ: {len(batch_predictions)}/{len(batch_tickers)} æˆåŠŸ")
            
            return {
                'predictions': batch_predictions,
                'model_performance': batch_performance,
                'feature_importance': batch_importance,
                'batch_metadata': {
                    'total_tickers': len(batch_tickers),
                    'successful_count': len(batch_predictions),
                    'success_rate': len(batch_predictions) / len(batch_tickers) if batch_tickers else 0,
                    'training_summary': batch_training_results,
                    'ml_models_used': list(batch_training_results.get('traditional_models', {}).keys()),
                    'source': 'full_ml_pipeline'
                }
            }
            
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡MLåˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'predictions': {}, 'model_performance': {}, 'feature_importance': {}}
        
        batch_results = {
            'predictions': {},
            'model_performance': {},
            'feature_importance': {}
        }
        
        successful_count = 0
        
        failed_tickers = []
        
        for ticker in batch_tickers:
            try:
                # æ£€æŸ¥æ¨¡å‹ç¼“å­˜
                cache_key = f"{ticker}_{start_date}_{end_date}"
                cached_result = None
                
                if hasattr(self, 'model_cache'):
                    # ç®€åŒ–ç¼“å­˜æ£€æŸ¥
                    # å¯ç”¨æ™ºèƒ½ç¼“å­˜æ£€æŸ¥
                    if self.model_cache:
                        cached_result = self.model_cache.get_analysis_result(cache_key)
                        if cached_result:
                            logger.info(f"ç¼“å­˜å‘½ä¸­: {ticker}")
                    else:
                        cached_result = None
                
                if cached_result:
                    batch_results['predictions'][ticker] = cached_result
                    successful_count += 1
                    continue
                
                # æµå¼åŠ è½½æ•°æ®
                data = self._load_ticker_data_optimized(ticker, start_date, end_date)
                validation = self.data_validator.validate_dataframe(data, f"{ticker}_data", min_rows=20)
                if not validation['valid']:
                    logger.warning(f"æ‰¹æ¬¡åˆ†æ: {ticker} æ•°æ®éªŒè¯å¤±è´¥: {validation['errors']}")
                    failed_tickers.append(ticker)
                    continue
                
                # è®¡ç®—ç‰¹å¾ï¼ˆä½¿ç”¨å…¨å±€ç»Ÿè®¡æ ‡å‡†åŒ–ï¼‰
                features = self._calculate_features_optimized(data, ticker, global_stats)
                feature_validation = self.data_validator.validate_dataframe(features, f"{ticker}_features", min_rows=10)
                if not feature_validation['valid']:
                    logger.warning(f"æ‰¹æ¬¡åˆ†æ: {ticker} ç‰¹å¾éªŒè¯å¤±è´¥: {feature_validation['errors']}")
                    continue
                
                # ç”Ÿæˆé¢„æµ‹
                prediction_result = self._generate_prediction_optimized(ticker, features)
                if prediction_result is not None and (isinstance(prediction_result, dict) or not hasattr(prediction_result, 'empty') or not prediction_result.empty):
                    # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                    prediction = prediction_result.get('prediction', 0.0)
                    confidence = prediction_result.get('confidence', 0.5)
                    importance = prediction_result.get('importance', {})
                    
                    # ç±»å‹æ£€æŸ¥å’Œè½¬æ¢
                    if not isinstance(prediction, (int, float)):
                        prediction = 0.0
                    if not isinstance(confidence, (int, float)):
                        confidence = 0.5
                    if not isinstance(importance, dict):
                        importance = {}
                    
                    batch_results['predictions'][ticker] = prediction
                    batch_results['model_performance'][ticker] = confidence
                    batch_results['feature_importance'][ticker] = importance
                    successful_count += 1
                
                # å†…å­˜æ¸…ç†
                del data, features
                if successful_count % 50 == 0 and self.memory_manager:
                    self.memory_manager.force_garbage_collection()
                    
            except Exception as e:
                logger.warning(f"æ‰¹æ¬¡åˆ†æå¤±è´¥ {ticker}: {e}")
                failed_tickers.append(ticker)
                continue
        
        # æ‰¹æ¬¡è´¨é‡æ£€æŸ¥
        success_rate = successful_count / len(batch_tickers) if batch_tickers else 0
        logger.info(f"æ‰¹æ¬¡åˆ†æå®Œæˆ: {successful_count}/{len(batch_tickers)} æˆåŠŸ (æˆåŠŸç‡: {success_rate:.1%})")
        
        if failed_tickers:
            logger.warning(f"æ‰¹æ¬¡å¤±è´¥è‚¡ç¥¨: {failed_tickers[:5]}{'...' if len(failed_tickers) > 5 else ''}")
        
        # å¦‚æœæˆåŠŸç‡è¿‡ä½ï¼Œæ·»åŠ è­¦å‘Š
        if success_rate < 0.3:
            logger.error(f"æ‰¹æ¬¡æˆåŠŸç‡è¿‡ä½ ({success_rate:.1%})ï¼Œå¯èƒ½å­˜åœ¨ç³»ç»Ÿæ€§é—®é¢˜")
        
        batch_results['batch_metadata'] = {
            'total_tickers': len(batch_tickers),
            'successful_count': successful_count,
            'failed_count': len(failed_tickers),
            'success_rate': success_rate,
            'failed_tickers': failed_tickers
        }
        
        return batch_results
    
    def _load_ticker_data_optimized(self, ticker: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """ä¼˜åŒ–ç‰ˆæ•°æ®åŠ è½½"""
        if hasattr(self, 'streaming_loader'):
            return self.streaming_loader.get_data(
                ticker, "price_data", start_date, end_date,
                lambda t, s, e: self._download_single_ticker(t, s, e)
            )
        else:
            return self._download_single_ticker(ticker, start_date, end_date)
    
    def _download_single_ticker(self, ticker: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """ä¸‹è½½å•ä¸ªè‚¡ç¥¨æ•°æ®"""
        try:
            # ä¿®å¤è°ƒç”¨æ–¹å¼ï¼šä½¿ç”¨startå’Œendå‚æ•°
            data = pc.download(ticker, start=start_date, end=end_date, interval='1d')
            return data if data is not None and not data.empty else None
        except Exception as e:
            logger.warning(f"æ•°æ®ä¸‹è½½å¤±è´¥ {ticker}: {e}")
            return None
    
    def _calculate_features_optimized(self, data: pd.DataFrame, ticker: str, 
                                     global_stats: Dict[str, Any] = None) -> Optional[pd.DataFrame]:
        """ä¼˜åŒ–ç‰ˆç‰¹å¾è®¡ç®— - é›†æˆç»Ÿä¸€ç‰¹å¾ç®¡é“"""
        try:
            if len(data) < 20:  # ğŸ”§ é™ä½ç‰¹å¾è®¡ç®—çš„æ•°æ®è¦æ±‚ï¼Œæé«˜é€šè¿‡ç‡
                return None
            
            # ğŸ”§ Step 1: ç”ŸæˆåŸºç¡€æŠ€æœ¯ç‰¹å¾
            features = pd.DataFrame(index=data.index)
            
            # ç¡®ä¿æœ‰closeåˆ—ï¼ˆæ”¯æŒå¤§å°å†™å…¼å®¹ï¼‰
            close_col = None
            if 'close' in data.columns:
                close_col = 'close'
            elif 'Close' in data.columns:
                close_col = 'Close'
            else:
                logger.warning(f"ç‰¹å¾è®¡ç®—å¤±è´¥ {ticker}: æ‰¾ä¸åˆ°close/Closeåˆ—")
                return None
                
            # âœ… NEW: åŸºç¡€ç‰¹å¾æŒ‰ç…§å·®å¼‚åŒ–æ»åç­–ç•¥è®¡ç®— - ç»Ÿä¸€ä½¿ç”¨MarketDataManager
            # æ‰€æœ‰åŸºç¡€æŠ€æœ¯ç‰¹å¾ä½¿ç”¨T-1æ»åï¼ˆä»·æ ¼/æŠ€æœ¯ç±»ï¼‰
            features['returns'] = data[close_col].pct_change().shift(1)  # T-1
            features['volatility'] = features['returns'].rolling(20).std().shift(1)  # T-1
            
            # ä½¿ç”¨ç»Ÿä¸€çš„æŠ€æœ¯æŒ‡æ ‡è®¡ç®—
            if hasattr(self, 'market_data_manager') and self.market_data_manager:
                tech_indicators = self.market_data_manager.calculate_technical_indicators(data)
                if 'rsi' in tech_indicators:
                    features['rsi'] = tech_indicators['rsi'].shift(1)  # T-1
                else:
                    features['rsi'] = self._calculate_rsi(data[close_col]).shift(1)  # å¤‡ç”¨æ–¹æ¡ˆ
            else:
                features['rsi'] = self._calculate_rsi(data[close_col]).shift(1)  # å¤‡ç”¨æ–¹æ¡ˆ
                
            features['sma_ratio'] = (data[close_col] / data[close_col].rolling(20).mean()).shift(1)  # T-1
            
            # æ¸…ç†åŸºç¡€ç‰¹å¾
            features = features.dropna()
            if len(features) < 10:
                return None
            
            # âœ… NEW: è®°å½•æ»åä¿¡æ¯ç”¨äºéªŒè¯
            if hasattr(self, 'alpha_engine') and hasattr(self.alpha_engine, 'lag_manager'):
                logger.debug(f"{ticker}: åŸºç¡€ç‰¹å¾ä½¿ç”¨T-1æ»åï¼Œä¸æŠ€æœ¯ç±»å› å­å¯¹é½")
            
            # ğŸ”§ Step 2: ç”ŸæˆAlphaå› å­æ•°æ®
            alpha_data = None
            try:
                alpha_data = self.alpha_engine.compute_all_alphas(data)
                if alpha_data is not None and not alpha_data.empty:
                    logger.debug(f"{ticker}: Alphaå› å­ç”ŸæˆæˆåŠŸ - {alpha_data.shape}")
                    
                    # âœ… PERFORMANCE FIX: åº”ç”¨å› å­æ­£äº¤åŒ–ï¼Œæ¶ˆé™¤å†—ä½™ï¼Œæå‡ä¿¡æ¯æ¯”ç‡
                    if PRODUCTION_FIXES_AVAILABLE:
                        try:
                            alpha_data = orthogonalize_factors_predictive_safe(
                                alpha_data,
                                method="pca_hybrid",
                                correlation_threshold=0.7
                            )
                            logger.debug(f"{ticker}: âœ… å› å­æ­£äº¤åŒ–å®Œæˆï¼Œæ¶ˆé™¤å†—ä½™å¹²æ‰°")
                        except Exception as orth_e:
                            logger.warning(f"{ticker}: å› å­æ­£äº¤åŒ–å¤±è´¥: {orth_e}")
                        
                        # âœ… PERFORMANCE FIX: åº”ç”¨æ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼Œæ¶ˆé™¤æ—¶é—´æ¼‚ç§»
                        try:
                            # è¯†åˆ«æ•°å€¼ç‰¹å¾åˆ—
                            alpha_numeric_cols = alpha_data.select_dtypes(include=[np.number]).columns.tolist()
                            alpha_numeric_cols = [col for col in alpha_numeric_cols 
                                                if col not in ['date', 'ticker']]
                            
                            if alpha_numeric_cols:
                                alpha_data = standardize_cross_sectional_predictive_safe(
                                    alpha_data,
                                    feature_cols=alpha_numeric_cols,
                                    method="robust_zscore",
                                    winsorize_quantiles=(0.01, 0.99)
                                )
                                logger.debug(f"{ticker}: âœ… æ¨ªæˆªé¢æ ‡å‡†åŒ–å®Œæˆï¼Œæ¶ˆé™¤æ—¶é—´æ¼‚ç§»")
                        except Exception as std_e:
                            logger.warning(f"{ticker}: æ¨ªæˆªé¢æ ‡å‡†åŒ–å¤±è´¥: {std_e}")
                            
            except Exception as e:
                logger.warning(f"{ticker}: Alphaå› å­ç”Ÿæˆå¤±è´¥: {e}")
            
            # ğŸ”§ Step 3: ä½¿ç”¨ç»Ÿä¸€ç‰¹å¾ç®¡é“å¤„ç†ç‰¹å¾
            if self.feature_pipeline is not None:
                try:
                    if not self.feature_pipeline.is_fitted:
                        # é¦–æ¬¡ä½¿ç”¨æ—¶æ‹Ÿåˆç®¡é“
                        processed_features, transform_info = self.feature_pipeline.fit_transform(
                            base_features=features,
                            alpha_data=alpha_data,
                            dates=features.index
                        )
                        logger.info(f"{ticker}: ç»Ÿä¸€ç‰¹å¾ç®¡é“æ‹Ÿåˆå®Œæˆ - {features.shape} -> {processed_features.shape}")
                    else:
                        # åç»­ä½¿ç”¨æ—¶åªè½¬æ¢
                        processed_features = self.feature_pipeline.transform(
                            base_features=features,
                            alpha_data=alpha_data,
                            dates=features.index
                        )
                        logger.debug(f"{ticker}: ç»Ÿä¸€ç‰¹å¾ç®¡é“è½¬æ¢å®Œæˆ - {features.shape} -> {processed_features.shape}")
                    
                    return processed_features
                    
                except Exception as e:
                    logger.error(f"{ticker}: ç»Ÿä¸€ç‰¹å¾ç®¡é“å¤„ç†å¤±è´¥: {e}")
                    # ğŸš¨ ä¸å…è®¸å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
                    raise ValueError(f"ç‰¹å¾ç®¡é“å¤„ç†å¤±è´¥ï¼Œæ— æ³•ç»§ç»­: {str(e)}")
            
            # ğŸš¨ ä¸å…è®¸å›é€€åˆ°ä¼ ç»Ÿç‰¹å¾å¤„ç†ï¼Œç›´æ¥æŠ¥é”™
            # ä½¿ç”¨å…¨å±€ç»Ÿè®¡è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆç¡®ä¿æ‰¹æ¬¡é—´ä¸€è‡´æ€§ï¼‰
            if global_stats and global_stats.get('feature_means'):
                features = self._standardize_features(features, global_stats)
            
            return features if len(features) > 5 else None  # ğŸ”§ é™ä½æœ€ç»ˆç‰¹å¾æ•°é‡è¦æ±‚
            
        except Exception as e:
            logger.warning(f"ç‰¹å¾è®¡ç®—å¤±è´¥ {ticker}: {e}")
            return None
    
    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
        """è®¡ç®—RSIæŒ‡æ ‡"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
    
    def _generate_prediction_optimized(self, ticker: str, features: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        ğŸ¤– ä½¿ç”¨è®­ç»ƒå¥½çš„æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡ŒçœŸæ­£çš„é¢„æµ‹
        ä¸å†ä½¿ç”¨ç¡¬ç¼–ç å…¬å¼ï¼Œè€Œæ˜¯ä½¿ç”¨é›†æˆMLæ¨¡å‹çš„é¢„æµ‹ç»“æœ
        """
        try:
            # BMA Enhancedç³»ç»Ÿé¢„æµ‹å·²åˆ é™¤ - åŠŸèƒ½èå…¥ç»Ÿä¸€è·¯å¾„
            
            # ğŸ”¥ ç¬¬äºŒä¼˜å…ˆçº§ï¼šä½¿ç”¨Alphaå› å­å¼•æ“ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼Œä¸å…è®¸å¤±è´¥ï¼‰
            if hasattr(self, 'alpha_engine') and self.alpha_engine and hasattr(self.alpha_engine, 'compute_all_alphas'):
                try:
                    alpha_prediction = self._predict_with_alpha_factors(ticker, features)
                    if alpha_prediction is not None:
                        logger.debug(f"ä½¿ç”¨Alphaå› å­é¢„æµ‹ {ticker}: {alpha_prediction['prediction']:.6f}")
                        return alpha_prediction
                    else:
                        # å¦‚æœAlphaå¼•æ“å­˜åœ¨ä½†è¿”å›Noneï¼Œè¿™è¡¨æ˜æœ‰ä¸¥é‡é—®é¢˜
                        logger.error(f"âŒ Alphaå¼•æ“å­˜åœ¨ä½†æ— æ³•ä¸º{ticker}ç”Ÿæˆé¢„æµ‹")
                except ValueError as e:
                    # Alphaé¢„æµ‹çš„ValueErrorè¡¨æ˜é…ç½®æˆ–è®­ç»ƒæœ‰é—®é¢˜ï¼Œå¿…é¡»ä¿®å¤
                    logger.error(f"âŒ Alphaå› å­é¢„æµ‹ä¸¥é‡é”™è¯¯ {ticker}: {e}")
                    # æ ¹æ®ç”¨æˆ·è¦æ±‚ï¼šä¸è¦å›é€€ï¼Œç›´æ¥æŠ¥é”™
                    raise ValueError(f"Alphaå› å­é¢„æµ‹å¤±è´¥ï¼Œç³»ç»Ÿè¦æ±‚ä¿®å¤: {e}") from e
            
            # ğŸš¨ CRITICAL FIX: ç”Ÿäº§ç¯å¢ƒéœ€è¦å®‰å…¨é™çº§ï¼Œé¿å…å•tickeræ•…éšœå¯¼è‡´ç³»ç»Ÿå´©æºƒ
            logger.error(f"âŒ {ticker} æ‰€æœ‰MLæ¨¡å‹ä¸å¯ç”¨ï¼Œå¯ç”¨ç´§æ€¥å®‰å…¨æ¨¡å¼")
            logger.error("âš ï¸ ç”Ÿäº§é£é™©è­¦å‘Š: ä½¿ç”¨é›¶é¢„æµ‹é¿å…ç³»ç»Ÿå´©æºƒï¼Œè¯¥è‚¡ç¥¨å°†è¢«æ’é™¤åœ¨æŠ•èµ„ç»„åˆå¤–")
            
            # è¿”å›é›¶é¢„æµ‹è€Œä¸æ˜¯å´©æºƒç³»ç»Ÿï¼Œè®©ä¸Šå±‚é€»è¾‘å¤„ç†
            return {
                'prediction': 0.0,
                'confidence': 0.0,
                'emergency_mode': True,
                'risk_warning': f'{ticker} é¢„æµ‹ç³»ç»Ÿæ•…éšœï¼Œå·²å¯ç”¨å®‰å…¨æ¨¡å¼',
                'exclude_from_portfolio': True
            }
            
        except Exception as e:
            logger.error(f"ğŸš¨ CRITICAL: é¢„æµ‹ç”Ÿæˆä¸¥é‡å¤±è´¥ {ticker}: {e}")
            logger.error("âš ï¸ ç”Ÿäº§é£é™©è­¦å‘Š: å¯ç”¨ç´§æ€¥å®‰å…¨æ¨¡å¼ï¼Œè¯¥è‚¡ç¥¨å°†è¢«æ’é™¤")
            
            # ç”Ÿäº§å®‰å…¨æ¨¡å¼ï¼šè¿”å›ç´§æ€¥çŠ¶æ€è€Œä¸æ˜¯å´©æºƒ
            return {
                'prediction': 0.0,
                'confidence': 0.0,
                'emergency_mode': True,
                'exception_type': type(e).__name__,
                'error_message': str(e),
                'risk_warning': f'{ticker} é¢„æµ‹ç³»ç»Ÿå¼‚å¸¸ï¼Œå·²å¯ç”¨å®‰å…¨æ¨¡å¼',
                'exclude_from_portfolio': True
            }
    
    
    # _predict_with_trained_models å·²åˆ é™¤ - åŠŸèƒ½é€šè¿‡BMA Enhancedç³»ç»Ÿæä¾›
    
    def _predict_with_alpha_factors(self, ticker: str, features: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        ğŸ¤– ä½¿ç”¨çœŸæ­£çš„æœºå™¨å­¦ä¹ Alphaå› å­è¿›è¡Œé¢„æµ‹
        ç¦æ­¢ç®€å•åŠ æƒå¹³å‡ï¼Œå¿…é¡»ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹
        """
        try:
            # ğŸš¨ ä¸¥æ ¼éªŒè¯ï¼šç¡®ä¿Alphaå¼•æ“ä¸æ˜¯Mock
            if not hasattr(self, 'alpha_engine') or self.alpha_engine is None:
                raise ValueError("âŒ Alphaå¼•æ“æœªåˆå§‹åŒ–ï¼æ— æ³•è¿›è¡ŒAlphaå› å­é¢„æµ‹")
            
            # Alphaå¼•æ“éªŒè¯å·²å®Œæˆ
            
            # å‡†å¤‡Alphaè¾“å…¥æ•°æ®
            alpha_input = self._prepare_single_ticker_alpha_data(ticker, features)
            if alpha_input is None or alpha_input.empty:
                raise ValueError(f"âŒ æ— æ³•ä¸º{ticker}å‡†å¤‡Alphaè¾“å…¥æ•°æ®")
            
            # ğŸ”¥ è®¡ç®—æ‰€æœ‰Alphaå› å­ï¼ˆè¿™ä¸€æ­¥åº”è¯¥æ˜¯ç»è¿‡è®­ç»ƒçš„ï¼‰
            logger.debug(f"è®¡ç®—{ticker}çš„Alphaå› å­...")
            alpha_signals = self.alpha_engine.compute_all_alphas(alpha_input)
            if alpha_signals is None or alpha_signals.empty:
                raise ValueError(f"âŒ Alphaå¼•æ“æ²¡æœ‰ä¸º{ticker}ç”Ÿæˆä»»ä½•ä¿¡å·")
            
            logger.info(f"Alphaå¼•æ“ä¸º{ticker}ç”Ÿæˆäº†{alpha_signals.shape[1]}ä¸ªå› å­ä¿¡å·")
            
            # è·å–æœ€æ–°çš„Alphaä¿¡å·
            latest_signals = alpha_signals.tail(1).iloc[0]
            valid_signals = latest_signals.dropna()
            
            if len(valid_signals) == 0:
                raise ValueError(f"âŒ {ticker}çš„æ‰€æœ‰Alphaä¿¡å·éƒ½æ˜¯NaN")
            
            # ğŸ”¥ ä½¿ç”¨æœºå™¨å­¦ä¹ è®­ç»ƒçš„æƒé‡ï¼ˆä¸æ˜¯ç¡¬ç¼–ç ï¼ï¼‰
            try:
                alpha_weights = self._get_alpha_factor_weights()
                logger.debug(f"è·å–åˆ°{len(alpha_weights)}ä¸ªAlphaå› å­æƒé‡")
            except ValueError as e:
                # é‡æ–°æŠ›å‡ºæƒé‡è·å–é”™è¯¯ï¼Œä¸å…è®¸å›é€€
                raise ValueError(f"âŒ Alphaå› å­æƒé‡è·å–å¤±è´¥: {e}")
            
            # ğŸ”¥ éªŒè¯æƒé‡å’Œä¿¡å·çš„åŒ¹é…æ€§
            matched_factors = 0
            weighted_prediction = 0.0
            total_weight = 0.0
            importance_dict = {}
            
            for alpha_name, weight in alpha_weights.items():
                if alpha_name in valid_signals.index:
                    signal_value = valid_signals[alpha_name]
                    weighted_prediction += signal_value * weight
                    total_weight += weight
                    importance_dict[alpha_name] = abs(signal_value * weight)
                    matched_factors += 1
            
            # ğŸš¨ ä¸¥æ ¼éªŒè¯ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„å› å­å‚ä¸é¢„æµ‹
            min_required_factors = max(3, len(alpha_weights) * 0.3)  # è‡³å°‘30%çš„å› å­æœ‰æ•ˆ
            if matched_factors < min_required_factors:
                raise ValueError(
                    f"âŒ Alphaå› å­åŒ¹é…ä¸è¶³ï¼\n"
                    f"åŒ¹é…å› å­: {matched_factors}/{len(alpha_weights)}\n"
                    f"æœ€å°è¦æ±‚: {min_required_factors}\n"
                    f"å¯èƒ½åŸå› : é…ç½®æ–‡ä»¶ä¸­çš„å› å­åç§°ä¸å®é™…ç”Ÿæˆçš„ä¸åŒ¹é…"
                )
            
            if total_weight == 0:
                raise ValueError("âŒ Alphaæƒé‡æ€»å’Œä¸º0ï¼Œæ— æ³•ç”Ÿæˆé¢„æµ‹")
            
            # è®¡ç®—æœ€ç»ˆé¢„æµ‹
            final_prediction = weighted_prediction / total_weight
            
            # ğŸ”¥ ä½¿ç”¨æ›´æ™ºèƒ½çš„æ ‡å‡†åŒ–ï¼ˆåŸºäºå†å²åˆ†å¸ƒï¼‰
            if hasattr(self.alpha_engine, 'signal_statistics'):
                # å¦‚æœæœ‰å†å²ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿ç”¨å®ƒä»¬è¿›è¡Œæ ‡å‡†åŒ–
                stats = self.alpha_engine.signal_statistics
                mean_signal = stats.get('mean', 0.0)
                std_signal = stats.get('std', 1.0)
                if std_signal > 0:
                    normalized_prediction = (final_prediction - mean_signal) / std_signal
                    final_prediction = 1 / (1 + np.exp(-normalized_prediction))  # sigmoid
                else:
                    final_prediction = max(0, min(1, (final_prediction + 1) / 2))
            else:
                # åŸºç¡€æ ‡å‡†åŒ–
                final_prediction = max(0, min(1, (final_prediction + 1) / 2))
            
            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºä¿¡å·è´¨é‡ï¼‰
            signal_strength = np.mean([abs(v) for v in importance_dict.values()])
            coverage_ratio = matched_factors / len(alpha_weights)
            confidence = min(0.95, max(0.4, signal_strength * coverage_ratio))
            
            logger.info(f"Alphaé¢„æµ‹å®Œæˆ {ticker}: {final_prediction:.6f} "
                       f"(ç½®ä¿¡åº¦: {confidence:.3f}, åŒ¹é…å› å­: {matched_factors})")
            
            return {
                'prediction': float(final_prediction),
                'confidence': float(confidence),
                'importance': importance_dict,
                'model_details': {
                    'alpha_count': len(valid_signals),
                    'matched_factors': matched_factors,
                    'total_factors': len(alpha_weights),
                    'coverage_ratio': float(coverage_ratio),
                    'signal_strength': float(signal_strength),
                    'source': 'trained_alpha_factors'
                }
            }
            
        except (ValueError, KeyError, AttributeError) as e:
            # ğŸš¨ ä¸šåŠ¡é€»è¾‘é”™è¯¯ï¼Œå¿…é¡»æŠ¥å‘Šå…·ä½“é”™è¯¯
            error_msg = f"âŒ Alphaå› å­é¢„æµ‹ä¸šåŠ¡é€»è¾‘é”™è¯¯ {ticker}: {str(e)}"
            logger.error(error_msg)
            # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä¸å…è®¸å›é€€åˆ°æŠ€æœ¯æŒ‡æ ‡
            raise ValueError(error_msg) from e
        except (ImportError, ModuleNotFoundError) as e:
            # ğŸš¨ ä¾èµ–é”™è¯¯
            error_msg = f"âŒ Alphaå› å­é¢„æµ‹ä¾èµ–é”™è¯¯ {ticker}: {str(e)}"
            logger.error(error_msg)
            raise ImportError(error_msg) from e
        except Exception as e:
            # ğŸš¨ å…¶ä»–æœªé¢„æœŸé”™è¯¯ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
            import traceback
            error_msg = f"âŒ Alphaå› å­é¢„æµ‹æœªçŸ¥é”™è¯¯ {ticker}: {str(e)}\n{traceback.format_exc()}"
            logger.error(error_msg)
            raise RuntimeError(error_msg) from e
    
    # _predict_with_ltr_model å’Œ _fallback_ltr_prediction å·²åˆ é™¤ - LTRåŠŸèƒ½é€šè¿‡BMA Enhancedç³»ç»Ÿæä¾›
    
    # _predict_with_enhanced_technical_model å·²åˆ é™¤ - æŠ€æœ¯æŒ‡æ ‡é¢„æµ‹é€šè¿‡BMA Enhancedç³»ç»Ÿæä¾›
    def _prepare_single_ticker_alpha_data(self, ticker: str, features: pd.DataFrame) -> Optional[pd.DataFrame]:
        """ä¸ºå•ä¸ªè‚¡ç¥¨å‡†å¤‡Alphaå› å­è®¡ç®—çš„è¾“å…¥æ•°æ®"""
        try:
            if features.empty:
                return None
            
            # Alphaå¼•æ“é€šå¸¸éœ€è¦ä»·æ ¼æ•°æ®åˆ—
            alpha_data = features.copy()
            
            # ç¡®ä¿æœ‰å¿…è¦çš„åˆ—ï¼ˆå¦‚æœæ²¡æœ‰åˆ™å°è¯•æ„é€ ï¼‰
            required_cols = ['close', 'high', 'low', 'volume', 'open']
            for col in required_cols:
                if col not in alpha_data.columns:
                    if col == 'close' and 'Close' in alpha_data.columns:
                        alpha_data['close'] = alpha_data['Close']
                    elif col in ['high', 'low', 'open'] and 'close' in alpha_data.columns:
                        # å¦‚æœæ²¡æœ‰OHLVæ•°æ®ï¼Œç”¨closeä»·æ ¼è¿‘ä¼¼
                        alpha_data[col] = alpha_data['close']
                    elif col == 'volume':
                        # å¦‚æœæ²¡æœ‰æˆäº¤é‡æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        alpha_data[col] = 1000000
            
            # æ·»åŠ tickeråˆ—ï¼ˆAlphaå¼•æ“å¯èƒ½éœ€è¦ï¼‰
            alpha_data['ticker'] = ticker
            
            return alpha_data
            
        except Exception as e:
            logger.debug(f"Alphaæ•°æ®å‡†å¤‡å¤±è´¥ {ticker}: {e}")
            return None
    
    def _get_alpha_factor_weights(self) -> Dict[str, float]:
        """
        ğŸš¨ ç¦æ­¢ç¡¬ç¼–ç æƒé‡ï¼å¿…é¡»ä»æœºå™¨å­¦ä¹ è®­ç»ƒä¸­è·å–Alphaå› å­æƒé‡
        """
        # ğŸ”¥ ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šä»è®­ç»ƒå¥½çš„BMAæ¨¡å‹è·å–æƒé‡
        if hasattr(self, 'alpha_engine') and self.alpha_engine:
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„BMAæƒé‡
                if hasattr(self.alpha_engine, 'bma_weights') and self.alpha_engine.bma_weights is not None:
                    logger.info("ä½¿ç”¨è®­ç»ƒå¥½çš„BMAæƒé‡")
                    return self.alpha_engine.bma_weights.to_dict()
                    
                # æ£€æŸ¥æ˜¯å¦æœ‰OOFè¯„åˆ†å¯ä»¥è½¬æ¢ä¸ºæƒé‡
                if hasattr(self.alpha_engine, 'alpha_scores') and self.alpha_engine.alpha_scores is not None:
                    logger.info("åŸºäºOOFè¯„åˆ†è®¡ç®—Alphaæƒé‡")
                    scores = self.alpha_engine.alpha_scores
                    # å°†ICè¯„åˆ†è½¬æ¢ä¸ºæ­£æƒé‡
                    positive_scores = np.abs(scores)
                    if positive_scores.sum() > 0:
                        normalized_weights = positive_scores / positive_scores.sum()
                        return normalized_weights.to_dict()
            except Exception as e:
                logger.error(f"è·å–è®­ç»ƒæƒé‡å¤±è´¥: {e}")
        
        # ğŸ”¥ ç¬¬äºŒä¼˜å…ˆçº§ï¼šä»é…ç½®æ–‡ä»¶è¯»å–æƒé‡æç¤ºå¹¶éªŒè¯
        if (hasattr(self, 'alpha_engine') and self.alpha_engine and 
            hasattr(self.alpha_engine, 'config') and 'alphas' in self.alpha_engine.config):
            alpha_configs = self.alpha_engine.config['alphas']
            config_weights = {}
            total_weight = 0.0
            
            for alpha_config in alpha_configs:
                alpha_name = alpha_config['name']
                weight_hint = alpha_config.get('weight_hint', 0.0)
                if weight_hint > 0:
                    config_weights[alpha_name] = weight_hint
                    total_weight += weight_hint
            
            if config_weights and total_weight > 0:
                # å½’ä¸€åŒ–æƒé‡
                normalized_weights = {k: v/total_weight for k, v in config_weights.items()}
                logger.info(f"ä½¿ç”¨é…ç½®æ–‡ä»¶æƒé‡æç¤ºï¼Œ{len(normalized_weights)} ä¸ªå› å­")
                return normalized_weights
        
        # ğŸš¨ å¦‚æœæ²¡æœ‰è®­ç»ƒæƒé‡ä¹Ÿæ²¡æœ‰é…ç½®ï¼Œç›´æ¥æŠ¥é”™ï¼
        raise ValueError(
            "âŒ ä¸¥é‡é”™è¯¯ï¼šæ— æ³•è·å–Alphaå› å­æƒé‡ï¼\n"
            "åŸå› ï¼š\n"
            "1. æ²¡æœ‰è®­ç»ƒå¥½çš„BMAæƒé‡\n" 
            "2. æ²¡æœ‰OOFè¯„åˆ†\n"
            "3. é…ç½®æ–‡ä»¶æ²¡æœ‰æƒé‡æç¤º\n"
            "è§£å†³æ–¹æ¡ˆï¼š\n"
            "1. ç¡®ä¿Alphaå¼•æ“å·²æ­£ç¡®è®­ç»ƒ\n"
            "2. æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„weight_hintè®¾ç½®\n"
            "3. ä¸å…è®¸ä½¿ç”¨ç¡¬ç¼–ç æƒé‡"
        )
    
    # ç¡¬ç¼–ç æŠ€æœ¯æƒé‡å‡½æ•°å·²åˆ é™¤ - ä½¿ç”¨MLè®­ç»ƒçš„æƒé‡
    
    # ç¡¬ç¼–ç tickerè°ƒæ•´å‡½æ•°å·²åˆ é™¤ - ä½¿ç”¨MLè®­ç»ƒçš„è°ƒæ•´å› å­
    
    # _predict_with_batch_trained_models å·²åˆ é™¤ - æ‰¹é‡é¢„æµ‹åŠŸèƒ½é€šè¿‡BMA Enhancedç³»ç»Ÿæä¾›
    
    def _generate_recommendations_from_predictions(self, predictions: Dict[str, float], top_n: int) -> List[Dict[str, Any]]:
        """ä»é¢„æµ‹ç»“æœç”Ÿæˆæ¨è"""
        recommendations = []
        
        # æŒ‰é¢„æµ‹å€¼æ’åº
        sorted_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)
        
        # è®¡ç®—æƒé‡æ€»å’Œï¼Œé˜²æ­¢é™¤é›¶é”™è¯¯
        try:
            total_predictions = sum(predictions.values())
            if total_predictions == 0:
                total_predictions = len(predictions)  # ä½¿ç”¨å‡æƒä½œä¸ºå¤‡é€‰
        except (TypeError, ValueError):
            total_predictions = len(predictions)
            
        for i, (ticker, prediction) in enumerate(sorted_predictions[:top_n]):
            try:
                weight = max(0.01, prediction / total_predictions) if total_predictions != 0 else 1.0 / top_n
            except (TypeError, ZeroDivisionError):
                weight = 1.0 / top_n  # å‡æƒå¤‡é€‰
                
            recommendations.append({
                'rank': i + 1,
                'ticker': ticker,
                'prediction_signal': prediction,
                'weight': weight,
                'rating': 'BUY' if prediction > 0.6 else 'HOLD' if prediction > 0.4 else 'SELL'
            })
        
        return recommendations
    
    def _save_optimized_results(self, results: Dict[str, Any], filename: str):
        """ä¿å­˜ä¼˜åŒ–ç‰ˆç»“æœ"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            import os
            os.makedirs(os.path.dirname(filename), exist_ok=True)
            
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # æ¨èåˆ—è¡¨
                if 'recommendations' in results:
                    recommendations_df = pd.DataFrame(results['recommendations'])
                    recommendations_df.to_excel(writer, sheet_name='æ¨èåˆ—è¡¨', index=False)
                
                # é¢„æµ‹ç»“æœ
                if 'predictions' in results:
                    predictions_df = pd.DataFrame(list(results['predictions'].items()), 
                                                columns=['è‚¡ç¥¨ä»£ç ', 'é¢„æµ‹å€¼'])
                    predictions_df.to_excel(writer, sheet_name='é¢„æµ‹ç»“æœ', index=False)
                
                # ä¼˜åŒ–ç»Ÿè®¡
                if 'optimization_stats' in results:
                    stats_data = []
                    for key, value in results['optimization_stats'].items():
                        if isinstance(value, dict):
                            for sub_key, sub_value in value.items():
                                stats_data.append([f"{key}_{sub_key}", str(sub_value)])
                        else:
                            stats_data.append([key, str(value)])
                    
                    stats_df = pd.DataFrame(stats_data, columns=['æŒ‡æ ‡', 'æ•°å€¼'])
                    stats_df.to_excel(writer, sheet_name='ä¼˜åŒ–ç»Ÿè®¡', index=False)
            
            logger.info(f"ç»“æœå·²ä¿å­˜: {filename}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def _standardize_features(self, features: pd.DataFrame, global_stats: Dict[str, Any]) -> pd.DataFrame:
        """ä½¿ç”¨å…¨å±€ç»Ÿè®¡æ ‡å‡†åŒ–ç‰¹å¾"""
        try:
            feature_means = global_stats.get('feature_means', {})
            feature_stds = global_stats.get('feature_stds', {})
            
            standardized_features = features.copy()
            
            for col in features.columns:
                if col in feature_means and col in feature_stds:
                    mean_val = feature_means[col]
                    std_val = feature_stds[col]
                    
                    if std_val > 0:  # é¿å…é™¤é›¶
                        standardized_features[col] = (features[col] - mean_val) / std_val
                    else:
                        standardized_features[col] = features[col] - mean_val
            
            return standardized_features
            
        except Exception as e:
            logger.warning(f"ç‰¹å¾æ ‡å‡†åŒ–å¤±è´¥: {e}")
            return features
        
        # ğŸ”¥ CRITICAL: ç”Ÿäº§å®‰å…¨ç³»ç»ŸéªŒè¯
        self._production_safety_validation()
        
        logger.info("UltraEnhancedé‡åŒ–æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def _production_safety_validation(self):
        """ğŸ”¥ CRITICAL: ç”Ÿäº§å®‰å…¨ç³»ç»ŸéªŒè¯ï¼Œé˜²æ­¢éƒ¨ç½²æ—¶å‡ºç°é—®é¢˜"""
        logger.info("ğŸ” å¼€å§‹ç”Ÿäº§å®‰å…¨ç³»ç»ŸéªŒè¯...")
        
        safety_issues = []
        
        # 1. ä¾èµ–å®Œæ•´æ€§æ£€æŸ¥
        dep_status = validate_dependency_integrity()
        if dep_status['critical_failure']:
            safety_issues.append("CRITICAL: æ‰€æœ‰å…³é”®ä¾èµ–ç¼ºå¤±ï¼Œç³»ç»Ÿæ— æ³•è¿è¡Œ")
        elif not dep_status['production_ready']:
            safety_issues.append(f"WARNING: {len(dep_status['missing_modules'])}ä¸ªå…³é”®ä¾èµ–ç¼ºå¤±: {dep_status['missing_modules']}")
        
        # 2. æ—¶é—´é…ç½®å®‰å…¨æ£€æŸ¥
        try:
            temporal_config = validate_temporal_configuration()
            logger.info(f"âœ… æ—¶é—´é…ç½®éªŒè¯é€šè¿‡: gap={temporal_config['cv_gap_days']}å¤©")
        except ValueError as e:
            safety_issues.append(f"CRITICAL: æ—¶é—´é…ç½®ä¸å®‰å…¨: {e}")
        
        # 3. çº¿ç¨‹æ± èµ„æºæ£€æŸ¥
        if hasattr(self, '_shared_thread_pool') and self._shared_thread_pool:
            logger.info(f"âœ… å…±äº«çº¿ç¨‹æ± å¯ç”¨ï¼Œæœ€å¤§å·¥ä½œçº¿ç¨‹: {self._shared_thread_pool._max_workers}")
        else:
            safety_issues.append("CRITICAL: å…±äº«çº¿ç¨‹æ± æœªåˆå§‹åŒ–ï¼Œå¯èƒ½å¯¼è‡´èµ„æºæ³„éœ²")
        
        # 4. å…³é”®é…ç½®æ£€æŸ¥
        if not hasattr(self, 'config') or not self.config:
            safety_issues.append("CRITICAL: ä¸»é…ç½®ç¼ºå¤±")
        else:
            if 'ensemble_weights' not in self.config:
                logger.warning("âš ï¸ ç¼ºå°‘é›†æˆæƒé‡é…ç½®ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
        
        # 5. Alphaå¼•æ“æ£€æŸ¥
        if not hasattr(self, 'alpha_engine') or self.alpha_engine is None:
            safety_issues.append("WARNING: Alphaå¼•æ“æœªåˆå§‹åŒ–ï¼Œé¢„æµ‹æ€§èƒ½å¯èƒ½ä¸‹é™")
        
        # 6. ç”Ÿäº§é—¨ç¦æ£€æŸ¥
        production_fixes = self.get_production_fixes_status()
        if not production_fixes.get('available', False):
            safety_issues.append("WARNING: ç”Ÿäº§çº§ä¿®å¤ç³»ç»Ÿä¸å¯ç”¨")
        
        # æŠ¥å‘ŠéªŒè¯ç»“æœ
        if safety_issues:
            critical_issues = [issue for issue in safety_issues if issue.startswith('CRITICAL')]
            warning_issues = [issue for issue in safety_issues if issue.startswith('WARNING')]
            
            if critical_issues:
                logger.error("ğŸš¨ å‘ç°å…³é”®ç”Ÿäº§å®‰å…¨é—®é¢˜:")
                for issue in critical_issues:
                    logger.error(f"  - {issue}")
                logger.error("âš ï¸ å»ºè®®åœ¨ä¿®å¤å…³é”®é—®é¢˜åå†éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ")
            
            if warning_issues:
                logger.warning("âš ï¸ å‘ç°ç”Ÿäº§è­¦å‘Š:")
                for issue in warning_issues:
                    logger.warning(f"  - {issue}")
        else:
            logger.info("âœ… ç”Ÿäº§å®‰å…¨éªŒè¯é€šè¿‡ï¼Œç³»ç»Ÿå¯å®‰å…¨éƒ¨ç½²")
        
        # å­˜å‚¨éªŒè¯ç»“æœ
        self._safety_validation_result = {
            'timestamp': pd.Timestamp.now(),
            'issues_found': len(safety_issues),
            'critical_issues': len([i for i in safety_issues if i.startswith('CRITICAL')]),
            'warning_issues': len([i for i in safety_issues if i.startswith('WARNING')]),
            'production_ready': len([i for i in safety_issues if i.startswith('CRITICAL')]) == 0,
            'details': safety_issues
        }
    
    def _generate_stock_recommendations(self, selection_result: Dict[str, Any], top_n: int) -> pd.DataFrame:
        """ç”Ÿæˆæ¸…æ™°çš„è‚¡ç¥¨é€‰æ‹©æ¨è"""
        try:
            # ä»è‚¡ç¥¨é€‰æ‹©ç»“æœä¸­æå–æ¨è
            if not selection_result or not selection_result.get('success', False):
                logger.warning("è‚¡ç¥¨é€‰æ‹©å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæœ‰æ•ˆæ¨è")
                return pd.DataFrame()  # è¿”å›ç©ºDataFrameè€Œä¸æ˜¯è™šå‡æ¨è
            
            # æå–é¢„æµ‹å’Œé€‰æ‹©ç»“æœ
            predictions = selection_result.get('predictions', {})
            selected_stocks = selection_result.get('selected_stocks', [])
            
            if not predictions:
                logger.error("âŒ ç¼ºå°‘é¢„æµ‹æ”¶ç›Šç‡ï¼Œæ— æ³•ç”Ÿæˆæ¨è")
                return pd.DataFrame()
            
            # æŒ‰T+10é¢„æµ‹æ”¶ç›Šç‡ä»é«˜åˆ°ä½æ’åºï¼ˆè¿™æ˜¯ç”¨æˆ·è¦çš„ï¼ï¼‰
            if isinstance(predictions, dict):
                sorted_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)
            elif hasattr(predictions, 'index'):
                # Seriesæ ¼å¼
                sorted_predictions = predictions.sort_values(ascending=False).head(top_n)
                sorted_predictions = [(idx, val) for idx, val in sorted_predictions.items()]
            else:
                logger.error("âŒ é¢„æµ‹æ•°æ®æ ¼å¼é”™è¯¯")
                return pd.DataFrame()
            
            # ç”Ÿæˆæ¸…æ™°çš„æ¨èæ ¼å¼
            recommendations = []
            if selected_stocks:
                # ä½¿ç”¨å·²é€‰æ‹©çš„è‚¡ç¥¨
                for stock_info in selected_stocks[:top_n]:
                    ticker = stock_info['ticker']
                    prediction = stock_info['prediction_score']
                    
                    # æ¸…æ™°çš„æ¨èé€»è¾‘
                    if prediction > 0.02:  # >2%é¢„æœŸæ”¶ç›Š
                        action = 'STRONG_BUY'
                    elif prediction > 0.01:  # >1%é¢„æœŸæ”¶ç›Š
                        action = 'BUY'
                    elif prediction < -0.02:  # <-2%é¢„æœŸæ”¶ç›Š  
                        action = 'AVOID'
                    else:
                        action = 'HOLD'
                    
                    recommendations.append({
                        'rank': stock_info['rank'],
                        'ticker': str(ticker),
                        'prediction_score': f"{prediction*100:.2f}%",  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                        'raw_prediction': float(prediction),  # åŸå§‹æ•°å€¼
                        'percentile': stock_info['percentile'],
                        'signal_strength': stock_info['signal_strength'],
                        'recommendation': action,
                        'prediction_signal': float(prediction)  # ç”¨äºæ’åºå’Œæ˜¾ç¤º
                    })
            else:
                # åå¤‡ï¼šä½¿ç”¨é¢„æµ‹å­—å…¸
                sorted_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)
                for i, (ticker, prediction) in enumerate(sorted_predictions[:top_n]):
                    action = 'BUY' if prediction > 0.01 else 'AVOID' if prediction < -0.01 else 'HOLD'
                    recommendations.append({
                        'rank': i + 1,
                        'ticker': str(ticker),
                        'prediction_score': f"{prediction*100:.2f}%",
                        'raw_prediction': float(prediction),
                        'percentile': (top_n - i) / top_n * 100,
                        'signal_strength': 'STRONG' if prediction > 0.02 else 'MODERATE' if prediction > 0.005 else 'WEAK',
                        'recommendation': action,
                        'prediction_signal': float(prediction)
                    })
            
            df = pd.DataFrame(recommendations)
            logger.info(f"âœ… ç”ŸæˆT+10æ”¶ç›Šç‡æ¨è: {len(df)} åªè‚¡ç¥¨ï¼Œæ”¶ç›Šç‡èŒƒå›´ {df['raw_prediction'].min()*100:.2f}% ~ {df['raw_prediction'].max()*100:.2f}%")
            
            return df
                
        except Exception as e:
            logger.error(f"æŠ•èµ„å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
            return pd.DataFrame({
                'ticker': ['ERROR'],
                'recommendation': ['HOLD'],
                'weight': [1.0],
                'confidence': [0.1]
            })
    
    def _save_results(self, recommendations: pd.DataFrame, selection_result: Dict[str, Any], 
                     analysis_results: Dict[str, Any]) -> str:
        """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
        try:
            from datetime import datetime
            import os
            
            # åˆ›å»ºç»“æœæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            result_file = f"result/bma_enhanced_analysis_{timestamp}.xlsx"
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs('result', exist_ok=True)
            
            # ä¿å­˜åˆ°Excel
            with pd.ExcelWriter(result_file, engine='openpyxl') as writer:
                # ä¿å­˜æŠ•èµ„å»ºè®®
                if not recommendations.empty:
                    recommendations.to_excel(writer, sheet_name='æŠ•èµ„å»ºè®®', index=False)
                
                # ä¿å­˜è‚¡ç¥¨é€‰æ‹©è¯¦æƒ…
                if selection_result and selection_result.get('success'):
                    selected_stocks = selection_result.get('selected_stocks', [])
                    if selected_stocks:
                        selection_df = pd.DataFrame(selected_stocks)
                        selection_df.to_excel(writer, sheet_name='è‚¡ç¥¨é€‰æ‹©è¯¦æƒ…', index=False)
                
                # ä¿å­˜åˆ†ææ‘˜è¦
                summary_data = {
                    'æŒ‡æ ‡': ['æ€»è€—æ—¶(ç§’)', 'è‚¡ç¥¨æ•°é‡', 'é¢„æµ‹é•¿åº¦', 'æˆåŠŸçŠ¶æ€'],
                    'å€¼': [
                        analysis_results.get('total_time', 0),
                        len(analysis_results.get('tickers', [])),
                        len(analysis_results.get('predictions', [])),
                        analysis_results.get('success', False)
                    ]
                }
                summary_df = pd.DataFrame(summary_data)
                summary_df.to_excel(writer, sheet_name='åˆ†ææ‘˜è¦', index=False)
            
            logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {result_file}")
            return result_file
            
        except Exception as e:
            logger.error(f"ç»“æœä¿å­˜å¤±è´¥: {e}")
            return f"ä¿å­˜å¤±è´¥: {str(e)}"
    
    def generate_stock_selection(self, predictions: pd.Series, top_n: int = 20) -> Dict[str, Any]:
        """ç”Ÿæˆè‚¡ç¥¨é€‰è‚¡ç»“æœ"""
        try:
            if predictions.empty:
                return {'success': False, 'error': 'é¢„æµ‹æ•°æ®ä¸ºç©º'}
            
            # æŒ‰é¢„æµ‹å€¼æ’åº
            ranked_predictions = predictions.sort_values(ascending=False)
            
            # ç”Ÿæˆè‚¡ç¥¨æ’å
            stock_rankings = []
            for rank, (ticker, prediction) in enumerate(ranked_predictions.items(), 1):
                percentile = (len(predictions) - rank + 1) / len(predictions) * 100
                stock_rankings.append({
                    'rank': rank,
                    'ticker': str(ticker),
                    'prediction_score': float(prediction),
                    'percentile': round(percentile, 2),
                    'signal_strength': 'STRONG' if percentile >= 90 else 'MODERATE' if percentile >= 70 else 'WEAK'
                })
            
            # é€‰å‡ºå‰nåªè‚¡ç¥¨
            top_stocks = stock_rankings[:top_n] if top_n else stock_rankings
            
            return {
                'success': True,
                'selected_stocks': top_stocks,
                'all_rankings': stock_rankings,
                'predictions': predictions.to_dict(),
                'selection_criteria': f'Top {min(top_n, len(predictions))} stocks by prediction score',
                'total_universe': len(predictions),
                'method': 'prediction_based_selection'
            }
            
        except Exception as e:
            logger.error(f"è‚¡ç¥¨é€‰è‚¡å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def get_health_report(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿå¥åº·çŠ¶å†µæŠ¥å‘Š"""
        # ç¡®ä¿health_metricså·²åˆå§‹åŒ–
        if not hasattr(self, 'health_metrics'):
            self.health_metrics = {
                'universe_load_fallbacks': 0,
                'risk_model_failures': 0,
                'optimization_fallbacks': 0,
                'alpha_computation_failures': 0,
                'neutralization_failures': 0,
                'prediction_failures': 0,
                'total_exceptions': 0,
                'successful_predictions': 0
            }
        
        total_operations = sum(self.health_metrics.values())
        failure_rate = (self.health_metrics['total_exceptions'] / max(total_operations, 1)) * 100
        
        report = {
            'health_metrics': self.health_metrics.copy(),
            'failure_rate_percent': failure_rate,
            'risk_level': 'LOW' if failure_rate < 5 else 'MEDIUM' if failure_rate < 15 else 'HIGH',
            'recommendations': []
        }
        
        # æ ¹æ®å¤±è´¥ç±»å‹ç»™å‡ºå»ºè®®
        if self.health_metrics['risk_model_failures'] > 2:
            report['recommendations'].append("æ£€æŸ¥UMDMé…ç½®å’Œå¸‚åœºæ•°æ®è¿æ¥")
        
        return report
    
    def build_risk_model(self, stock_data: Dict[str, pd.DataFrame] = None, 
                          start_date: str = None, end_date: str = None) -> Dict[str, Any]:
        """æ„å»ºMulti-factoré£é™©æ¨¡å‹ï¼ˆæ¥è‡ªProfessionalå¼•æ“ï¼‰ - ä½¿ç”¨å·²æœ‰æ•°æ®é¿å…é‡å¤ä¸‹è½½"""
        logger.info("æ„å»ºMulti-factoré£é™©æ¨¡å‹")
        
        if not hasattr(self, 'market_data_manager') or self.market_data_manager is None:
            raise ValueError("MarketDataManager not available")
        
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å·²æœ‰æ•°æ®ï¼Œé¿å…é‡å¤ä¸‹è½½
        if stock_data and len(stock_data) > 0:
            logger.info(f"ä½¿ç”¨å·²æœ‰è‚¡ç¥¨æ•°æ®æ„å»ºé£é™©æ¨¡å‹: {len(stock_data)}åªè‚¡ç¥¨")
            returns_data = []
            valid_tickers = []
            
            for ticker, data in stock_data.items():
                try:
                    if len(data) > 100:  # ç¡®ä¿æ•°æ®å……è¶³
                        close_col = 'close' if 'close' in data.columns else 'Close'
                        returns = data[close_col].pct_change().fillna(0)
                        returns_data.append(returns)
                        valid_tickers.append(ticker)
                except Exception as e:
                    logger.debug(f"å¤„ç†{ticker}æ”¶ç›Šç‡å¤±è´¥: {e}")
                    continue
        else:
            # å¦‚æœæ²¡æœ‰ä¼ å…¥æ•°æ®ï¼Œæ‰ä½¿ç”¨MarketDataManagerè·å–
            logger.info("æœªæä¾›è‚¡ç¥¨æ•°æ®ï¼Œä½¿ç”¨MarketDataManagerè·å–")
            tickers = self.market_data_manager.get_available_tickers(max_tickers=self.model_config.max_risk_model_tickers)
            if not tickers:
                raise ValueError("No tickers available from MarketDataManager")
            
            # ä½¿ç”¨ç»Ÿä¸€çš„æ—¶é—´èŒƒå›´
            if not start_date or not end_date:
                end_date = pd.Timestamp.now().strftime('%Y-%m-%d')
                start_date = (pd.Timestamp.now() - pd.Timedelta(days=self.model_config.risk_model_history_days)).strftime('%Y-%m-%d')
            
            # æ‰¹é‡ä¸‹è½½ä»¥æé«˜æ•ˆç‡
            stock_data = self.market_data_manager.download_batch_historical_data(tickers, start_date, end_date)
            returns_data = []
            valid_tickers = []
            
            for ticker, data in stock_data.items():
                try:
                    if len(data) > 100:
                        close_col = 'close' if 'close' in data.columns else 'Close'
                        returns = data[close_col].pct_change().fillna(0)
                        returns_data.append(returns)
                        valid_tickers.append(ticker)
                except Exception as e:
                    logger.debug(f"å¤„ç†{ticker}æ”¶ç›Šç‡å¤±è´¥: {e}")
                    continue
        
        if not returns_data:
            raise ValueError("No valid returns data")
        
        returns_matrix = pd.concat(returns_data, axis=1, keys=valid_tickers)
        returns_matrix = returns_matrix.fillna(0.0)
        
        # æ„å»ºé£é™©å› å­
        risk_factors = self._build_risk_factors(returns_matrix)
        
        # ä¼°è®¡å› å­è½½è·
        factor_loadings = self._estimate_factor_loadings(returns_matrix, risk_factors)
        
        # ä¼°è®¡å› å­åæ–¹å·®
        factor_covariance = self._estimate_factor_covariance(risk_factors)
        
        # ä¼°è®¡ç‰¹å¼‚é£é™©
        specific_risk = self._estimate_specific_risk(returns_matrix, factor_loadings, risk_factors)
        
        self.risk_model_results = {
            'factor_loadings': factor_loadings,
            'factor_covariance': factor_covariance,
            'specific_risk': specific_risk,
            'risk_factors': risk_factors
        }
        
        logger.info("é£é™©æ¨¡å‹æ„å»ºå®Œæˆ")
        return self.risk_model_results
    
    def _build_risk_factors(self, returns_matrix: pd.DataFrame) -> pd.DataFrame:
        """[P1] æ„å»ºé£é™©å› å­ - çœŸå®Bã€Fã€Sæ•°æ®ï¼ˆè¡Œä¸š/å›½å®¶/é£æ ¼çº¦æŸï¼‰"""
        factors = pd.DataFrame(index=returns_matrix.index)
        tickers = returns_matrix.columns.tolist()
        
        # 1. å¸‚åœºå› å­
        factors['market'] = returns_matrix.mean(axis=1)
        
        # 2. [ENHANCED] P1 è§„æ¨¡å› å­ (ä½¿ç”¨çœŸå®å¸‚å€¼æ•°æ®)
        size_factor = self._build_size_factor(tickers, returns_matrix.index)
        if size_factor is not None:
            factors['size'] = size_factor
        else:
            factors['size'] = self._build_real_size_factor(tickers, returns_matrix.index)
        
        # 3. [ENHANCED] P1 ä»·å€¼å› å­ (å¸‚å‡€ç‡ã€å¸‚ç›ˆç‡) - ä½¿ç”¨çœŸå®åŸºæœ¬é¢æ•°æ®
        factors['value'] = self._build_real_value_factor(tickers, returns_matrix.index)
        
        # 4. [ENHANCED] P1 è´¨é‡å› å­ (ROEã€æ¯›åˆ©ç‡ã€è´¢åŠ¡å¥åº·åº¦) - ä½¿ç”¨çœŸå®è´¢åŠ¡æ•°æ®
        factors['quality'] = self._build_real_quality_factor(tickers, returns_matrix.index)
        
        # 5. [ENHANCED] P1 Betaå› å­ (å¸‚åœºæ•æ„Ÿæ€§)
        beta_factor = self._build_beta_factor(returns_matrix)
        factors['beta'] = beta_factor
        
        # 6. [ENHANCED] P1 åŠ¨é‡å› å­ (12-1æœˆåŠ¨é‡ç­–ç•¥) - ä½¿ç”¨çœŸå®ä»·æ ¼æ•°æ®  
        factors['momentum'] = self._build_real_momentum_factor(tickers, returns_matrix.index)
        
        # 7. [ENHANCED] P1 æ³¢åŠ¨ç‡å› å­ (å†å²æ³¢åŠ¨ç‡)
        volatility_factor = self._build_volatility_factor(returns_matrix)
        factors['volatility'] = volatility_factor
        
        # 8. [ENHANCED] P1 è¡Œä¸šå› å­ (ä»çœŸå®å…ƒæ•°æ®æ„å»º)
        industry_factors = self._build_industry_factors(tickers, returns_matrix.index)
        for industry_name, industry_factor in industry_factors.items():
            factors[f'industry_{industry_name}'] = industry_factor
        
        # æ ‡å‡†åŒ–å› å­ï¼ˆç¡®ä¿æ•°å€¼ç¨³å®šæ€§ï¼‰
        for col in factors.columns:
            if col != 'market':  # ä¿æŒå¸‚åœºå› å­ä¸å˜
                factors[col] = (factors[col] - factors[col].mean()) / (factors[col].std() + 1e-8)
        
        logger.info(f"é£é™©å› å­æ„å»ºå®Œæˆï¼ŒåŒ…å«{len(factors.columns)}ä¸ªå› å­: {list(factors.columns)}")
        return factors
    
    def _build_size_factor(self, tickers: List[str], date_index: pd.DatetimeIndex) -> Optional[pd.Series]:
        """æ„å»ºçœŸå®çš„è§„æ¨¡å› å­ - ç»Ÿä¸€ä½¿ç”¨MarketDataManager"""
        try:
            if not hasattr(self, 'market_data_manager') or self.market_data_manager is None:
                logger.warning("MarketDataManagerä¸å¯ç”¨ï¼Œè·³è¿‡è§„æ¨¡å› å­")
                return None
                
            size_data = []
            
            for date in date_index:
                daily_sizes = []
                for ticker in tickers:
                    try:
                        # ç»Ÿä¸€ä½¿ç”¨self.market_data_manager
                        stock_info = self.market_data_manager.get_stock_info(ticker)
                        if stock_info and stock_info.market_cap:
                            daily_sizes.append(np.log(stock_info.market_cap))
                        else:
                            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨å†å²æ•°æ®ä¼°ç®—
                            historical_data = self.market_data_manager.download_historical_data(
                                ticker, 
                                (date - pd.Timedelta(days=5)).strftime('%Y-%m-%d'),
                                date.strftime('%Y-%m-%d')
                            )
                            if historical_data is not None and not historical_data.empty:
                                latest = historical_data.iloc[-1]
                                if 'volume' in historical_data.columns:
                                    market_proxy = latest['close'] * latest['volume']
                                    daily_sizes.append(np.log(max(market_proxy, 1e6)))
                    except Exception as e:
                        logger.debug(f"è·å–{ticker}è§„æ¨¡æ•°æ®å¤±è´¥: {e}")
                        daily_sizes.append(np.log(1e8))  # é»˜è®¤å€¼
                
                if daily_sizes:
                    sizes_array = np.array(daily_sizes)
                    size_factor_value = np.mean(sizes_array) - np.median(sizes_array)
                    size_data.append(size_factor_value)
                else:
                    size_data.append(0.0)
            
            return pd.Series(size_data, index=date_index, name='size_factor')
            
        except Exception as e:
            logger.warning(f"æ„å»ºçœŸå®è§„æ¨¡å› å­å¤±è´¥: {e}")
            return None
    
    def _build_real_size_factor(self, tickers: List[str], date_index: pd.DatetimeIndex) -> pd.Series:
        """æ„å»ºçœŸå®çš„è§„æ¨¡å› å­ - ç»Ÿä¸€ä½¿ç”¨MarketDataManager"""
        try:
            if not hasattr(self, 'market_data_manager') or self.market_data_manager is None:
                raise ValueError("MarketDataManagerä¸å¯ç”¨ï¼Œæ— æ³•æ„å»ºSizeå› å­")
            
            size_data = []
            for date in date_index:
                daily_market_caps = []
                for ticker in tickers:
                    try:
                        # ç»Ÿä¸€ä½¿ç”¨self.market_data_manager
                        stock_info = self.market_data_manager.get_stock_info(ticker)
                        if stock_info and stock_info.market_cap > 0:
                            daily_market_caps.append(np.log(stock_info.market_cap))
                    except Exception as e:
                        logger.debug(f"è·å–{ticker}å¸‚å€¼å¤±è´¥: {e}")
                        continue
                
                if daily_market_caps:
                    size_factor = np.mean(daily_market_caps) 
                    size_data.append(size_factor)
                else:
                    raise ValueError(f"æ— æ³•è·å–ä»»ä½•è‚¡ç¥¨çš„çœŸå®å¸‚å€¼æ•°æ®")
            
            factor_series = pd.Series(size_data, index=date_index, name='real_size_factor')
            logger.info(f"âœ… çœŸå®è§„æ¨¡å› å­æ„å»ºæˆåŠŸï¼Œæ•°æ®ç‚¹: {len(factor_series)}")
            return factor_series
            
        except Exception as e:
            logger.error(f"çœŸå®è§„æ¨¡å› å­æ„å»ºå¤±è´¥: {e}")
            raise ValueError(f"Sizeå› å­æ„å»ºå¤±è´¥: {str(e)}")
    
    def _build_real_value_factor(self, tickers: List[str], date_index: pd.DatetimeIndex) -> pd.Series:
        """æ„å»ºçœŸå®çš„ä»·å€¼å› å­ - ç»Ÿä¸€é€šè¿‡MarketDataManagerè·å–æ•°æ®"""
        try:
            if not hasattr(self, 'market_data_manager') or self.market_data_manager is None:
                logger.error("MarketDataManagerä¸å¯ç”¨ï¼Œæ— æ³•è·å–åŸºæœ¬é¢æ•°æ®")
                raise ValueError("MarketDataManagerä¸å¯ç”¨ï¼Œæ— æ³•æ„å»ºValueå› å­")
            
            # ç®€åŒ–å®ç°ï¼šä½¿ç”¨stock_infoä¸­çš„åŸºæœ¬é¢æ•°æ®
            value_data = []
            for date in date_index:
                daily_value_scores = []
                for ticker in tickers:
                    try:
                        # ç»Ÿä¸€ä½¿ç”¨market_data_managerè·å–è‚¡ç¥¨ä¿¡æ¯
                        stock_info = self.market_data_manager.get_stock_info(ticker)
                        if stock_info:
                            # åŸºäºå¸‚å€¼æ„å»ºä»·å€¼ä»£ç†å› å­
                            if stock_info.market_cap and stock_info.market_cap > 0:
                                # ç®€åŒ–çš„ä»·å€¼åˆ†æ•°ï¼šå°å¸‚å€¼ = é«˜ä»·å€¼
                                value_score = -np.log(stock_info.market_cap)
                                daily_value_scores.append(value_score)
                    except Exception as e:
                        logger.debug(f"è·å–{ticker}ä»·å€¼æ•°æ®å¤±è´¥: {e}")
                        continue
                
                if daily_value_scores:
                    value_factor = np.mean(daily_value_scores)
                    value_data.append(value_factor)
                else:
                    value_data.append(0.0)
            
            factor_series = pd.Series(value_data, index=date_index, name='real_value_factor')
            logger.info(f"âœ… ä»·å€¼å› å­æ„å»ºæˆåŠŸï¼Œæ•°æ®ç‚¹: {len(factor_series)}")
            return factor_series
            
        except Exception as e:
            logger.error(f"ä»·å€¼å› å­æ„å»ºå¤±è´¥: {e}")
            raise ValueError(f"Valueå› å­æ„å»ºå¤±è´¥: {str(e)}")
    
    def _build_real_quality_factor(self, tickers: List[str], date_index: pd.DatetimeIndex) -> pd.Series:
        """æ„å»ºè´¨é‡å› å­ - ç»Ÿä¸€é€šè¿‡MarketDataManager"""
        try:
            if not hasattr(self, 'market_data_manager') or self.market_data_manager is None:
                logger.error("MarketDataManagerä¸å¯ç”¨ï¼Œæ— æ³•æ„å»ºè´¨é‡å› å­")
                raise ValueError("MarketDataManagerä¸å¯ç”¨ï¼Œæ— æ³•æ„å»ºQualityå› å­")
            
            # ç®€åŒ–å®ç°ï¼šä½¿ç”¨è¡Œä¸šä¿¡æ¯æ„å»ºè´¨é‡ä»£ç†å› å­
            quality_data = []
            for date in date_index:
                daily_quality_scores = []
                for ticker in tickers:
                    try:
                        # ç»Ÿä¸€ä½¿ç”¨market_data_managerè·å–è‚¡ç¥¨ä¿¡æ¯
                        stock_info = self.market_data_manager.get_stock_info(ticker)
                        if stock_info and stock_info.sector:
                            # åŸºäºè¡Œä¸šæ„å»ºè´¨é‡ä»£ç†å› å­
                            # æŠ€æœ¯è¡Œä¸šå¾—åˆ†è¾ƒé«˜
                            quality_score = 1.0 if stock_info.sector == 'Technology' else 0.5
                            daily_quality_scores.append(quality_score)
                    except Exception as e:
                        logger.debug(f"è·å–{ticker}è´¨é‡æ•°æ®å¤±è´¥: {e}")
                        continue
                
                if daily_quality_scores:
                    quality_factor = np.mean(daily_quality_scores)
                    quality_data.append(quality_factor)
                else:
                    quality_data.append(0.0)
            
            factor_series = pd.Series(quality_data, index=date_index, name='real_quality_factor')
            logger.info(f"âœ… è´¨é‡å› å­æ„å»ºæˆåŠŸï¼Œæ•°æ®ç‚¹: {len(factor_series)}")
            return factor_series
            
        except Exception as e:
            logger.error(f"è´¨é‡å› å­æ„å»ºå¤±è´¥: {e}")
            raise ValueError(f"Qualityå› å­æ„å»ºå¤±è´¥: {str(e)}")
    
    def _build_real_momentum_factor(self, tickers: List[str], date_index: pd.DatetimeIndex) -> pd.Series:
        """æ„å»ºåŠ¨é‡å› å­ - ç»Ÿä¸€ä½¿ç”¨MarketDataManager"""
        try:
            if not hasattr(self, 'market_data_manager') or self.market_data_manager is None:
                logger.warning("MarketDataManagerä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–åŠ¨é‡å› å­")
                return pd.Series(np.random.randn(len(date_index)) * 0.01, index=date_index, name='momentum_factor')
            
            momentum_data = []
            for date in date_index:
                daily_momentums = []
                for ticker in tickers:
                    try:
                        # ä½¿ç”¨MarketDataManagerä¸‹è½½å†å²æ•°æ®è®¡ç®—åŠ¨é‡
                        end_date = date.strftime('%Y-%m-%d')
                        start_date = (date - pd.Timedelta(days=300)).strftime('%Y-%m-%d')  # è·å–è¶³å¤Ÿçš„å†å²æ•°æ®
                        
                        historical_data = self.market_data_manager.download_historical_data(ticker, start_date, end_date)
                        if historical_data is not None and len(historical_data) >= 252:
                            close_prices = historical_data['close']
                            # è®¡ç®—12-1æœˆåŠ¨é‡
                            current_price = close_prices.iloc[-21]  # 1ä¸ªæœˆå‰
                            past_12m_price = close_prices.iloc[-252]  # 12ä¸ªæœˆå‰
                            
                            momentum_12m = (current_price / past_12m_price) - 1
                            daily_momentums.append(momentum_12m)
                            
                    except Exception as e:
                        logger.debug(f"è·å–{ticker}åŠ¨é‡æ•°æ®å¤±è´¥: {e}")
                        continue
                
                if daily_momentums:
                    momentum_factor = np.mean(daily_momentums)
                    momentum_data.append(momentum_factor)
                else:
                    momentum_data.append(0.0)
            
            factor_series = pd.Series(momentum_data, index=date_index, name='real_momentum_factor')
            logger.info(f"âœ… åŠ¨é‡å› å­æ„å»ºæˆåŠŸï¼Œæ•°æ®ç‚¹: {len(factor_series)}")
            return factor_series
            
        except Exception as e:
            logger.error(f"åŠ¨é‡å› å­æ„å»ºå¤±è´¥: {e}")
            raise ValueError(f"Momentumå› å­æ„å»ºå¤±è´¥: {str(e)}")
    
    # é›¶å€¼å›é€€å‡½æ•°å·²åˆ é™¤ - æ ¹æ®ç”¨æˆ·è¦æ±‚ä¸å…è®¸å›é€€æœºåˆ¶
    
    def _build_beta_factor(self, returns_matrix: pd.DataFrame) -> pd.Series:
        """æ„å»ºBetaå› å­ - FIXED: ç¨³å¥è®¡ç®—æ›¿ä»£ä¸­ä½æ•°æ–¹æ³•"""
        # ğŸ”¥ CRITICAL FIX: ä½¿ç”¨ä¸“é—¨çš„ç¨³å¥Betaè®¡ç®—å™¨
        from .robust_beta_calculator import RobustBetaCalculator
        
        logger.info("ä½¿ç”¨ç¨³å¥Betaè®¡ç®—å™¨è®¡ç®—Betaå› å­")
        
        try:
            # åˆ›å»ºç¨³å¥è®¡ç®—å™¨
            beta_calculator = RobustBetaCalculator(
                window_size=self.model_config.beta_calculation_window,
                min_samples=30,
                use_robust_regression=True,
                market_cap_weighted=hasattr(self, 'market_caps')
            )
            
            # è·å–å¸‚å€¼æ•°æ®(å¦‚æœå¯ç”¨)
            market_caps = getattr(self, 'market_caps', None)
            
            # è®¡ç®—ç¨³å¥Beta
            robust_betas = beta_calculator.calculate_beta_series(
                returns_matrix, market_caps
            )
            
            logger.info(f"âœ… ç¨³å¥Betaè®¡ç®—å®Œæˆ - å¹³å‡å€¼: {robust_betas.mean():.3f}")
            return robust_betas
            
        except Exception as e:
            logger.error(f"ç¨³å¥Betaè®¡ç®—å¤±è´¥: {e}")
            # é™çº§åˆ°æ”¹è¿›çš„ç®€å•æ–¹æ³•
            return self._build_beta_factor_fallback(returns_matrix)
        
    def _build_beta_factor_fallback(self, returns_matrix: pd.DataFrame) -> pd.Series:
        """Betaè®¡ç®—çš„é™çº§æ–¹æ¡ˆ - æ”¹è¿›çš„ç®€å•æ–¹æ³•"""
        from scipy.stats import trim_mean
        
        logger.info("ä½¿ç”¨æ”¹è¿›çš„é™çº§Betaè®¡ç®—æ–¹æ³•")
        
        try:
            # ä½¿ç”¨æˆªå°¾å‡å€¼æ›¿ä»£ä¸­ä½æ•°è®¡ç®—å¸‚åœºæ”¶ç›Š
            market_returns = returns_matrix.apply(
                lambda row: trim_mean(row.dropna(), 0.1) if len(row.dropna()) >= 3 else np.nan,
                axis=1
            )
        except:
            market_returns = returns_matrix.mean(axis=1)
        
        betas = []
        min_samples = max(30, getattr(self.model_config, 'beta_calculation_window', 252) // 4)
        
        for date in returns_matrix.index:
            try:
                # ä½¿ç”¨æ»šåŠ¨çª—å£è®¡ç®—Beta
                end_idx = returns_matrix.index.get_loc(date)
                start_idx = max(0, end_idx - getattr(self.model_config, 'beta_calculation_window', 252))
                
                # ç¡®ä¿è¶³å¤Ÿçš„æ ·æœ¬æ•°
                if end_idx - start_idx < min_samples:
                    betas.append(1.0)
                    continue
                    
                period_data = returns_matrix.iloc[start_idx:end_idx]
                period_market = market_returns.iloc[start_idx:end_idx]
                
                # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
                if len(period_market.dropna()) < min_samples:
                    betas.append(1.0)
                    continue
                    
                # è®¡ç®—å„è‚¡ç¥¨ç›¸å¯¹å¸‚åœºçš„å¹³å‡Beta  
                stock_betas = []
                for ticker in period_data.columns:
                    try:
                        stock_ret = period_data[ticker].dropna()
                        market_ret = period_market.loc[stock_ret.index].dropna()
                        
                        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„é‡å æ•°æ®
                        if len(stock_ret) < min_samples // 2 or len(market_ret) < min_samples // 2:
                            stock_betas.append(1.0)
                            continue
                            
                        # æ•°æ®å¯¹é½å’Œç¨³å¥åæ–¹å·®è®¡ç®—
                        common_index = stock_ret.index.intersection(market_ret.index)
                        if len(common_index) < min_samples // 2:
                            stock_betas.append(1.0)
                            continue
                            
                        aligned_stock = stock_ret.loc[common_index].dropna()
                        aligned_market = market_ret.loc[common_index].dropna()
                        
                        # å†æ¬¡æ£€æŸ¥å¯¹é½åçš„æ•°æ®
                        final_common = aligned_stock.index.intersection(aligned_market.index)
                        if len(final_common) < min_samples // 2:
                            stock_betas.append(1.0)
                            continue
                            
                        final_stock = aligned_stock.loc[final_common]
                        final_market = aligned_market.loc[final_common]
                        
                        # ROBUSTåæ–¹å·®è®¡ç®—
                        try:
                            cov_matrix = np.cov(final_stock.values, final_market.values, ddof=1)
                            if cov_matrix.shape == (2, 2) and not np.isnan(cov_matrix).any():
                                cov = cov_matrix[0, 1]
                                var_market = cov_matrix[1, 1]
                            else:
                                # ä½¿ç”¨pandasç›¸å…³ç³»æ•°æ–¹æ³•ä½œä¸ºbackup
                                correlation = final_stock.corr(final_market)
                                if pd.isna(correlation):
                                    stock_betas.append(1.0)
                                    continue
                                stock_std = final_stock.std()
                                market_std = final_market.std()
                                if market_std > 1e-8:
                                    cov = correlation * stock_std * market_std
                                    var_market = market_std ** 2
                                else:
                                    stock_betas.append(1.0)
                                    continue
                            # CRITICAL FIX: æ›´ä¸¥æ ¼çš„æ•°å€¼ç¨³å®šæ€§é˜ˆå€¼
                            if abs(var_market) > 1e-6:  # å¢åŠ é˜ˆå€¼ï¼Œé¿å…æ•°å€¼ä¸ç¨³å®š
                                beta = cov / var_market
                                # CRITICAL FIX: Betaå¼‚å¸¸å€¼å¤„ç†
                                if -5 <= beta <= 5:  # åˆç†çš„betaèŒƒå›´
                                    stock_betas.append(beta)
                                else:
                                    stock_betas.append(1.0)  # å¼‚å¸¸betaä½¿ç”¨1.0
                            else:
                                stock_betas.append(1.0)
                                
                        except Exception as e:
                            logger.debug(f"åæ–¹å·®è®¡ç®—å¼‚å¸¸ {ticker}: {e}")
                            stock_betas.append(1.0)
                        
                    except Exception as e:
                        logger.debug(f"è‚¡ç¥¨Betaè®¡ç®—é”™è¯¯ {ticker}: {e}")
                        stock_betas.append(1.0)
                
                # CRITICAL FIX: ä½¿ç”¨ä¸­ä½æ•°ä»£æ›¿å‡å€¼ï¼Œæ›´robust
                if stock_betas:
                    final_beta = np.median(stock_betas)
                    # ç¡®ä¿betaåœ¨åˆç†èŒƒå›´å†…
                    final_beta = np.clip(final_beta, 0.1, 3.0)
                    betas.append(final_beta)
                else:
                    betas.append(1.0)
                    
            except Exception as e:
                logger.debug(f"æ—¥æœŸ{date}çš„Betaè®¡ç®—å¤±è´¥: {e}")
                betas.append(1.0)
        
        return pd.Series(betas, index=returns_matrix.index, name='beta_factor')
    
    
    
    def _build_volatility_factor(self, returns_matrix: pd.DataFrame) -> pd.Series:
        """æ„å»ºæ³¢åŠ¨ç‡å› å­"""
        volatility = returns_matrix.rolling(window=self.model_config.volatility_window).std().mean(axis=1)
        return volatility.fillna(0)
    
    def _build_industry_factors(self, tickers: List[str], date_index: pd.DatetimeIndex) -> Dict[str, pd.Series]:
        """æ„å»ºè¡Œä¸šå› å­ï¼ˆæ¥è‡ªçœŸå®å…ƒæ•°æ®ï¼‰ - ç»Ÿä¸€ä½¿ç”¨MarketDataManager"""
        industry_factors = {}
        
        try:
            if not hasattr(self, 'market_data_manager') or self.market_data_manager is None:
                logger.warning("MarketDataManagerä¸å¯ç”¨ï¼Œè·³è¿‡è¡Œä¸šå› å­")
                return {'neutral': pd.Series(np.zeros(len(date_index)), index=date_index, name='neutral')}
            
            # ä»MarketDataManagerè·å–è¡Œä¸šä¿¡æ¯
            ticker_industries = {}
            for ticker in tickers:
                try:
                    stock_info = self.market_data_manager.get_stock_info(ticker)
                    if stock_info and stock_info.sector:
                        ticker_industries[ticker] = stock_info.sector
                    else:
                        ticker_industries[ticker] = 'Technology'  # é»˜è®¤å€¼
                except Exception as e:
                    logger.debug(f"è·å–{ticker}è¡Œä¸šä¿¡æ¯å¤±è´¥: {e}")
                    ticker_industries[ticker] = 'Technology'
            
            # è·å–æ‰€æœ‰è¡Œä¸š
            unique_industries = list(set(ticker_industries.values()))
            
            for industry in unique_industries:
                if industry and industry != 'Unknown':
                    # åˆ é™¤æ¨¡æ‹Ÿè¡Œä¸šå› å­æ•°æ® - æ— æ³•è·å–çœŸå®æ•°æ®
                    # è·³è¿‡è¡Œä¸šå› å­æ„å»ºï¼Œé¿å…ä½¿ç”¨éšæœºæ¨¡æ‹Ÿæ•°æ®
                    logger.debug(f"è·³è¿‡è¡Œä¸šå› å­æ„å»º: {industry} (ç¼ºå°‘çœŸå®è¡Œä¸šæ”¶ç›Šæ•°æ®)")
            
            logger.info(f"æ„å»ºäº†{len(industry_factors)}ä¸ªè¡Œä¸šå› å­: {list(industry_factors.keys())}")
            
        except Exception as e:
            logger.warning(f"æ„å»ºè¡Œä¸šå› å­å¤±è´¥: {e}")
        
        # ğŸ”¥ FIX: è¿”å›ç©ºè¡Œä¸šå› å­è€Œä¸æ˜¯éšæœºæ•°æ®
        if not industry_factors:
            # è¿”å›é›¶å› å­è€Œä¸æ˜¯éšæœºæ•°
            industry_factors['neutral'] = pd.Series(
                np.zeros(len(date_index)), 
                index=date_index, name='industry_neutral'
            )
            logger.warning("No industry factors available, using neutral (zero) factor")
        
        return industry_factors
    
    def _create_earnings_window_dummy(self, date_index: pd.DatetimeIndex, ticker: str, days: int = 5) -> pd.Series:
        """[ENHANCED] P2 åˆ›å»ºè´¢æŠ¥/å…¬å‘Šçª—å£dummyå˜é‡"""
        try:
            # è´¢æŠ¥å‘å¸ƒé€šå¸¸åœ¨å­£åº¦ç»“æŸåçš„45å¤©å†…
            earnings_dates = []
            
            # ä¼°ç®—å­£åº¦è´¢æŠ¥æ—¥æœŸï¼ˆå®é™…ä¸­åº”ä»è´¢æŠ¥æ—¥å†APIè·å–ï¼‰
            for year in range(2023, 2025):
                for quarter_end in ['03-31', '06-30', '09-30', '12-31']:
                    try:
                        quarter_date = pd.to_datetime(f'{year}-{quarter_end}')
                        # è´¢æŠ¥é€šå¸¸åœ¨å­£åº¦ç»“æŸå30-45å¤©å‘å¸ƒ
                        for offset_days in [30, 35, 40, 45]:
                            earnings_date = quarter_date + pd.Timedelta(days=offset_days)
                            earnings_dates.append(earnings_date)
                    except:
                        continue
            
            # ä¸ºæ¯ä¸ªæ—¥æœŸè®¡ç®—æ˜¯å¦åœ¨è´¢æŠ¥çª—å£å†…
            window_flags = []
            for date in date_index:
                is_earnings_window = False
                for earnings_date in earnings_dates:
                    if abs(int((date - earnings_date) / pd.Timedelta(days=1))) <= days:
                        is_earnings_window = True
                        break
                window_flags.append(int(is_earnings_window))
            
            return pd.Series(window_flags, index=date_index, name=f'earnings_window_{days}')
            
        except Exception as e:
            logger.warning(f"åˆ›å»ºè´¢æŠ¥çª—å£dummyå¤±è´¥ {ticker}: {e}")
            # å›é€€åˆ°å…¨é›¶ï¼ˆæ— è´¢æŠ¥çª—å£ä¿¡æ¯ï¼‰
            return pd.Series(
                np.zeros(len(date_index), dtype=int),  # Use zeros instead of random
                index=date_index, 
                name=f'earnings_window_{days}'
            )
        
    
    def _estimate_factor_loadings(self, returns_matrix: pd.DataFrame, 
                                 risk_factors: pd.DataFrame) -> pd.DataFrame:
        """ä¼°è®¡å› å­è½½è·"""
        loadings = {}
        
        for ticker in returns_matrix.columns:
            stock_returns = returns_matrix[ticker].dropna()
            aligned_factors = risk_factors.loc[stock_returns.index].dropna().fillna(0)
            
            if len(stock_returns) < 50 or len(aligned_factors) < 50:
                loadings[ticker] = np.zeros(len(risk_factors.columns))
                continue
            
            try:
                # ç¡®ä¿æ•°æ®é•¿åº¦åŒ¹é…
                min_len = min(len(stock_returns), len(aligned_factors))
                stock_returns = stock_returns.iloc[:min_len]
                aligned_factors = aligned_factors.iloc[:min_len]
                
                # ä½¿ç”¨ç¨³å¥å›å½’ä¼°è®¡è½½è·
                model = HuberRegressor(epsilon=1.35, alpha=0.0001)
                model.fit(aligned_factors.values, stock_returns.values)
                
                loadings[ticker] = model.coef_
                
            except Exception as e:
                logger.warning(f"Failed to estimate loadings for {ticker}: {e}")
                loadings[ticker] = np.zeros(len(risk_factors.columns))
        
        loadings_df = pd.DataFrame(loadings, index=risk_factors.columns).T
        return loadings_df
    
    def _estimate_factor_covariance(self, risk_factors: pd.DataFrame) -> pd.DataFrame:
        """ä¼°è®¡å› å­åæ–¹å·®çŸ©é˜µ"""
        # ä½¿ç”¨Ledoit-Wolfæ”¶ç¼©ä¼°è®¡
        cov_estimator = LedoitWolf()
        factor_cov_matrix = cov_estimator.fit(risk_factors.fillna(0)).covariance_
        
        # ç¡®ä¿æ­£å®šæ€§
        eigenvals, eigenvecs = np.linalg.eigh(factor_cov_matrix)
        eigenvals = np.maximum(eigenvals, 1e-8)
        factor_cov_matrix = eigenvecs @ np.diag(eigenvals) @ eigenvecs.T
        
        return pd.DataFrame(factor_cov_matrix, 
                           index=risk_factors.columns, 
                           columns=risk_factors.columns)
    
    def _estimate_specific_risk(self, returns_matrix: pd.DataFrame,
                               factor_loadings: pd.DataFrame, 
                               risk_factors: pd.DataFrame) -> pd.Series:
        """ä¼°è®¡ç‰¹å¼‚é£é™©"""
        specific_risks = {}
        
        for ticker in returns_matrix.columns:
            if ticker not in factor_loadings.index:
                specific_risks[ticker] = 0.2  # é»˜è®¤ç‰¹å¼‚é£é™©
                continue
            
            stock_returns = returns_matrix[ticker].dropna()
            loadings = factor_loadings.loc[ticker]
            aligned_factors = risk_factors.loc[stock_returns.index].fillna(0)
            
            if len(stock_returns) < 50:
                specific_risks[ticker] = 0.2
                continue
            
            # è®¡ç®—æ®‹å·®
            min_len = min(len(stock_returns), len(aligned_factors))
            factor_returns = (aligned_factors.iloc[:min_len] @ loadings).values
            residuals = stock_returns.iloc[:min_len].values - factor_returns
            
            # ç‰¹å¼‚é£é™©ä¸ºæ®‹å·®æ ‡å‡†å·®
            specific_var = np.nan_to_num(np.var(residuals), nan=0.04)
            specific_risks[ticker] = np.sqrt(specific_var)
        
        return pd.Series(specific_risks)
    
    def detect_market_regime(self, stock_data: Dict[str, pd.DataFrame] = None, 
                           start_date: str = None, end_date: str = None) -> MarketRegime:
        """æ£€æµ‹å¸‚åœºçŠ¶æ€ï¼ˆæ¥è‡ªProfessionalå¼•æ“ï¼‰ - ä½¿ç”¨å·²æœ‰æ•°æ®é¿å…é‡å¤ä¸‹è½½"""
        logger.info("æ£€æµ‹å¸‚åœºçŠ¶æ€")
        
        if not hasattr(self, 'market_data_manager') or self.market_data_manager is None:
            return MarketRegime(0, "Unknown", 0.5, {'volatility': 0.2, 'trend': 0.0})
        
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å·²æœ‰æ•°æ®
        if stock_data and len(stock_data) > 0:
            logger.info(f"ä½¿ç”¨å·²æœ‰è‚¡ç¥¨æ•°æ®æ£€æµ‹å¸‚åœºçŠ¶æ€: {len(stock_data)}åªè‚¡ç¥¨")
            market_returns = []
            
            for ticker, data in list(stock_data.items())[:20]:  # é™åˆ¶è‚¡ç¥¨æ•°é‡æé«˜æ€§èƒ½
                try:
                    if len(data) > 100:
                        # âœ… FIX: å…¼å®¹'Close'å’Œ'close'åˆ—å
                        price_col = 'close' if 'close' in data.columns else 'Close' if 'Close' in data.columns else None
                        
                        if price_col:
                            returns = data[price_col].pct_change().fillna(0)
                            market_returns.append(returns)
                        else:
                            logger.debug(f"Missing Close price column for {ticker}")
                except Exception as e:
                    logger.debug(f"å¤„ç†{ticker}å¸‚åœºæ•°æ®å¤±è´¥: {e}")
                    continue
        else:
            # å¦‚æœæ²¡æœ‰ä¼ å…¥æ•°æ®ï¼Œæ‰ä½¿ç”¨MarketDataManagerè·å–
            logger.info("æœªæä¾›è‚¡ç¥¨æ•°æ®ï¼Œä½¿ç”¨MarketDataManagerè·å–å¸‚åœºæ•°æ®")
            tickers = self.market_data_manager.get_available_tickers(max_tickers=self.model_config.max_market_regime_tickers)
            if not tickers:
                return MarketRegime(0, "Unknown", 0.5, {'volatility': 0.2, 'trend': 0.0})
            
            # ä½¿ç”¨ç»Ÿä¸€çš„æ—¶é—´èŒƒå›´
            if not start_date or not end_date:
                end_date = pd.Timestamp.now().strftime('%Y-%m-%d')
                start_date = (pd.Timestamp.now() - pd.Timedelta(days=self.model_config.market_regime_history_days)).strftime('%Y-%m-%d')
            
            # æ‰¹é‡ä¸‹è½½å†å²æ•°æ®
            stock_data = self.market_data_manager.download_batch_historical_data(tickers, start_date, end_date)
            market_returns = []
            
            for ticker, data in stock_data.items():
                try:
                    if len(data) > 100:
                        price_col = 'close' if 'close' in data.columns else 'Close' if 'Close' in data.columns else None
                        
                        if price_col:
                            returns = data[price_col].pct_change().fillna(0)
                            market_returns.append(returns)
                        else:
                            logger.debug(f"Missing Close price column for {ticker}")
                except Exception as e:
                    logger.debug(f"å¤„ç†{ticker}å¸‚åœºæ•°æ®å¤±è´¥: {e}")
                    continue
        
        if not market_returns:
            logger.warning("æ— æ³•è·å–ä»»ä½•æœ‰æ•ˆä»·æ ¼æ•°æ®ï¼Œé»˜è®¤ä¸ºä½æ³¢åŠ¨çŠ¶æ€")
            return MarketRegime(1, "ä½æ³¢åŠ¨", 0.8, {'volatility': 0.15, 'trend': 0.0})
        
        market_index = pd.concat(market_returns, axis=1).mean(axis=1).dropna()
        
        if len(market_index) < 100:
            return MarketRegime(1, "Normal", 1.0, {'volatility': 0.15, 'trend': 0.0})
        
        # åŸºäºæ³¢åŠ¨ç‡å’Œè¶‹åŠ¿çš„çŠ¶æ€æ£€æµ‹
        rolling_vol = market_index.rolling(21).std()
        rolling_trend = market_index.rolling(21).mean()
        
        # å®šä¹‰çŠ¶æ€é˜ˆå€¼
        vol_low = rolling_vol.quantile(0.33)
        vol_high = rolling_vol.quantile(0.67)
        trend_low = rolling_trend.quantile(0.33)
        trend_high = rolling_trend.quantile(0.67)
        
        # å½“å‰çŠ¶æ€
        current_vol = rolling_vol.iloc[-1]
        current_trend = rolling_trend.iloc[-1]
        
        if current_vol < vol_low:
            if current_trend > trend_high:
                regime = MarketRegime(1, "Bull_Low_Vol", 0.8, 
                                    {'volatility': current_vol, 'trend': current_trend})
            elif current_trend < trend_low:
                regime = MarketRegime(2, "Bear_Low_Vol", 0.8,
                                    {'volatility': current_vol, 'trend': current_trend})
            else:
                regime = MarketRegime(3, "Normal_Low_Vol", 0.8,
                                    {'volatility': current_vol, 'trend': current_trend})
        elif current_vol > vol_high:
            if current_trend > trend_high:
                regime = MarketRegime(4, "Bull_High_Vol", 0.8,
                                    {'volatility': current_vol, 'trend': current_trend})
            elif current_trend < trend_low:
                regime = MarketRegime(5, "Bear_High_Vol", 0.8,
                                    {'volatility': current_vol, 'trend': current_trend})
            else:
                regime = MarketRegime(6, "Volatile", 0.8,
                                    {'volatility': current_vol, 'trend': current_trend})
        else:
            regime = MarketRegime(0, "Normal", 0.7,
                                {'volatility': current_vol, 'trend': current_trend})
        
        self.current_regime = regime
        logger.info(f"æ£€æµ‹åˆ°å¸‚åœºçŠ¶æ€: {regime.name} (æ¦‚ç‡: {regime.probability:.2f})")
        
        return regime
    
    def _get_regime_alpha_weights(self, regime: MarketRegime) -> Dict[str, float]:
        """æ ¹æ®å¸‚åœºçŠ¶æ€è°ƒæ•´Alphaæƒé‡ - ä½¿ç”¨MLè®­ç»ƒçš„åŠ¨æ€æƒé‡"""
        # å¦‚æœæ²¡æœ‰MLè®­ç»ƒçš„æƒé‡ï¼Œä½¿ç”¨ç®€å•å‡è¡¡æƒé‡
        default_features = [
            'momentum_21d', 'momentum_63d', 'momentum_126d',
            'reversion_5d', 'reversion_10d', 'reversion_21d', 
            'volatility_factor', 'volume_trend', 'quality_factor'
        ]
        
        # TODO: è¿™é‡Œåº”è¯¥ä»æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­è·å–æ ¹æ®å¸‚åœºçŠ¶æ€è®­ç»ƒçš„åŠ¨æ€æƒé‡
        # è€Œä¸æ˜¯ä½¿ç”¨å›ºå®šè§„åˆ™
        
        return {col: 1.0 for col in default_features}
    
    def _generate_base_predictions(self, training_results: Dict[str, Any]) -> pd.Series:
        """ç”ŸæˆåŸºç¡€é¢„æµ‹ç»“æœ - ä¿®å¤ç‰ˆæœ¬"""
        try:
            if not training_results:
                logger.warning("è®­ç»ƒç»“æœä¸ºç©º")
                return pd.Series()
            
            logger.info("ğŸ” å¼€å§‹æå–æœºå™¨å­¦ä¹ é¢„æµ‹...")
            logger.info(f"è®­ç»ƒç»“æœé”®: {list(training_results.keys())}")
            
            # ğŸ”¥ CRITICAL FIX: æ”¹è¿›é¢„æµ‹æå–é€»è¾‘ï¼Œæ”¯æŒå•è‚¡ç¥¨åœºæ™¯
            
            # 1. é¦–å…ˆæ£€æŸ¥ç›´æ¥é¢„æµ‹ç»“æœ
            if 'predictions' in training_results:
                direct_predictions = training_results['predictions']
                if direct_predictions is not None and hasattr(direct_predictions, '__len__') and len(direct_predictions) > 0:
                    logger.info(f"âœ… ä»ç›´æ¥é¢„æµ‹æºæå–: {len(direct_predictions)} æ¡")
                    if hasattr(direct_predictions, 'index'):
                        return pd.Series(direct_predictions)
                    else:
                        # åˆ›å»ºåˆç†çš„ç´¢å¼•
                        return pd.Series(direct_predictions, name='predictions')
            
            # 2. æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„è®­ç»ƒç»“æœï¼ˆæ”¾å®½æˆåŠŸæ¡ä»¶ï¼‰
            success_indicators = [
                training_results.get('success', False),
                any(key in training_results for key in ['traditional_models', 'learning_to_rank', 'stacking', 'regime_aware']),
                'mode' in training_results and training_results['mode'] != 'COMPLETE_FAILURE'
            ]
            
            if not any(success_indicators):
                logger.warning("âš ï¸ è®­ç»ƒç»“æœæ˜¾ç¤ºå¤±è´¥ï¼Œä½†ä»å°è¯•æå–å¯ç”¨é¢„æµ‹...")
            
            # 3. æ‰©å±•é¢„æµ‹æºæœç´¢ - æ›´å…¨é¢çš„æœç´¢ç­–ç•¥
            prediction_sources = [
                ('traditional_models', 'models'),
                ('learning_to_rank', 'predictions'),
                ('stacking', 'predictions'), 
                ('regime_aware', 'predictions'),
                ('alignment_report', 'predictions'),  # ä»å¯¹é½æŠ¥å‘Šä¸­æŸ¥æ‰¾
                ('daily_tickers_stats', None),  # ç»Ÿè®¡ä¿¡æ¯ä¸­å¯èƒ½æœ‰é¢„æµ‹
                ('model_stats', 'predictions'),  # æ¨¡å‹ç»Ÿè®¡ä¸­çš„é¢„æµ‹
                ('recommendations', None)  # æ¨èç»“æœä¸­çš„é¢„æµ‹
            ]
            
            extracted_predictions = []
            
            for source_key, pred_key in prediction_sources:
                if source_key not in training_results:
                    continue
                    
                source_data = training_results[source_key]
                logger.info(f"ğŸ” æ£€æŸ¥ {source_key}: ç±»å‹={type(source_data)}")
                
                if isinstance(source_data, dict):
                    # ä¼ ç»ŸMLæ¨¡å‹ç»“æœå¤„ç†
                    if source_key == 'traditional_models' and source_data.get('success', False):
                        models = source_data.get('models', {})
                        best_model = source_data.get('best_model')
                        
                        logger.info(f"ä¼ ç»Ÿæ¨¡å‹: æœ€ä½³æ¨¡å‹={best_model}, å¯ç”¨æ¨¡å‹={list(models.keys())}")
                        
                        if best_model and best_model in models:
                            model_data = models[best_model]
                            if 'predictions' in model_data:
                                predictions = model_data['predictions']
                                if hasattr(predictions, '__len__') and len(predictions) > 0:
                                    logger.info(f"âœ… ä»{best_model}æ¨¡å‹æå–é¢„æµ‹ï¼Œé•¿åº¦: {len(predictions)}")
                                    
                                    # ğŸ”¥ CRITICAL FIX: ç¡®ä¿é¢„æµ‹ç»“æœæœ‰æ­£ç¡®çš„ç´¢å¼•
                                    if hasattr(predictions, 'index'):
                                        return pd.Series(predictions)
                                    else:
                                        # åˆ›å»ºåŸºäºè‚¡ç¥¨çš„ç´¢å¼•
                                        if hasattr(self, 'feature_data') and self.feature_data is not None:
                                            if 'ticker' in self.feature_data.columns:
                                                tickers = self.feature_data['ticker'].unique()[:len(predictions)]
                                                return pd.Series(predictions, index=tickers, name='ml_predictions')
                                        # ä½¿ç”¨æ•°å€¼ç´¢å¼•
                                        return pd.Series(predictions, name='ml_predictions')
                        
                        # å¦‚æœæœ€ä½³æ¨¡å‹å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ¨¡å‹
                        for model_name, model_data in models.items():
                            if 'predictions' in model_data:
                                predictions = model_data['predictions']
                                if hasattr(predictions, '__len__') and len(predictions) > 0:
                                    logger.info(f"âœ… ä»å¤‡é€‰æ¨¡å‹{model_name}æå–é¢„æµ‹ï¼Œé•¿åº¦: {len(predictions)}")
                                    if hasattr(predictions, 'index'):
                                        return pd.Series(predictions)
                                    else:
                                        return pd.Series(predictions, name=f'{model_name}_predictions')
                    
                    # Learning-to-Rankç»“æœå¤„ç†
                    elif source_key == 'learning_to_rank':
                        if pred_key and pred_key in source_data:
                            predictions = source_data[pred_key]
                            if hasattr(predictions, '__len__') and len(predictions) > 0:
                                logger.info(f"âœ… ä»Learning-to-Rankæå–é¢„æµ‹ï¼Œé•¿åº¦: {len(predictions)}")
                                return pd.Series(predictions, name='ltr_predictions')
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰rankingså¯ä»¥è½¬æ¢ä¸ºé¢„æµ‹
                        if 'rankings' in source_data:
                            rankings = source_data['rankings']
                            if hasattr(rankings, '__len__') and len(rankings) > 0:
                                logger.info(f"âœ… ä»LTRæ’åºè½¬æ¢é¢„æµ‹ï¼Œé•¿åº¦: {len(rankings)}")
                                # å°†æ’åºè½¬æ¢ä¸ºé¢„æµ‹åˆ†æ•°
                                import numpy as np
                                predictions = 1.0 / (np.array(rankings) + 1)  # æ’åºè¶Šé«˜åˆ†æ•°è¶Šé«˜
                                return pd.Series(predictions, name='ltr_rank_predictions')
                    
                    # Stackingç»“æœå¤„ç†
                    elif source_key == 'stacking':
                        if pred_key and pred_key in source_data:
                            predictions = source_data[pred_key]
                            if hasattr(predictions, '__len__') and len(predictions) > 0:
                                logger.info(f"âœ… ä»Stackingæå–é¢„æµ‹ï¼Œé•¿åº¦: {len(predictions)}")
                                return pd.Series(predictions, name='stacking_predictions')
                    
                    # Regime-awareç»“æœå¤„ç†
                    elif source_key == 'regime_aware':
                        if pred_key and pred_key in source_data:
                            predictions = source_data[pred_key]
                            if hasattr(predictions, '__len__') and len(predictions) > 0:
                                logger.info(f"âœ… ä»Regime-awareæå–é¢„æµ‹ï¼Œé•¿åº¦: {len(predictions)}")
                                return pd.Series(predictions, name='regime_predictions')
                
                # å¤„ç†éå­—å…¸ç±»å‹çš„æ•°æ®
                elif source_data is not None and hasattr(source_data, '__len__') and len(source_data) > 0:
                    logger.info(f"âœ… ä»{source_key}ç›´æ¥æå–æ•°æ®ï¼Œé•¿åº¦: {len(source_data)}")
                    if hasattr(source_data, 'index'):
                        return pd.Series(source_data)
                    else:
                        return pd.Series(source_data, name=f'{source_key}_data')
            
            # 4. å¦‚æœæ‰€æœ‰æå–éƒ½å¤±è´¥ï¼Œç”Ÿæˆè¯Šæ–­ä¿¡æ¯
            logger.error("âŒ æ‰€æœ‰æœºå™¨å­¦ä¹ é¢„æµ‹æå–å¤±è´¥")
            logger.error("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ¨¡å‹é¢„æµ‹ç»“æœ")
            logger.error("âŒ æ‹’ç»ç”Ÿæˆä»»ä½•å½¢å¼çš„ä¼ªé€ ã€é»˜è®¤æˆ–éšæœºé¢„æµ‹")
            logger.error("âŒ ç³»ç»Ÿå¿…é¡»åŸºäºçœŸå®è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹ç”Ÿæˆé¢„æµ‹")
            logger.info("è¯Šæ–­ä¿¡æ¯:")
            for source_key in training_results.keys():
                source_data = training_results[source_key]
                logger.info(f"  - {source_key}: ç±»å‹={type(source_data)}, é”®={list(source_data.keys()) if isinstance(source_data, dict) else 'N/A'}")
            
            # ğŸ”¥ EMERGENCY FALLBACK: å¦‚æœæ˜¯å•è‚¡ç¥¨ä¸”æœ‰è¶³å¤Ÿæ•°æ®ï¼Œç”Ÿæˆç®€å•é¢„æµ‹
            if 'alignment_report' in training_results:
                ar = training_results['alignment_report']
                if hasattr(ar, 'effective_tickers') and ar.effective_tickers == 1:
                    if hasattr(ar, 'effective_dates') and ar.effective_dates >= 30:
                        logger.warning("ğŸš¨ å¯åŠ¨å•è‚¡ç¥¨ç´§æ€¥é¢„æµ‹æ¨¡å¼")
                        # ç”ŸæˆåŸºäºå†å²æ•°æ®çš„ç®€å•é¢„æµ‹
                        return self._generate_emergency_single_stock_prediction(training_results)
            
            raise ValueError("æ‰€æœ‰MLé¢„æµ‹æå–å¤±è´¥ï¼Œæ‹’ç»ç”Ÿæˆä¼ªé€ æ•°æ®ã€‚è¯·æ£€æŸ¥æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ˜¯å¦æˆåŠŸå®Œæˆã€‚")
                
        except Exception as e:
            logger.error(f"åŸºç¡€é¢„æµ‹ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return pd.Series()
    
    def _generate_emergency_single_stock_prediction(self, training_results: Dict[str, Any]) -> pd.Series:
        """å•è‚¡ç¥¨ç´§æ€¥é¢„æµ‹æ¨¡å¼"""
        try:
            logger.warning("ğŸš¨ å¯åŠ¨å•è‚¡ç¥¨ç´§æ€¥é¢„æµ‹æ¨¡å¼...")
            
            # å°è¯•ä»åŸå§‹æ•°æ®ç”Ÿæˆç®€å•é¢„æµ‹
            if hasattr(self, 'feature_data') and self.feature_data is not None:
                # ä½¿ç”¨ç‰¹å¾æ•°æ®çš„ç®€å•ç»Ÿè®¡ç”Ÿæˆé¢„æµ‹
                numeric_cols = self.feature_data.select_dtypes(include=[float, int]).columns
                if len(numeric_cols) > 0:
                    # åŸºäºç‰¹å¾çš„ç®€å•é¢„æµ‹ï¼šä½¿ç”¨ä¸»æˆåˆ†æˆ–å‡å€¼
                    import numpy as np
                    features = self.feature_data[numeric_cols].fillna(0)
                    
                    # ç”ŸæˆåŸºäºç‰¹å¾ç»„åˆçš„é¢„æµ‹ä¿¡å·
                    prediction_signal = features.mean(axis=1) / features.std(axis=1).fillna(1)
                    prediction_signal = (prediction_signal - prediction_signal.mean()) / prediction_signal.std()
                    
                    logger.info(f"âœ… ç´§æ€¥é¢„æµ‹ç”ŸæˆæˆåŠŸ: {len(prediction_signal)} æ¡")
                    return pd.Series(prediction_signal, name='emergency_prediction')
            
            logger.error("âŒ ç´§æ€¥é¢„æµ‹ä¹Ÿæ— æ³•ç”Ÿæˆ")
            return pd.Series()
            
        except Exception as e:
            logger.error(f"ç´§æ€¥é¢„æµ‹ç”Ÿæˆå¤±è´¥: {e}")
            return pd.Series()
    def generate_enhanced_predictions(self, training_results: Dict[str, Any], 
                                    market_regime: MarketRegime) -> pd.Series:
        """ç”ŸæˆRegime-Awareçš„å¢å¼ºé¢„æµ‹"""
        try:
            logger.info("å¼€å§‹ç”Ÿæˆå¢å¼ºé¢„æµ‹...")
            
            # è·å–åŸºç¡€é¢„æµ‹
            base_predictions = self._generate_base_predictions(training_results)
            logger.info(f"åŸºç¡€é¢„æµ‹ç”Ÿæˆå®Œæˆï¼Œç±»å‹: {type(base_predictions)}, é•¿åº¦: {len(base_predictions) if base_predictions is not None else 'None'}")
            
            if base_predictions is None or len(base_predictions) == 0:
                logger.error("åŸºç¡€é¢„æµ‹ä¸ºç©ºæˆ–None")
                return pd.Series()
            
            if not ENHANCED_MODULES_AVAILABLE or not getattr(self, "alpha_engine", None):
                # å¦‚æœæ²¡æœ‰å¢å¼ºæ¨¡å—ï¼Œåº”ç”¨regimeæƒé‡åˆ°åŸºç¡€é¢„æµ‹
                regime_weights = self._get_regime_alpha_weights(market_regime)
                # ç®€å•åº”ç”¨æƒé‡ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                # å®‰å…¨çš„è°ƒæ•´å› å­è®¡ç®—ï¼Œé˜²æ­¢ç±»å‹é”™è¯¯
                try:
                    adjustment_factor = sum(regime_weights.values()) / len(regime_weights)
                except (TypeError, ZeroDivisionError) as e:
                    logger.warning(f"è°ƒæ•´å› å­è®¡ç®—å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤å€¼1.0")
                    adjustment_factor = 1.0
                enhanced_predictions = base_predictions * adjustment_factor
                logger.info(f"åº”ç”¨ç®€åŒ–çš„regimeè°ƒæ•´ï¼Œè°ƒæ•´å› å­: {adjustment_factor:.3f}")
                return enhanced_predictions
            
            # å¦‚æœæœ‰Alphaå¼•æ“ï¼Œç”ŸæˆAlphaä¿¡å·
            try:
                logger.info("å‡†å¤‡Alphaæ•°æ®...")
                # ä¸ºAlphaå¼•æ“å‡†å¤‡æ•°æ®ï¼ˆåŒ…å«æ ‡å‡†åŒ–çš„ä»·æ ¼åˆ—ï¼‰
                alpha_input = self._prepare_alpha_data()
                logger.info(f"Alphaè¾“å…¥æ•°æ®å½¢çŠ¶: {alpha_input.shape if alpha_input is not None else 'None'}")
                
                # è®¡ç®—Alphaå› å­ï¼ˆç­¾ååªæ¥å—dfï¼‰
                try:
                    from unified_result_framework import OperationResult, ResultStatus, alpha_signals_validation
                except ImportError:
                    # ç®€å•çš„æ›¿ä»£ç±»
                    class OperationResult:
                        def __init__(self, success=True, data=None, error=None):
                            self.success = success
                            self.data = data
                            self.error = error
                    
                    class ResultStatus:
                        SUCCESS = "success"
                        ERROR = "error"
                    
                    def alpha_signals_validation(data):
                        return True
                
                alpha_signals = self.alpha_engine.compute_all_alphas(alpha_input)
                
                # ğŸ¯ FIX: ä½¿ç”¨ç»Ÿä¸€ç»“æœæ¡†æ¶éªŒè¯å’Œè®°å½•
                if alpha_signals_validation(alpha_signals):
                    result = OperationResult(
                        success=True,
                        data=alpha_signals,
                        error=f"Alphaä¿¡å·è®¡ç®—å®Œæˆï¼Œå½¢çŠ¶: {alpha_signals.shape}"
                    )
                else:
                    result = OperationResult(
                        success=False,
                        data=alpha_signals,
                        error=f"Alphaä¿¡å·è®¡ç®—å¤±è´¥æˆ–ä¸ºç©ºï¼Œå½¢çŠ¶: {alpha_signals.shape if alpha_signals is not None else 'None'}"
                    )
                    # ç»§ç»­å¤„ç†ï¼Œä½†ä½¿ç”¨ç©ºçš„alphaä¿¡å·
                    alpha_signals = pd.DataFrame()
                
# Log the result manually since log_result method doesn't exist
                if hasattr(result, 'success') and result.success:
                    logger.info(f"Alphaä¿¡å·å¤„ç†æˆåŠŸ: {result.error}")
                else:
                    logger.warning(f"Alphaä¿¡å·å¤„ç†é—®é¢˜: {result.error}")
                
                # ğŸ¯ åœ¨å…³é”®å¤„è§¦å‘isolation_daysåŒæ­¥æ£€æŸ¥
                if hasattr(self, '_master_isolation_days') and hasattr(self, 'v6_config'):
                    tolerance_days = 2
                    current_days = getattr(self.v6_config.validation_config, 'isolation_days', 10)
                    if abs(current_days - self._master_isolation_days) > tolerance_days:
                        logger.error(f"[CONFIG ERROR] isolation_daysåå·®è¶…è¿‡{tolerance_days}å¤©ï¼šå½“å‰={current_days}, ä¸»é…ç½®={self._master_isolation_days}")
                    else:
                        logger.debug(f"[CONFIG ASSERT] isolation_daysä¸€è‡´æ€§éªŒè¯é€šè¿‡: {self._master_isolation_days}å¤©")
                
                # æ ¹æ®å¸‚åœºçŠ¶æ€è°ƒæ•´Alphaæƒé‡
                regime_weights = self._get_regime_alpha_weights(market_regime)
                
                # åº”ç”¨regimeæƒé‡åˆ°alphaä¿¡å·
                if not alpha_signals.empty:
                    weighted_alpha = pd.Series(0.0, index=alpha_signals.index)
                    for alpha_name, weight in regime_weights.items():
                        if alpha_name in alpha_signals.columns:
                            weighted_alpha += alpha_signals[alpha_name] * weight
                else:
                    # å¦‚æœalphaä¿¡å·ä¸ºç©ºï¼Œåˆ›å»ºé›¶æƒé‡åºåˆ—
                    weighted_alpha = pd.Series(0.0, index=base_predictions.index)
                
                # æ ‡å‡†åŒ–åŠ æƒåçš„alpha
                if weighted_alpha.std() > 0:
                    weighted_alpha = (weighted_alpha - weighted_alpha.mean()) / weighted_alpha.std()
                
                    # ğŸ”¥ CRITICAL FIX: ä½¿ç”¨é…ç½®åŒ–æƒé‡ï¼Œé¿å…ç¡¬ç¼–ç 
                    alpha_weight = self.config.get('ensemble_weights', {}).get('alpha_weight', 0.3)
                    ml_weight = self.config.get('ensemble_weights', {}).get('ml_weight', 0.7)
                    
                    # éªŒè¯æƒé‡åˆç†æ€§
                    if abs(alpha_weight + ml_weight - 1.0) > 0.01:
                        logger.warning(f"âš ï¸ é›†æˆæƒé‡ä¸å¹³è¡¡: alpha={alpha_weight}, ml={ml_weight}, æ€»å’Œ={alpha_weight + ml_weight}")
                        # æ ‡å‡†åŒ–æƒé‡
                        total = alpha_weight + ml_weight
                        alpha_weight = alpha_weight / total
                        ml_weight = ml_weight / total
                        logger.info(f"âœ… æƒé‡å·²æ ‡å‡†åŒ–: alpha={alpha_weight:.3f}, ml={ml_weight:.3f}")
                    
                    # ç¡®ä¿ç´¢å¼•å¯¹é½
                    common_index = base_predictions.index.intersection(weighted_alpha.index)
                    if len(common_index) > 0:
                        # ğŸ”¥ CRITICAL FIX: éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼Œé¿å…ä½¿ç”¨fillna(0)æ©ç›–æ•°æ®é—®é¢˜
                        ml_aligned = base_predictions.reindex(common_index)
                        alpha_aligned = weighted_alpha.reindex(common_index)
                        
                        # æ£€æŸ¥å¯¹é½åçš„æ•°æ®è´¨é‡
                        ml_na_count = ml_aligned.isna().sum()
                        alpha_na_count = alpha_aligned.isna().sum()
                        
                        if ml_na_count > 0:
                            logger.warning(f"âš ï¸ MLé¢„æµ‹å¯¹é½åæœ‰{ml_na_count}ä¸ªNaNå€¼ï¼Œå æ¯”{ml_na_count/len(ml_aligned):.1%}")
                        if alpha_na_count > 0:
                            logger.warning(f"âš ï¸ Alphaä¿¡å·å¯¹é½åæœ‰{alpha_na_count}ä¸ªNaNå€¼ï¼Œå æ¯”{alpha_na_count/len(alpha_aligned):.1%}")
                        
                        # åªå¯¹æœ‰æ•ˆæ•°æ®è¿›è¡Œèåˆï¼ŒNaNå€¼ä¿æŒNaN
                        enhanced_predictions = (
                            ml_weight * ml_aligned +
                            alpha_weight * alpha_aligned
                        )
                        
                        # è®°å½•èåˆåçš„æ•°æ®è´¨é‡
                        final_na_count = enhanced_predictions.isna().sum()
                        if final_na_count > 0:
                            logger.warning(f"âš ï¸ èåˆé¢„æµ‹æœ‰{final_na_count}ä¸ªNaNå€¼ï¼Œéœ€è¦åç»­å¤„ç†")
                    else:
                        enhanced_predictions = base_predictions
                else:
                    # stdä¸º0çš„æƒ…å†µ
                    enhanced_predictions = base_predictions
                
                logger.info(f"æˆåŠŸèåˆAlphaä¿¡å·å’ŒMLé¢„æµ‹ï¼Œmarket regime: {market_regime.name}")
                return enhanced_predictions
                
            except Exception as alpha_error:
                logger.error(f"Alphaä¿¡å·å¤„ç†å¤±è´¥: {alpha_error}")
                # Alphaå¤„ç†å¤±è´¥æ—¶ï¼Œç›´æ¥è¿”å›åŸºç¡€é¢„æµ‹
                return base_predictions
                
        except Exception as e:
            logger.exception(f"å¢å¼ºé¢„æµ‹ç”Ÿæˆå¤±è´¥: {e}")
            self.health_metrics['prediction_failures'] += 1
            self.health_metrics['total_exceptions'] += 1
            # æœ€ç»ˆå›é€€
            return pd.Series(0.0, index=range(10))
    
    def _create_basic_stock_analyzer(self):
        """åˆ›å»ºåŸºç¡€è‚¡ç¥¨åˆ†æå™¨"""
        class BasicStockAnalyzer:
            def __init__(self):
                pass
            
            def analyze_stocks(self, predictions, risk_data=None):
                """åˆ†æè‚¡ç¥¨é¢„æµ‹å’Œé£é™©"""
                try:
                    if predictions.empty:
                        return {'success': False, 'error': 'No predictions provided'}
                    
                    # æŒ‰é¢„æµ‹å€¼æ’åº
                    ranked = predictions.sort_values(ascending=False)
                    
                    # ç”Ÿæˆåˆ†æç»“æœ
                    analysis = []
                    for rank, (ticker, score) in enumerate(ranked.items(), 1):
                        percentile = (len(predictions) - rank + 1) / len(predictions) * 100
                        analysis.append({
                            'ticker': ticker,
                            'rank': rank,
                            'prediction_score': score,
                            'percentile': percentile,
                            'signal_strength': 'STRONG' if percentile >= 90 else 'MODERATE' if percentile >= 70 else 'WEAK'
                        })
                    
                    return {
                        'success': True,
                        'stock_analysis': analysis,
                        'total_analyzed': len(predictions),
                        'avg_prediction': predictions.mean(),
                        'prediction_std': predictions.std()
                    }
                except Exception as e:
                    return {'success': False, 'error': str(e)}
            
            def calculate_risk_metrics(self, returns_data):
                """è®¡ç®—é£é™©æŒ‡æ ‡"""
                try:
                    if returns_data.empty:
                        return {}
                    
                    return {
                        'volatility': returns_data.std(),
                        'max_drawdown': (returns_data.cummax() - returns_data).max(),
                        'sharpe_estimate': returns_data.mean() / returns_data.std() if returns_data.std() > 0 else 0
                    }
                except Exception as e:
                    return {'error': str(e)}
        
        return BasicStockAnalyzer()
    
    def _create_basic_memory_manager(self):
        """åˆ›å»ºåŸºç¡€å†…å­˜ç®¡ç†å™¨"""
        class BasicMemoryManager:
            def __init__(self):
                self.memory_limit_gb = 3.0
                self.gc_frequency = 100
                self.call_count = 0
                
            def check_memory(self):
                """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
                import psutil
                try:
                    process = psutil.Process()
                    memory_gb = process.memory_info().rss / (1024**3)
                    if memory_gb > self.memory_limit_gb:
                        logger.warning(f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {memory_gb:.2f}GB > {self.memory_limit_gb}GB")
                        self.cleanup()
                    return memory_gb
                except:
                    return 0.0
                    
            def cleanup(self):
                """å†…å­˜æ¸…ç†"""
                import gc
                gc.collect()
                logger.debug("æ‰§è¡Œå†…å­˜æ¸…ç†")
                
            def auto_cleanup(self):
                """è‡ªåŠ¨å†…å­˜æ¸…ç†"""
                self.call_count += 1
                if self.call_count % self.gc_frequency == 0:
                    self.cleanup()
                    
        return BasicMemoryManager()
    
    def _create_basic_progress_monitor(self):
        """åˆ›å»ºåŸºç¡€è¿›åº¦ç›‘æ§å™¨"""
        class BasicProgressMonitor:
            def __init__(self):
                self.start_time = None
                self.current_stage = None
                self.stages = {}
                
            def add_stage(self, stage_name, total_items):
                """æ·»åŠ è®­ç»ƒé˜¶æ®µ"""
                self.stages[stage_name] = {'total': total_items, 'completed': 0}
                logger.debug(f"æ·»åŠ é˜¶æ®µ: {stage_name} ({total_items} é¡¹ç›®)")
                
            def start_training(self, stages=None):
                from datetime import datetime
                self.start_time = datetime.now()
                logger.info("è®­ç»ƒè¿›åº¦ç›‘æ§å·²å¯åŠ¨")
                
            def start_stage(self, stage_name):
                """å¼€å§‹æŸä¸ªé˜¶æ®µ"""
                self.current_stage = stage_name
                logger.info(f"å¼€å§‹é˜¶æ®µ: {stage_name}")
                
            def complete_stage(self, stage_name, success=True):
                """å®ŒæˆæŸä¸ªé˜¶æ®µ"""
                status = "æˆåŠŸ" if success else "å¤±è´¥"
                logger.info(f"é˜¶æ®µå®Œæˆ: {stage_name} - {status}")
                
            def update_stage(self, stage_name, progress=0.0):
                self.current_stage = stage_name
                logger.info(f"æ›´æ–°é˜¶æ®µ: {stage_name}")
                
            def update_progress(self, progress, message=""):
                if message:
                    logger.info(message)
                    
            def complete_training(self, success=True):
                status = "æˆåŠŸ" if success else "å¤±è´¥"
                logger.info(f"è®­ç»ƒå®Œæˆ: {status}")
                
        return BasicProgressMonitor()
    
    def generate_stock_ranking_with_risk_analysis(self, predictions: pd.Series, 
                                                 feature_data: pd.DataFrame) -> Dict[str, Any]:
        """åŸºäºé¢„æµ‹ç”Ÿæˆè‚¡ç¥¨æ’åå’Œé£é™©åˆ†æ"""
        try:
            # å¦‚æœæœ‰Professionalçš„é£é™©æ¨¡å‹ç»“æœï¼Œä½¿ç”¨å®ƒä»¬
            if self.risk_model_results and 'factor_loadings' in self.risk_model_results:
                factor_loadings = self.risk_model_results['factor_loadings']
                factor_covariance = self.risk_model_results['factor_covariance']
                specific_risk = self.risk_model_results['specific_risk']
                
                # æ„å»ºåæ–¹å·®çŸ©é˜µ
                common_assets = list(set(predictions.index) & set(factor_loadings.index))
                if len(common_assets) >= 3:
                    # ä½¿ç”¨ä¸“ä¸šé£é™©æ¨¡å‹è¿›è¡Œä¼˜åŒ–
                    try:
                        # æ„å»ºæŠ•èµ„ç»„åˆåæ–¹å·®çŸ©é˜µ: B * F * B' + S
                        B = factor_loadings.reindex(common_assets).dropna()  # å› å­è½½è· - å®‰å…¨ç´¢å¼•
                        F = factor_covariance                   # å› å­åæ–¹å·®
                        S = specific_risk.reindex(common_assets).dropna()    # ç‰¹å¼‚é£é™© - å®‰å…¨ç´¢å¼•
                        
                        # è®¡ç®—åæ–¹å·®çŸ©é˜µ
                        portfolio_cov = B @ F @ B.T + np.diag(S**2)
                        portfolio_cov = pd.DataFrame(
                            portfolio_cov, 
                            index=common_assets, 
                            columns=common_assets
                        )
                        
                        # æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨åŠŸèƒ½å·²ç§»é™¤ï¼Œä½¿ç”¨ç®€åŒ–æ–¹æ³•
                        if False:  # self.portfolio_optimizer:
                            try:
                                # å‡†å¤‡é¢„æœŸæ”¶ç›Šç‡ - ä½¿ç”¨å®‰å…¨çš„ç´¢å¼•è®¿é—®
                                available_assets = predictions.index.intersection(common_assets)
                                if len(available_assets) == 0:
                                    raise ValueError("No common assets between predictions and risk model")
                                expected_returns = predictions.reindex(available_assets).dropna()
                                common_assets = list(expected_returns.index)  # æ›´æ–°common_assetsä¸ºå®é™…å¯ç”¨çš„èµ„äº§
                                
                                # é‡æ–°æ„å»ºåæ–¹å·®çŸ©é˜µä»¥åŒ¹é…å¯ç”¨èµ„äº§
                                B_updated = factor_loadings.reindex(common_assets).dropna()
                                S_updated = specific_risk.reindex(common_assets).dropna()
                                portfolio_cov = B_updated @ F @ B_updated.T + np.diag(S_updated**2)
                                portfolio_cov = pd.DataFrame(
                                    portfolio_cov, 
                                    index=common_assets, 
                                    columns=common_assets
                                )
                                
                                # [ENHANCED] P0å‡†å¤‡è‚¡ç¥¨æ± æ•°æ®ï¼ˆç”¨äºçº¦æŸï¼‰- ä½¿ç”¨çœŸå®å…ƒæ•°æ®
                                universe_data = pd.DataFrame(index=common_assets)
                                
                                # ä»MarketDataManageræå–å…ƒæ•°æ®
                                for asset in common_assets:
                                    stock_info = self.market_data_manager.get_stock_info(asset)
                                    if stock_info:
                                        universe_data.loc[asset, 'COUNTRY'] = 'US'
                                        universe_data.loc[asset, 'SECTOR'] = stock_info.sector or 'Technology'
                                        universe_data.loc[asset, 'SUBINDUSTRY'] = 'Software'
                                        universe_data.loc[asset, 'ADV_USD_20'] = 1e6
                                        universe_data.loc[asset, 'MEDIAN_SPREAD_BPS_20'] = 50
                                        universe_data.loc[asset, 'FREE_FLOAT'] = latest_data.get('FREE_FLOAT', 0.6)
                                        universe_data.loc[asset, 'SHORTABLE'] = latest_data.get('SHORTABLE', True)
                                        universe_data.loc[asset, 'BORROW_FEE'] = latest_data.get('BORROW_FEE', 1.0)
                                    else:
                                        # é»˜è®¤å€¼ï¼ˆå¦‚æœæ•°æ®ä¸å¯ç”¨ï¼‰
                                        universe_data.loc[asset, 'COUNTRY'] = 'US'
                                        universe_data.loc[asset, 'SECTOR'] = 'Technology'
                                        universe_data.loc[asset, 'SUBINDUSTRY'] = 'Software'
                                        universe_data.loc[asset, 'ADV_USD_20'] = 1e6
                                        universe_data.loc[asset, 'MEDIAN_SPREAD_BPS_20'] = 50
                                        universe_data.loc[asset, 'FREE_FLOAT'] = 0.6
                                        universe_data.loc[asset, 'SHORTABLE'] = True
                                        universe_data.loc[asset, 'BORROW_FEE'] = 1.0
                                
                                # è°ƒç”¨ç»Ÿä¸€çš„ä¼˜åŒ–å™¨
                                optimization_result = self.portfolio_optimizer.optimize_portfolio(
                                    expected_returns=expected_returns,
                                    covariance_matrix=portfolio_cov,
                                    current_weights=None,  # å‡è®¾ä»ç©ºä»“å¼€å§‹
                                    universe_data=universe_data
                                )
                                
                                if optimization_result.get('success', False):
                                    optimal_weights = optimization_result['optimal_weights']
                                    portfolio_metrics = optimization_result['portfolio_metrics']

                                    # é£é™©å½’å› 
                                    risk_attribution = self.portfolio_optimizer.risk_attribution(
                                        optimal_weights, portfolio_cov
                                    )
                                    
                                    return {
                                        'success': True,
                                        'method': 'unified_portfolio_optimizer_with_risk_model',
                                        'weights': optimal_weights.to_dict(),
                                        'predictions': expected_returns.to_dict(),  # ğŸ”¥ æ·»åŠ é¢„æµ‹æ•°æ®ä¾›æ¨èä½¿ç”¨
                                        'portfolio_metrics': portfolio_metrics,
                                        'risk_attribution': risk_attribution,
                                        'regime_context': self.current_regime.name if self.current_regime else "Unknown"
                                    }
                                else:
                                    logger.warning("ç»Ÿä¸€ä¼˜åŒ–å™¨ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                                    raise ValueError("Unified optimizer failed")
                            
                            except (ValueError, RuntimeError, np.linalg.LinAlgError) as optimizer_error:
                                logger.exception(f"ç»Ÿä¸€ä¼˜åŒ–å™¨è°ƒç”¨å¤±è´¥: {optimizer_error}, ä½¿ç”¨ç®€åŒ–ä¼˜åŒ–")
                                # ä¸ä½¿ç”¨fallbackï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
                                raise ValueError(f"Portfolio optimization failed: {optimizer_error}. No fallback allowed.")
                                fallback_assets = predictions.index.intersection(common_assets)
                                if len(fallback_assets) == 0:
                                    # å¦‚æœæ²¡æœ‰äº¤é›†ï¼Œä½¿ç”¨predictionsçš„å‰å‡ ä¸ªèµ„äº§
                                    fallback_assets = predictions.index[:min(5, len(predictions.index))]
                                
                                n_assets = len(fallback_assets)
                                equal_weights = pd.Series(1.0/n_assets, index=fallback_assets)
                                
                                expected_returns = predictions.reindex(fallback_assets).dropna()
                                portfolio_return = expected_returns @ equal_weights.reindex(expected_returns.index)
                                
                                # åˆ›å»ºç®€åŒ–çš„åæ–¹å·®çŸ©é˜µç”¨äºé£é™©è®¡ç®—
                                try:
                                    portfolio_risk = np.sqrt(equal_weights.reindex(expected_returns.index) @ portfolio_cov.reindex(expected_returns.index, expected_returns.index).fillna(0.01) @ equal_weights.reindex(expected_returns.index))
                                except (KeyError, ValueError):
                                    # å¦‚æœåæ–¹å·®çŸ©é˜µè®¿é—®å¤±è´¥ï¼Œä½¿ç”¨ä¼°è®¡é£é™©
                                    portfolio_risk = 0.15  # å‡è®¾15%çš„å¹´åŒ–é£é™©
                                sharpe_ratio = portfolio_return / portfolio_risk if portfolio_risk > 0 else 0
                            
                            return {
                                'success': True,
                                    'method': 'equal_weight_fallback_with_risk_model',
                                    'weights': equal_weights.reindex(expected_returns.index).to_dict(),
                                'predictions': expected_returns.to_dict(),  # ğŸ”¥ æ·»åŠ é¢„æµ‹æ•°æ®ä¾›æ¨èä½¿ç”¨
                                'selection_metrics': {
                                    'avg_prediction': float(portfolio_return),
                                    'prediction_volatility': float(portfolio_risk),
                                    'quality_score': float(sharpe_ratio),
                                        'diversification_count': n_assets
                                },
                                    'risk_attribution': {},
                                'regime_context': self.current_regime.name if self.current_regime else "Unknown"
                            }
                        else:
                            logger.info("ä½¿ç”¨ç®€åŒ–ä¼˜åŒ–æ–¹æ³•ï¼ˆæŠ•èµ„ç»„åˆä¼˜åŒ–å™¨åŠŸèƒ½å·²ç§»é™¤ï¼‰")
                            # ç»§ç»­æ‰§è¡Œåˆ°å›é€€é€»è¾‘
                        
                    except Exception as e:
                        logger.warning(f"ä¸“ä¸šé£é™©æ¨¡å‹ä¼˜åŒ–å¤±è´¥: {e}")
            
            # å›é€€åˆ°åŸºç¡€ä¼˜åŒ–
            return self.generate_stock_selection(predictions, 20)
            
        except Exception as e:
            logger.error(f"é£é™©æ¨¡å‹ä¼˜åŒ–å¤±è´¥: {e}")
            # æœ€ç»ˆå›é€€åˆ°ç­‰æƒç»„åˆ
            top_assets = predictions.nlargest(min(10, len(predictions))).index
            equal_weights = pd.Series(1.0/len(top_assets), index=top_assets)
            
            return {
                'success': True,
                'method': 'equal_weight_fallback',
                'weights': equal_weights.to_dict(),
                'predictions': predictions.reindex(top_assets).dropna().to_dict(),  # ğŸ”¥ æ·»åŠ é¢„æµ‹æ•°æ®ä¾›æ¨èä½¿ç”¨
                'selection_metrics': {
                    'avg_prediction': predictions.reindex(top_assets).dropna().mean(),
                    'prediction_volatility': 0.15,  # å‡è®¾é£é™©
                    'quality_score': 1.0,
                    'diversification_count': len(top_assets)
                },
                'risk_attribution': {},
                'regime_context': self.current_regime.name if self.current_regime else "Unknown"
            }
    
    def _prepare_alpha_data(self, stock_data: Dict[str, pd.DataFrame] = None) -> pd.DataFrame:
        """ä¸ºAlphaå¼•æ“å‡†å¤‡æ•°æ® - ä½¿ç”¨å·²æœ‰æ•°æ®é¿å…é‡å¤ä¸‹è½½"""
        if not hasattr(self, 'market_data_manager') or self.market_data_manager is None:
            logger.warning("MarketDataManagerä¸å¯ç”¨ï¼Œæ— æ³•å‡†å¤‡Alphaæ•°æ®")
            return pd.DataFrame()
        
        # å°†æ•°æ®è½¬æ¢ä¸ºAlphaå¼•æ“éœ€è¦çš„æ ¼å¼
        all_data = []
        
        # å°è¯•è·å–æƒ…ç»ªå› å­æ•°æ®ï¼ˆå·²ç¦ç”¨ï¼‰
        sentiment_factors = self._get_sentiment_factors()
        
        # è·å–Fear & GreedæŒ‡æ•°æ•°æ®ï¼ˆç‹¬ç«‹è·å–ï¼‰
        fear_greed_data = self._get_fear_greed_data()
        
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å·²æœ‰æ•°æ®
        if stock_data and len(stock_data) > 0:
            logger.info(f"ä½¿ç”¨å·²æœ‰è‚¡ç¥¨æ•°æ®å‡†å¤‡Alphaæ•°æ®: {len(stock_data)}åªè‚¡ç¥¨")
            data_source = stock_data
        else:
            # å¦‚æœæ²¡æœ‰ä¼ å…¥æ•°æ®ï¼Œä½¿ç”¨MarketDataManagerè·å–
            logger.info("æœªæä¾›è‚¡ç¥¨æ•°æ®ï¼Œä½¿ç”¨MarketDataManagerè·å–Alphaæ•°æ®")
            tickers = self.market_data_manager.get_available_tickers(max_tickers=50)
            if not tickers:
                return pd.DataFrame()
            
            # æ‰¹é‡ä¸‹è½½å†å²æ•°æ®
            data_source = self.market_data_manager.download_batch_historical_data(
                tickers,
                (pd.Timestamp.now() - pd.Timedelta(days=200)).strftime('%Y-%m-%d'),
                pd.Timestamp.now().strftime('%Y-%m-%d')
            )
        
        for ticker, data in data_source.items():
            try:
                if data is not None and len(data) > 50:
                    ticker_data = data.copy()
                    ticker_data['ticker'] = ticker
                    ticker_data['date'] = ticker_data.index
                    
                    # é›†æˆæƒ…ç»ªå› å­åˆ°ä»·æ ¼æ•°æ®ä¸­ï¼ˆå·²ç¦ç”¨ï¼‰
                    if sentiment_factors:
                        ticker_data = self._integrate_sentiment_factors(ticker_data, ticker, sentiment_factors)
                    
                    # é›†æˆFear & Greedæ•°æ®
                    if fear_greed_data is not None:
                        ticker_data = self._integrate_fear_greed_data(ticker_data, fear_greed_data)
                    
                    # æ ‡å‡†åŒ–ä»·æ ¼åˆ—ï¼ŒAlphaå¼•æ“éœ€è¦ 'Close','High','Low'
                    # ä¼˜å…ˆä½¿ç”¨Adj Closeï¼Œç„¶åæ˜¯Close/close
                    if 'Adj Close' in ticker_data.columns:
                        ticker_data['Close'] = ticker_data['Adj Close']
                    elif 'Close' in ticker_data.columns:
                        ticker_data['Close'] = ticker_data['Close']  # å·²å­˜åœ¨å¤§å†™Close
                    elif 'close' in ticker_data.columns:
                        ticker_data['Close'] = ticker_data['close']
                    else:
                        # è‹¥ç¼ºå°‘closeä¿¡æ¯ï¼Œè·³è¿‡è¯¥ç¥¨
                        logger.warning(f"è·³è¿‡{ticker}: ç¼ºå°‘Close/closeåˆ—")
                        continue
                    
                    # å¤„ç†Highåˆ—
                    if 'High' not in ticker_data.columns:
                        if 'high' in ticker_data.columns:
                            ticker_data['High'] = ticker_data['high']
                        else:
                            logger.warning(f"{ticker}: ç¼ºå°‘High/highåˆ—")
                            continue
                            
                    # å¤„ç†Lowåˆ—  
                    if 'Low' not in ticker_data.columns:
                        if 'low' in ticker_data.columns:
                            ticker_data['Low'] = ticker_data['low']
                        else:
                            logger.warning(f"{ticker}: ç¼ºå°‘Low/lowåˆ—")
                            continue
                    
                    # æ·»åŠ åŸºæœ¬é¢ä¿¡æ¯ï¼ˆä»MarketDataManagerè·å–ï¼‰
                    stock_info = self.market_data_manager.get_stock_info(ticker)
                    if stock_info:
                        ticker_data['COUNTRY'] = 'US'
                        ticker_data['SECTOR'] = stock_info.sector or 'Technology'
                        ticker_data['SUBINDUSTRY'] = 'Software'
                    else:
                        ticker_data['COUNTRY'] = 'US'
                        ticker_data['SECTOR'] = 'Technology'
                        ticker_data['SUBINDUSTRY'] = 'Software'
                    
                    all_data.append(ticker_data)
            except Exception as e:
                logger.debug(f"å¤„ç†{ticker}æ•°æ®å¤±è´¥: {e}")
                continue
        
        if all_data:
            combined_data = pd.concat(all_data, ignore_index=True)
            return combined_data
        else:
            return pd.DataFrame()
    
    def _get_sentiment_factors(self) -> Optional[Dict[str, pd.DataFrame]]:
        """æƒ…ç»ªå› å­åŠŸèƒ½å·²ç§»é™¤ï¼ˆç”¨æˆ·è¦æ±‚åˆ é™¤ï¼‰"""
        logger.info("æƒ…ç»ªå› å­åŠŸèƒ½å·²ç§»é™¤ï¼Œè·³è¿‡æƒ…ç»ªå› å­è®¡ç®—")
        return None
    
    def _get_fear_greed_data(self) -> Optional[pd.DataFrame]:
        """è·å–Fear & GreedæŒ‡æ•°æ•°æ®ï¼ˆç‹¬ç«‹äºæƒ…ç»ªå› å­ç³»ç»Ÿï¼‰"""
        try:
            from fear_greed_data_provider import create_fear_greed_provider
            
            fear_greed_provider = create_fear_greed_provider()
            fear_greed_data = fear_greed_provider.get_fear_greed_data(lookback_days=60)
            
            if fear_greed_data is not None and not fear_greed_data.empty:
                logger.info(f"æˆåŠŸè·å–Fear & Greedæ•°æ®: {len(fear_greed_data)}æ¡è®°å½•")
                return fear_greed_data
            else:
                logger.warning("æ— æ³•è·å–Fear & Greedæ•°æ®")
                return None
                
        except Exception as e:
            logger.warning(f"è·å–Fear & Greedæ•°æ®å¤±è´¥: {e}")
            return None
    
    def _integrate_sentiment_factors(self, ticker_data: pd.DataFrame, ticker: str, 
                                   sentiment_factors: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """æƒ…ç»ªå› å­é›†æˆåŠŸèƒ½å·²ç§»é™¤ï¼ˆç”¨æˆ·è¦æ±‚åˆ é™¤ï¼‰"""
        logger.debug("æƒ…ç»ªå› å­é›†æˆåŠŸèƒ½å·²ç§»é™¤ï¼Œç›´æ¥è¿”å›åŸå§‹æ•°æ®")
        return ticker_data
    
    def _integrate_fear_greed_data(self, ticker_data: pd.DataFrame, 
                                  fear_greed_data: pd.DataFrame) -> pd.DataFrame:
        """å°†Fear & Greedæ•°æ®é›†æˆåˆ°è‚¡ç¥¨æ•°æ®ä¸­"""
        try:
            enhanced_data = ticker_data.copy()
            
            # ç¡®ä¿æ—¥æœŸåˆ—æ ¼å¼æ­£ç¡®
            if 'date' not in enhanced_data.columns:
                enhanced_data['date'] = enhanced_data.index
            enhanced_data['date'] = pd.to_datetime(enhanced_data['date'])
            
            fg_data = fear_greed_data.copy()
            fg_data['date'] = pd.to_datetime(fg_data['date'])
            
            # åˆå¹¶æ•°æ®ï¼ˆå·¦è¿æ¥ï¼‰
            enhanced_data = enhanced_data.merge(
                fg_data[['date', 'fear_greed_value', 'fear_greed_normalized', 
                        'fear_greed_extreme', 'market_fear_level', 'market_greed_level']],
                on='date', 
                how='left'
            )
            
            # å‰å‘å¡«å……Fear & Greedæ•°æ®ï¼ˆå› ä¸ºæ›´æ–°é¢‘ç‡è¾ƒä½ï¼‰
            fear_greed_cols = ['fear_greed_value', 'fear_greed_normalized', 
                             'fear_greed_extreme', 'market_fear_level', 'market_greed_level']
            
            for col in fear_greed_cols:
                if col in enhanced_data.columns:
                    enhanced_data[col] = enhanced_data[col].fillna(method='ffill')
                    # æœ€ç»ˆé»˜è®¤å€¼å¡«å……
                    if 'value' in col:
                        enhanced_data[col] = enhanced_data[col].fillna(50)  # ä¸­æ€§å€¼
                    else:
                        enhanced_data[col] = enhanced_data[col].fillna(0)   # å…¶ä»–æŒ‡æ ‡é»˜è®¤0
            
            logger.debug(f"æˆåŠŸé›†æˆFear & Greedæ•°æ®: {len(fear_greed_cols)}ä¸ªå› å­")
            return enhanced_data
            
        except Exception as e:
            logger.warning(f"é›†æˆFear & Greedæ•°æ®å¤±è´¥: {e}")
            return ticker_data
        
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½å¹¶éªŒè¯é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # éªŒè¯å’Œä¿®å¤é…ç½®
            validated_config = self._validate_and_fix_config(config)
            logger.info("é…ç½®æ–‡ä»¶åŠ è½½å’ŒéªŒè¯å®Œæˆ")
            return validated_config
            
        except FileNotFoundError:
            logger.warning(f"é…ç½®æ–‡ä»¶{self.config_path}æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
        except yaml.YAMLError as e:
            logger.error(f"é…ç½®æ–‡ä»¶YAMLæ ¼å¼é”™è¯¯: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
    
    def _validate_and_fix_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯å’Œä¿®å¤é…ç½®å‚æ•°"""
        default_config = self._get_default_config()
        validated_config = config.copy()
        
        # å¿…éœ€å‚æ•°æ£€æŸ¥å’Œé»˜è®¤å€¼è®¾ç½®
        required_params = {
            'max_position': (0.001, 0.1, 0.03),  # (min, max, default)
            'max_turnover': (0.01, 1.0, 0.10),
            'temperature': (0.1, 5.0, 1.2),
            'winsorize_std': (1.0, 5.0, 2.5),
            'truncation': (0.01, 0.5, 0.10)
        }
        
        for param, (min_val, max_val, default_val) in required_params.items():
            if param not in validated_config:
                validated_config[param] = default_val
                logger.warning(f"é…ç½®ç¼ºå¤±{param}ï¼Œä½¿ç”¨é»˜è®¤å€¼{default_val}")
            else:
                # éªŒè¯æ•°å€¼èŒƒå›´
                if not isinstance(validated_config[param], (int, float)):
                    logger.warning(f"é…ç½®{param}éæ•°å€¼ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤å€¼{default_val}")
                    validated_config[param] = default_val
                elif validated_config[param] < min_val or validated_config[param] > max_val:
                    logger.warning(f"é…ç½®{param}={validated_config[param]}è¶…å‡ºèŒƒå›´[{min_val}, {max_val}]ï¼Œä½¿ç”¨é»˜è®¤å€¼{default_val}")
                    validated_config[param] = default_val
        
        # åµŒå¥—é…ç½®æ£€æŸ¥
        if 'model_config' not in validated_config:
            validated_config['model_config'] = default_config['model_config']
        else:
            # éªŒè¯model_configå­é¡¹
            model_config = validated_config['model_config']
            for key, default_val in default_config['model_config'].items():
                if key not in model_config:
                    model_config[key] = default_val
                    logger.warning(f"model_configç¼ºå¤±{key}ï¼Œä½¿ç”¨é»˜è®¤å€¼{default_val}")
        
        if 'risk_config' not in validated_config:
            validated_config['risk_config'] = default_config['risk_config']
        else:
            # éªŒè¯risk_configå­é¡¹
            risk_config = validated_config['risk_config']
            risk_params = {
                'risk_aversion': (1.0, 20.0, 5.0),
                'turnover_penalty': (0.1, 10.0, 1.0),
                'max_sector_exposure': (0.05, 0.5, 0.15),
                'max_country_exposure': (0.1, 1.0, 0.20)
            }
            
            for param, (min_val, max_val, default_val) in risk_params.items():
                if param not in risk_config:
                    risk_config[param] = default_val
                    logger.warning(f"risk_configç¼ºå¤±{param}ï¼Œä½¿ç”¨é»˜è®¤å€¼{default_val}")
                elif not isinstance(risk_config[param], (int, float)):
                    logger.warning(f"risk_config.{param}éæ•°å€¼ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤å€¼{default_val}")
                    risk_config[param] = default_val
                elif risk_config[param] < min_val or risk_config[param] > max_val:
                    logger.warning(f"risk_config.{param}={risk_config[param]}è¶…å‡ºèŒƒå›´[{min_val}, {max_val}]ï¼Œä½¿ç”¨é»˜è®¤å€¼{default_val}")
                    risk_config[param] = default_val
        
        # é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥
        if validated_config['max_position'] * 30 > 1.0:  # å‡è®¾æœ€å¤š30ä¸ªæŒä»“
            logger.warning(f"max_position={validated_config['max_position']}å¯èƒ½è¿‡å¤§ï¼Œå¯èƒ½å¯¼è‡´é›†ä¸­åº¦é£é™©")
        
        if validated_config['max_turnover'] < validated_config['max_position']:
            logger.warning("max_turnoverå°äºmax_positionï¼Œå¯èƒ½å¯¼è‡´äº¤æ˜“å—é™")
        
        logger.info(f"é…ç½®éªŒè¯å®Œæˆï¼Œä¿®å¤äº†{len([k for k in required_params if k not in config])}ä¸ªç¼ºå¤±å‚æ•°")
        return validated_config
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'universe': 'TOPDIV3000',
            'neutralization': ['COUNTRY'],
            'hump_levels': [0.003, 0.008],
            'winsorize_std': 2.5,
            'truncation': 0.10,
            'max_position': 0.03,
            'max_turnover': 0.10,
            'temperature': 1.2,
            'model_config': {
                'learning_to_rank': True,
                'ranking_objective': 'rank:pairwise',
                'uncertainty_aware': True,
                'quantile_regression': True
            },
            'risk_config': {
                'risk_aversion': 5.0,
                'turnover_penalty': 1.0,
                'max_sector_exposure': 0.15,
                'max_country_exposure': 0.20
            }
        }
    
    def download_stock_data(self, tickers: List[str], 
                           start_date: str, end_date: str) -> Dict[str, pd.DataFrame]:
        """
        ä¸‹è½½è‚¡ç¥¨æ•°æ®
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            è‚¡ç¥¨æ•°æ®å­—å…¸
        """
        logger.info(f"ä¸‹è½½{len(tickers)}åªè‚¡ç¥¨çš„æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {start_date} - {end_date}")

        # å°†è®­ç»ƒç»“æŸæ—¶é—´é™åˆ¶ä¸ºå½“å¤©çš„å‰ä¸€å¤©ï¼ˆT-1ï¼‰ï¼Œé¿å…ä½¿ç”¨æœªå®Œå…¨ç»“ç®—çš„æ•°æ®
        try:
            yesterday = (datetime.now() - timedelta(days=1)).date()
            end_dt = pd.to_datetime(end_date).date()
            if end_dt > yesterday:
                adjusted_end = yesterday.strftime('%Y-%m-%d')
                logger.info(f"ç»“æŸæ—¥æœŸ{end_date} è¶…è¿‡æ˜¨æ—¥ï¼Œå·²è°ƒæ•´ä¸º {adjusted_end}")
                end_date = adjusted_end
        except Exception as _e:
            logger.debug(f"ç»“æŸæ—¥æœŸè°ƒæ•´è·³è¿‡: {_e}")
        
        # æ•°æ®éªŒè¯
        if not tickers or len(tickers) == 0:
            logger.error("è‚¡ç¥¨ä»£ç åˆ—è¡¨ä¸ºç©º")
            return {}
        
        if not start_date or not end_date:
            logger.error("å¼€å§‹æ—¥æœŸæˆ–ç»“æŸæ—¥æœŸä¸ºç©º")
            return {}
        
        all_data = {}
        failed_downloads = []
        
        # APIé™åˆ¶ä¼˜åŒ–ï¼šæ‰¹é‡å¤„ç†+å»¶è¿Ÿ+é‡è¯•æœºåˆ¶
        import time
        import random
        
        total_tickers = len(tickers)
        batch_size = 50  # æ‰¹é‡å¤§å°å‡å°‘APIå‹åŠ›
        api_delay = 0.12  # å¢åŠ å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶
        max_retries = getattr(self, 'config', {}).get('error_handling', {}).get('max_retries', 3)  # æœ€å¤§é‡è¯•æ¬¡æ•°
        
        # æ‰¹é‡å¤„ç†è‚¡ç¥¨
        for batch_idx in range(0, total_tickers, batch_size):
            batch_tickers = tickers[batch_idx:batch_idx + batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_idx//batch_size + 1}/{(total_tickers-1)//batch_size + 1}: {len(batch_tickers)} è‚¡ç¥¨")
            
            for ticker_idx, ticker in enumerate(batch_tickers):
                # åŠ¨æ€å»¶è¿Ÿï¼šé¿å…APIé€Ÿç‡é™åˆ¶
                if ticker_idx > 0:
                    time.sleep(api_delay + random.uniform(0, 0.05))
                
                # é‡è¯•æœºåˆ¶
                for retry in range(max_retries):
                    try:
                        # éªŒè¯è‚¡ç¥¨ä»£ç æ ¼å¼
                        if not ticker or not isinstance(ticker, str) or len(ticker.strip()) == 0:
                            logger.warning(f"æ— æ•ˆçš„è‚¡ç¥¨ä»£ç : {ticker}")
                            failed_downloads.append(ticker)
                            break  # è·³å‡ºé‡è¯•å¾ªç¯
                        
                        ticker = ticker.strip().upper()  # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
                        
                        logger.info(f"[DEBUG] å¼€å§‹ä¸‹è½½ {ticker} æ•°æ®...")
                        
                        # ä½¿ç”¨æ”¹è¿›çš„çº¿ç¨‹æ± æœºåˆ¶ï¼Œæ”¯æŒæ›´å¥½çš„èµ„æºç®¡ç†
                        import threading
                        from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError, as_completed
                        import signal
                        import time
                        
                        def download_data_with_validation():
                            """å¸¦éªŒè¯çš„æ•°æ®ä¸‹è½½å‡½æ•°"""
                            try:
                                stock = PolygonTicker(ticker)
                                hist = stock.history(start=start_date, end=end_date, interval='1d')
                                
                                # åŸºç¡€æ•°æ®éªŒè¯
                                if hist is None:
                                    raise ValueError("è¿”å›çš„å†å²æ•°æ®ä¸ºNone")
                                if hasattr(hist, '__len__') and len(hist) == 0:
                                    raise ValueError("è¿”å›çš„å†å²æ•°æ®ä¸ºç©º")
                                    
                                return hist
                            except Exception as e:
                                logger.debug(f"{ticker} æ•°æ®ä¸‹è½½å†…éƒ¨é”™è¯¯: {e}")
                                raise
                        
                        hist = None
                        download_success = False
                        
                        try:
                            logger.info(f"[DEBUG] å¯åŠ¨ {ticker} æ•°æ®ä¸‹è½½ï¼ˆ30ç§’è¶…æ—¶ï¼‰...")
                            
                            # ğŸ”¥ CRITICAL FIX: ä½¿ç”¨å…±äº«çº¿ç¨‹æ± ï¼Œé˜²æ­¢èµ„æºæ³„éœ²
                            # ä¸å†ä¸ºæ¯ä¸ªtickeråˆ›å»ºç‹¬ç«‹çº¿ç¨‹æ± ï¼Œä½¿ç”¨å…±äº«æ± 
                            future = self._shared_thread_pool.submit(download_data_with_validation)
                            
                            try:
                                # ç­‰å¾…ç»“æœï¼Œ30ç§’è¶…æ—¶
                                hist = future.result(timeout=30)
                                download_success = True
                                logger.info(f"[DEBUG] {ticker} å†å²æ•°æ®è·å–å®Œæˆï¼Œæ•°æ®é•¿åº¦: {len(hist) if hist is not None else 0}")
                                
                            except FutureTimeoutError:
                                logger.warning(f"[TIMEOUT] {ticker} æ•°æ®ä¸‹è½½è¶…æ—¶ï¼ˆ30ç§’ï¼‰")
                                # å°è¯•å–æ¶ˆä»»åŠ¡
                                future.cancel()
                                raise
                                    
                        except FutureTimeoutError:
                            if retry < max_retries - 1:
                                logger.info(f"{ticker} è¶…æ—¶ï¼Œé‡è¯• {retry + 1}/{max_retries}")
                                time.sleep(1)  # çŸ­æš‚ç­‰å¾…åé‡è¯•
                                continue
                            else:
                                failed_downloads.append(ticker)
                                break
                        except (ConnectionError, TimeoutError, OSError) as conn_e:
                            logger.warning(f"[NETWORK] {ticker} ç½‘ç»œè¿æ¥é—®é¢˜: {conn_e}")
                            if retry < max_retries - 1:
                                time.sleep(2)  # ç½‘ç»œé—®é¢˜ç­‰å¾…æ›´é•¿æ—¶é—´
                                continue
                            else:
                                failed_downloads.append(ticker)
                                break
                        except Exception as thread_e:
                            logger.warning(f"[ERROR] {ticker} ä¸‹è½½å¼‚å¸¸: {thread_e}")
                            if retry < max_retries - 1:
                                continue
                            else:
                                failed_downloads.append(ticker)
                                break
                        
                        # æ•°æ®è´¨é‡æ£€æŸ¥
                        if hist is None or len(hist) == 0:
                            if retry < max_retries - 1:
                                logger.warning(f"{ticker}: æ— æ•°æ®ï¼Œé‡è¯• {retry + 1}/{max_retries}")
                                time.sleep(1.0 * (retry + 1))  # é€’å¢å»¶è¿Ÿ
                                continue
                            else:
                                logger.warning(f"{ticker}: æ— æ•°æ®ï¼Œæœ€ç»ˆå¤±è´¥")
                                failed_downloads.append(ticker)
                                break
                        
                        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨ - æ”¯æŒå¤§å°å†™å…¼å®¹
                        required_cols_upper = ['Open', 'High', 'Low', 'Close', 'Volume']
                        required_cols_lower = ['open', 'high', 'low', 'close', 'volume']
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„åˆ—ï¼ˆå¤§å†™æˆ–å°å†™ï¼‰
                        has_required = True
                        missing_info = []
                        
                        for i, (upper_col, lower_col) in enumerate(zip(required_cols_upper, required_cols_lower)):
                            if upper_col not in hist.columns and lower_col not in hist.columns:
                                has_required = False
                                missing_info.append(f"{upper_col}/{lower_col}")
                        
                        if not has_required:
                            if retry < max_retries - 1:
                                logger.warning(f"{ticker}: ç¼ºå°‘å¿…è¦åˆ— {missing_info}ï¼Œé‡è¯• {retry + 1}/{max_retries}")
                                time.sleep(0.5 * (retry + 1))
                                continue
                            else:
                                logger.warning(f"{ticker}: ç¼ºå°‘å¿…è¦åˆ— {missing_info}, ç°æœ‰åˆ—: {list(hist.columns)}")
                                failed_downloads.append(ticker)
                                break
                        
                        # æ ‡å‡†åŒ–åˆ—å - æ™ºèƒ½å¤„ç†å¤§å°å†™
                        rename_mapping = {}
                        for upper_col, lower_col in zip(required_cols_upper, required_cols_lower):
                            if upper_col in hist.columns:
                                rename_mapping[upper_col] = lower_col
                            # å¦‚æœå·²ç»æ˜¯å°å†™ï¼Œä¸éœ€è¦é‡å‘½å
                        
                        if rename_mapping:
                            hist = hist.rename(columns=rename_mapping)
                        
                        # æ£€æŸ¥æ•°æ®è´¨é‡ - ä½¿ç”¨æ ‡å‡†åŒ–åçš„åˆ—åï¼Œå¢å¼ºæ•°æ®éªŒè¯
                        if 'close' not in hist.columns or hist['close'].isna().all():
                            if retry < max_retries - 1:
                                logger.warning(f"{ticker}: closeåˆ—é—®é¢˜ï¼Œé‡è¯• {retry + 1}/{max_retries}")
                                time.sleep(0.5 * (retry + 1))
                                continue
                            else:
                                logger.warning(f"{ticker}: closeåˆ—ç¼ºå¤±æˆ–æ‰€æœ‰æ”¶ç›˜ä»·éƒ½æ˜¯NaN")
                                failed_downloads.append(ticker)
                                break
                        
                        # å¢å¼ºæ•°æ®è´¨é‡æ£€æŸ¥
                        # 1. æ£€æŸ¥æ•°æ®å……åˆ†æ€§ - ğŸ”§ è°ƒæ•´ä¸ºæ›´åˆç†çš„è¦æ±‚
                        MIN_REQUIRED_DAYS = 90  # è°ƒæ•´ä¸º3ä¸ªæœˆæ•°æ® (åŸä¸º252å¤©/1å¹´ï¼Œè¿‡äºä¸¥æ ¼)
                        if len(hist) < MIN_REQUIRED_DAYS:
                            logger.warning(f"{ticker}: æ•°æ®ä¸è¶³ï¼Œåªæœ‰{len(hist)}å¤©ï¼Œéœ€è¦è‡³å°‘{MIN_REQUIRED_DAYS}å¤©")
                            failed_downloads.append(ticker)
                            break
                        
                        # 2. æ£€æŸ¥æ•°æ®å¼‚å¸¸å€¼å’Œè´¨é‡
                        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
                        for col in numeric_cols:
                            if col in hist.columns:
                                # æ£€æŸ¥è´Ÿå€¼
                                if (hist[col] < 0).any():
                                    logger.warning(f"{ticker}: å‘ç°è´Ÿå€¼åœ¨{col}åˆ—")
                                    hist[col] = hist[col].clip(lower=0)  # ä¿®å¤è´Ÿå€¼
                                
                                # æ£€æŸ¥å¼‚å¸¸çš„å¤§å¹…æ³¢åŠ¨ (>20å€å˜åŠ¨)
                                if col in ['open', 'high', 'low', 'close'] and len(hist) > 1:
                                    price_ratio = hist[col] / hist[col].shift(1)
                                    extreme_moves = (price_ratio > 20) | (price_ratio < 0.05)
                                    if extreme_moves.sum() > 0:
                                        logger.warning(f"{ticker}: å‘ç°{extreme_moves.sum()}ä¸ªå¼‚å¸¸ä»·æ ¼æ³¢åŠ¨åœ¨{col}åˆ—")
                                        # ç”¨å‰å€¼å¡«å……å¼‚å¸¸å€¼
                                        hist.loc[extreme_moves, col] = hist[col].shift(1)
                        
                        # 3. æ£€æŸ¥ä»·æ ¼é€»è¾‘å…³ç³» (High >= Close >= Low)
                        invalid_price_logic = (hist['high'] < hist['close']) | (hist['close'] < hist['low']) | (hist['high'] < hist['low'])
                        if invalid_price_logic.any():
                            logger.warning(f"{ticker}: å‘ç°{invalid_price_logic.sum()}ä¸ªä»·æ ¼é€»è¾‘é”™è¯¯")
                            # ä¿®å¤ä»·æ ¼é€»è¾‘é”™è¯¯
                            hist.loc[invalid_price_logic, 'high'] = hist[['open', 'high', 'low', 'close']].max(axis=1)
                            hist.loc[invalid_price_logic, 'low'] = hist[['open', 'high', 'low', 'close']].min(axis=1)
                        
                        # æ·»åŠ åŸºç¡€ç‰¹å¾
                        hist['ticker'] = ticker
                        hist['date'] = hist.index
                        hist['amount'] = hist['close'] * hist['volume']  # æˆäº¤é¢
                        
                        # [ENHANCED] P0è‚¡ç¥¨æ± æ‰“æ ‡ï¼šè®¡ç®—ADV_USD_20å’ŒMEDIAN_SPREAD_BPS_20
                        hist['ADV_USD_20'] = hist['amount'].rolling(window=20, min_periods=1).mean()
                        
                        # è®¡ç®—ç‚¹å·®ï¼ˆç®€åŒ–ä¼°è®¡ï¼šé«˜ä½ä»·å·®ä½œä¸ºä»£ç†ï¼‰
                        hist['spread_estimate'] = (hist['high'] - hist['low']) / hist['close'] * 10000  # è½¬ä¸ºbp
                        hist['MEDIAN_SPREAD_BPS_20'] = hist['spread_estimate'].rolling(window=20, min_periods=1).median()
                        
                        # æ·»åŠ å…¶ä»–æµåŠ¨æ€§å’Œè´¨é‡æŒ‡æ ‡  
                        hist['FREE_FLOAT'] = self._get_free_float_for_ticker(ticker)
                        hist['SHORTABLE'] = self._get_shortable_status(ticker)
                        hist['BORROW_FEE'] = self._get_borrow_fee(ticker)
                        
                        # æ·»åŠ çœŸå®å…ƒæ•°æ®ï¼ˆæ›¿æ¢éšæœºæ¨¡æ‹Ÿï¼‰
                        hist['COUNTRY'] = self._get_country_for_ticker(ticker)
                        hist['SECTOR'] = self._get_sector_for_ticker(ticker)
                        hist['SUBINDUSTRY'] = self._get_subindustry_for_ticker(ticker)
                        
                        all_data[ticker] = hist
                        logger.debug(f"{ticker}: æ•°æ®å¤„ç†æˆåŠŸ")
                        break  # æˆåŠŸåè·³å‡ºé‡è¯•å¾ªç¯
                        
                    except Exception as e:
                        if retry < max_retries - 1:
                            logger.warning(f"ä¸‹è½½{ticker}å¤±è´¥ (é‡è¯• {retry + 1}/{max_retries}): {e}")
                            time.sleep(2.0 * (retry + 1))  # é€’å¢å»¶è¿Ÿ
                        else:
                            logger.warning(f"ä¸‹è½½{ticker}æœ€ç»ˆå¤±è´¥: {e}")
                            failed_downloads.append(ticker)
        
        # æ•°æ®è¦†ç›–ç‡æ£€æŸ¥
        total_requested = len(tickers)
        successful_downloads = len(all_data)
        failed_count = len(failed_downloads)
        coverage_rate = successful_downloads / total_requested if total_requested > 0 else 0
        
        if failed_downloads:
            logger.warning(f"ä¸‹è½½å¤±è´¥çš„è‚¡ç¥¨ ({failed_count}/{total_requested}): {failed_downloads[:10]}{'...' if failed_count > 10 else ''}")
        
        logger.info(f"æ•°æ®ä¸‹è½½å®Œæˆ: {successful_downloads}/{total_requested} (è¦†ç›–ç‡: {coverage_rate:.1%})")
        
        # æ•°æ®è´¨é‡éªŒè¯ï¼šå¦‚æœè¦†ç›–ç‡è¿‡ä½ï¼Œå‘å‡ºè­¦å‘Š
        if coverage_rate < 0.5:
            logger.error(f"æ•°æ®è¦†ç›–ç‡è¿‡ä½ ({coverage_rate:.1%})ï¼Œå»ºè®®æ£€æŸ¥APIé…ç½®æˆ–ç½‘ç»œè¿æ¥")
        elif coverage_rate < 0.7:
            logger.warning(f"æ•°æ®è¦†ç›–ç‡è¾ƒä½ ({coverage_rate:.1%})ï¼Œå¯èƒ½å½±å“æ¨¡å‹è´¨é‡")
        
        self.raw_data = all_data
        return all_data
    
    def _get_country_for_ticker(self, ticker: str) -> str:
        """è·å–è‚¡ç¥¨çš„å›½å®¶ï¼ˆçœŸå®æ•°æ®æºï¼‰"""
        try:
            if MARKET_MANAGER_AVAILABLE:
                # ä½¿ç”¨ç»Ÿä¸€å¸‚åœºæ•°æ®ç®¡ç†å™¨
                if hasattr(self, 'market_data_manager') and self.market_data_manager:
                    stock_info = self.market_data_manager.get_stock_info(ticker)
                    if stock_info and hasattr(stock_info, 'country') and stock_info.country:
                        return stock_info.country
            
            # é€šè¿‡Polygonå®¢æˆ·ç«¯è·å–å…¬å¸è¯¦æƒ…
            try:
                url = f"https://api.polygon.io/v3/reference/tickers/{ticker}"
                response = pc.get(url)
                if response.status_code == 200:
                    data = response.json()
                    if 'results' in data:
                        locale = data['results'].get('locale', 'us')
                        return locale.upper()
            except Exception as e:
                logger.debug(f"è·å–{ticker}å¸‚åœºä¿¡æ¯APIè°ƒç”¨å¤±è´¥: {e}")
                # ç»§ç»­ä½¿ç”¨é»˜è®¤å€¼ï¼Œä½†è®°å½•é”™è¯¯ç”¨äºè°ƒè¯•
            
            # é»˜è®¤ä¸ºç¾å›½å¸‚åœºï¼ˆå¤§éƒ¨åˆ†è‚¡ç¥¨ï¼‰
            return 'US'
        except Exception as e:
            logger.warning(f"è·å–{ticker}å›½å®¶ä¿¡æ¯å¤±è´¥: {e}")
            return 'US'
    
    def _get_sector_for_ticker(self, ticker: str) -> str:
        """è·å–è‚¡ç¥¨çš„è¡Œä¸šï¼ˆçœŸå®æ•°æ®æºï¼‰"""
        try:
            if MARKET_MANAGER_AVAILABLE:
                # ä½¿ç”¨ç»Ÿä¸€å¸‚åœºæ•°æ®ç®¡ç†å™¨
                if hasattr(self, 'market_data_manager') and self.market_data_manager:
                    stock_info = self.market_data_manager.get_stock_info(ticker)
                    if stock_info and hasattr(stock_info, 'sector') and stock_info.sector:
                        return stock_info.sector
            
            # é€šè¿‡Polygonå®¢æˆ·ç«¯è·å–å…¬å¸è¯¦æƒ…
            try:
                url = f"https://api.polygon.io/v3/reference/tickers/{ticker}"
                response = pc.get(url)
                if response.status_code == 200:
                    data = response.json()
                    if 'results' in data:
                        sic_description = data['results'].get('sic_description', '')
                        if sic_description:
                            # å°†SICæè¿°æ˜ å°„ä¸ºä¸»è¦è¡Œä¸š
                            sector = self._map_sic_to_sector(sic_description)
                            return sector
            except Exception:
                pass
            
           
            return sector_mapping.get(ticker, 'Technology')  # é»˜è®¤ç§‘æŠ€
        except Exception as e:
            logger.warning(f"è·å–{ticker}è¡Œä¸šä¿¡æ¯å¤±è´¥: {e}")
            return 'Technology'
    
    def _get_subindustry_for_ticker(self, ticker: str) -> str:
        """è·å–è‚¡ç¥¨çš„å­è¡Œä¸šï¼ˆçœŸå®æ•°æ®æºï¼‰"""
        try:
            if MARKET_MANAGER_AVAILABLE:
                # ä½¿ç”¨ç»Ÿä¸€å¸‚åœºæ•°æ®ç®¡ç†å™¨
                if hasattr(self, 'market_data_manager') and self.market_data_manager:
                    stock_info = self.market_data_manager.get_stock_info(ticker)
                    if stock_info and hasattr(stock_info, 'subindustry') and stock_info.subindustry:
                        return stock_info.subindustry
            
            # é€šè¿‡Polygonå®¢æˆ·ç«¯è·å–è¯¦ç»†è¡Œä¸šåˆ†ç±»
            try:
                url = f"https://api.polygon.io/v3/reference/tickers/{ticker}"
                response = pc.get(url)
                if response.status_code == 200:
                    data = response.json()
                    if 'results' in data:
                        sic_description = data['results'].get('sic_description', '')
                        if sic_description:
                            return sic_description[:50]  # å–å‰50å­—ç¬¦ä½œä¸ºå­è¡Œä¸š
            except Exception:
                pass
            
            # é»˜è®¤æ˜ å°„
           
            return subindustry_mapping.get(ticker, 'Software')
        except Exception as e:
            logger.warning(f"è·å–{ticker}å­è¡Œä¸šä¿¡æ¯å¤±è´¥: {e}")
            return 'Software'
    
    def _map_sic_to_sector(self, sic_description: str) -> str:
        """å°†SICæè¿°æ˜ å°„ä¸ºGICSè¡Œä¸š"""
        sic_lower = sic_description.lower()
        
        if any(word in sic_lower for word in ['computer', 'software', 'internet', 'technology']):
            return 'Technology'
        elif any(word in sic_lower for word in ['bank', 'finance', 'insurance', 'investment']):
            return 'Financials'
        elif any(word in sic_lower for word in ['drug', 'pharmaceutical', 'health', 'medical']):
            return 'Health Care'
        elif any(word in sic_lower for word in ['oil', 'gas', 'energy', 'petroleum']):
            return 'Energy'
        elif any(word in sic_lower for word in ['retail', 'consumer', 'restaurant']):
            return 'Consumer Discretionary'
        elif any(word in sic_lower for word in ['utility', 'electric', 'water']):
            return 'Utilities'
        elif any(word in sic_lower for word in ['real estate', 'reit']):
            return 'Real Estate'
        elif any(word in sic_lower for word in ['manufacturing', 'industrial', 'machinery']):
            return 'Industrials'
        elif any(word in sic_lower for word in ['material', 'chemical', 'mining']):
            return 'Materials'
        else:
            return 'Technology'  # é»˜è®¤
    
    def _get_free_float_for_ticker(self, ticker: str) -> float:
        """è·å–è‚¡ç¥¨çš„è‡ªç”±æµé€šå¸‚å€¼æ¯”ä¾‹"""
        try:
            if MARKET_MANAGER_AVAILABLE:
                if hasattr(self, 'market_data_manager') and self.market_data_manager:
                    stock_info = self.market_data_manager.get_stock_info(ticker)
                    if stock_info and hasattr(stock_info, 'free_float_shares'):
                        # è®¡ç®—è‡ªç”±æµé€šæ¯”ä¾‹
                        total_shares = getattr(stock_info, 'shares_outstanding', None)
                        if total_shares and stock_info.free_float_shares:
                            return stock_info.free_float_shares / total_shares
            
            # é€šè¿‡Polygonè·å–è‚¡ä»½ä¿¡æ¯
            try:
                url = f"https://api.polygon.io/v3/reference/tickers/{ticker}"
                response = pc.get(url)
                if response.status_code == 200:
                    data = response.json()
                    if 'results' in data:
                        shares_outstanding = data['results'].get('share_class_shares_outstanding')
                        weighted_shares = data['results'].get('weighted_shares_outstanding')
                        if shares_outstanding and weighted_shares:
                            return min(weighted_shares / shares_outstanding, 1.0)
            except Exception:
                pass
            
            # é»˜è®¤ä¼°ç®—60%ä¸ºè‡ªç”±æµé€š
            return 0.6
        except Exception as e:
            logger.warning(f"è·å–{ticker}è‡ªç”±æµé€šä¿¡æ¯å¤±è´¥: {e}")
            return 0.6
    
   
    
    def _get_borrow_fee(self, ticker: str) -> float:
        """è·å–è‚¡ç¥¨å€Ÿåˆ¸è´¹ç‡ï¼ˆå¹´åŒ–%ï¼‰"""
        try:
            # æ ¹æ®è‚¡ç¥¨æµåŠ¨æ€§å’Œçƒ­åº¦ä¼°ç®—å€Ÿåˆ¸è´¹ç‡
            # å®é™…åº”ç”¨ä¸­åº”æ¥å…¥åˆ¸å•†æˆ–ç¬¬ä¸‰æ–¹æ•°æ®æº
            # Use fixed estimates instead of random
            high_fee_stocks = ['TSLA', 'AMC', 'GME']  # é«˜è´¹ç‡è‚¡ç¥¨
            if ticker in high_fee_stocks:
                return 10.0  # Fixed 10% annualized for high-fee stocks
            else:
                return 1.0   # Fixed 1% annualized for normal stocks
        except Exception:
            return 1.0  # é»˜è®¤1%
    
    def create_traditional_features(self, data_dict: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """
        åˆ›å»ºä¼ ç»ŸæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        
        Args:
            data_dict: è‚¡ç¥¨æ•°æ®å­—å…¸
            
        Returns:
            ç‰¹å¾æ•°æ®æ¡†
        """
        logger.info("åˆ›å»ºä¼ ç»ŸæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾")
        
        all_features = []
        
        for ticker, df in data_dict.items():
            if len(df) < 20:  # ğŸ”§ è¿›ä¸€æ­¥é™ä½æœ€å°æ•°æ®è¦æ±‚ï¼Œä»30å¤©æ”¹ä¸º20å¤©ï¼Œæé«˜é€šè¿‡ç‡
                logger.warning(f"è·³è¿‡ {ticker}: æ•°æ®ä¸è¶³20å¤© ({len(df)}å¤©)")
                continue
            
            df_copy = df.copy().sort_values('date')
            
            # ä»·æ ¼ç‰¹å¾
            df_copy['returns'] = df_copy['close'].pct_change()
            df_copy['log_returns'] = np.log(df_copy['close'] / df_copy['close'].shift(1))
            
            # ç§»åŠ¨å¹³å‡
            for window in [5, 10, 20, 50]:
                df_copy[f'ma_{window}'] = df_copy['close'].rolling(window).mean()
                df_copy[f'ma_ratio_{window}'] = df_copy['close'] / df_copy[f'ma_{window}']
            
            # æ³¢åŠ¨ç‡
            for window in [10, 20, 50]:
                df_copy[f'vol_{window}'] = df_copy['log_returns'].rolling(window).std()
            
            # RSI
            def calculate_rsi(prices, window=14):
                delta = prices.diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
                rs = gain / loss
                return 100 - (100 / (1 + rs))
            
            df_copy['rsi_14'] = calculate_rsi(df_copy['close'])
            
            # æˆäº¤é‡ç‰¹å¾
            if 'volume' in df_copy.columns:
                df_copy['volume_ma_20'] = df_copy['volume'].rolling(20).mean()
                df_copy['volume_ratio'] = df_copy['volume'] / df_copy['volume_ma_20']
            
            # ä»·æ ¼ä½ç½®
            for window in [20, 50]:
                high_roll = df_copy['high'].rolling(window).max()
                low_roll = df_copy['low'].rolling(window).min()
                df_copy[f'price_position_{window}'] = (df_copy['close'] - low_roll) / (high_roll - low_roll + 1e-8)
            
            # åŠ¨é‡æŒ‡æ ‡
            for period in [5, 10, 20]:
                df_copy[f'momentum_{period}'] = df_copy['close'] / df_copy['close'].shift(period) - 1
            
            # [ENHANCED] P2 åŠ¨é‡åŠ é€Ÿåº¦ï¼ˆAccelerationï¼‰
            df_copy['momentum_10_day'] = df_copy['close'] / df_copy['close'].shift(10) - 1
            df_copy['momentum_20_day'] = df_copy['close'] / df_copy['close'].shift(20) - 1
            df_copy['acceleration_10'] = df_copy['momentum_10_day'] - df_copy['momentum_20_day']
            
            # [ENHANCED] P2 æ³¢åŠ¨èšç±»/é£é™©ä»£ç†ï¼ˆVolatility Clusteringï¼‰
            df_copy['realized_vol_20'] = (df_copy['returns'] ** 2).rolling(20).sum()
            df_copy['realized_vol_60'] = (df_copy['returns'] ** 2).rolling(60).sum()
            df_copy['vol_change'] = df_copy['realized_vol_20'] / df_copy['realized_vol_60'] - 1
            df_copy['vol_regime'] = (df_copy['realized_vol_20'] > df_copy['realized_vol_20'].rolling(252).median()).astype(int)
            
            # [ENHANCED] P2 èµ„é‡‘æµï¼ˆMoney Flowï¼‰
            if 'volume' in df_copy.columns:
                df_copy['money_flow'] = df_copy['close'] * df_copy['volume']
                df_copy['money_flow_ma_20'] = df_copy['money_flow'].rolling(20).mean()
                df_copy['money_flow_deviation'] = (df_copy['money_flow'] - df_copy['money_flow_ma_20']) / df_copy['money_flow_ma_20']
                df_copy['money_flow_rank'] = df_copy['money_flow'].rolling(60).rank(pct=True)
            
            # [ENHANCED] P2 å…¬å‘Š/è´¢æŠ¥çª—å£dummyï¼ˆEarnings Windowï¼‰
            df_copy['earnings_window_3'] = self._create_earnings_window_dummy(df_copy.index, ticker, days=3)
            df_copy['earnings_window_5'] = self._create_earnings_window_dummy(df_copy.index, ticker, days=5)
            df_copy['earnings_window_10'] = self._create_earnings_window_dummy(df_copy.index, ticker, days=10)
            
            # [ENHANCED] P2 å€Ÿåˆ¸è´¹ç‡ç‰¹å¾ï¼ˆå¦‚æœæ•°æ®å¯ç”¨ï¼‰
            if 'BORROW_FEE' in df_copy.columns:
                df_copy['borrow_fee_normalized'] = df_copy['BORROW_FEE'] / 100  # è½¬ä¸ºæ¯”ä¾‹
                df_copy['high_borrow_fee'] = (df_copy['BORROW_FEE'] > 5.0).astype(int)  # é«˜è´¹ç‡æ ‡è®°
            else:
                # æ— æ•°æ®æ—¶æŠ›å‡ºå¼‚å¸¸ï¼Œä¸ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
                logger.warning(f"Missing BORROW_FEE data for ticker {ticker}ï¼Œä½¿ç”¨é»˜è®¤å€¼0.005")
                borrow_fee = 0.005  # é»˜è®¤å€Ÿè´·è´¹ç”¨0.5%
            
            # ğŸ”´ ä¿®å¤ä¸¥é‡æ—¶é—´æ³„éœ²ï¼šä½¿ç”¨ç»Ÿä¸€çš„T10é…ç½®
            config = get_config()
            FEATURE_LAG = config.FEATURE_LAG        # ç‰¹å¾ä½¿ç”¨T-5åŠä¹‹å‰æ•°æ®
            SAFETY_GAP = config.SAFETY_GAP          # é¢å¤–å®‰å…¨é—´éš”ï¼ˆé˜²æ­¢ä¿¡æ¯æ³„éœ²ï¼‰
            PRED_START = config.PREDICTION_HORIZON  # é¢„æµ‹ä»T+10å¼€å§‹  
            PRED_END = config.PREDICTION_HORIZON    # é¢„æµ‹åˆ°T+10ç»“æŸ
            prediction_horizon = PRED_END            # å‘åå…¼å®¹
            
            # éªŒè¯æ—¶é—´å¯¹é½æ­£ç¡®æ€§
            total_gap = FEATURE_LAG + SAFETY_GAP + PRED_START
            if total_gap <= 0:
                raise ValueError(f"æ—¶é—´å¯¹é½é”™è¯¯ï¼šæ€»é—´éš” {total_gap} <= 0ï¼Œå­˜åœ¨æ•°æ®æ³„éœ²é£é™©")
            
            logger.info(f"æ—¶é—´å¯¹é½é…ç½®: ç‰¹å¾lag={FEATURE_LAG}, å®‰å…¨gap={SAFETY_GAP}, é¢„æµ‹[T+{PRED_START}, T+{PRED_END}]")
            
            # å®‰å…¨çš„ç›®æ ‡æ„å»ºï¼šTæ—¶åˆ»ä½¿ç”¨T-5ç‰¹å¾ï¼Œé¢„æµ‹Tåˆ°T+10çš„10å¤©ç´¯è®¡æ”¶ç›Š
            # ç¡®ä¿ç‰¹å¾å’Œç›®æ ‡ä¹‹é—´æœ‰è¶³å¤Ÿçš„æ—¶é—´é—´éš”ï¼ˆè‡³å°‘10æœŸï¼‰
            # æ­£ç¡®çš„10å¤©å‰å‘æ”¶ç›Šï¼š(P[T+10] - P[T]) / P[T]
            df_copy['target'] = (
                df_copy['close'].shift(-PRED_END) / 
                df_copy['close'] - 1
            )
            # ç­‰ä»·äº: df_copy['target'] = df_copy['close'].pct_change(PRED_END).shift(-PRED_END)
            
            # æ—¶é—´éªŒè¯ï¼šT+10é¢„æµ‹çš„æ­£ç¡®æ—¶é—´å¯¹é½
            # ç‰¹å¾ä½¿ç”¨T-5æ•°æ®ï¼Œé¢„æµ‹T+10æ”¶ç›Šï¼Œæ€»é—´éš”åº”ä¸º15å¤©
            feature_time = -FEATURE_LAG - SAFETY_GAP  # T-5
            prediction_time = PRED_START               # T+10
            total_time_gap = prediction_time - feature_time  # 10 - (-5) = 15å¤©
            
            if total_time_gap < 12:  # è‡³å°‘12å¤©é—´éš”ç¡®ä¿å®‰å…¨ï¼ˆ2å‘¨é¢„æµ‹ï¼‰
                logger.warning(f"æ—¶é—´é—´éš”åå°ï¼šç‰¹å¾T{feature_time} -> é¢„æµ‹T+{prediction_time}ï¼Œé—´éš”{total_time_gap}å¤©")
            else:
                logger.info(f"[OK] T+10æ—¶é—´å¯¹é½éªŒè¯é€šè¿‡ï¼šç‰¹å¾T{feature_time} -> é¢„æµ‹T+{prediction_time}ï¼Œé—´éš”{total_time_gap}å¤©")
            
            # ğŸ”¥ å…³é”®ï¼šå¼ºåˆ¶ç‰¹å¾æ»åä»¥åŒ¹é…å¢å¼ºçš„æ—¶é—´çº¿
            # ç‰¹å¾ä½¿ç”¨T-5æ•°æ®ï¼Œç›®æ ‡ä½¿ç”¨T+10åˆ°T+10ï¼Œé—´éš”15æœŸï¼ˆå®‰å…¨ï¼‰
            feature_lag = FEATURE_LAG + SAFETY_GAP  # æ‰€æœ‰ç‰¹å¾é¢å¤–æ»å4æœŸ
            
            # åœ¨åç»­feature_colså¤„ç†ä¸­ä¼šç»Ÿä¸€åº”ç”¨æ»å
            
            # æ·»åŠ è¾…åŠ©ä¿¡æ¯
            df_copy['ticker'] = ticker
            df_copy['date'] = df_copy.index
            # å¿…é¡»ä»çœŸå®æ•°æ®æºè·å–è¡Œä¸šå’Œå›½å®¶ä¿¡æ¯
            if 'COUNTRY' not in df_copy.columns or 'SECTOR' not in df_copy.columns:
                raise ValueError(f"Missing COUNTRY/SECTOR data for ticker {ticker}. Real data required.")
            df_copy['SUBINDUSTRY'] = ticker[:3] if len(ticker) >= 3 else 'SOFTWARE'
            
            all_features.append(df_copy)
        
        if all_features:
            # ğŸ”§ ä¿®å¤å¤šè‚¡ç¥¨è¯†åˆ«é—®é¢˜ï¼šä¿ç•™panelç»“æ„è€Œéç®€å•å †å 
            # ä½¿ç”¨æ—¥æœŸ+è‚¡ç¥¨çš„MultiIndexæ¥ä¿æŒæ¨ªæˆªé¢ç»“æ„
            combined_features = pd.concat(all_features, ignore_index=False)
            
            # ç¡®ä¿æœ‰tickerå’Œdateåˆ—ï¼Œå¹¶è®¾ç½®æ­£ç¡®çš„ç´¢å¼•ç»“æ„
            if 'date' in combined_features.columns and 'ticker' in combined_features.columns:
                # è®¾ç½®MultiIndex: (date, ticker)
                combined_features = combined_features.set_index(['date', 'ticker'])
                logger.info(f"âœ… è®¾ç½®MultiIndex panelç»“æ„: {len(combined_features.index.get_level_values('ticker').unique())} åªè‚¡ç¥¨, {len(combined_features.index.get_level_values('date').unique())} ä¸ªæ—¥æœŸ")
            else:
                logger.warning("ç¼ºå°‘dateæˆ–tickeråˆ—ï¼Œä½¿ç”¨ç®€å•è¿æ¥")
                combined_features = pd.concat(all_features, ignore_index=True)
            # ğŸ”§ ä¿®å¤ç‰¹å¾çŸ©é˜µæ±¡æŸ“ï¼šä¸¥æ ¼ç­›é€‰æ•°å€¼ç‰¹å¾åˆ—
            def get_clean_numeric_features(df):
                """è·å–å¹²å‡€çš„æ•°å€¼ç‰¹å¾åˆ—ï¼Œæ’é™¤æ‰€æœ‰éæ•°å€¼å’Œæ ‡è¯†åˆ—"""
                # æ˜ç¡®æ’é™¤çš„åˆ—
                exclude_cols = {'ticker', 'date', 'target', 'COUNTRY', 'SECTOR', 'SUBINDUSTRY', 
                               'symbol', 'stock_code', 'name', 'industry', 'sector'}
                
                # åªé€‰æ‹©æ•°å€¼ç±»å‹çš„åˆ—
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                
                # è¿›ä¸€æ­¥è¿‡æ»¤ï¼šç¡®ä¿ä¸åŒ…å«ä»»ä½•å­—ç¬¦ä¸²æˆ–æ ‡è¯†ç¬¦
                clean_cols = []
                for col in numeric_cols:
                    if col not in exclude_cols and not col.lower().endswith('_name'):
                        # éªŒè¯åˆ—æ•°æ®æ˜¯å¦çœŸæ­£ä¸ºæ•°å€¼
                        if df[col].dtype in ['int64', 'float64', 'int32', 'float32']:
                            clean_cols.append(col)
                
                logger.info(f"ç‰¹å¾ç­›é€‰ï¼šæ€»åˆ—æ•°{len(df.columns)} -> æ•°å€¼åˆ—{len(numeric_cols)} -> æ¸…æ´ç‰¹å¾{len(clean_cols)}")
                return clean_cols
            
            feature_cols = get_clean_numeric_features(combined_features)
            # ğŸ”¥ å¼ºåŒ–ç‰¹å¾æ»åï¼šç¡®ä¿ä¸¥æ ¼çš„æ—¶é—´å¯¹é½
            try:
                # T-2åŸºç¡€æ»å + formation_lag(2) = æ€»å…±T-4æ»å
                # è¿™ç¡®ä¿ç‰¹å¾ä¿¡æ¯ä¸¥æ ¼æ—©äºç›®æ ‡æ—¶é—´çª—å£
                total_lag = 2 + 2  # base_lag + formation_lag
                if combined_features.index.names == ['date', 'ticker']:
                    # MultiIndexç»“æ„ï¼šæŒ‰tickeråˆ†ç»„è¿›è¡Œæ»å
                    combined_features[feature_cols] = combined_features.groupby(level='ticker')[feature_cols].shift(total_lag)
                    logger.info(f"âœ… MultiIndexç»“æ„æ»ååº”ç”¨å®Œæˆï¼Œæ€»æ»åæœŸæ•°: {total_lag}")
                else:
                    # æ™®é€šç»“æ„ï¼šæŒ‰tickeråˆ—åˆ†ç»„
                    combined_features[feature_cols] = combined_features.groupby('ticker')[feature_cols].shift(total_lag)
                    logger.info(f"âœ… æ™®é€šç»“æ„æ»ååº”ç”¨å®Œæˆï¼Œæ€»æ»åæœŸæ•°: {total_lag}")
            except Exception as e:
                logger.warning(f"ç‰¹å¾æ»åå¤„ç†å¤±è´¥: {e}")
                try:
                    # å›é€€åˆ°åŸºç¡€æ»å
                    if combined_features.index.names == ['date', 'ticker']:
                        combined_features[feature_cols] = combined_features.groupby(level='ticker')[feature_cols].shift(2)
                    else:
                        combined_features[feature_cols] = combined_features.groupby('ticker')[feature_cols].shift(2)
                    logger.info("å›é€€åˆ°åŸºç¡€æ»åå¤„ç†å®Œæˆ")
                except Exception as e2:
                    logger.error(f"æ»åå¤„ç†å®Œå…¨å¤±è´¥: {e2}")
                    # ç»§ç»­è€Œä¸åº”ç”¨æ»å
            # åŸºç¡€æ¸…æ´— - åªåˆ é™¤ç‰¹å¾å…¨ä¸ºNaNçš„è¡Œï¼Œä¿ç•™ç›®æ ‡å˜é‡
            # åˆ é™¤ç‰¹å¾å…¨ä¸ºNaNçš„è¡Œï¼Œä½†ä¿ç•™æœ‰æ•ˆç›®æ ‡çš„è¡Œ
            feature_na_mask = combined_features[feature_cols].isna().all(axis=1)
            combined_features = combined_features[~feature_na_mask]

            # ğŸ”— åˆå¹¶å®Œæ•´çš„Polygon 40+ä¸“ä¸šå› å­é›†ï¼ˆç»Ÿä¸€æ¥æº - T+1ä¼˜åŒ–ï¼‰
            try:
                # ä¿®å¤å¯¼å…¥é”™è¯¯ï¼šä½¿ç”¨æ­£ç¡®çš„æ¨¡å—è·¯å¾„
                try:
                    from autotrader.unified_polygon_factors import UnifiedPolygonFactors as PolygonCompleteFactors
                except ImportError:
                    from unified_polygon_factors import UnifiedPolygonFactors as PolygonCompleteFactors
                
                # çŸ­æœŸå› å­æš‚æ—¶ä½¿ç”¨åŸºç¡€å®ç°
                class PolygonShortTermFactors:
                    def calculate_all_short_term_factors(self, symbol):
                        # åŸºç¡€å®ç°ï¼Œè¿”å›ç©ºå­—å…¸
                        return {}
                    
                    def create_t_plus_5_prediction(self, symbol, results):
                        # åŸºç¡€å®ç°ï¼Œè¿”å›é»˜è®¤é¢„æµ‹
                        return {'signal_strength': 0.0, 'confidence': 0.5}
                
                short_term_factors = PolygonShortTermFactors()
                
                complete_factors = PolygonCompleteFactors()
                short_term_factors = PolygonShortTermFactors()
                symbols = sorted(combined_features['ticker'].unique().tolist())
                
                logger.info(f"å¼€å§‹é›†æˆPolygonç»Ÿä¸€å› å­åº“ï¼Œè‚¡ç¥¨æ•°é‡: {len(symbols)}")
                
                # è·å–å› å­åº“æ‘˜è¦ï¼ˆä½¿ç”¨å¯ç”¨æ–¹æ³•ï¼‰
                try:
                    # å°è¯•è·å–å› å­ç»Ÿè®¡ä¿¡æ¯
                    factor_stats = getattr(complete_factors, 'stats', {})
                    total_factors = factor_stats.get('total_calculations', 0)
                    logger.info(f"ç»Ÿä¸€å› å­åº“åŒ…å« {total_factors} ä¸ªå› å­è®¡ç®—")
                except Exception as e:
                    logger.info("ç»Ÿä¸€å› å­åº“å·²åˆå§‹åŒ–")
                
                # ç»Ÿä¸€å› å­é›†åˆ
                all_polygon_factors = {}
                factor_calculation_success = {}
                
                # å¯¹å‰å‡ åªä»£è¡¨æ€§è‚¡ç¥¨è®¡ç®—å› å­
                sample_symbols = symbols[:min(3, len(symbols))]  # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥é¿å…APIé™åˆ¶
                
                for symbol in sample_symbols:
                    try:
                        logger.info(f"ä¸º {symbol} è®¡ç®—ç»Ÿä¸€å› å­...")
                        
                        # ä½¿ç”¨ç»Ÿä¸€å› å­åº“çš„æ–¹æ³•
                        symbol_factors = complete_factors.calculate_all_signals(symbol)
                        
                        if symbol_factors:
                            logger.info(f"{symbol} æˆåŠŸè®¡ç®— {len(symbol_factors)} ä¸ªå› å­")
                            
                            # æå–å› å­å€¼ä½œä¸ºç‰¹å¾
                            for factor_name, result in symbol_factors.items():
                                if result.value is not None and result.data_quality_score > 0.5:
                                    col_name = f"polygon_{factor_name}"
                                    # ä½¿ç”¨å› å­å€¼
                                    factor_value = result.value
                                    if not np.isnan(factor_value) and np.isfinite(factor_value):
                                        all_polygon_factors[col_name] = factor_value
                                        factor_calculation_success[factor_name] = True
                        
                        # T+1çŸ­æœŸå› å­
                        try:
                            t5_results = short_term_factors.calculate_all_short_term_factors(symbol)
                            if t5_results:
                                prediction = short_term_factors.create_t_plus_5_prediction(symbol, t5_results)
                                
                                # T+1ä¸“ç”¨å› å­
                                for factor_name, result in t5_results.items():
                                    col_name = f"t5_{factor_name}"
                                    if hasattr(result, 't_plus_5_signal'):
                                        signal_value = result.t_plus_5_signal
                                        if not np.isnan(signal_value) and np.isfinite(signal_value):
                                            all_polygon_factors[col_name] = signal_value
                                
                                # T+1ç»¼åˆé¢„æµ‹ä¿¡å·
                                if 'signal_strength' in prediction:
                                    all_polygon_factors['t5_prediction_signal'] = prediction['signal_strength']
                                    all_polygon_factors['t5_prediction_confidence'] = prediction.get('confidence', 0.5)
                        except Exception as t5_e:
                            logger.warning(f"{symbol} T+5å› å­è®¡ç®—å¤±è´¥: {t5_e}")
                        
                        time.sleep(0.5)  # APIé™åˆ¶
                        
                    except Exception as e:
                        logger.warning(f"{symbol}å®Œæ•´å› å­è®¡ç®—å¤±è´¥: {e}")
                        continue
                
                # å°†è®¡ç®—æˆåŠŸçš„å› å­æ·»åŠ åˆ°ç‰¹å¾çŸ©é˜µ
                if all_polygon_factors:
                    logger.info(f"æˆåŠŸè®¡ç®—Polygonå› å­: {len(all_polygon_factors)} ä¸ª")
                    logger.info(f"å› å­ç±»å‹åˆ†å¸ƒ: {list(factor_calculation_success.keys())}")
                    
                    # æ·»åŠ åˆ°combined_features
                    for col_name, value in all_polygon_factors.items():
                        if col_name not in combined_features.columns:
                            # å¯¹æ‰€æœ‰è‚¡ç¥¨å¹¿æ’­è¯¥å› å­å€¼ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                            combined_features[col_name] = value
                    
                    # è®°å½•æˆåŠŸæ·»åŠ çš„å› å­æ•°é‡
                    added_factors = len(all_polygon_factors)
                    logger.info(f"[OK] æˆåŠŸæ·»åŠ  {added_factors} ä¸ªPolygonä¸“ä¸šå› å­åˆ°ç‰¹å¾çŸ©é˜µ")
                    
                    # æ˜¾ç¤ºå› å­åˆ†ç±»ç»Ÿè®¡
                    momentum_factors = len([k for k in all_polygon_factors.keys() if 'momentum' in k])
                    fundamental_factors = len([k for k in all_polygon_factors.keys() if any(x in k for x in ['earnings', 'ebit', 'yield'])])
                    quality_factors = len([k for k in all_polygon_factors.keys() if any(x in k for x in ['piotroski', 'altman', 'quality'])])
                    risk_factors = len([k for k in all_polygon_factors.keys() if any(x in k for x in ['volatility', 'beta', 'risk'])])
                    t5_factors = len([k for k in all_polygon_factors.keys() if 't5_' in k])
                    
                    logger.info(f"å› å­åˆ†å¸ƒ - åŠ¨é‡:{momentum_factors}, åŸºæœ¬é¢:{fundamental_factors}, è´¨é‡:{quality_factors}, é£é™©:{risk_factors}, T+5:{t5_factors}")
                else:
                    logger.warning("æœªèƒ½æˆåŠŸè®¡ç®—ä»»ä½•Polygonå› å­")
                
            except Exception as _e:
                logger.error(f"Polygonå®Œæ•´å› å­åº“é›†æˆå¤±è´¥: {_e}")
                import traceback
                logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
            # [ENHANCED] P2 æ ‡å‡†ç‰¹å¾å¤„ç†æµç¨‹ï¼šæ»åå¯¹é½â†’å»æå€¼â†’è¡Œä¸š/è§„æ¨¡ä¸­æ€§åŒ–â†’æ ‡å‡†åŒ–
            logger.info("[ENHANCED] åº”ç”¨P2æ ‡å‡†ç‰¹å¾å¤„ç†æµç¨‹")
            try:
                # é¢„å…ˆè·å–ä¸€æ¬¡æ‰€æœ‰tickerçš„è¡Œä¸šä¿¡æ¯ï¼Œé¿å…åœ¨å¾ªç¯ä¸­é‡å¤è·å–
                all_tickers = combined_features['ticker'].unique().tolist()
                stock_info_cache = {}
                if hasattr(self, 'market_data_manager') and self.market_data_manager:
                    try:
                        stock_info_cache = self.market_data_manager.get_batch_stock_info(all_tickers)
                        logger.info(f"é¢„è·å–{len(all_tickers)}åªè‚¡ç¥¨çš„è¡Œä¸šä¿¡æ¯å®Œæˆ")
                    except Exception as e:
                        logger.warning(f"é¢„è·å–è¡Œä¸šä¿¡æ¯å¤±è´¥: {e}")
                else:
                    logger.debug("å¸‚åœºæ•°æ®ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡è¡Œä¸šä¿¡æ¯è·å–")
                
                # P2æ ‡å‡†æµç¨‹ï¼šæŒ‰æ—¥æœŸåˆ†ç»„ï¼Œé€æ—¥è¿›è¡Œå®Œæ•´çš„å¤„ç†ç®¡é“
                neutralized_features = []
                
                # ğŸ” DEBUG: æ·»åŠ æ—¥æœŸå¤„ç†è¿›åº¦ç›‘æ§
                unique_dates = combined_features['date'].unique()
                total_dates = len(unique_dates)
                logger.info(f"[DEBUG] å¼€å§‹å¤„ç† {total_dates} ä¸ªäº¤æ˜“æ—¥çš„ç‰¹å¾æ•°æ®")
                
                for date_idx, (date, group) in enumerate(combined_features.groupby('date')):
                    group_features = group[feature_cols].copy()
                    group_meta = group[['ticker', 'SECTOR', 'COUNTRY']].copy()
                    
                    # P2 Step 1: æ»åå¯¹é½ï¼ˆå·²åœ¨å‰é¢å®Œæˆï¼‰
                    # ğŸ” DEBUG: å‡çº§ä¸ºINFOçº§åˆ«æ—¥å¿—ï¼Œä¾¿äºç›‘æ§è¿›åº¦
                    if date_idx % max(1, total_dates // 10) == 0:  # æ¯10%è¿›åº¦æ‰“å°ä¸€æ¬¡
                        logger.info(f"[PROGRESS] å¤„ç†æ—¥æœŸ {date} ({date_idx+1}/{total_dates}, {((date_idx+1)/total_dates*100):.1f}%), è‚¡ç¥¨æ•°: {len(group)}")
                    else:
                        logger.debug(f"Processing {len(group)} stocks for date {date}")
                    
                    # P2 Step 2: å¢å¼ºå»æå€¼å¤„ç†ï¼ˆMAD/Winsorize + åˆ†å¸ƒæ£€æŸ¥ï¼‰
                    for col in feature_cols:
                        if group_features[col].notna().sum() > 2:
                            # ä½¿ç”¨MADï¼ˆä¸­ä½æ•°ç»å¯¹åå·®ï¼‰è¿›è¡Œç¨³å¥çš„å¼‚å¸¸å€¼æ£€æµ‹
                            median_val = group_features[col].median()
                            mad_val = (group_features[col] - median_val).abs().median()
                            
                            # å¢å¼º: å…ˆæ£€æŸ¥ç‰¹å¾åˆ†å¸ƒç¨³å®šæ€§
                            feature_std = group_features[col].std()
                            feature_skewness = group_features[col].skew() if len(group_features[col].dropna()) > 3 else 0
                            
                            # å¯¹äºé«˜åº¦åæ–œçš„ç‰¹å¾ä½¿ç”¨æ›´ä¿å®ˆçš„é˜ˆå€¼
                            if abs(feature_skewness) > 2:  # é«˜åæ–œ
                                threshold_multiplier = 2  # æ›´ä¿å®ˆ
                                logger.debug(f"ç‰¹å¾{col}åæ–œåº¦{feature_skewness:.2f}ï¼Œä½¿ç”¨ä¿å®ˆé˜ˆå€¼")
                            else:
                                threshold_multiplier = 3  # æ ‡å‡†é˜ˆå€¼
                            
                            if mad_val > 0:
                                # ä½¿ç”¨è°ƒæ•´åçš„MADé˜ˆå€¼
                                threshold = threshold_multiplier * 1.4826 * mad_val  # 1.4826ä½¿MADä¸æ ‡å‡†å·®ä¸€è‡´
                                lower_bound = median_val - threshold
                                upper_bound = median_val + threshold
                                
                                # è®°å½•å¼‚å¸¸å€¼æ•°é‡
                                outliers = (group_features[col] < lower_bound) | (group_features[col] > upper_bound)
                                outlier_count = outliers.sum()
                                if outlier_count > 0:
                                    logger.debug(f"ç‰¹å¾{col}åœ¨{date}å‘ç°{outlier_count}ä¸ªå¼‚å¸¸å€¼ï¼Œé˜ˆå€¼[{lower_bound:.3f}, {upper_bound:.3f}]")
                                
                                group_features[col] = group_features[col].clip(lower=lower_bound, upper=upper_bound)
                            else:
                                # å›é€€åˆ°åˆ†ä½æ•°æˆªæ–­ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„åˆ†ä½æ•°
                                if abs(feature_skewness) > 2:
                                    q_lower, q_upper = 0.02, 0.98  # æ›´ä¿å®ˆ
                                else:
                                    q_lower, q_upper = 0.01, 0.99  # æ ‡å‡†
                                
                                q_vals = group_features[col].quantile([q_lower, q_upper])
                                outliers = (group_features[col] < q_vals.iloc[0]) | (group_features[col] > q_vals.iloc[1])
                                if outliers.sum() > 0:
                                    logger.debug(f"ç‰¹å¾{col}åœ¨{date}ä½¿ç”¨åˆ†ä½æ•°æ³•å‘ç°{outliers.sum()}ä¸ªå¼‚å¸¸å€¼")
                                
                                group_features[col] = group_features[col].clip(lower=q_vals.iloc[0], upper=q_vals.iloc[1])
                    
                    # P2 Step 3: è¡Œä¸š/è§„æ¨¡ä¸­æ€§åŒ–
                    for col in feature_cols:
                        if group_features[col].notna().sum() > 5:  # è‡³å°‘éœ€è¦5ä¸ªè§‚æµ‹å€¼
                            try:
                                # æ„å»ºä¸­æ€§åŒ–å›å½’çŸ©é˜µ
                                X_neutralize = pd.get_dummies(group_meta['SECTOR'], prefix='sector', drop_first=True)
                                
                                # æ·»åŠ è§„æ¨¡å› å­ï¼ˆå¦‚æœæ•°æ®å¯ç”¨ï¼‰
                                if 'market_cap' in group.columns:
                                    X_neutralize['log_market_cap'] = np.log(group['market_cap'].fillna(group['market_cap'].median()))
                                elif 'money_flow' in group_features.columns:
                                    # ä½¿ç”¨èµ„é‡‘æµä½œä¸ºè§„æ¨¡ä»£ç†
                                    X_neutralize['log_money_flow'] = np.log(group_features['money_flow'].fillna(group_features['money_flow'].median()) + 1)
                                
                                # æ‰§è¡Œå›å½’ä¸­æ€§åŒ–
                                if len(X_neutralize.columns) > 0 and X_neutralize.shape[0] > X_neutralize.shape[1]:
                                    from sklearn.linear_model import LinearRegression
                                    reg = LinearRegression(fit_intercept=True)
                                    
                                    # åªå¯¹æœ‰æ•ˆæ•°æ®è¿›è¡Œå›å½’
                                    valid_mask = group_features[col].notna() & X_neutralize.notna().all(axis=1)
                                    if valid_mask.sum() > X_neutralize.shape[1] + 1:
                                        reg.fit(X_neutralize[valid_mask], group_features.loc[valid_mask, col])
                                        
                                        # è®¡ç®—æ®‹å·®ä½œä¸ºä¸­æ€§åŒ–åçš„å› å­å€¼
                                        predictions = reg.predict(X_neutralize[valid_mask])
                                        group_features.loc[valid_mask, col] = group_features.loc[valid_mask, col] - predictions
                                        
                            except Exception as e:
                                logger.debug(f"Factor {col} neutralization failed: {e}")
                                # å¦‚æœä¸­æ€§åŒ–å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸå§‹å€¼
                                pass
                    
                    # P2 Step 4: æ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰
                    for col in feature_cols:
                        if group_features[col].notna().sum() > 2:
                            mean_val = group_features[col].mean()
                            std_val = group_features[col].std()
                            if std_val > 0:
                                group_features[col] = (group_features[col] - mean_val) / std_val
                            else:
                                group_features[col] = 0.0
                    
                    # 3. è¡Œä¸šä¸­æ€§åŒ–ï¼ˆä½¿ç”¨é¢„è·å–çš„è¡Œä¸šä¿¡æ¯ï¼‰
                    if stock_info_cache:
                        try:
                            tickers = group['ticker'].tolist()
                            industries = {}
                            for ticker in tickers:
                                info = stock_info_cache.get(ticker)
                                if info:
                                    sector = info.gics_sub_industry or info.gics_industry or info.sector
                                    industries[ticker] = sector or 'Unknown'
                                else:
                                    industries[ticker] = 'Unknown'
                            
                            # æŒ‰è¡Œä¸šå»å‡å€¼
                            group_with_industry = group_features.copy()
                            group_with_industry['industry'] = group['ticker'].map(industries)
                            
                            for col in feature_cols:
                                if group_with_industry[col].notna().sum() > 2:
                                    industry_means = group_with_industry.groupby('industry')[col].transform('mean')
                                    group_features[col] = group_features[col] - industry_means
                                    
                        except Exception as e:
                            logger.debug(f"è¡Œä¸šä¸­æ€§åŒ–è·³è¿‡: {e}")
                    
                    # ä¿ç•™éç‰¹å¾åˆ—
                    group_result = group[['date', 'ticker']].copy()
                    group_result[feature_cols] = group_features[feature_cols]
                    neutralized_features.append(group_result)
                
                # åˆå¹¶ç»“æœ
                neutralized_df = pd.concat(neutralized_features, ignore_index=True)
                combined_features[feature_cols] = neutralized_df[feature_cols]
                
                logger.info(f"ç®€åŒ–ä¸­æ€§åŒ–å®Œæˆï¼Œå¤„ç†{len(feature_cols)}ä¸ªç‰¹å¾")
                
            except Exception as e:
                logger.warning(f"ç®€åŒ–ä¸­æ€§åŒ–å¤±è´¥: {e}")
                logger.info("ä½¿ç”¨åŸå§‹ç‰¹å¾ï¼Œè¿›è¡Œæ—¶é—´å®‰å…¨æ ‡å‡†åŒ–")
                # âœ… CRITICAL FIX: ä½¿ç”¨æ—¶é—´å®‰å…¨æ ‡å‡†åŒ–ä»£æ›¿å…¨æ ·æœ¬StandardScaler
                # ä½¿ç”¨temporal_safe_preprocessingä¸­çš„æ¨ªæˆªé¢æ ‡å‡†åŒ–æ–¹æ³•
                try:
                    logger.info("åº”ç”¨æ¨ªæˆªé¢æ ‡å‡†åŒ–...")
                    standardized_features = self.temporal_preprocessor.cross_sectional_standardize(
                        combined_features, 'date', feature_cols
                    )
                    combined_features[feature_cols] = standardized_features[feature_cols]
                    logger.info(f"æ¨ªæˆªé¢æ ‡å‡†åŒ–å®Œæˆï¼Œå¤„ç†{len(feature_cols)}ä¸ªç‰¹å¾")
                except Exception as std_e:
                    logger.warning(f"æ¨ªæˆªé¢æ ‡å‡†åŒ–å¤±è´¥: {std_e}ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾")
                try:
                    # === æ™ºèƒ½å¤šé‡å…±çº¿æ€§å¤„ç†é›†æˆ ===
                    logger.info("å¼€å§‹åº”ç”¨æ™ºèƒ½å¤šé‡å…±çº¿æ€§å¤„ç†...")
                    try:
                        # å‡†å¤‡ç‰¹å¾æ•°æ®è¿›è¡Œå…±çº¿æ€§åˆ†æ
                        feature_data = combined_features[feature_cols].copy()
                        
                        # åº”ç”¨æ™ºèƒ½å¤šé‡å…±çº¿æ€§å¤„ç†
                        processed_features, process_info = self.apply_intelligent_multicollinearity_processing(feature_data)
                        
                        # æ ¹æ®å¤„ç†ç»“æœæ›´æ–°ç‰¹å¾çŸ©é˜µ
                        if process_info['success']:
                            if process_info['method_used'] == 'pca':
                                # PCAå¤„ç†ï¼šæ›¿æ¢ä¸ºä¸»æˆåˆ†
                                logger.info("åº”ç”¨PCAä¸»æˆåˆ†æ›¿æ¢åŸå§‹ç‰¹å¾...")
                                
                                # ä¿ç•™éç‰¹å¾åˆ—ï¼ˆå¦‚date, tickerç­‰ï¼‰
                                non_feature_cols = [col for col in combined_features.columns if col not in feature_cols]
                                base_df = combined_features[non_feature_cols].copy()
                                
                                # æ·»åŠ ä¸»æˆåˆ†
                                for col in processed_features.columns:
                                    base_df[col] = processed_features[col]
                                
                                combined_features = base_df
                                
                                # æ›´æ–°ç‰¹å¾åˆ—åˆ—è¡¨ä¸ºä¸»æˆåˆ†
                                feature_cols = processed_features.columns.tolist()
                                
                                pca_info = process_info['pca_info']
                                logger.info(f"âœ“ PCAå¤„ç†å®Œæˆ: è§£é‡Šæ–¹å·®{pca_info['variance_explained_total']:.3f}")
                                logger.info(f"âœ“ ç‰¹å¾ç»´åº¦: {process_info['original_shape'][1]} -> {process_info['final_shape'][1]}")
                                
                            else:
                                # æ ‡å‡†åŒ–å¤„ç†ï¼šç›´æ¥æ›´æ–°ç‰¹å¾
                                combined_features[feature_cols] = processed_features[feature_cols]
                                logger.info(f"âœ“ æ ‡å‡†åŒ–å¤„ç†å®Œæˆ: {process_info['method_used']}")
                            
                            logger.info(f"âœ“ å¤šé‡å…±çº¿æ€§å¤„ç†æˆåŠŸ: {', '.join(process_info['processing_details'])}")
                            
                        else:
                            # âœ… FIXED: å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨æ—¶é—´å®‰å…¨æ ‡å‡†åŒ–
                            logger.warning("å¤šé‡å…±çº¿æ€§å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨æ¨ªæˆªé¢æ ‡å‡†åŒ–")
                            try:
                                standardized_features = self.temporal_preprocessor.cross_sectional_standardize(
                                    combined_features, 'date', feature_cols
                                )
                                combined_features[feature_cols] = standardized_features[feature_cols]
                            except Exception as std_e:
                                logger.warning(f"æ¨ªæˆªé¢æ ‡å‡†åŒ–å¤±è´¥: {std_e}")
                            
                    except Exception as e:
                        logger.warning(f"å¤šé‡å…±çº¿æ€§å¤„ç†å¼‚å¸¸: {e}")
                        # âœ… FIXED: å›é€€åˆ°æ—¶é—´å®‰å…¨å¤„ç†
                        try:
                            standardized_features = self.temporal_preprocessor.cross_sectional_standardize(
                                combined_features, 'date', feature_cols
                            )
                            combined_features[feature_cols] = standardized_features[feature_cols]
                        except Exception as std_e:
                            logger.warning(f"å›é€€æ ‡å‡†åŒ–å¤±è´¥: {std_e}")
                    # === å¤šé‡å…±çº¿æ€§å¤„ç†ç»“æŸ ===
                except Exception:
                    pass
            
            logger.info(f"ä¼ ç»Ÿç‰¹å¾åˆ›å»ºå®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {combined_features.shape}")
            return combined_features
        else:
            logger.error("æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾æ•°æ®")
            return pd.DataFrame()
    
    def get_data_and_features(self, tickers: List[str], start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """
        è·å–æ•°æ®å¹¶åˆ›å»ºç‰¹å¾çš„ç»„åˆæ–¹æ³•
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            åŒ…å«ç‰¹å¾çš„DataFrame
        """
        try:
            logger.info(f"å¼€å§‹è·å–æ•°æ®å’Œç‰¹å¾ï¼Œè‚¡ç¥¨: {len(tickers)}åªï¼Œæ—¶é—´: {start_date} - {end_date}")
            
            # 1. ä¸‹è½½è‚¡ç¥¨æ•°æ®
            stock_data = self.download_stock_data(tickers, start_date, end_date)
            if not stock_data:
                logger.error("è‚¡ç¥¨æ•°æ®ä¸‹è½½å¤±è´¥")
                return None
            
            logger.info(f"âœ… è‚¡ç¥¨æ•°æ®ä¸‹è½½å®Œæˆ: {len(stock_data)}åªè‚¡ç¥¨")
            
            # 2. åˆ›å»ºä¼ ç»Ÿç‰¹å¾
            feature_data = self.create_traditional_features(stock_data)
            if feature_data.empty:
                logger.error("ä¼ ç»Ÿç‰¹å¾åˆ›å»ºå¤±è´¥")
                return None
            
            logger.info(f"âœ… ä¼ ç»Ÿç‰¹å¾åˆ›å»ºå®Œæˆ: {feature_data.shape}")
            
            # 3. é›†æˆAlphaæ‘˜è¦ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            alpha_integration_success = False
            try:
                alpha_result = self._integrate_alpha_summary_features(feature_data, stock_data)
                
                # ğŸ”§ CRITICAL FIX: ä¿®å¤Alphaé›†æˆçŠ¶æ€åˆ¤æ–­é€»è¾‘ï¼Œé¿å…çŸ›ç›¾æ—¥å¿—
                if alpha_result is not None and not alpha_result.empty:
                    # æ£€æŸ¥æ˜¯å¦çœŸçš„åŒ…å«Alphaç‰¹å¾ï¼ˆé€šè¿‡åˆ—æ•°å˜åŒ–ï¼‰
                    original_cols = feature_data.shape[1]
                    result_cols = alpha_result.shape[1]
                    
                    if result_cols > original_cols:
                        # åˆ—æ•°å¢åŠ ï¼Œè¯´æ˜æˆåŠŸæ·»åŠ äº†Alphaç‰¹å¾
                        feature_data = alpha_result
                        alpha_integration_success = True
                        added_features = result_cols - original_cols
                        logger.info(f"âœ… Alphaæ‘˜è¦ç‰¹å¾é›†æˆæˆåŠŸï¼Œæœ€ç»ˆå½¢çŠ¶: {feature_data.shape}")
                        logger.info(f"   - æ–°å¢Alphaç‰¹å¾: {added_features}ä¸ª")
                    else:
                        # åˆ—æ•°ç›¸åŒï¼Œè¯´æ˜æ²¡æœ‰æˆåŠŸæ·»åŠ Alphaç‰¹å¾
                        logger.warning("âš ï¸ Alphaæ‘˜è¦ç‰¹å¾æœªç”Ÿæˆæ–°ç‰¹å¾ï¼Œä½¿ç”¨ä¼ ç»Ÿç‰¹å¾")
                else:
                    # alpha_resultä¸ºNoneæˆ–emptyï¼Œæ˜ç¡®è¡¨ç¤ºAlphaé›†æˆå¤±è´¥
                    logger.warning("âš ï¸ Alphaæ‘˜è¦ç‰¹å¾é›†æˆå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿç‰¹å¾")
            except Exception as e:
                logger.warning(f"âš ï¸ Alphaæ‘˜è¦ç‰¹å¾é›†æˆå¼‚å¸¸: {e}ï¼Œä½¿ç”¨ä¼ ç»Ÿç‰¹å¾")
            
            # è®°å½•é›†æˆçŠ¶æ€ç”¨äºåç»­éªŒè¯
            if hasattr(self, '_debug_info'):
                self._debug_info['alpha_integration_success'] = alpha_integration_success
            
            return feature_data
            
        except Exception as e:
            logger.error(f"è·å–æ•°æ®å’Œç‰¹å¾å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _integrate_alpha_summary_features(self, 
                                        feature_data: pd.DataFrame, 
                                        stock_data: Dict[str, pd.DataFrame]) -> Optional[pd.DataFrame]:
        """
        ğŸ”¥ Route A: Alphaæ‘˜è¦ç‰¹å¾é›†æˆåˆ°ä¼ ç»ŸML pipeline
        
        æ ¸å¿ƒè®¾è®¡ï¼š
        1. æœ€å°ä¾µå…¥æ€§ï¼šåœ¨X_cleanåŸºç¡€ä¸Šæ·»åŠ 5-10ä¸ªAlphaæ‘˜è¦ç‰¹å¾ â†’ X_fused
        2. ä¸¥æ ¼æ—¶é—´å¯¹é½ï¼šç¡®ä¿Alphaç‰¹å¾ä»…ä½¿ç”¨å†å²æ•°æ®
        3. æ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼šæŒ‰äº¤æ˜“æ—¥è¿›è¡Œå»æå€¼å’Œæ ‡å‡†åŒ–
        4. é™ç»´å‹ç¼©ï¼šä»45+ä¸ªAlpha â†’ 6-10ä¸ªæ½œå› å­
        5. æ‘˜è¦ç»Ÿè®¡ï¼šæ•æ‰Alphaä¿¡å·çš„è´¨é‡å’Œä¸€è‡´æ€§
        
        Args:
            feature_data: ä¼ ç»Ÿç‰¹å¾æ•°æ® (date, ticker, traditional_features, target)
            stock_data: åŸå§‹è‚¡ç¥¨æ•°æ®å­—å…¸ (ç”¨äºè®¡ç®—Alpha)
            
        Returns:
            X_fused: èåˆäº†Alphaæ‘˜è¦ç‰¹å¾çš„ç‰¹å¾æ•°æ®
        """
        logger.info("å¼€å§‹Route A Alphaæ‘˜è¦ç‰¹å¾é›†æˆ...")
        
        # â­ ä¼˜å…ˆä½¿ç”¨é«˜çº§Alphaç³»ç»Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.advanced_alpha_system is not None:
            logger.info("ä½¿ç”¨é«˜çº§Alphaç³»ç»Ÿï¼ˆä¸“ä¸šæœºæ„çº§ï¼‰")
            try:
                # å‡†å¤‡æ•°æ®
                raw_data = self._prepare_data_for_advanced_alpha(stock_data)
                returns = feature_data['target'] if 'target' in feature_data.columns else pd.Series()
                
                # ä½¿ç”¨é«˜çº§Alphaç³»ç»Ÿå¤„ç†
                advanced_features = self.advanced_alpha_system.process_complete_pipeline(
                    raw_data=raw_data,
                    returns=returns,
                    market_data=None  # å¯ä»¥ä¼ å…¥å¸‚åœºæ•°æ®
                )
                
                # åˆå¹¶åˆ°ä¸»ç‰¹å¾æ•°æ®
                if advanced_features is not None and not advanced_features.empty:
                    logger.info(f"âœ… é«˜çº§Alphaç³»ç»Ÿç”Ÿæˆ {advanced_features.shape[1]} ä¸ªç‰¹å¾")
                    
                    # å¯¹é½ç´¢å¼•
                    advanced_features.index = feature_data.index[:len(advanced_features)]
                    
                    # åˆå¹¶ç‰¹å¾
                    X_fused = pd.concat([feature_data, advanced_features], axis=1)
                    
                    # è·å–æ€§èƒ½æŠ¥å‘Š
                    perf_summary = self.advanced_alpha_system.performance_monitor.get_performance_summary()
                    if perf_summary:
                        current = perf_summary.get('current', {})
                        logger.info(f"  Rank IC: {current.get('rank_ic', 0):.4f}")
                        logger.info(f"  Sharpe: {current.get('sharpe_ratio', 0):.2f}")
                    
                    return X_fused
                    
            except Exception as e:
                logger.warning(f"é«˜çº§Alphaç³»ç»Ÿå¤„ç†å¤±è´¥: {e}, å›é€€åˆ°åŸºç¡€å¤„ç†")
        
        # å›é€€åˆ°åŸå§‹Alphaæ‘˜è¦å¤„ç†å™¨
        try:
            # å¯¼å…¥Alphaæ‘˜è¦å¤„ç†å™¨
            from alpha_summary_features import create_alpha_summary_processor, AlphaSummaryConfig
            
            # é…ç½®Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨
            alpha_config = AlphaSummaryConfig(
                max_alpha_features=18,  # ä½¿ç”¨ä¸“ä¸šæ ‡å‡†ï¼š18ä¸ªç‰¹å¾
                winsorize_lower=0.01,
                winsorize_upper=0.99,
                use_pca=True,
                pca_variance_explained=0.65,
                use_ic_weighted_composite=True,
                include_dispersion=True,
                include_agreement=True,
                include_quality=True,
                strict_time_validation=True,
                data_type='float32'
            )
            
            processor = create_alpha_summary_processor(alpha_config.__dict__)
            
            # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—Alphaå› å­ä¿¡å·
            alpha_signals = self._compute_alpha_signals_for_integration(stock_data, feature_data)
            if alpha_signals is None or alpha_signals.empty:
                logger.warning("Alphaä¿¡å·è®¡ç®—å¤±è´¥ï¼Œè·³è¿‡Alphaç‰¹å¾é›†æˆ")
                return feature_data
            
            # ç¬¬äºŒæ­¥ï¼šåˆ›å»ºå¸‚åœºèƒŒæ™¯æ•°æ®ï¼ˆç”¨äºä¸­æ€§åŒ–ï¼‰
            market_context = self._create_market_context_data(stock_data, feature_data)
            
            # ç¬¬ä¸‰æ­¥ï¼šæå–ç›®æ ‡æ—¥æœŸï¼ˆç”¨äºæ—¶é—´éªŒè¯ï¼‰
            target_dates = pd.to_datetime(feature_data['date']).unique()
            
            # ç¬¬å››æ­¥ï¼šå¤„ç†Alphaä¿¡å· â†’ æ‘˜è¦ç‰¹å¾
            alpha_summary_features = processor.process_alpha_to_summary(
                alpha_df=alpha_signals,
                market_data=market_context,
                target_dates=pd.Series(target_dates)
            )
            
            if alpha_summary_features.empty:
                logger.warning("Alphaæ‘˜è¦ç‰¹å¾ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡é›†æˆ")
                # ğŸ”§ CRITICAL FIX: è¿”å›Noneä»¥æ˜ç¡®è¡¨ç¤ºAlphaé›†æˆå¤±è´¥ï¼Œè€Œä¸æ˜¯è¿”å›åŸå§‹æ•°æ®
                return None
            
            # ç¬¬äº”æ­¥ï¼šå¯¹é½å’Œåˆå¹¶ç‰¹å¾ï¼ˆX_clean + alpha_features â†’ X_fusedï¼‰
            X_fused = self._merge_alpha_and_traditional_features(
                feature_data, alpha_summary_features
            )
            
            if X_fused is None or X_fused.empty:
                logger.warning("ç‰¹å¾åˆå¹¶å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿç‰¹å¾")
                # ğŸ”§ CRITICAL FIX: è¿”å›Noneä»¥æ˜ç¡®è¡¨ç¤ºç‰¹å¾åˆå¹¶å¤±è´¥
                return None
            
            # è·å–å¤„ç†ç»Ÿè®¡
            stats = processor.get_processing_stats()
            logger.info(f"Alphaæ‘˜è¦ç‰¹å¾é›†æˆç»Ÿè®¡:")
            logger.info(f"  - åŸå§‹Alphaæ•°é‡: {stats.get('total_alphas_processed', 0)}")
            logger.info(f"  - ç”Ÿæˆæ‘˜è¦ç‰¹å¾: {stats.get('features_generated', 0)}")
            logger.info(f"  - æ—¶é—´è¿è§„: {stats.get('time_violations', 0)}")
            logger.info(f"  - å‹ç¼©æ–¹å·®è§£é‡Š: {stats.get('compression_variance_explained', 0):.3f}")
            
            return X_fused
            
        except Exception as e:
            logger.error(f"Alphaæ‘˜è¦ç‰¹å¾é›†æˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return feature_data
    
    def _compute_alpha_signals_for_integration(self, 
                                             stock_data: Dict[str, pd.DataFrame], 
                                             feature_data: pd.DataFrame) -> Optional[pd.DataFrame]:
        """è®¡ç®—Alphaä¿¡å·ç”¨äºæ‘˜è¦ç‰¹å¾æå–"""
        try:
            if not hasattr(self, 'alpha_engine') or not self.alpha_engine:
                logger.warning("Alphaå¼•æ“ä¸å¯ç”¨")
                return None
            
            # å‡†å¤‡Alphaè®¡ç®—æ‰€éœ€çš„æ•°æ®æ ¼å¼
            alpha_input_data = []
            
            for ticker, df in stock_data.items():
                if len(df) < 30:
                    continue
                    
                df_copy = df.copy().sort_values('date')
                df_copy['ticker'] = ticker
                
                # æ ‡å‡†åŒ–åˆ—åï¼šå°†å°å†™åˆ—åæ˜ å°„åˆ°Alphaå¼•æ“éœ€è¦çš„å¤§å†™åˆ—å
                column_mapping = {
                    'close': 'Close',
                    'high': 'High', 
                    'low': 'Low',
                    'open': 'Open',
                    'volume': 'Volume',
                    'adj_close': 'Close',  # ä½¿ç”¨è°ƒæ•´åæ”¶ç›˜ä»·
                    'adjclose': 'Close',   # å¦ä¸€ç§æ ¼å¼
                    'Adj Close': 'Close'   # yfinanceæ ¼å¼
                }
                
                # åº”ç”¨åˆ—åæ˜ å°„
                for old_col, new_col in column_mapping.items():
                    if old_col in df_copy.columns and new_col not in df_copy.columns:
                        df_copy[new_col] = df_copy[old_col]
                
                # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
                required_cols = ['date', 'ticker', 'Close', 'Volume', 'High', 'Low']
                missing_cols = [col for col in required_cols if col not in df_copy.columns]
                
                if missing_cols:
                    logger.warning(f"è‚¡ç¥¨{ticker}ç¼ºå°‘å¿…è¦åˆ— {missing_cols}ï¼Œè·³è¿‡")
                    continue
                
                # å¦‚æœæ²¡æœ‰Openåˆ—ï¼Œç”¨Closeæ›¿ä»£ï¼ˆä¸€äº›æ•°æ®æºå¯èƒ½ç¼ºå¤±ï¼‰
                if 'Open' not in df_copy.columns:
                    df_copy['Open'] = df_copy['Close']
                    logger.debug(f"è‚¡ç¥¨{ticker}ç¼ºå°‘Openåˆ—ï¼Œä½¿ç”¨Closeæ›¿ä»£")
                
                # é€‰æ‹©æœ€ç»ˆéœ€è¦çš„åˆ—
                final_cols = ['date', 'ticker', 'Close', 'Volume', 'High', 'Low', 'Open']
                alpha_input_data.append(df_copy[final_cols])
            
            if not alpha_input_data:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„Alphaè¾“å…¥æ•°æ®")
                return None
            
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            combined_alpha_data = pd.concat(alpha_input_data, ignore_index=True)
            combined_alpha_data['date'] = pd.to_datetime(combined_alpha_data['date'])
            combined_alpha_data = combined_alpha_data.sort_values(['date', 'ticker'])
            
            # ä½¿ç”¨Alphaå¼•æ“è®¡ç®—æ‰€æœ‰Alphaå› å­
            alpha_signals = self.alpha_engine.compute_all_alphas(combined_alpha_data)
            
            if alpha_signals is None or alpha_signals.empty:
                logger.warning("Alphaå¼•æ“æœªç”Ÿæˆä»»ä½•ä¿¡å·")
                return None
            
            # ç¡®ä¿ç´¢å¼•æ ¼å¼æ­£ç¡®ï¼ˆmulti-index: date, tickerï¼‰
            if not isinstance(alpha_signals.index, pd.MultiIndex):
                alpha_signals = alpha_signals.set_index(['date', 'ticker'])
            
            logger.info(f"Alphaä¿¡å·è®¡ç®—å®Œæˆ: {alpha_signals.shape}")
            return alpha_signals
            
        except Exception as e:
            logger.error(f"Alphaä¿¡å·è®¡ç®—å¤±è´¥: {e}")
            return None
    
    def _create_market_context_data(self, 
                                  stock_data: Dict[str, pd.DataFrame], 
                                  feature_data: pd.DataFrame) -> Optional[pd.DataFrame]:
        """åˆ›å»ºå¸‚åœºèƒŒæ™¯æ•°æ®ç”¨äºAlphaä¸­æ€§åŒ–"""
        try:
            market_context_data = []
            
            for ticker, df in stock_data.items():
                if len(df) < 10:
                    continue
                    
                df_copy = df.copy().sort_values('date')
                df_copy['ticker'] = ticker
                
                # è®¡ç®—å¸‚å€¼ä»£ç†ï¼ˆä»·æ ¼*æˆäº¤é‡ï¼‰
                if 'Close' in df_copy.columns and 'Volume' in df_copy.columns:
                    df_copy['market_cap'] = df_copy['Close'] * df_copy['Volume']
                else:
                    df_copy['market_cap'] = 1.0  # é»˜è®¤å€¼
                
                # ç®€åŒ–çš„è¡Œä¸šåˆ†ç±»ï¼ˆåŸºäºè‚¡ç¥¨ä»£ç å‰ç¼€ï¼‰
                if ticker.startswith(('A', 'B', 'C')):
                    df_copy['industry'] = 'Tech'
                elif ticker.startswith(('D', 'E', 'F')):
                    df_copy['industry'] = 'Finance'
                elif ticker.startswith(('G', 'H', 'I')):
                    df_copy['industry'] = 'Healthcare'
                elif ticker.startswith(('J', 'K', 'L')):
                    df_copy['industry'] = 'Consumer'
                else:
                    df_copy['industry'] = 'Others'
                
                market_context_data.append(df_copy[['date', 'ticker', 'market_cap', 'industry']])
            
            if market_context_data:
                combined_context = pd.concat(market_context_data, ignore_index=True)
                combined_context['date'] = pd.to_datetime(combined_context['date'])
                return combined_context
            else:
                return None
                
        except Exception as e:
            logger.warning(f"å¸‚åœºèƒŒæ™¯æ•°æ®åˆ›å»ºå¤±è´¥: {e}")
            return None
    
    def _merge_alpha_and_traditional_features(self, 
                                            feature_data: pd.DataFrame, 
                                            alpha_summary_features: pd.DataFrame) -> Optional[pd.DataFrame]:
        """åˆå¹¶ä¼ ç»Ÿç‰¹å¾å’ŒAlphaæ‘˜è¦ç‰¹å¾ (X_clean + alpha_features â†’ X_fused)"""
        try:
            # ç¡®ä¿ä¸¤ä¸ªæ•°æ®æ¡†éƒ½æœ‰æ­£ç¡®çš„é”®ç”¨äºåˆå¹¶
            feature_data_copy = feature_data.copy()
            feature_data_copy['date'] = pd.to_datetime(feature_data_copy['date'])
            
            # åˆ›å»ºåˆå¹¶é”®
            feature_data_copy['merge_key'] = feature_data_copy['date'].astype(str) + '_' + feature_data_copy['ticker'].astype(str)
            
            # Alphaæ‘˜è¦ç‰¹å¾çš„ç´¢å¼•æ ¼å¼å¤„ç†
            if isinstance(alpha_summary_features.index, pd.MultiIndex):
                alpha_df_for_merge = alpha_summary_features.reset_index()
                alpha_df_for_merge['date'] = pd.to_datetime(alpha_df_for_merge['date'])
                alpha_df_for_merge['merge_key'] = alpha_df_for_merge['date'].astype(str) + '_' + alpha_df_for_merge['ticker'].astype(str)
            else:
                logger.warning("Alphaæ‘˜è¦ç‰¹å¾ç´¢å¼•æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡åˆå¹¶")
                return feature_data
            
            # æ‰§è¡Œå·¦è¿æ¥ï¼ˆä»¥ä¼ ç»Ÿç‰¹å¾ä¸ºä¸»ï¼‰
            merged_data = feature_data_copy.merge(
                alpha_df_for_merge.drop(['date', 'ticker'], axis=1), 
                on='merge_key', 
                how='left'
            )
            
            # åˆ é™¤ä¸´æ—¶åˆå¹¶é”®
            merged_data = merged_data.drop('merge_key', axis=1)
            
            # å¤„ç†ç¼ºå¤±çš„Alphaç‰¹å¾ï¼ˆä½¿ç”¨æ¨ªæˆªé¢ä¸­ä½æ•°å¡«å……ï¼‰
            alpha_cols = [col for col in alpha_df_for_merge.columns 
                         if col.startswith('alpha_') and col not in ['date', 'ticker', 'merge_key']]
            
            for alpha_col in alpha_cols:
                if alpha_col in merged_data.columns:
                    # æŒ‰æ—¥æœŸåˆ†ç»„ï¼Œä½¿ç”¨æ¨ªæˆªé¢ä¸­ä½æ•°å¡«å……
                    merged_data[alpha_col] = merged_data.groupby('date')[alpha_col].transform(
                        lambda x: x.fillna(x.median())
                    )
                    # å¦‚æœä»æœ‰NaNï¼Œç”¨0å¡«å……
                    merged_data[alpha_col] = merged_data[alpha_col].fillna(0)
            
            logger.info(f"ç‰¹å¾åˆå¹¶å®Œæˆ: {feature_data.shape} + Alpha â†’ {merged_data.shape}")
            logger.info(f"æ–°å¢Alphaæ‘˜è¦ç‰¹å¾: {alpha_cols}")
            
            return merged_data
            
        except Exception as e:
            logger.error(f"ç‰¹å¾åˆå¹¶å¤±è´¥: {e}")
            return feature_data
    
    def detect_multicollinearity(self, X: pd.DataFrame, vif_threshold: float = 10.0) -> Dict[str, Any]:
        """
        æ£€æµ‹å› å­é—´çš„å¤šé‡å…±çº¿æ€§
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            vif_threshold: VIFé˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼è®¤ä¸ºå­˜åœ¨å…±çº¿æ€§
            
        Returns:
            å…±çº¿æ€§æ£€æµ‹ç»“æœ
        """
        try:
            results = {
                'high_vif_features': [],
                'correlation_matrix': None,
                'highly_correlated_pairs': [],
                'vif_scores': {},
                'needs_pca': False,
                'max_correlation': 0.0
            }
            
            logger.info(f"å¼€å§‹å…±çº¿æ€§æ£€æµ‹ï¼Œç‰¹å¾æ•°é‡: {X.shape[1]}")
            
            # 1. è®¡ç®—VIF (æ–¹å·®è†¨èƒ€å› å­)
            if X.shape[1] > 1 and X.shape[0] > X.shape[1] + 5:
                try:
                    X_clean = X.select_dtypes(include=[np.number]).fillna(0)
                    if X_clean.shape[1] > 1:
                        vif_scores = {}
                        high_vif_features = []
                        
                        for i, feature in enumerate(X_clean.columns):
                            try:
                                vif = variance_inflation_factor(X_clean.values, i)
                                vif_scores[feature] = vif if not np.isnan(vif) and not np.isinf(vif) else 999
                                if vif_scores[feature] > vif_threshold:
                                    high_vif_features.append(feature)
                            except:
                                vif_scores[feature] = 999
                                high_vif_features.append(feature)
                        
                        results['high_vif_features'] = high_vif_features
                        results['vif_scores'] = vif_scores
                        
                        logger.info(f"VIFæ£€æµ‹å®Œæˆï¼Œå‘ç°{len(high_vif_features)}ä¸ªé«˜å…±çº¿æ€§ç‰¹å¾")
                        
                except Exception as e:
                    logger.warning(f"VIFè®¡ç®—å¤±è´¥: {e}")
            
            # 2. è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
            try:
                corr_matrix = X.corr()
                results['correlation_matrix'] = corr_matrix
                
                # æ‰¾å‡ºé«˜ç›¸å…³æ€§ç‰¹å¾å¯¹
                threshold = 0.8
                highly_corr_pairs = []
                max_corr = 0.0
                
                for i in range(len(corr_matrix.columns)):
                    for j in range(i+1, len(corr_matrix.columns)):
                        corr_val = abs(corr_matrix.iloc[i, j])
                        max_corr = max(max_corr, corr_val)
                        if corr_val > threshold:
                            highly_corr_pairs.append({
                                'feature1': corr_matrix.columns[i],
                                'feature2': corr_matrix.columns[j],
                                'correlation': corr_matrix.iloc[i, j]
                            })
                
                results['highly_correlated_pairs'] = highly_corr_pairs
                results['max_correlation'] = max_corr
                logger.info(f"å‘ç°{len(highly_corr_pairs)}ä¸ªé«˜ç›¸å…³æ€§ç‰¹å¾å¯¹ï¼Œæœ€å¤§ç›¸å…³æ€§: {max_corr:.3f}")
                
            except Exception as e:
                logger.warning(f"ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}")
            
            # 3. åˆ¤æ–­æ˜¯å¦éœ€è¦PCA
            high_vif_ratio = len(results['high_vif_features']) / max(X.shape[1], 1)
            high_corr_ratio = len(results['highly_correlated_pairs']) / max(X.shape[1], 1)
            
            results['needs_pca'] = (
                high_vif_ratio > 0.3 or  # è¶…è¿‡30%ç‰¹å¾æœ‰é«˜VIF
                high_corr_ratio > 0.2 or # è¶…è¿‡20%ç‰¹å¾å¯¹é«˜ç›¸å…³
                results['max_correlation'] > 0.9  # å­˜åœ¨æé«˜ç›¸å…³æ€§
            )
            
            logger.info(f"å…±çº¿æ€§è¯„ä¼°: VIFæ¯”ä¾‹={high_vif_ratio:.2f}, ç›¸å…³æ€§æ¯”ä¾‹={high_corr_ratio:.2f}, éœ€è¦PCA={results['needs_pca']}")
            
            return results
            
        except Exception as e:
            logger.error(f"å…±çº¿æ€§æ£€æµ‹å¤±è´¥: {e}")
            return {'needs_pca': False, 'high_vif_features': [], 'highly_correlated_pairs': [], 'max_correlation': 0.0}

    def _basic_correlation_filter(self, features: pd.DataFrame, process_info: Dict) -> Tuple[pd.DataFrame, Dict]:
        """
        åŸºç¡€ç›¸å…³æ€§è¿‡æ»¤æ–¹æ³• - æ—¶åºå®‰å…¨çš„å›é€€æ–¹æ¡ˆ
        """
        logger.info("æ‰§è¡ŒåŸºç¡€ç›¸å…³æ€§è¿‡æ»¤ï¼ˆæ—¶åºå®‰å…¨ï¼‰")
        
        # è®¡ç®—ç‰¹å¾é—´ç›¸å…³æ€§
        corr_matrix = features.corr().abs()
        
        # å¯»æ‰¾é«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹
        threshold = 0.85
        high_corr_pairs = []
        features_to_remove = set()
        
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                if corr_val > threshold and not pd.isna(corr_val):
                    col1, col2 = corr_matrix.columns[i], corr_matrix.columns[j]
                    high_corr_pairs.append((col1, col2, corr_val))
                    
                    # ç§»é™¤æ–¹å·®è¾ƒå°çš„ç‰¹å¾
                    if col1 not in features_to_remove and col2 not in features_to_remove:
                        var1 = features[col1].var()
                        var2 = features[col2].var()
                        
                        if var1 < var2:
                            features_to_remove.add(col1)
                        else:
                            features_to_remove.add(col2)
        
        # è¿‡æ»¤ç‰¹å¾
        retained_features = [col for col in features.columns if col not in features_to_remove]
        filtered_features = features[retained_features]
        
        process_info.update({
            'method_used': 'basic_correlation_filter',
            'final_shape': filtered_features.shape,
            'processing_details': [
                f"åŸºç¡€ç›¸å…³æ€§è¿‡æ»¤ï¼Œé˜ˆå€¼={threshold}",
                f"å‘ç°{len(high_corr_pairs)}ä¸ªé«˜ç›¸å…³ç‰¹å¾å¯¹",
                f"ç§»é™¤{len(features_to_remove)}ä¸ªå†—ä½™ç‰¹å¾"
            ],
            'success': True,
            'data_leakage_risk': 'LOW',
            'features_removed': len(features_to_remove)
        })
        
        logger.info(f"åŸºç¡€ç›¸å…³æ€§è¿‡æ»¤å®Œæˆ: {features.shape} -> {filtered_features.shape}")
        
        return filtered_features, process_info

    def apply_pca_transformation(self, X: pd.DataFrame, variance_threshold: float = 0.95) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        âœ… FIXED: æ—¶é—´å®‰å…¨çš„PCAå˜æ¢ - å·²ä¿®å¤æ•°æ®æ³„éœ²é—®é¢˜
        
        ä½¿ç”¨expanding windowæˆ–rolling windowæ–¹æ³•é¿å…æœªæ¥ä¿¡æ¯æ³„éœ²
        æ³¨æ„ï¼šå»ºè®®ä½¿ç”¨apply_intelligent_multicollinearity_processingè·å¾—æ›´å¥½çš„æ•ˆæœ
        
        Args:
            X: è¾“å…¥ç‰¹å¾çŸ©é˜µï¼ˆéœ€è¦æœ‰æ—¥æœŸç´¢å¼•ï¼‰
            variance_threshold: ä¿ç•™çš„æ–¹å·®æ¯”ä¾‹
            
        Returns:
            Tuple[æ­£äº¤åŒ–åçš„ç‰¹å¾çŸ©é˜µ, PCAä¿¡æ¯]
        """
        try:
            pca_info = {
                'n_components': 0,
                'explained_variance_ratio': [],
                'cumulative_variance': [],
                'transformation_applied': False,
                'variance_explained_total': 0.0,
                'original_features': [],
                'component_names': [],
                'safety_mode': 'time_aware'
            }
            
            logger.info(f"å¼€å§‹æ—¶é—´å®‰å…¨PCAå˜æ¢ï¼Œè¾“å…¥å½¢çŠ¶: {X.shape}")
            
            # 1. æ•°æ®é¢„å¤„ç†
            X_clean = X.select_dtypes(include=[np.number]).fillna(0)
            if X_clean.shape[1] < 2:
                logger.info("ç‰¹å¾æ•°é‡ä¸è¶³2ä¸ªï¼Œè·³è¿‡PCA")
                return X, pca_info
            
            # 2. âœ… æ—¶é—´å®‰å…¨æ ‡å‡†åŒ– - ä½¿ç”¨expanding window
            if 'date' in X.columns:
                # å¦‚æœæœ‰æ—¥æœŸåˆ—ï¼ŒæŒ‰æ—¥æœŸæ’åº
                X_clean = X_clean.sort_values('date') if 'date' in X_clean.columns else X_clean.sort_index()
            
            # ä½¿ç”¨expanding windowè¿›è¡Œæ ‡å‡†åŒ–ï¼Œé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²
            X_scaled = np.zeros_like(X_clean.values)
            min_samples = 60  # è‡³å°‘60ä¸ªæ ·æœ¬æ‰å¼€å§‹æ ‡å‡†åŒ–
            
            for i in range(len(X_clean)):
                if i < min_samples:
                    # åˆæœŸæ ·æœ¬ä¸è¶³ï¼Œä½¿ç”¨0å¡«å……
                    X_scaled[i, :] = 0
                else:
                    # åªä½¿ç”¨å†å²æ•°æ®è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
                    historical_data = X_clean.iloc[:i].values
                    mean = np.mean(historical_data, axis=0)
                    std = np.std(historical_data, axis=0)
                    std[std == 0] = 1  # é¿å…é™¤é›¶
                    X_scaled[i, :] = (X_clean.iloc[i].values - mean) / std
            
            # 3. âœ… æ—¶é—´å®‰å…¨PCA - ä½¿ç”¨å¢é‡PCAæˆ–ç®€åŒ–æ–¹æ³•
            # ä¸ºé¿å…å¤æ‚çš„å¢é‡PCAï¼Œè¿™é‡Œä½¿ç”¨ç›¸å…³æ€§ç­›é€‰æ›¿ä»£
            if len(X_clean) < 100:
                # æ ·æœ¬å¤ªå°‘ï¼Œä¸è¿›è¡ŒPCA
                logger.info("æ ·æœ¬ä¸è¶³100ï¼Œè·³è¿‡PCAå˜æ¢")
                return X_clean, pca_info
            
            # è®¡ç®—ç‰¹å¾ç›¸å…³çŸ©é˜µï¼ˆåªä½¿ç”¨å†å²æ•°æ®ï¼‰
            corr_matrix = pd.DataFrame(X_scaled).corr().abs()
            
            # è¯†åˆ«é«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹
            upper_tri = corr_matrix.where(
                np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
            )
            
            # ç§»é™¤ç›¸å…³æ€§>0.95çš„å†—ä½™ç‰¹å¾
            to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]
            
            # ä¿ç•™ç‹¬ç«‹ç‰¹å¾
            X_reduced = X_clean.drop(columns=X_clean.columns[to_drop])
            
            # 4. è®°å½•å¤„ç†ä¿¡æ¯
            n_components = X_reduced.shape[1]
            pca_info.update({
                'n_components': n_components,
                'transformation_applied': True,
                'original_features': X_clean.columns.tolist(),
                'removed_features': X_clean.columns[to_drop].tolist(),
                'variance_explained_total': 0.95,  # è¿‘ä¼¼å€¼
                'method': 'correlation_reduction',
                'safety_mode': 'time_aware_expanding_window'
            })
            
            logger.info(f"æ—¶é—´å®‰å…¨å¤„ç†å®Œæˆ: {X_clean.shape[1]} -> {n_components}ä¸ªç‰¹å¾")
            logger.info(f"ç§»é™¤{len(to_drop)}ä¸ªé«˜åº¦ç›¸å…³ç‰¹å¾")
            
            return X_reduced, pca_info
            
        except Exception as e:
            logger.error(f"æ—¶é—´å®‰å…¨PCAå˜æ¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return X, pca_info

    def apply_intelligent_multicollinearity_processing(self, features: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        ğŸ”§ ä¿®å¤ï¼šæ—¶åºå®‰å…¨çš„å…±çº¿æ€§å¤„ç† - é¿å…æ•°æ®æ³„éœ²
        ä½¿ç”¨æ—¶åºå®‰å…¨çš„é¢„å¤„ç†æ–¹æ³•æ›¿ä»£å­˜åœ¨æ³„éœ²é£é™©çš„PCA
        
        Args:
            features: è¾“å…¥ç‰¹å¾çŸ©é˜µ
            
        Returns:
            Tuple[å¤„ç†åçš„ç‰¹å¾çŸ©é˜µ, å¤„ç†ä¿¡æ¯]
        """
        try:
            process_info = {
                'method_used': 'temporal_safe_processing',
                'original_shape': features.shape,
                'final_shape': features.shape,
                'multicollinearity_detected': False,
                'pca_info': None,
                'processing_details': [],
                'success': False,
                'data_leakage_risk': 'FIXED'
            }
            
            logger.info(f"ğŸ”§ å¼€å§‹æ—¶åºå®‰å…¨å…±çº¿æ€§å¤„ç†ï¼Œè¾“å…¥å½¢çŠ¶: {features.shape}")
            
            if features.shape[1] < 2:
                logger.info("ç‰¹å¾æ•°é‡ä¸è¶³ï¼Œè·³è¿‡å…±çº¿æ€§å¤„ç†")
                process_info['method_used'] = 'skip_insufficient_features'
                process_info['success'] = True
                return features, process_info
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ—¶åºå®‰å…¨çš„é¢„å¤„ç†å™¨æ›¿ä»£å±é™©çš„PCAæ–¹æ³•
            try:
                from temporal_safe_preprocessing import create_temporal_safe_preprocessor
                
                # åˆ›å»ºæ—¶åºå®‰å…¨é¢„å¤„ç†å™¨
                safe_preprocessor = create_temporal_safe_preprocessor({
                    'standardization_mode': 'cross_sectional',  # æ¨ªæˆªé¢æ ‡å‡†åŒ–
                    'enable_pca': False,  # ç¦ç”¨PCAé¿å…æ•°æ®æ³„éœ²
                    'pca_alternative': 'correlation_filter'  # ä½¿ç”¨ç›¸å…³æ€§è¿‡æ»¤æ›¿ä»£PCA
                })
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ—¥æœŸåˆ—
                if 'date' in features.columns:
                    date_col = 'date'
                elif any('date' in col.lower() for col in features.columns):
                    date_col = [col for col in features.columns if 'date' in col.lower()][0]
                else:
                    # å¦‚æœæ²¡æœ‰æ—¥æœŸåˆ—ï¼Œåˆ›å»ºä¸€ä¸ªå‡çš„æ—¥æœŸåºåˆ—ç”¨äºå¤„ç†
                    features_copy = features.copy()
                    features_copy['date'] = pd.date_range('2023-01-01', periods=len(features), freq='D')
                    date_col = 'date'
                    features = features_copy
                
                # æ‰§è¡Œæ—¶åºå®‰å…¨å˜æ¢
                processed_features, transform_info = safe_preprocessor.fit_transform(
                    features, 
                    features[date_col],
                    date_col
                )
                
                # ç§»é™¤ä¸´æ—¶æ·»åŠ çš„æ—¥æœŸåˆ—ï¼ˆå¦‚æœæ˜¯æˆ‘ä»¬æ·»åŠ çš„ï¼‰
                if 'date' not in self.original_columns:
                    processed_features = processed_features.drop('date', axis=1, errors='ignore')
                
                process_info.update({
                    'method_used': 'temporal_safe_correlation_filter',
                    'final_shape': processed_features.shape,
                    'processing_details': [
                        f"ä½¿ç”¨æ—¶åºå®‰å…¨é¢„å¤„ç†å™¨",
                        f"æ ‡å‡†åŒ–æ¨¡å¼: {transform_info['standardization_info']['method']}",
                        f"å…±çº¿æ€§å¤„ç†: {transform_info['collinearity_info']['method']}"
                    ],
                    'success': True,
                    'data_leakage_risk': 'MINIMAL',
                    'features_removed': transform_info['collinearity_info'].get('features_removed', 0)
                })
                
                logger.info(f"âœ… æ—¶åºå®‰å…¨å¤„ç†å®Œæˆ: {features.shape} -> {processed_features.shape}")
                logger.info(f"   æ•°æ®æ³„éœ²é£é™©: MINIMAL (å·²ä¿®å¤)")
                
                return processed_features, process_info
                
            except ImportError:
                logger.warning("æ—¶åºå®‰å…¨é¢„å¤„ç†å™¨ä¸å¯ç”¨ï¼Œå›é€€åˆ°åŸºç¡€ç›¸å…³æ€§è¿‡æ»¤")
                # å›é€€åˆ°åŸºç¡€çš„ç›¸å…³æ€§è¿‡æ»¤æ–¹æ³•
                return self._basic_correlation_filter(features, process_info)
                
            # 1. æ£€æµ‹å…±çº¿æ€§ (ä¿ç•™åŸæœ‰é€»è¾‘ä½œä¸ºå¤‡ç”¨)
            multicollinearity_results = self.detect_multicollinearity(features, vif_threshold=10.0)
            process_info['multicollinearity_detected'] = multicollinearity_results['needs_pca']
            process_info['processing_details'].append(f"å…±çº¿æ€§æ£€æµ‹: éœ€è¦å¤„ç†={multicollinearity_results['needs_pca']}")
            
            # 2. æ ¹æ®æ£€æµ‹ç»“æœé€‰æ‹©å¤„ç†æ–¹æ³•
            if multicollinearity_results['needs_pca']:
                # âœ… FIXED: ä½¿ç”¨å®‰å…¨çš„ç›¸å…³æ€§è¿‡æ»¤æ›¿ä»£PCA
                logger.warning("æ£€æµ‹åˆ°ä¸¥é‡å…±çº¿æ€§ï¼Œä½¿ç”¨ç›¸å…³æ€§è¿‡æ»¤æ›¿ä»£å±é™©çš„PCA")
                return self._basic_correlation_filter(features, process_info)
                
            else:
                # âœ… FIXED: æœªæ£€æµ‹åˆ°ä¸¥é‡å…±çº¿æ€§ï¼Œä½¿ç”¨å®‰å…¨çš„åŸºç¡€è¿‡æ»¤
                logger.info("æœªæ£€æµ‹åˆ°ä¸¥é‡å…±çº¿æ€§ï¼Œä½¿ç”¨åŸºç¡€å¤„ç†")
                return self._basic_correlation_filter(features, process_info)
            
        except Exception as e:
            logger.error(f"å¤šé‡å…±çº¿æ€§å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            process_info['success'] = False
            process_info['error'] = str(e)
            return features, process_info

    def _validate_temporal_alignment(self, feature_data: pd.DataFrame) -> bool:
        """ğŸ”§ ä¿®å¤æ—¶é—´å¯¹é½éªŒè¯ï¼šæ™ºèƒ½é€‚åº”æ•°æ®é¢‘ç‡å’Œå‘¨æœ«é—´éš™"""
        try:
            # æ£€æŸ¥æ¯ä¸ªtickerçš„æ—¶é—´å¯¹é½
            alignment_issues = 0
            total_checked = 0
            
            for ticker in feature_data['ticker'].unique()[:5]:  # æ£€æŸ¥å‰5ä¸ªè‚¡ç¥¨
                ticker_data = feature_data[feature_data['ticker'] == ticker].sort_values('date')
                if len(ticker_data) < 10:
                    continue
                
                total_checked += 1
                
                # ğŸ”§ æ™ºèƒ½æ—¶é—´é—´éš”æ£€æµ‹ï¼šæ ¹æ®å®é™…æ•°æ®é¢‘ç‡è°ƒæ•´
                dates = pd.to_datetime(ticker_data['date']).sort_values()
                if len(dates) < 2:
                    continue
                    
                # è®¡ç®—å®é™…æ•°æ®é¢‘ç‡
                date_diffs = dates.diff().dt.days.dropna()
                median_diff = date_diffs.median()
                
                # æ ¹æ®æ•°æ®é¢‘ç‡è®¾å®šæœŸæœ›é—´éš”
                if median_diff <= 1:  # æ—¥é¢‘æ•°æ®
                    base_lag = 4  # 4ä¸ªå·¥ä½œæ—¥
                    tolerance = 4  # å®¹å¿å‘¨æœ«å’Œå‡æœŸ
                elif median_diff <= 7:  # å‘¨é¢‘æ•°æ®  
                    base_lag = 4 * 7  # 4å‘¨
                    tolerance = 7  # 1å‘¨å®¹å·®
                else:  # æœˆé¢‘æˆ–æ›´ä½é¢‘
                    base_lag = 30  # çº¦1ä¸ªæœˆ
                    tolerance = 15  # åŠæœˆå®¹å·®
                
                # æ£€æŸ¥æœ€æ–°æ•°æ®çš„æ—¶é—´é—´éš”
                if len(ticker_data) > 5:
                    # ä»å€’æ•°ç¬¬5ä¸ªå’Œæœ€åä¸€ä¸ªæ¯”è¾ƒï¼ˆæ›´ç°å®çš„æ»åæ£€æŸ¥ï¼‰
                    feature_date = ticker_data['date'].iloc[-5]
                    target_date = ticker_data['date'].iloc[-1]
                    
                    # è½¬æ¢ä¸ºdatetimeè¿›è¡Œè®¡ç®—
                    feature_dt = pd.to_datetime(feature_date)
                    target_dt = pd.to_datetime(target_date)
                    actual_diff = int((target_dt - feature_dt) / pd.Timedelta(days=1))
                    
                    logger.info(f"æ—¶é—´å¯¹é½æ£€æŸ¥ {ticker}: ç‰¹å¾={feature_date}, ç›®æ ‡={target_date}, å®é™…é—´éš”={actual_diff}å¤©, æœŸæœ›â‰ˆ{base_lag}å¤©(Â±{tolerance}å¤©)")
                    
                    # æ›´å®½æ¾çš„å¯¹é½éªŒè¯ï¼ˆè€ƒè™‘å®é™…å¸‚åœºæƒ…å†µï¼‰
                    if abs(actual_diff - base_lag) > tolerance:
                        logger.warning(f"æ—¶é—´å¯¹é½åå·® {ticker}: {actual_diff}å¤© vs æœŸæœ›{base_lag}Â±{tolerance}å¤©")
                        alignment_issues += 1
                    else:
                        logger.info(f"æ—¶é—´å¯¹é½æ­£å¸¸ {ticker}: åå·®{abs(actual_diff - base_lag)}å¤© < å®¹å·®{tolerance}å¤©")
                    
            # å¦‚æœè¶…è¿‡50%çš„è‚¡ç¥¨å­˜åœ¨æ—¶é—´å¯¹é½é—®é¢˜ï¼Œè¿”å›False
            if total_checked > 0:
                error_rate = alignment_issues / total_checked
                if error_rate > 0.5:
                    logger.error(f"âŒ æ—¶é—´å¯¹é½éªŒè¯å¤±è´¥: {alignment_issues}/{total_checked} ({error_rate*100:.1f}%) è‚¡ç¥¨å­˜åœ¨é—®é¢˜")
                    return False
                else:
                    logger.info(f"âœ… æ—¶é—´å¯¹é½éªŒè¯é€šè¿‡: {total_checked-alignment_issues}/{total_checked} ({(1-error_rate)*100:.1f}%) è‚¡ç¥¨é€šè¿‡éªŒè¯")
                    return True
            
            return True
            
        except Exception as e:
            logger.error(f"æ—¶é—´å¯¹é½éªŒè¯å¼‚å¸¸: {e}")
            return False

    def _collect_data_info(self, feature_data: pd.DataFrame) -> Dict[str, Any]:
        """æ”¶é›†æ•°æ®ä¿¡æ¯ç”¨äºæ¨¡å—çŠ¶æ€è¯„ä¼°"""
        try:
            data_info = {}
            
            # åŸºç¡€ç»Ÿè®¡
            data_info['n_samples'] = len(feature_data)
            data_info['n_features'] = len([col for col in feature_data.columns 
                                         if col not in ['ticker', 'date', 'target']])
            
            # æ—¥æœŸå’Œè‚¡ç¥¨ä¿¡æ¯
            if 'date' in feature_data.columns:
                dates = pd.to_datetime(feature_data['date'])
                data_info['date_range'] = (dates.min(), dates.max())
                data_info['unique_dates'] = dates.nunique()
                
                # æ¯æ—¥ç»„è§„æ¨¡ï¼ˆç”¨äºLTRè¯„ä¼°ï¼‰
                daily_groups = feature_data.groupby('date').size()
                data_info['daily_group_sizes'] = daily_groups.tolist()
                data_info['min_daily_group_size'] = daily_groups.min() if len(daily_groups) > 0 else 0
                data_info['avg_daily_group_size'] = daily_groups.mean() if len(daily_groups) > 0 else 0
                
                # æ—¥æœŸè¦†ç›–ç‡ï¼ˆæ»¡è¶³ç»„è§„æ¨¡è¦æ±‚çš„æ—¥æœŸæ¯”ä¾‹ï¼‰
                valid_dates = (daily_groups >= 20).sum()  # ä½¿ç”¨é˜ˆå€¼20
                data_info['date_coverage_ratio'] = valid_dates / len(daily_groups) if len(daily_groups) > 0 else 0.0
            
            # éªŒè¯é›†å¤§å°ä¼°ç®—ï¼ˆç”¨äºIsotonicæ ¡å‡†ï¼‰
            data_info['validation_samples'] = max(100, int(data_info['n_samples'] * 0.2))
            
            # å¯¼å…¥DataInfoCalculatorç”¨äºçœŸå®è®¡ç®—
            from fix_hardcoded_data_info import DataInfoCalculator
            calculator = DataInfoCalculator()
            
            # OOFè¦†ç›–ç‡ - ä½¿ç”¨çœŸå®è®¡ç®—
            data_info['oof_coverage'] = calculator.calculate_oof_coverage(
                getattr(self, 'oof_predictions', None) if hasattr(self, 'oof_predictions') else None,
                data_info['n_samples']
            )
            
            # ä»·æ ¼/æˆäº¤é‡æ•°æ®æ£€æŸ¥ï¼ˆRegime-awareéœ€è¦ï¼‰
            price_volume_cols = ['close', 'volume', 'Close', 'Volume']
            data_info['has_price_volume'] = any(col in feature_data.columns for col in price_volume_cols)
            
            # Regimeæ ·æœ¬ä¼°ç®— - ä½¿ç”¨çœŸå®è®¡ç®—
            data_info['regime_samples'] = calculator.calculate_regime_samples(
                feature_data,
                getattr(self, 'regime_labels', None) if hasattr(self, 'regime_labels') else None
            )
            
            # è®¡ç®—çœŸå®çš„regimeç¨³å®šæ€§
            data_info['regime_stability'] = calculator.calculate_regime_stability(
                feature_data, 
                getattr(self, 'regime_detector', None) if hasattr(self, 'regime_detector') else None
            )
            
            # Stackingç›¸å…³ - ä½¿ç”¨çœŸå®è®¡ç®—
            validation_data = feature_data.sample(n=min(1000, len(feature_data))) if len(feature_data) > 0 else feature_data
            
            data_info['base_models_ic_ir'] = calculator.calculate_base_models_ic_ir(
                getattr(self, 'base_models', None) if hasattr(self, 'base_models') else None,
                validation_data
            )
            
            data_info['oof_valid_samples'] = int(data_info['n_samples'] * 0.7)
            
            data_info['model_correlations'] = calculator.calculate_model_correlations(
                getattr(self, 'base_models', None) if hasattr(self, 'base_models') else None,
                validation_data
            )
            
            # å†…å­˜ä½¿ç”¨
            try:
                memory_usage = psutil.virtual_memory().used / 1024**2  # MB
                data_info['memory_usage_mb'] = memory_usage
            except:
                data_info['memory_usage_mb'] = 500  # é»˜è®¤å€¼
            
            data_info['other_modules_stable'] = True  # å‡è®¾å…¶ä»–æ¨¡å—ç¨³å®š
            
            return data_info
            
        except Exception as e:
            logger.error(f"æ•°æ®ä¿¡æ¯æ”¶é›†å¤±è´¥: {e}")
            return {
                'n_samples': len(feature_data) if feature_data is not None else 0,
                'n_features': 0,
                'daily_group_sizes': [],
                'date_coverage_ratio': 0.0,
                'validation_samples': 100,
                'oof_coverage': 0.0,
                'has_price_volume': False,
                'regime_samples': {},
                'regime_stability': 0.0,
                'base_models_ic_ir': {},
                'oof_valid_samples': 0,
                'model_correlations': [],
                'memory_usage_mb': 500,
                'other_modules_stable': False
            }
    
    def _calculate_cross_sectional_ic(self, predictions: np.ndarray, 
                                     returns: np.ndarray, 
                                     dates: pd.Series) -> Tuple[Optional[float], int]:
        """
        ğŸ”¥ CRITICAL: è®¡ç®—æ¨ªæˆªé¢RankICï¼Œé¿å…æ—¶é—´åºåˆ—ICçš„é”™è¯¯
        
        Returns:
            (cross_sectional_ic, valid_days): æ¨ªæˆªé¢ICå‡å€¼å’Œæœ‰æ•ˆå¤©æ•°
        """
        try:
            if len(predictions) != len(returns) or len(predictions) != len(dates):
                logger.error(f"âŒ ICè®¡ç®—ç»´åº¦ä¸åŒ¹é…: pred={len(predictions)}, ret={len(returns)}, dates={len(dates)}")
                return None, 0
            
            # åˆ›å»ºDataFrame
            df = pd.DataFrame({
                'prediction': predictions,
                'return': returns,
                'date': pd.to_datetime(dates) if not isinstance(dates.iloc[0], pd.Timestamp) else dates
            })
            
            # æŒ‰æ—¥æœŸåˆ†ç»„è®¡ç®—æ¯æ—¥æ¨ªæˆªé¢IC
            daily_ics = []
            valid_days = 0
            
            for date, group in df.groupby('date'):
                if len(group) < 2:  # éœ€è¦è‡³å°‘2åªè‚¡ç¥¨
                    continue
                    
                # è®¡ç®—å½“æ—¥æ¨ªæˆªé¢Spearmanç›¸å…³æ€§
                pred_ranks = group['prediction'].rank()
                ret_ranks = group['return'].rank()
                
                daily_ic = pred_ranks.corr(ret_ranks, method='spearman')
                
                if not pd.isna(daily_ic):
                    daily_ics.append(daily_ic)
                    valid_days += 1
            
            if len(daily_ics) == 0:
                logger.warning("âŒ æ— æœ‰æ•ˆçš„æ¨ªæˆªé¢ICè®¡ç®—æ—¥æœŸ")
                # ğŸ”¥ CRITICAL FIX: å•è‚¡ç¥¨æƒ…å†µçš„å¤„ç†
                if hasattr(self, 'feature_data') and self.feature_data is not None and 'ticker' in self.feature_data.columns:
                    unique_tickers = self.feature_data['ticker'].nunique()
                    if unique_tickers == 1:
                        logger.info("ğŸ”„ æ£€æµ‹åˆ°å•è‚¡ç¥¨æƒ…å†µï¼Œä½¿ç”¨æ—¶é—´åºåˆ—ç›¸å…³æ€§ä½œä¸ºICä»£æ›¿")
                        # å¯¹äºå•è‚¡ç¥¨ï¼Œè®¡ç®—æ—¶é—´åºåˆ—ç›¸å…³æ€§
                        time_series_ic = np.corrcoef(predictions, returns)[0, 1]
                        if not np.isnan(time_series_ic):
                            logger.info(f"ğŸ“Š å•è‚¡ç¥¨æ—¶é—´åºåˆ—IC: {time_series_ic:.3f}")
                            return time_series_ic, len(predictions)
                return None, 0
            
            # è®¡ç®—å¹³å‡æ¨ªæˆªé¢IC
            mean_ic = np.mean(daily_ics)
            
            logger.debug(f"æ¨ªæˆªé¢ICè®¡ç®—: {valid_days} æœ‰æ•ˆå¤©æ•°, ICèŒƒå›´: {np.min(daily_ics):.3f}~{np.max(daily_ics):.3f}")
            
            return mean_ic, valid_days
            
        except Exception as e:
            logger.error(f"âŒ æ¨ªæˆªé¢ICè®¡ç®—å¤±è´¥: {e}")
            return None, 0
    
    def _linear_regression_calibration(self, predictions: np.ndarray, 
                                     true_labels: np.ndarray) -> Tuple[np.ndarray, Dict[str, float]]:
        """
        ğŸ”¥ CRITICAL: å›å½’ä»»åŠ¡çš„çº¿æ€§ç¼©æ”¾æ ¡å‡†ï¼ˆæ›¿ä»£åˆ†ç±»Brier Scoreï¼‰
        
        Returns:
            (calibrated_predictions, regression_metrics)
        """
        try:
            from sklearn.linear_model import LinearRegression
            from sklearn.metrics import r2_score, mean_squared_error
            
            # æ£€æŸ¥è¾“å…¥
            if len(predictions) != len(true_labels) or len(predictions) < 10:
                logger.warning(f"æ ¡å‡†æ•°æ®ä¸è¶³: {len(predictions)} æ¡æ ·æœ¬")
                return predictions, {'r2_score': 0.0, 'mse': float('inf')}
            
            # çº¿æ€§å›å½’æ ¡å‡†: calibrated = a * prediction + b  
            X_calib = predictions.reshape(-1, 1)
            y_calib = true_labels
            
            # è®­ç»ƒæ ¡å‡†æ¨¡å‹
            calibration_model = LinearRegression()
            calibration_model.fit(X_calib, y_calib)
            
            # æ ¡å‡†åçš„é¢„æµ‹
            calibrated_preds = calibration_model.predict(X_calib)
            
            # è®¡ç®—å›å½’æŒ‡æ ‡ï¼ˆä¸æ˜¯åˆ†ç±»æŒ‡æ ‡ï¼‰
            r2 = r2_score(true_labels, calibrated_preds)
            mse = mean_squared_error(true_labels, calibrated_preds)
            
            # è®¡ç®—é¢„æµ‹åŒºé—´è¦†ç›–ç‡ï¼ˆå›å½’ä»»åŠ¡çš„é‡è¦æŒ‡æ ‡ï¼‰
            residuals = np.abs(calibrated_preds - true_labels)
            coverage_80 = np.percentile(residuals, 80)  # 80%åˆ†ä½æ•°
            
            metrics = {
                'r2_score': r2,
                'mse': mse,
                'calibration_slope': calibration_model.coef_[0],
                'calibration_intercept': calibration_model.intercept_,
                'coverage_80_percentile': coverage_80
            }
            
            logger.debug(f"çº¿æ€§æ ¡å‡†: æ–œç‡={metrics['calibration_slope']:.3f}, æˆªè·={metrics['calibration_intercept']:.3f}")
            
            return calibrated_preds, metrics
            
        except Exception as e:
            logger.error(f"âŒ çº¿æ€§å›å½’æ ¡å‡†å¤±è´¥: {e}")
            return predictions, {'r2_score': 0.0, 'mse': float('inf')}
    
    def _extract_bma_weights_from_training(self, training_results: Dict[str, Any]) -> Dict[str, float]:
        """ä»è®­ç»ƒç»“æœä¸­æå–BMAæƒé‡"""
        try:
            weights = {}
            total_weight = 0.0
            
            # ä»å„ä¸ªæ¨¡å‹è®­ç»ƒç»“æœä¸­æå–æƒé‡
            for model_type, result in training_results.items():
                if isinstance(result, dict) and result.get('success', False):
                    # åŸºäºCVåˆ†æ•°æˆ–ICåˆ†æ•°è®¡ç®—æƒé‡
                    if 'cv_score' in result:
                        weight = max(0, result['cv_score'])  # ç¡®ä¿éè´Ÿ
                    elif 'ic_score' in result:
                        weight = max(0, abs(result['ic_score']))  # ICç»å¯¹å€¼
                    else:
                        weight = 0.1  # é»˜è®¤æœ€å°æƒé‡
                    
                    weights[model_type] = weight
                    total_weight += weight
            
            # CRITICAL FIX: å¥å£®çš„æƒé‡å½’ä¸€åŒ–å’Œè¾¹ç•Œæƒ…å†µå¤„ç†
            if total_weight > 1e-8:  # ä½¿ç”¨æ›´ä¸¥æ ¼çš„æ•°å€¼é˜ˆå€¼
                # æ ‡å‡†å½’ä¸€åŒ–
                normalized_weights = {k: v/total_weight for k, v in weights.items()}
                
                # éªŒè¯å½’ä¸€åŒ–ç»“æœ
                norm_sum = sum(normalized_weights.values())
                if abs(norm_sum - 1.0) > 1e-6:
                    logger.warning(f"æƒé‡å½’ä¸€åŒ–å¼‚å¸¸: æ€»å’Œ={norm_sum:.8f}, é‡æ–°å½’ä¸€åŒ–")
                    # å¼ºåˆ¶é‡æ–°å½’ä¸€åŒ–
                    normalized_weights = {k: v/norm_sum for k, v in normalized_weights.items()}
                
                weights = normalized_weights
            else:
                # CRITICAL FIX: æ”¹è¿›çš„fallbackç­–ç•¥
                logger.warning(f"æ— æœ‰æ•ˆæ¨¡å‹æƒé‡ (total_weight={total_weight:.8f})ï¼Œå¯ç”¨fallbackç­–ç•¥")
                # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•è®­ç»ƒç»“æœï¼Œå³ä½¿æœªæ ‡è®°ä¸ºæˆåŠŸ
                if training_results:
                    available_models = []
                    for model_type, result in training_results.items():
                        if isinstance(result, dict):
                            available_models.append(model_type)
                    
                    if available_models:
                        # ç»™æ‰€æœ‰å¯ç”¨æ¨¡å‹åˆ†é…ç­‰æƒé‡
                        equal_weight = 1.0 / len(available_models)
                        weights = {model: equal_weight for model in available_models}
                        logger.info(f"å•è‚¡ç¥¨ç­‰æƒé‡fallback: {len(available_models)} ä¸ªæ¨¡å‹")
                    else:
                        # æœ€åçš„fallbackï¼šåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„åŸºçº¿æ¨¡å‹
                        weights = {'baseline_fallback': 1.0}
                        logger.warning("åˆ›å»ºbaseline fallbackæ¨¡å‹æƒé‡")
            
            return weights
            
        except Exception as e:
            logger.error(f"æƒé‡æå–å¤±è´¥: {e}")
            return {}
    
    def _extract_model_performance(self, training_results: Dict[str, Any]) -> Dict[str, Dict]:
        """æå–æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
        try:
            performance = {}
            for model_type, result in training_results.items():
                if isinstance(result, dict):
                    performance[model_type] = {
                        'cv_score': result.get('cv_score', 0.0),
                        'ic_score': result.get('ic_score', 0.0),
                        'success': result.get('success', False),
                        'samples': result.get('train_samples', 0)
                    }
            return performance
        except Exception as e:
            logger.error(f"æ€§èƒ½æŒ‡æ ‡æå–å¤±è´¥: {e}")
            return {}
    
    def _calculate_ensemble_diversity(self, training_results: Dict[str, Any]) -> Dict[str, float]:
        """è®¡ç®—é›†æˆå¤šæ ·æ€§æŒ‡æ ‡"""
        try:
            # ç®€åŒ–çš„å¤šæ ·æ€§è®¡ç®—
            successful_models = sum(1 for r in training_results.values() 
                                  if isinstance(r, dict) and r.get('success', False))
            
            return {
                'model_count': len(training_results),
                'successful_models': successful_models,
                'success_rate': successful_models / max(1, len(training_results)),
                'diversity_score': min(1.0, successful_models / 3)  # è‡³å°‘3ä¸ªæ¨¡å‹æ‰ç®—å¤šæ ·
            }
        except Exception as e:
            logger.error(f"å¤šæ ·æ€§è®¡ç®—å¤±è´¥: {e}")
            return {'diversity_score': 0.0}

    def _safe_data_preprocessing(self, X: pd.DataFrame, y: pd.Series, 
                               dates: pd.Series, tickers: pd.Series) -> Tuple[pd.DataFrame, pd.Series, pd.Series, pd.Series]:
        """å®‰å…¨çš„æ•°æ®é¢„å¤„ç†"""
        try:
            # å¯¹ç‰¹å¾è¿›è¡Œå®‰å…¨çš„ä¸­ä½æ•°å¡«å……ï¼ˆåªå¤„ç†æ•°å€¼åˆ—ï¼‰
            numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
            non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()
            
            X_imputed = X.copy()
            
            # åªå¯¹æ•°å€¼åˆ—åº”ç”¨ä¸­ä½æ•°å¡«å……
            if numeric_cols:
                imputer = SimpleImputer(strategy='median')
                X_imputed[numeric_cols] = pd.DataFrame(
                    imputer.fit_transform(X[numeric_cols]), 
                    columns=numeric_cols, 
                    index=X.index
                )
            
            # å¯¹éæ•°å€¼åˆ—ä½¿ç”¨å¸¸æ•°å¡«å……
            if non_numeric_cols:
                for col in non_numeric_cols:
                    X_imputed[col] = X_imputed[col].fillna('Unknown')
        
        # ç›®æ ‡å˜é‡å¿…é¡»æœ‰æ•ˆ
            target_valid = ~y.isna()
            
            X_clean = X_imputed[target_valid]
            y_clean = y[target_valid]
            dates_clean = dates[target_valid]
            tickers_clean = tickers[target_valid]
            
                # ç¡®ä¿X_cleanåªåŒ…å«æ•°å€¼ç‰¹å¾
            numeric_columns = X_clean.select_dtypes(include=[np.number]).columns
            X_clean = X_clean[numeric_columns]
            
                # å½»åº•çš„NaNå¤„ç†
            initial_shape = X_clean.shape
            X_clean = X_clean.dropna(axis=1, how='all')  # ç§»é™¤å…¨ä¸ºNaNçš„åˆ—
            X_clean = X_clean.dropna(axis=0, how='all')  # ç§»é™¤å…¨ä¸ºNaNçš„è¡Œ
                
            if X_clean.isnull().any().any():
                X_clean = X_clean.ffill().bfill().fillna(0)
                logger.info(f"NaNå¡«å……å®Œæˆ: {initial_shape} -> {X_clean.shape}")
            
                logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆ: {len(X_clean)}æ ·æœ¬, {len(X_clean.columns)}ç‰¹å¾")
                
                return X_clean, y_clean, dates_clean, tickers_clean
                
        except Exception as e:
            logger.error(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            # è¿”å›åŸºç¡€æ¸…ç†ç‰ˆæœ¬
            target_valid = ~y.isna()
            return X[target_valid].fillna(0), y[target_valid], dates[target_valid], tickers[target_valid]
    
    def _train_standard_models(self, X: pd.DataFrame, y: pd.Series, 
                             dates: pd.Series, tickers: pd.Series) -> Dict[str, Any]:
        """è®­ç»ƒæ ‡å‡†æœºå™¨å­¦ä¹ æ¨¡å‹"""
        try:
            from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
            from sklearn.linear_model import LinearRegression
            from sklearn.model_selection import cross_val_score
            
            # ç¡®ä¿åªä½¿ç”¨æ•°å€¼ç‰¹å¾
            if X.empty:
                logger.error("è¾“å…¥ç‰¹å¾ä¸ºç©º")
                return {'success': False, 'error': 'è¾“å…¥ç‰¹å¾ä¸ºç©º'}
                
            # ä¸¥æ ¼è¿‡æ»¤æ•°å€¼åˆ—
            numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
            if len(numeric_cols) == 0:
                logger.error("æ²¡æœ‰æ•°å€¼ç‰¹å¾å¯ç”¨äºæ¨¡å‹è®­ç»ƒ")
                return {'success': False, 'error': 'æ²¡æœ‰æ•°å€¼ç‰¹å¾'}
            
            X_numeric = X[numeric_cols].fillna(0)  # å¡«å……NaNå€¼
            logger.info(f"MLè®­ç»ƒä½¿ç”¨ç‰¹å¾: {len(X.columns)} -> {len(numeric_cols)} ä¸ªæ•°å€¼ç‰¹å¾")
            
            # ç¡®ä¿ç›®æ ‡å˜é‡ä¹Ÿæ˜¯æ•°å€¼å‹ä¸”æ— NaN
            y_clean = pd.to_numeric(y, errors='coerce').fillna(0)
            
            models = {
                'random_forest': RandomForestRegressor(
                    n_estimators=self.config.get('models', {}).get('random_forest', {}).get('n_estimators', 50),
                    random_state=42,
                    max_depth=self.config.get('models', {}).get('random_forest', {}).get('max_depth', 10)
                ),
                'gradient_boosting': GradientBoostingRegressor(
                    n_estimators=self.config.get('models', {}).get('gradient_boosting', {}).get('n_estimators', 50),
                    random_state=42,
                    max_depth=self.config.get('models', {}).get('gradient_boosting', {}).get('max_depth', 5)
                ),
                'linear_regression': LinearRegression()
            }
            
            results = {}
            for name, model in models.items():
                try:
                    # æ£€æŸ¥æ•°æ®å½¢çŠ¶
                    if X_numeric.shape[0] < 10 or X_numeric.shape[1] == 0:
                        logger.warning(f"{name} è·³è¿‡ï¼šæ•°æ®ä¸è¶³ (shape: {X_numeric.shape})")
                        results[name] = {'model': None, 'cv_score': 0.0, 'predictions': np.zeros(len(y_clean))}
                        continue
                    
                    # è®­ç»ƒæ¨¡å‹
                    model.fit(X_numeric, y_clean)
                    
                    # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯è¯„åˆ†ï¼ˆå¦‚æœæ•°æ®è¶³å¤Ÿï¼‰
                    if X_numeric.shape[0] >= 30:  # éœ€è¦è¶³å¤Ÿæ•°æ®è¿›è¡Œæ—¶é—´åºåˆ—åˆ†å‰²
                        logger.info(f"ä½¿ç”¨æ—¶é—´åºåˆ—CVè®­ç»ƒ{name}æ¨¡å‹ï¼Œæ•°æ®é‡: {X_numeric.shape[0]}")
                        
                        # ä½¿ç”¨æ—¶é—´åºåˆ—åˆ†å‰²è€Œä¸æ˜¯éšæœºåˆ†å‰²
                        if PURGED_CV_AVAILABLE and PURGED_CV_VERSION == "FIXED":
                            # ğŸ”¥ CRITICAL FIX: ä½¿ç”¨å¤šèµ„äº§å®‰å…¨çš„æ—¶é—´åºåˆ—CV
                            try:
                                # å¯¼å…¥å®‰å…¨çš„å¤šèµ„äº§CV
                                from multi_asset_safe_cv import create_safe_multi_asset_cv, SafeMultiAssetValidator
                                
                                # å‡†å¤‡å¤šèµ„äº§CVæ‰€éœ€çš„æ•°æ®æ ¼å¼
                                cv_data = X_numeric.copy()
                                if 'date' not in cv_data.columns and dates_clean is not None:
                                    cv_data['date'] = dates_clean
                                
                                # ğŸš¨ CRITICAL FIX: é˜²æ­¢å¤šè‚¡ç¥¨CVä¿¡æ¯æ³„éœ²
                                if 'ticker' not in cv_data.columns:
                                    # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯å¤šè‚¡ç¥¨æ•°æ®
                                    if hasattr(X_numeric.index, 'names') and len(X_numeric.index.names) > 1:
                                        # MultiIndexæƒ…å†µï¼šä»ç´¢å¼•ä¸­æå–tickerä¿¡æ¯
                                        if 'ticker' in X_numeric.index.names:
                                            cv_data['ticker'] = X_numeric.index.get_level_values('ticker')
                                        elif 'symbol' in X_numeric.index.names:
                                            cv_data['ticker'] = X_numeric.index.get_level_values('symbol')
                                        else:
                                            # å¦‚æœæ— æ³•ç¡®å®šè‚¡ç¥¨èº«ä»½ï¼Œå¼ºåˆ¶å›é€€åˆ°å•èµ„äº§CV
                                            logger.warning("âš ï¸ æ— æ³•è¯†åˆ«è‚¡ç¥¨èº«ä»½ï¼Œå›é€€åˆ°å•èµ„äº§æ—¶é—´åºåˆ—CVä»¥é˜²ä¿¡æ¯æ³„éœ²")
                                            cv_data['ticker'] = 'SINGLE_ASSET_MODE'
                                    else:
                                        # å•ä¸€æ—¶é—´åºåˆ—æ•°æ®ï¼Œä½¿ç”¨å•èµ„äº§æ¨¡å¼
                                        cv_data['ticker'] = 'SINGLE_ASSET_MODE'
                                
                                # ğŸ”¥ CRITICAL FIX: åˆ›å»ºå®‰å…¨çš„å¤šèµ„äº§CVåˆ†å‰²å™¨ï¼ˆä½¿ç”¨å…¨å±€ç»Ÿä¸€æ—¶é—´é…ç½®ï¼‰
                                temporal_config = validate_temporal_configuration()
                                safe_cv = create_safe_multi_asset_cv(
                                    n_splits=3,           # å‡å°‘åˆ†å‰²æ•°ä»¥ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
                                    test_size_days=21,    # 21å¤©éªŒè¯æœŸ
                                    gap_days=temporal_config['cv_gap_days'],        # å…¨å±€ç»Ÿä¸€11å¤©é—´éš”
                                    embargo_days=temporal_config['cv_embargo_days']  # å…¨å±€ç»Ÿä¸€11å¤©ç¦æ­¢æœŸ
                                )
                                
                                validator = SafeMultiAssetValidator()
                                scores = []
                                split_count = 0
                                
                                logger.info(f"ğŸ“Š å¤šèµ„äº§å®‰å…¨CVè®¾ç½®: {len(cv_data)} æ¡æ•°æ®")
                                
                                # ä½¿ç”¨å®‰å…¨çš„å¤šèµ„äº§CVåˆ†å‰²
                                for train_idx, test_idx in safe_cv.split(cv_data):
                                    # éªŒè¯åˆ†å‰²çš„å®‰å…¨æ€§
                                    if not validator.validate_no_leakage(cv_data, train_idx, test_idx):
                                        logger.warning(f"å‘ç°æ—¶é—´æ³„éœ²ï¼Œè·³è¿‡æ­¤åˆ†å‰²")
                                        continue
                                    
                                    if not validator.validate_sufficient_data(train_idx, test_idx, min_train=50, min_val=10):
                                        logger.warning(f"æ•°æ®ä¸è¶³ï¼Œè·³è¿‡æ­¤åˆ†å‰²")
                                        continue
                                    
                                    # æ£€æŸ¥èµ„äº§åˆ†å¸ƒ
                                    distribution = validator.check_asset_distribution(cv_data, train_idx, test_idx)
                                    logger.info(f"èµ„äº§åˆ†å¸ƒ: {distribution}")
                                    
                                    if len(train_idx) > 10 and len(test_idx) > 5:
                                        X_train, X_test = X_numeric.iloc[train_idx], X_numeric.iloc[test_idx]
                                        y_train, y_test = y_clean.iloc[train_idx], y_clean.iloc[test_idx]
                                        
                                        # é‡æ–°è®­ç»ƒæ¨¡å‹å¹¶è¯„åˆ†
                                        temp_model = type(model)(**model.get_params())
                                        temp_model.fit(X_train, y_train)
                                        score = temp_model.score(X_test, y_test)
                                        scores.append(score)
                                        split_count += 1
                                        logger.info(f"  æ—¶é—´åºåˆ—CV fold {split_count}: RÂ² = {score:.3f}")
                                
                                cv_score = np.mean(scores) if scores else 0.0
                                logger.info(f"  {name}æ—¶é—´åºåˆ—CVå¹³å‡å¾—åˆ†: {cv_score:.3f} ({len(scores)} folds)")
                                
                            except Exception as e:
                                logger.error(f"âŒ CRITICAL: Purged CVå¤±è´¥ï¼Œè¿™ä¼šå¯¼è‡´æ•°æ®æ³„éœ²é£é™©: {e}")
                                
                                # ğŸš€ åº”ç”¨ç”Ÿäº§çº§ä¿®å¤ï¼šä½¿ç”¨å®‰å…¨CVæˆ–æ‹’ç»è®­ç»ƒ
                                if PRODUCTION_FIXES_AVAILABLE and self.cv_preventer:
                                    try:
                                        logger.info("ğŸ”§ ä½¿ç”¨ç”Ÿäº§çº§ä¿®å¤ï¼šåˆ›å»ºå®‰å…¨CVåˆ†å‰²å™¨")
                                        safe_cv = self.cv_preventer.create_safe_cv_splitter(n_splits=3)
                                        scores = cross_val_score(model, X_numeric, y_clean, cv=safe_cv, scoring='r2')
                                        cv_score = scores.mean()
                                        logger.info("âœ… ä½¿ç”¨å®‰å…¨CVåˆ†å‰²å™¨æˆåŠŸï¼Œé¿å…æ•°æ®æ³„éœ²")
                                    except Exception as cv_e:
                                        logger.error(f"âŒ å®‰å…¨CVåˆ›å»ºä¹Ÿå¤±è´¥: {cv_e}")
                                        logger.warning("âš ï¸ ä¸ºå®‰å…¨èµ·è§ï¼Œä½¿ç”¨å•ä¸€è®­ç»ƒ-éªŒè¯åˆ†å‰²ï¼ˆæ— CVï¼‰")
                                        # å®‰å…¨çš„å•ä¸€åˆ†å‰²
                                        split_idx = int(len(X_numeric) * 0.8)
                                        X_train, X_val = X_numeric[:split_idx], X_numeric[split_idx:]
                                        y_train, y_val = y_clean[:split_idx], y_clean[split_idx:]
                                        model.fit(X_train, y_train)
                                        cv_score = model.score(X_val, y_val)
                                        logger.info(f"å•ä¸€åˆ†å‰²éªŒè¯å¾—åˆ†: {cv_score:.3f}")
                                else:
                                    # å¦‚æœæ²¡æœ‰ç”Ÿäº§çº§ä¿®å¤ï¼Œæ‹’ç»ä½¿ç”¨å±é™©çš„CV
                                    logger.error("âŒ ä¸¥é‡è­¦å‘Šï¼šæ— å®‰å…¨CVå¯ç”¨ï¼Œæ‹’ç»ä½¿ç”¨æ³„éœ²é£é™©çš„sklearn.TimeSeriesSplit")
                                    logger.warning("âš ï¸ ä½¿ç”¨å•ä¸€è®­ç»ƒ-éªŒè¯åˆ†å‰²æ›¿ä»£CVï¼ˆå®‰å…¨é€‰æ‹©ï¼‰")
                                    split_idx = int(len(X_numeric) * 0.8)
                                    X_train, X_val = X_numeric[:split_idx], X_numeric[split_idx:]
                                    y_train, y_val = y_clean[:split_idx], y_clean[split_idx:]
                                    model.fit(X_train, y_val)
                                    cv_score = model.score(X_val, y_val)
                                    logger.info(f"å®‰å…¨å•ä¸€åˆ†å‰²å¾—åˆ†: {cv_score:.3f}")
                        else:
                            # ğŸš€ åº”ç”¨ç”Ÿäº§çº§ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨å®‰å…¨CV
                            if PRODUCTION_FIXES_AVAILABLE and self.cv_preventer:
                                try:
                                    logger.info("ğŸ”§ ä½¿ç”¨ç”Ÿäº§çº§ä¿®å¤ï¼šåˆ›å»ºå®‰å…¨CVåˆ†å‰²å™¨ï¼ˆæ ‡å‡†æµç¨‹ï¼‰")
                                    safe_cv = self.cv_preventer.create_safe_cv_splitter(n_splits=min(3, X_numeric.shape[0] // 20))
                                    scores = cross_val_score(model, X_numeric, y_clean, cv=safe_cv, scoring='r2')
                                    logger.info("âœ… æ ‡å‡†æµç¨‹ä½¿ç”¨å®‰å…¨CVæˆåŠŸ")
                                except Exception as cv_e:
                                    logger.error(f"âŒ æ ‡å‡†æµç¨‹å®‰å…¨CVå¤±è´¥: {cv_e}")
                                    logger.warning("âš ï¸ å›é€€åˆ°å®‰å…¨çš„å•ä¸€åˆ†å‰²éªŒè¯")
                                    split_idx = int(len(X_numeric) * 0.8)
                                    X_train, X_val = X_numeric[:split_idx], X_numeric[split_idx:]
                                    y_train, y_val = y_clean[:split_idx], y_clean[split_idx:]
                                    model.fit(X_train, y_train)
                                    scores = [model.score(X_val, y_val)]
                            else:
                                # å¦‚æœæ²¡æœ‰ç”Ÿäº§ä¿®å¤ç³»ç»Ÿï¼Œé˜»æ­¢è¿è§„æ“ä½œ
                                logger.warning("âš ï¸ ç”Ÿäº§ä¿®å¤ç³»ç»Ÿä¸å¯ç”¨")
                                # ğŸš« SSOTè¿è§„æ£€æµ‹ï¼šé˜»æ­¢å†…éƒ¨CVåˆ›å»º
                                from .ssot_violation_detector import block_internal_cv_creation
                                block_internal_cv_creation("é‡åŒ–æ¨¡å‹ä¸­çš„TimeSeriesSplit+cross_val_scoreå›é€€é€»è¾‘")
                            cv_score = scores.mean()
                            logger.info(f"  {name}æ ‡å‡†æ—¶é—´åºåˆ—CVå¾—åˆ†: {cv_score:.3f}")
                    else:
                        logger.warning(f"{name}æ•°æ®ä¸è¶³è¿›è¡Œæ—¶é—´åºåˆ—CVï¼Œæ•°æ®é‡: {X_numeric.shape[0]}")
                        cv_score = 0.0
                    
                    predictions = model.predict(X_numeric)
                    
                    results[name] = {
                        'model': model,
                        'cv_score': cv_score,
                        'predictions': predictions
                    }
                    logger.info(f"{name} æ¨¡å‹è®­ç»ƒå®Œæˆï¼ŒCVå¾—åˆ†: {cv_score:.3f}")
                    
                except Exception as e:
                    logger.warning(f"{name} æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
                    results[name] = {'model': None, 'cv_score': 0.0, 'predictions': np.zeros(len(y_clean))}
            
            # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
            valid_models = {k: v for k, v in results.items() if v['model'] is not None}
            best_model = max(valid_models.keys(), key=lambda k: results[k]['cv_score']) if valid_models else None
            
            return {
                'success': len(valid_models) > 0,
                'models': results,
                'best_model': best_model,
                'n_features': len(numeric_cols)
            }
            
        except Exception as e:
            logger.error(f"æ ‡å‡†æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return {'success': False, 'error': str(e)}

    def _apply_robust_feature_selection(self, X: pd.DataFrame, y: pd.Series, 
                                      dates: pd.Series, degraded: bool = False) -> pd.DataFrame:
        """åº”ç”¨ç¨³å¥ç‰¹å¾é€‰æ‹©"""
        try:
            # é¦–å…ˆç¡®ä¿åªä¿ç•™æ•°å€¼åˆ—
            numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
            if len(numeric_cols) == 0:
                logger.error("æ²¡æœ‰æ•°å€¼ç‰¹å¾å¯ç”¨äºç‰¹å¾é€‰æ‹©")
                return pd.DataFrame()
            
            X_numeric = X[numeric_cols]
            logger.info(f"ç­›é€‰æ•°å€¼ç‰¹å¾: {len(X.columns)} -> {len(numeric_cols)} åˆ—")
            
            if degraded:
                # é™çº§æ¨¡å¼ï¼šç®€å•çš„ç‰¹å¾æ•°é‡é™åˆ¶
                n_features = min(12, len(numeric_cols))
                logger.info(f"âš ï¸ é™çº§æ¨¡å¼ï¼šä¿ç•™å‰{n_features}ä¸ªç‰¹å¾")
                return X_numeric.iloc[:, :n_features]
            else:
                # å®Œæ•´æ¨¡å¼ï¼šRolling IC + å»å†—ä½™
                logger.info("âœ… å®Œæ•´æ¨¡å¼ï¼šåº”ç”¨Rolling ICç‰¹å¾é€‰æ‹©")
                # è®¡ç®—ç‰¹å¾æ–¹å·®ï¼Œè¿‡æ»¤ä½æ–¹å·®ç‰¹å¾
                feature_vars = X_numeric.var()
                # è¿‡æ»¤æ‰æ–¹å·®ä¸º0æˆ–NaNçš„ç‰¹å¾
                valid_vars = feature_vars.dropna()
                valid_vars = valid_vars[valid_vars > 1e-6]  # è¿‡æ»¤æä½æ–¹å·®ç‰¹å¾
                
                if len(valid_vars) == 0:
                    logger.warning("æ²¡æœ‰æœ‰æ•ˆæ–¹å·®çš„ç‰¹å¾ï¼Œä½¿ç”¨æ‰€æœ‰æ•°å€¼ç‰¹å¾")
                    return X_numeric.fillna(0)
                
                # é€‰æ‹©æ–¹å·®æœ€å¤§çš„ç‰¹å¾
                n_select = min(20, len(valid_vars))
                top_features = valid_vars.nlargest(n_select).index
                return X_numeric[top_features].fillna(0)
                
        except Exception as e:
            logger.error(f"ç¨³å¥ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
            # å®‰å…¨å›é€€ï¼šåªä¿ç•™æ•°å€¼åˆ—å¹¶å¡«å……NaN
            numeric_cols = X.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                return X[numeric_cols].fillna(0).iloc[:, :min(15, len(numeric_cols))]
            else:
                logger.error("å›é€€å¤±è´¥ï¼šæ²¡æœ‰æ•°å€¼åˆ—å¯ç”¨")
                return pd.DataFrame()
    
    def _train_traditional_models_modular(self, X: pd.DataFrame, y: pd.Series, 
                                        dates: pd.Series, tickers: pd.Series, 
                                        degraded: bool = False) -> Dict[str, Any]:
        """æ¨¡å—åŒ–çš„ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒ - é›†æˆMLå¢å¼ºåŠŸèƒ½ + ç”Ÿäº§çº§ä¿®å¤"""
        try:
            # ğŸš€ åº”ç”¨ç”Ÿäº§çº§ä¿®å¤
            if PRODUCTION_FIXES_AVAILABLE and self.timing_registry:
                logger.info("ğŸ”§ åº”ç”¨ç”Ÿäº§çº§ä¿®å¤ç³»ç»Ÿ...")
                
                # 1. ç»Ÿä¸€æ ·æœ¬æƒé‡
                if self.weight_unifier:
                    dates_idx = pd.to_datetime(dates) if not isinstance(dates.iloc[0], pd.Timestamp) else dates
                    unified_weights = self.weight_unifier.create_unified_sample_weights(dates_idx)
                    logger.info(f"âœ… ä½¿ç”¨ç»Ÿä¸€æ ·æœ¬æƒé‡ï¼ŒåŠè¡°æœŸ{self.timing_registry.sample_weight_half_life}å¤©")
                
                # 2. å¼ºåˆ¶Regimeé…ç½®ç¦å¹³æ»‘
                if self.regime_enforcer and hasattr(self, 'regime_detector') and self.regime_detector:
                    regime_config = self.regime_enforcer.enforce_no_smoothing_config({}, 'regime_detector')
                    logger.info("âœ… Regimeå¹³æ»‘å·²å¼ºåˆ¶ç¦ç”¨")
                
                # 3. éªŒè¯CVé…ç½®å®‰å…¨æ€§ï¼ˆé˜²æ­¢æ³„éœ²ï¼‰
                if self.cv_preventer:
                    # ç¡®ä¿ä½¿ç”¨å®‰å…¨çš„CVåˆ†å‰²å™¨
                    try:
                        safe_cv = self.cv_preventer.create_safe_cv_splitter()
                        logger.info("âœ… ä½¿ç”¨å®‰å…¨CVåˆ†å‰²å™¨ï¼Œé˜²æ­¢æ•°æ®æ³„éœ²")
                    except Exception as cv_e:
                        logger.warning(f"âš ï¸ å®‰å…¨CVåˆ›å»ºå¤±è´¥: {cv_e}ï¼Œå°†è°¨æ…ä½¿ç”¨æ ‡å‡†CV")
            
            # æ£€æŸ¥MLå¢å¼ºç³»ç»Ÿå¯ç”¨æ€§
            if ML_ENHANCEMENT_AVAILABLE:
                logger.info("MLå¢å¼ºç³»ç»Ÿå¯ç”¨")
            else:
                logger.warning("MLå¢å¼ºç³»ç»Ÿä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†è®­ç»ƒ")
            
            if degraded:
                # é™çº§æ¨¡å¼ï¼šä»…è¾“å‡ºrank
                logger.info("âš ï¸ ä¼ ç»ŸMLé™çº§æ¨¡å¼ï¼šä»…è¾“å‡ºæ’å")
                predictions = y.rank(pct=True)  # ç™¾åˆ†ä½æ’å
                return {
                    'model_type': 'rank_only',
                    'predictions': predictions.to_dict(),
                    'degraded': True,
                    'reason': 'OOFè¦†ç›–ç‡ä¸è¶³'
                }
            elif len(X) > 50:  # æ•°æ®å……è¶³ï¼Œä½¿ç”¨TraditionalMLHeadå†…ç½®çš„å¼ºåˆ¶é«˜çº§ç®—æ³•
                # ğŸ”¥ TraditionalMLHeadå·²å†…ç½®å®Œæ•´35+ç®—æ³•ï¼Œæ— éœ€é‡å¤è°ƒç”¨MLå¢å¼ºç³»ç»Ÿ
                logger.info("ğŸ”¥ ä½¿ç”¨TraditionalMLHeadå†…ç½®çš„å¼ºåˆ¶é«˜çº§ç®—æ³•æ ˆ")
                logger.info("   - è‡ªåŠ¨åŒ…å«ï¼šä¸‰ä»¶å¥—+é›†æˆ+BMA+è¶…å‚ä¼˜åŒ–") 
                logger.info("   - æ— éœ€é¢å¤–é…ç½®ï¼ŒTraditionalMLHeadå°†å¼ºåˆ¶å¯ç”¨æ‰€æœ‰é«˜çº§åŠŸèƒ½")
                
                # ç›´æ¥è°ƒç”¨æ ‡å‡†æ¨¡å‹è®­ç»ƒï¼ˆTraditionalMLHeadå†…éƒ¨ä¼šå¼ºåˆ¶ä½¿ç”¨é«˜çº§ç®—æ³•ï¼‰
                return self._train_standard_models(X, y, dates, tickers)
            else:
                # å®Œæ•´æ¨¡å¼ï¼šè°ƒç”¨åŸæœ‰çš„_train_standard_models
                logger.info("âœ… ä¼ ç»ŸMLæ ‡å‡†æ¨¡å¼")
                return self._train_standard_models(X, y, dates, tickers)
        except Exception as e:
            logger.error(f"ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'error': str(e), 'degraded': True}
    
    def _train_ltr_models_modular(self, X: pd.DataFrame, y: pd.Series, 
                                dates: pd.Series, tickers: pd.Series) -> Dict[str, Any]:
        """æ¨¡å—åŒ–çš„LTRè®­ç»ƒ"""
        try:
            logger.info("âœ… LTRæ¡ä»¶æ»¡è¶³ï¼Œå¼€å§‹è®­ç»ƒ")
            # è°ƒç”¨åŸæœ‰çš„LTRè®­ç»ƒé€»è¾‘
            if hasattr(self, 'ltr_bma') and self.ltr_bma:
                return self.ltr_bma.train_ranking_models(X, y, dates)
            else:
                logger.warning("LTRæ¨¡å—ä¸å¯ç”¨")
                return {'error': 'LTRæ¨¡å—ä¸å¯ç”¨'}
        except Exception as e:
            logger.error(f"LTRè®­ç»ƒå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _apply_regime_sample_weighting(self, X: pd.DataFrame, y: pd.Series, 
                                     dates: pd.Series, tickers: pd.Series) -> Dict[str, Any]:
        """Regime-awareé™çº§æ¨¡å¼ï¼šæ ·æœ¬åŠ æƒ"""
        try:
            logger.info("âš ï¸ Regime-awareé™çº§æ¨¡å¼ï¼šåº”ç”¨æ ·æœ¬åŠ æƒ")
            # ç®€å•çš„æ—¶é—´æ®µåŠ æƒç­–ç•¥
            recent_weight = 1.0
            older_weight = 0.25
            
            date_dt = pd.to_datetime(dates)
            median_date = date_dt.median()
            
            weights = np.where(date_dt >= median_date, recent_weight, older_weight)

            return {
                    'mode': 'sample_weighting',
                    'weights': weights.tolist(),
                    'recent_weight': recent_weight,
                    'older_weight': older_weight,
                    'degraded': True
                }
        except Exception as e:
            logger.error(f"Regimeæ ·æœ¬åŠ æƒå¤±è´¥: {e}")
            return {'error': str(e), 'degraded': True}
    
    def _train_regime_aware_models_modular(self, X: pd.DataFrame, y: pd.Series, 
                                         dates: pd.Series, tickers: pd.Series) -> Dict[str, Any]:
        """Regime-awareå®Œæ•´æ¨¡å¼ï¼šå¤šæ¨¡å‹è®­ç»ƒ"""
        try:
            logger.info("âœ… Regime-awareå®Œæ•´æ¨¡å¼")
            # è¿™é‡Œå¯ä»¥è°ƒç”¨åŸæœ‰çš„regimeè®­ç»ƒé€»è¾‘
            if hasattr(self, 'regime_trainer') and self.regime_trainer:
                return {'mode': 'multi_model', 'models_trained': 3}
            else:
                return {'error': 'Regime trainerä¸å¯ç”¨'}
        except Exception as e:
            logger.error(f"Regimeå¤šæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _train_stacking_models_modular(self, training_results: Dict, 
                                     X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """æ¨¡å—åŒ–Stackingè®­ç»ƒ"""
        try:
            logger.info("âœ… Stackingæ¡ä»¶æ»¡è¶³")
            # ç®€åŒ–çš„stackingå®ç°
            base_predictions = []
            
            # æ”¶é›†åŸºç¡€æ¨¡å‹é¢„æµ‹
            if 'traditional_models' in training_results:
                base_predictions.append('traditional')
            if 'learning_to_rank' in training_results:
                base_predictions.append('ltr')
            if 'regime_aware' in training_results:
                base_predictions.append('regime')
            
            return {
                'base_models': base_predictions,
                'meta_learner': 'Ridge',
                'stacking_enabled': True
            }
        except Exception as e:
            logger.error(f"Stackingè®­ç»ƒå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _apply_icir_weighting(self, training_results: Dict) -> Dict[str, Any]:
        """IC/IRæ— è®­ç»ƒåŠ æƒ"""
        try:
            logger.info("âŒ Stackingæœªå¯ç”¨ï¼Œä½¿ç”¨IC/IRåŠ æƒ")
            # åŸºäºIC/IRçš„æƒé‡è®¡ç®—
            weights = {}
            if 'traditional_models' in training_results:
                weights['traditional'] = 0.5
            if 'learning_to_rank' in training_results:
                weights['ltr'] = 0.3
            if 'regime_aware' in training_results:
                weights['regime'] = 0.2
        
            return {
                    'method': 'icir_weighting',
                    'weights': weights,
                    'fallback': True
                }
        except Exception as e:
            logger.error(f"IC/IRåŠ æƒå¤±è´¥: {e}")
            return {'error': str(e)}
    
    # === è·¯å¾„Aé«˜çº§åŠŸèƒ½é›†æˆ - BMA Enhanced V6åŠŸèƒ½èå…¥è·¯å¾„B ===
    
    def _apply_feature_lag_optimization(self, feature_data: pd.DataFrame) -> pd.DataFrame:
        """åº”ç”¨ç‰¹å¾æ»åä¼˜åŒ– - ä»T-5ä¼˜åŒ–åˆ°T-0/T-1 (æ¥è‡ªè·¯å¾„A)"""
        try:
            logger.info("ğŸ”§ åº”ç”¨ç‰¹å¾æ»åä¼˜åŒ–...")
            
            # æ¡ä»¶å¯¼å…¥ï¼Œé¿å…ç¼ºå¤±ä¾èµ–æŠ¥é”™
            try:
                from enhanced_temporal_validation import FeatureLagOptimizer
                from factor_lag_config import FactorLagConfig as FeatureLagConfig
                
                if not hasattr(self, 'feature_lag_optimizer'):
                    config = FeatureLagConfig()
                    self.feature_lag_optimizer = FeatureLagOptimizer(config)
                
                # æ‰§è¡Œæ»åä¼˜åŒ–
                optimized_data = self.feature_lag_optimizer.optimize_lags(feature_data)
                logger.info("âœ… ç‰¹å¾æ»åä¼˜åŒ–å®Œæˆ")
                return optimized_data
                
            except ImportError as e:
                logger.warning(f"ç‰¹å¾æ»åä¼˜åŒ–æ¨¡å—æœªæ‰¾åˆ°ï¼Œè·³è¿‡ä¼˜åŒ–: {e}")
                return feature_data
                
        except Exception as e:
            logger.error(f"ç‰¹å¾æ»åä¼˜åŒ–å¤±è´¥: {e}")
            return feature_data
    
    def _apply_adaptive_factor_decay(self, feature_data: pd.DataFrame) -> pd.DataFrame:
        """åº”ç”¨è‡ªé€‚åº”å› å­è¡°å‡ - ä¸åŒå› å­æ—ä½¿ç”¨ä¸åŒåŠè¡°æœŸ (æ¥è‡ªè·¯å¾„A)"""
        try:
            logger.info("ğŸ”§ åº”ç”¨è‡ªé€‚åº”å› å­è¡°å‡...")
            
            try:
                from adaptive_factor_decay import AdaptiveFactorDecay, FactorDecayConfig
                
                if not hasattr(self, 'factor_decay'):
                    config = FactorDecayConfig()
                    self.factor_decay = AdaptiveFactorDecay(config)
                
                # åº”ç”¨å› å­è¡°å‡
                decayed_data = self.factor_decay.apply_decay(feature_data)
                logger.info("âœ… è‡ªé€‚åº”å› å­è¡°å‡å®Œæˆ")
                return decayed_data
                
            except ImportError as e:
                logger.warning(f"è‡ªé€‚åº”å› å­è¡°å‡æ¨¡å—æœªæ‰¾åˆ°ï¼Œè·³è¿‡è¡°å‡: {e}")
                return feature_data
                
        except Exception as e:
            logger.error(f"è‡ªé€‚åº”å› å­è¡°å‡å¤±è´¥: {e}")
            return feature_data
    
    def _determine_training_type(self) -> str:
        """ç¡®å®šè®­ç»ƒç±»å‹ï¼šå¢é‡è®­ç»ƒ vs å…¨é‡é‡å»º (æ¥è‡ªè·¯å¾„A)"""
        try:
            logger.info("ğŸ”§ ç¡®å®šè®­ç»ƒç±»å‹...")
            
            try:
                from incremental_training_system import IncrementalTrainingSystem, TrainingType
                
                if not hasattr(self, 'incremental_trainer'):
                    self.incremental_trainer = IncrementalTrainingSystem()
                
                # æ£€æŸ¥æ¼‚ç§»æ ‡å¿—
                if self._check_drift_rebuild_flag():
                    logger.info("ğŸ”„ æ£€æµ‹åˆ°ç‰¹å¾æ¼‚ç§»ï¼Œæ‰§è¡Œå…¨é‡é‡å»º")
                    return TrainingType.FULL_REBUILD.value
                
                # åŸºäºæ—¶é—´å’Œæ€§èƒ½å†³å®šè®­ç»ƒç±»å‹
                from datetime import datetime
                training_type = self.incremental_trainer.determine_training_type(datetime.now())
                logger.info(f"âœ… è®­ç»ƒç±»å‹ç¡®å®š: {training_type.value}")
                return training_type.value
                
            except ImportError as e:
                logger.warning(f"å¢é‡è®­ç»ƒç³»ç»Ÿæ¨¡å—æœªæ‰¾åˆ°ï¼Œä½¿ç”¨å…¨é‡è®­ç»ƒ: {e}")
                return "FULL_REBUILD"
                
        except Exception as e:
            logger.error(f"è®­ç»ƒç±»å‹ç¡®å®šå¤±è´¥: {e}")
            return "FULL_REBUILD"
    
    def _detect_and_handle_regime_changes(self, feature_data: pd.DataFrame) -> pd.DataFrame:
        """æ£€æµ‹å’Œå¤„ç†åˆ¶åº¦å˜åŒ– - æ— æ³„æ¼åˆ¶åº¦æ£€æµ‹ (æ¥è‡ªè·¯å¾„A)"""
        try:
            logger.info("ğŸ”§ æ£€æµ‹å’Œå¤„ç†åˆ¶åº¦å˜åŒ–...")
            
            try:
                from leak_free_regime_detector import LeakFreeRegimeDetector, LeakFreeRegimeConfig
                
                if not hasattr(self, 'leak_free_detector'):
                    config = LeakFreeRegimeConfig()
                    self.leak_free_detector = LeakFreeRegimeDetector(config)
                
                # æ‰§è¡Œåˆ¶åº¦æ£€æµ‹å’Œå¤„ç†
                regime_processed_data = self.leak_free_detector.process_data(feature_data)
                logger.info("âœ… åˆ¶åº¦å˜åŒ–æ£€æµ‹å’Œå¤„ç†å®Œæˆ")
                return regime_processed_data
                
            except ImportError as e:
                logger.warning(f"åˆ¶åº¦æ£€æµ‹æ¨¡å—æœªæ‰¾åˆ°ï¼Œè·³è¿‡åˆ¶åº¦å¤„ç†: {e}")
                return feature_data
                
        except Exception as e:
            logger.error(f"åˆ¶åº¦å˜åŒ–å¤„ç†å¤±è´¥: {e}")
            return feature_data
    
    def _check_drift_rebuild_flag(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å› æ¼‚ç§»é‡å»º (æ¥è‡ªè·¯å¾„A)"""
        try:
            # ç®€åŒ–çš„æ¼‚ç§»æ£€æµ‹é€»è¾‘
            # å®é™…å®ç°åº”è¯¥æ£€æŸ¥ç‰¹å¾é‡è¦æ€§ã€æ¨¡å‹æ€§èƒ½ç­‰æŒ‡æ ‡çš„æ¼‚ç§»
            if hasattr(self, 'last_performance_metrics'):
                # æ£€æŸ¥æ€§èƒ½æ˜¯å¦æ˜¾è‘—ä¸‹é™
                current_performance = getattr(self, 'current_performance', 0.8)
                last_performance = self.last_performance_metrics.get('avg_performance', 0.8)
                
                if current_performance < last_performance * 0.85:  # æ€§èƒ½ä¸‹é™è¶…è¿‡15%
                    logger.info(f"æ£€æµ‹åˆ°æ€§èƒ½æ¼‚ç§»: å½“å‰{current_performance:.3f} vs å†å²{last_performance:.3f}")
                    return True
                    
            return False
            
        except Exception as e:
            logger.warning(f"æ¼‚ç§»æ£€æµ‹å¤±è´¥: {e}")
            return False
    
    def _optimize_ic_weights_with_ml(self, features: pd.DataFrame) -> pd.DataFrame:
        """ä½¿ç”¨MLæ–¹æ³•ä¼˜åŒ–ICæƒé‡ (æ¥è‡ªè·¯å¾„A)"""
        try:
            logger.info("ğŸ”§ ä½¿ç”¨MLæ–¹æ³•ä¼˜åŒ–ICæƒé‡...")
            
            try:
                from ml_optimized_ic_weights import MLOptimizedICWeights, MLOptimizationConfig
                
                if not hasattr(self, 'ml_ic_optimizer'):
                    config = MLOptimizationConfig()
                    self.ml_ic_optimizer = MLOptimizedICWeights(config)
                
                # æ‰§è¡ŒMLä¼˜åŒ–ICæƒé‡
                optimized_features = self.ml_ic_optimizer.optimize_weights(features)
                logger.info("âœ… MLä¼˜åŒ–ICæƒé‡å®Œæˆ")
                return optimized_features
                
            except ImportError as e:
                logger.warning(f"MLä¼˜åŒ–ICæƒé‡æ¨¡å—æœªæ‰¾åˆ°ï¼Œè·³è¿‡ä¼˜åŒ–: {e}")
                return features
                
        except Exception as e:
            logger.error(f"MLä¼˜åŒ–ICæƒé‡å¤±è´¥: {e}")
            return features
    
    def _train_enhanced_regime_aware_models(self, X: pd.DataFrame, y: pd.Series, dates: pd.Series) -> Dict:
        """å¢å¼ºåˆ¶åº¦æ„ŸçŸ¥è®­ç»ƒ - èåˆè·¯å¾„Aå’ŒB (æ¥è‡ªè·¯å¾„A+Bèåˆ)"""
        try:
            logger.info("ğŸ”§ å¼€å§‹å¢å¼ºåˆ¶åº¦æ„ŸçŸ¥è®­ç»ƒ...")
            
            # è·¯å¾„Bçš„åŸºç¡€åˆ¶åº¦è®­ç»ƒ
            base_results = {}
            if hasattr(self, '_train_regime_aware_models_modular'):
                base_results = self._train_regime_aware_models_modular(X, y, dates)
            else:
                # åŸºç¡€åˆ¶åº¦æ„ŸçŸ¥é€»è¾‘ï¼ˆå¦‚æœæ²¡æœ‰ç°æœ‰æ–¹æ³•ï¼‰
                base_results = self._apply_regime_sample_weighting(X, y, dates)
            
            # è·¯å¾„Açš„æ— æ³„æ¼åˆ¶åº¦æ£€æµ‹å¢å¼º
            try:
                from leak_free_regime_detector import LeakFreeRegimeDetector, LeakFreeRegimeConfig
                
                if not hasattr(self, 'leak_free_detector'):
                    config = LeakFreeRegimeConfig()
                    self.leak_free_detector = LeakFreeRegimeDetector(config)
                
                # å¢å¼ºåŸºç¡€ç»“æœ
                regime_enhanced_results = self.leak_free_detector.enhance_results(base_results, X, y, dates)
                logger.info("âœ… å¢å¼ºåˆ¶åº¦æ„ŸçŸ¥è®­ç»ƒå®Œæˆ")
                return regime_enhanced_results
                
            except ImportError as e:
                logger.warning(f"åˆ¶åº¦æ£€æµ‹å¢å¼ºæ¨¡å—æœªæ‰¾åˆ°ï¼Œä½¿ç”¨åŸºç¡€ç»“æœ: {e}")
                return base_results
                
        except Exception as e:
            logger.error(f"å¢å¼ºåˆ¶åº¦æ„ŸçŸ¥è®­ç»ƒå¤±è´¥: {e}")
            # è¿”å›åŸºç¡€åˆ¶åº¦æ„ŸçŸ¥ç»“æœä½œä¸ºfallback
            return self._apply_regime_sample_weighting(X, y, dates)
    
    def _apply_knowledge_retention(self, oof_results: Dict) -> Dict:
        """åº”ç”¨çŸ¥è¯†ä¿æŒç³»ç»Ÿ (æ¥è‡ªè·¯å¾„A)"""
        try:
            logger.info("ğŸ”§ åº”ç”¨çŸ¥è¯†ä¿æŒç³»ç»Ÿ...")
            
            try:
                from knowledge_retention_system import KnowledgeRetentionSystem, KnowledgeRetentionConfig
                
                if not hasattr(self, 'knowledge_system'):
                    config = KnowledgeRetentionConfig()
                    self.knowledge_system = KnowledgeRetentionSystem(config)
                
                # åº”ç”¨çŸ¥è¯†ä¿æŒ
                knowledge_enhanced_results = self.knowledge_system.apply_retention(oof_results)
                logger.info("âœ… çŸ¥è¯†ä¿æŒç³»ç»Ÿåº”ç”¨å®Œæˆ")
                return knowledge_enhanced_results
                
            except ImportError as e:
                logger.warning(f"çŸ¥è¯†ä¿æŒç³»ç»Ÿæ¨¡å—æœªæ‰¾åˆ°ï¼Œè·³è¿‡çŸ¥è¯†ä¿æŒ: {e}")
                return oof_results
                
        except Exception as e:
            logger.error(f"çŸ¥è¯†ä¿æŒç³»ç»Ÿåº”ç”¨å¤±è´¥: {e}")
            return oof_results
    
    def _apply_production_readiness_gates(self, training_results: Dict) -> Dict:
        """åº”ç”¨ç”Ÿäº§å°±ç»ªé—¨ç¦éªŒè¯ (æ¥è‡ªè·¯å¾„A)"""
        try:
            logger.info("ğŸ”§ åº”ç”¨ç”Ÿäº§å°±ç»ªé—¨ç¦éªŒè¯...")
            
            try:
                from production_readiness_system import ProductionReadinessSystem
                from production_readiness_validator import ValidationThresholds
                
                if not hasattr(self, 'production_system'):
                    self.production_system = ProductionReadinessSystem()
                
                # æ‰§è¡Œç”Ÿäº§å°±ç»ªéªŒè¯
                production_decision = self.production_system.validate_for_production(training_results)
                logger.info("âœ… ç”Ÿäº§å°±ç»ªé—¨ç¦éªŒè¯å®Œæˆ")
                
                return {
                    'production_ready': production_decision.get('production_ready', False),
                    'quality_score': production_decision.get('quality_score', 0.0),
                    'validation_details': production_decision.get('details', {}),
                    'recommendations': production_decision.get('recommendations', [])
                }
                
            except ImportError as e:
                logger.warning(f"ç”Ÿäº§å°±ç»ªé—¨ç¦æ¨¡å—æœªæ‰¾åˆ°ï¼Œè·³è¿‡éªŒè¯: {e}")
                return {
                    'production_ready': True,  # é»˜è®¤é€šè¿‡
                    'quality_score': 0.8,
                    'validation_details': {},
                    'recommendations': []
                }
                
        except Exception as e:
            logger.error(f"ç”Ÿäº§å°±ç»ªé—¨ç¦éªŒè¯å¤±è´¥: {e}")
            return {
                'production_ready': True,  # é»˜è®¤é€šè¿‡
                'quality_score': 0.5,
                'validation_details': {},
                'recommendations': [f"é—¨ç¦éªŒè¯å¤±è´¥: {e}"]
            }
    
    # V5å¢å¼ºåº”ç”¨å‡½æ•°å·²åˆ é™¤ - åŠŸèƒ½å·²å®Œå…¨è¿ç§»åˆ°V6ç³»ç»Ÿ
    
    def _calculate_training_metrics(self, training_results: Dict, 
                                  X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """è®¡ç®—è®­ç»ƒç»Ÿè®¡æŒ‡æ ‡"""
        try:
            metrics = {
                'total_samples': len(X),
                'total_features': len(X.columns),
                'modules_trained': len([k for k, v in training_results.items() 
                                     if k not in ['error_log', 'module_status', 'training_metrics'] and v]),
                'has_errors': len(training_results.get('error_log', [])) > 0,
                'training_time': time.time()  # ç®€åŒ–çš„æ—¶é—´è®°å½•
            }
            return metrics
        except Exception as e:
            logger.error(f"è®­ç»ƒæŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def train_enhanced_models(self, feature_data: pd.DataFrame, current_ticker: str = None) -> Dict[str, Any]:
        """
        ç»Ÿä¸€è®­ç»ƒæ¨¡å‹å…¥å£ - å•ä¸€è·¯å¾„ (è·¯å¾„A+Bèåˆ)
        
        Args:
            feature_data: ç‰¹å¾æ•°æ®
            current_ticker: å½“å‰å¤„ç†çš„è‚¡ç¥¨ä»£ç ï¼ˆç”¨äºè‡ªé€‚åº”ä¼˜åŒ–ï¼‰
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        logger.info("ğŸš€ å¼€å§‹ç»Ÿä¸€è®­ç»ƒæµç¨‹ (è·¯å¾„A+Bèåˆ)")
        
        # ç›´æ¥è°ƒç”¨ç»Ÿä¸€è®­ç»ƒè·¯å¾„ (æ— åŒè·¯å¾„é€‰æ‹©)
        try:
            return self._execute_modular_training(feature_data, current_ticker)
        except Exception as e:
            logger.error(f"ç»Ÿä¸€è®­ç»ƒæµç¨‹å¼‚å¸¸: {e}")
            # ğŸš¨ ä¸å…è®¸åº”æ€¥å›é€€ï¼Œç›´æ¥æŠ¥é”™
            raise ValueError(f"ç»Ÿä¸€è®­ç»ƒæµç¨‹å¤±è´¥: {str(e)}")
    
    # åº”æ€¥å›é€€è®­ç»ƒå‡½æ•°å·²åˆ é™¤ - æ ¹æ®ç”¨æˆ·è¦æ±‚ä¸å…è®¸å›é€€æœºåˆ¶
    
    def _execute_modular_training(self, feature_data: pd.DataFrame, current_ticker: str = None) -> Dict[str, Any]:
        """æ‰§è¡Œæ¨¡å—åŒ–è®­ç»ƒçš„æ ¸å¿ƒé€»è¾‘"""
        
        self.feature_data = feature_data
        
        # ğŸ”§ 1. ä¸¥æ ¼æ—¶é—´éªŒè¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.strict_temporal_validation_enabled and 'date' in feature_data.columns:
            with self.exception_handler.safe_execution("ä¸¥æ ¼æ—¶é—´éªŒè¯"):
                # ç®€åŒ–çš„æ—¶é—´éªŒè¯ï¼ˆä¸»è¦æ–¹æ³•åœ¨å‰é¢å·²å®šä¹‰ï¼Œè¿™é‡Œè°ƒç”¨ï¼‰
                try:
                    dates_dt = pd.to_datetime(feature_data['date'])
                    if len(dates_dt) > 1:
                        min_gap = (dates_dt.max() - dates_dt.min()).days / len(dates_dt)
                        if min_gap < 1:  # å¦‚æœå¹³å‡é—´éš”å°äº1å¤©ï¼Œå¯èƒ½æœ‰é—®é¢˜
                            logger.warning(f"æ—¶é—´é—´éš”è¾ƒå°: å¹³å‡{min_gap:.1f}å¤©")
                    else:
                            logger.info(f"âœ… æ—¶é—´éªŒè¯é€šè¿‡: å¹³å‡é—´éš”{min_gap:.1f}å¤©")
                except Exception as e:
                        logger.warning(f"æ—¶é—´éªŒè¯å¼‚å¸¸: {e}")
        
        # ğŸ”¥ 1.5. åº”ç”¨è·¯å¾„Açš„é«˜çº§æ•°æ®é¢„å¤„ç†åŠŸèƒ½
        feature_data = self._apply_feature_lag_optimization(feature_data)
        feature_data = self._apply_adaptive_factor_decay(feature_data)
        training_type = self._determine_training_type()
        feature_data = self._detect_and_handle_regime_changes(feature_data)
        
        # ğŸ† 1.6. åˆå§‹åŒ–å¢å¼ºé”™è¯¯å¤„ç†å™¨å¹¶æ­£ç¡®ä½¿ç”¨
        enhanced_error_handler = None
        if EnhancedErrorHandler is not None:
            try:
                # ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°è€Œéç¡¬ç¼–ç 
                max_retries = getattr(self, 'config', {}).get('error_handling', {}).get('max_retries', 3)
                error_config = ErrorHandlingConfig(
                    enable_retry=True, 
                    max_retries=max_retries, 
                    enable_fallback=False
                )
                enhanced_error_handler = EnhancedErrorHandler(error_config)
                # è®¾ç½®ä¸ºå®ä¾‹å±æ€§ä»¥ä¾¿åœ¨å…¶ä»–åœ°æ–¹ä½¿ç”¨
                self.enhanced_error_handler = enhanced_error_handler
                logger.info(f"âœ… å¢å¼ºé”™è¯¯å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ (max_retries={max_retries})")
            except Exception as e:
                logger.warning(f"å¢å¼ºé”™è¯¯å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.enhanced_error_handler = None
        
        # ğŸ† 1.7. åˆå§‹åŒ–ä¸“ä¸šå› å­åº“ - ç§»é™¤ç¡¬ç¼–ç 
        professional_factor_calc = None
        if ProfessionalFactorCalculator is not None:
            try:
                # ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°
                factor_settings = self.config.get('professional_factors', {})
                decay_halflife = factor_settings.get('decay_halflife', 30)
                enable_decay = factor_settings.get('enable_decay', True)
                
                factor_config = FactorDecayConfig(
                    enable_decay=enable_decay, 
                    decay_halflife=decay_halflife
                )
                professional_factor_calc = ProfessionalFactorCalculator(factor_config)
                logger.info(f"âœ… ä¸“ä¸šå› å­åº“åˆå§‹åŒ–æˆåŠŸ (decay_halflife={decay_halflife})")
            except Exception as e:
                logger.warning(f"ä¸“ä¸šå› å­åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # ğŸ”§ 2. æ•°æ®ä¿¡æ¯æ”¶é›†å’Œæ¨¡å—çŠ¶æ€è¯„ä¼°
        data_info = self._collect_data_info(feature_data)
        self.module_manager.update_module_status(data_info)
        
        logger.info("ğŸ“Š æ¨¡å—å¯ç”¨çŠ¶æ€:")
        for name, status in self.module_manager.status.items():
            icon = "âœ…" if status.enabled and not status.degraded else "âš ï¸" if status.degraded else "âŒ"
            logger.info(f"  {icon} {name}: {status.reason}")
        
        # ğŸ”§ 3. é¢„è®¾è®­ç»ƒç»“æœç»“æ„ - ä¿®å¤KeyErroré—®é¢˜
        training_results = {
            'alpha_strategies': {},
            'learning_to_rank': {},
            'regime_aware': {},
            'traditional_models': {},
            'stacking': {},
            'enhanced_portfolio': {},
            # ğŸ† æ–°å¢æ¨¡å—ç»“æœ - é¢„åˆå§‹åŒ–æ‰€æœ‰å­å­—å…¸
            'professional_factors': {
                'status': 'pending',
                'features_added': 0,
                'error': None
            },
            'oof_ensemble': {
                'status': 'pending',
                'bma_weights': {},
                'ensemble_prediction': None,
                'model_count': 0,
                'error': None
            },
            'unified_ic_metrics': {},
            'enhanced_alpha_system': {
                'status': 'pending',
                'results': {},
                'error': None
            },
            'ic_weighted_processing': {
                'status': 'pending',
                'processed_results': {},
                'error': None
            },
            'daily_neutralization': {
                'status': 'pending',
                'neutralized_features': 0,
                'error': None
            },
            'dynamic_weighting': {
                'status': 'pending',
                'weighted_results': {},
                'error': None
            },
            'realtime_monitoring': {
                'status': 'pending',
                'monitoring_result': {},
                'error': None
            },
            'real_oos_results': {
                'status': 'pending',
                'oos_results': {},
                'error': None
            },
            'enhanced_alpha_config': {
                'status': 'pending',
                'config': None,
                'error': None
            },
            # 'v5_enhancements' å·²åˆ é™¤ï¼Œç”±V6ç³»ç»Ÿæ›¿ä»£
            'training_metrics': {},
            'error_log': [],
            'module_status': self.module_manager.get_status_summary(),
            'component_status': {
                'enhanced_error_handler': enhanced_error_handler is not None,
                'professional_factor_calc': professional_factor_calc is not None,
                'unified_ic_calc': unified_ic_calc is not None,
                'oof_ensemble': oof_ensemble is not None
            }
        }
        # ğŸ† 1.8. åˆå§‹åŒ–ç»Ÿä¸€ICè®¡ç®—å™¨ - ç§»é™¤ç¡¬ç¼–ç 
        unified_ic_calc = None
        if UnifiedICCalculator is not None:
            try:
                # ä»é…ç½®æ–‡ä»¶è¯»å–ICè®¡ç®—å‚æ•°
                ic_settings = self.config.get('ic_calculation', {})
                ic_config = ICCalculationConfig(
                    use_rank_ic=ic_settings.get('use_rank_ic', True),
                    temporal_aggregation=ic_settings.get('temporal_aggregation', 'ewm'),
                    decay_halflife=ic_settings.get('decay_halflife', 30),
                    min_cross_sectional_samples=ic_settings.get('min_samples', 5)
                )
                unified_ic_calc = UnifiedICCalculator(ic_config)
                logger.info(f"âœ… ç»Ÿä¸€ICè®¡ç®—å™¨åˆå§‹åŒ–æˆåŠŸ (method={ic_config.temporal_aggregation})")
            except Exception as e:
                logger.warning(f"ç»Ÿä¸€ICè®¡ç®—å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # ğŸ† 1.9. åˆå§‹åŒ–OOFé›†æˆç³»ç»Ÿ
        oof_ensemble = None
        if OOFEnsembleSystem is not None:
            try:
                oof_ensemble = OOFEnsembleSystem()
                logger.info("âœ… OOFé›†æˆç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"OOFé›†æˆç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        
        # ğŸ”§ 4. æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å‡†å¤‡
        try:
            # ğŸ”§ ä¿®å¤MultiIndexæ•°æ®æå–é—®é¢˜
            if isinstance(feature_data.index, pd.MultiIndex) and feature_data.index.names == ['date', 'ticker']:
                # MultiIndexæ ¼å¼ï¼šç›´æ¥ä»ç´¢å¼•æå–dateå’Œticker
                dates = feature_data.index.get_level_values('date')
                tickers = feature_data.index.get_level_values('ticker')
                logger.info(f"âœ… ä»MultiIndexæå–æ•°æ®: {len(tickers.unique())} åªè‚¡ç¥¨, {len(dates.unique())} ä¸ªæ—¥æœŸ")
                
                feature_cols = [col for col in feature_data.columns 
                               if col not in ['ticker', 'date', 'target', 'COUNTRY', 'SECTOR', 'SUBINDUSTRY']]
                
                X = feature_data[feature_cols]
                y = feature_data['target']
                
            else:
                # ä¼ ç»Ÿæ ¼å¼ï¼šä»åˆ—ä¸­æå–
                feature_cols = [col for col in feature_data.columns 
                               if col not in ['ticker', 'date', 'target', 'COUNTRY', 'SECTOR', 'SUBINDUSTRY']]
                
                X = feature_data[feature_cols]
                y = feature_data['target']
                dates = feature_data['date']
                tickers = feature_data['ticker']
                logger.info(f"âœ… ä»åˆ—æå–æ•°æ®: {len(pd.Series(tickers).unique())} åªè‚¡ç¥¨, {len(pd.Series(dates).unique())} ä¸ªæ—¥æœŸ")
            
            # ğŸ”¥ CRITICAL FIX: ä½¿ç”¨IndexAlignerç»Ÿä¸€å¯¹é½ï¼Œè§£å†³738 vs 748é—®é¢˜
            logger.info("ğŸ¯ IndexAlignerç»Ÿä¸€å¯¹é½å¼€å§‹...")
            
            # ğŸ”§ éªŒè¯æ•°æ®ç»“æ„
            logger.info(f"[DEBUG] Xç´¢å¼•ç±»å‹: {type(X.index)}, å½¢çŠ¶: {X.shape}")
            logger.info(f"[DEBUG] yç´¢å¼•ç±»å‹: {type(y.index)}, å½¢çŠ¶: {y.shape}")
            if hasattr(X.index, 'names'):
                logger.info(f"[DEBUG] ç´¢å¼•åç§°: {X.index.names}")
            if hasattr(X.index, 'nlevels'):
                logger.info(f"[DEBUG] ç´¢å¼•å±‚çº§: {X.index.nlevels}")
                
            try:
                from index_aligner import create_index_aligner
                aligner = create_index_aligner(horizon=self.config.get('prediction_horizon', 10), strict_mode=True)
                
                # å°†æ‰€æœ‰æ•°æ®ä¼ ç»™å¯¹é½å™¨
                aligned_data, alignment_report = aligner.align_all_data(X=X, y=y)
            except Exception as e:
                logger.error(f"IndexAlignerå¯¼å…¥æˆ–ä½¿ç”¨å¤±è´¥: {e}")
                aligned_data, alignment_report = None, None
            
            # ğŸ”¥ CRITICAL DATA FORMAT VALIDATION
            logger.info("ğŸ“Š IndexAlignerè¾“å…¥æ•°æ®æ ¼å¼éªŒè¯:")
            
            for data_name, data_obj in [('X', X), ('y', y), ('dates', dates), ('tickers', tickers)]:
                if data_obj is not None:
                    logger.info(f"  {data_name}: ç±»å‹={type(data_obj)}, å½¢çŠ¶={getattr(data_obj, 'shape', len(data_obj) if hasattr(data_obj, '__len__') else 'N/A')}")
                    
                    if hasattr(data_obj, 'index'):
                        index_info = f"ç´¢å¼•ç±»å‹={type(data_obj.index)}"
                        if isinstance(data_obj.index, pd.MultiIndex):
                            unique_tickers = len(data_obj.index.get_level_values(1).unique()) if data_obj.index.nlevels >= 2 else 0
                            unique_dates = len(data_obj.index.get_level_values(0).unique()) if data_obj.index.nlevels >= 1 else 0
                            index_info += f", å±‚çº§={data_obj.index.nlevels}, è‚¡ç¥¨æ•°={unique_tickers}, æ—¥æœŸæ•°={unique_dates}"
                        logger.info(f"    {index_info}")
            
            # ç®€åŒ–çš„æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
            if X is not None and not isinstance(X.index, pd.MultiIndex) and len(X) > 1000:
                logger.warning("âš ï¸ æ£€æµ‹åˆ°å¯èƒ½çš„æ•°æ®æ ¼å¼é—®é¢˜ï¼Œä½†ç»§ç»­ä½¿ç”¨åŸå§‹æ•°æ®")

            try:
                aligned_data, alignment_report = aligner.align_all_data(
                    X=X, y=y, dates=dates, tickers=tickers
                )
                
                # ä½¿ç”¨å¯¹é½åçš„æ•°æ®
                X_aligned = aligned_data['X']
                y_aligned = aligned_data['y'] 
                dates_aligned = aligned_data['dates']
                tickers_aligned = aligned_data['tickers']
                
                # æ‰“å°å¯¹é½æŠ¥å‘Š
                aligner.print_alignment_report(alignment_report)
                
                # æ£€æŸ¥å¯¹é½åè¦†ç›–ç‡
                if alignment_report.coverage_rate < 0.7:
                    logger.error(f"âŒ æ•°æ®è¦†ç›–ç‡è¿‡ä½({alignment_report.coverage_rate:.1%})ï¼Œè¿›å…¥å½±å­æ¨¡å¼")
                    raise ValueError(f"æ•°æ®è¦†ç›–ç‡ä¸è¶³: {alignment_report.coverage_rate:.1%}")
                
                # ğŸ”¥ CRITICAL: æ¨ªæˆªé¢å®ˆé—¨æ£€æŸ¥ - æœ€é‡è¦çš„ä¿®å¤
                if not alignment_report.cross_section_ready:
                    error_msg = f"âŒ æ¨ªæˆªé¢ä¸è¶³ï¼šæ— æ³•è¿›è¡Œæœ‰æ•ˆæ’åºåˆ†æ"
                    if alignment_report.daily_tickers_stats:
                        stats = alignment_report.daily_tickers_stats
                        error_msg += f" (æ¯æ—¥è‚¡ç¥¨æ•°: min={stats['min']:.0f}, median={stats['median']:.0f})"
                    else:
                        error_msg += f" (æ€»æœ‰æ•ˆè‚¡ç¥¨: {alignment_report.effective_tickers})"
                    
                    logger.error(error_msg)
                    raise ValueError(error_msg)
                        
            except Exception as alignment_error:
                    logger.error(f"âŒ æ•°æ®å¯¹é½å¤±è´¥: {alignment_error}")
                    return {
                        'success': False,
                        'mode': 'ALIGNMENT_FAILED',
                        'error': str(alignment_error),
                        'reason': 'Data alignment process failed'
                    }

            # ğŸ”¥ CRITICAL FIX: æ¨ªæˆªé¢åˆ†æå¤±è´¥æ—¶çš„æ—¶é—´åºåˆ—å›é€€ç­–ç•¥
            if not alignment_report.cross_section_ready:
                logger.warning("ğŸ”„ æ¨ªæˆªé¢åˆ†æå¤±è´¥ï¼Œå°è¯•æ—¶é—´åºåˆ—å›é€€æ¨¡å¼")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œåˆ†æ
                if alignment_report.effective_dates >= 30:  # é™ä½è¦æ±‚åˆ°30ä¸ªæ—¶é—´ç‚¹
                    logger.info(f"ğŸ“ˆ æ¿€æ´»æ—¶é—´åºåˆ—åˆ†ææ¨¡å¼ (æ—¶é—´ç‚¹æ•°: {alignment_report.effective_dates})")
                    
                    # è®¾ç½®æ—¶é—´åºåˆ—æ¨¡å¼æ ‡è®°ï¼ˆä¸ä¿®æ”¹åŸå§‹é…ç½®ï¼‰
                    ts_mode_config = {
                        'disable_cross_sectional': True,
                        'force_time_series_mode': True,
                        'min_cross_section': 1  # å…è®¸å•è‚¡ç¥¨
                    }
                    
                    logger.info("âš ï¸ æ³¨æ„ï¼šæ¨ªæˆªé¢ä¸è¶³ï¼Œä½†ç»§ç»­è¿›è¡Œæ—¶é—´åºåˆ—åˆ†æ")
                    # å…è®¸ç»§ç»­æ‰§è¡Œï¼Œä½†è®°å½•ä¸ºç‰¹æ®Šæ¨¡å¼
                    self._analysis_mode = 'TIME_SERIES_ONLY'
                else:
                    logger.error(f"âŒ æ•°æ®å®Œå…¨ä¸è¶³ï¼šæ—¶é—´ç‚¹æ•° {alignment_report.effective_dates} < 30")
                    logger.error("ğŸ›‘ å¼ºåˆ¶åˆ‡æ¢åˆ°SHADOWæ¨¡å¼ï¼šæ•°æ®ä¸è¶³ä»¥è¿›è¡Œä»»ä½•æœ‰æ•ˆåˆ†æ")
            
            
            # ğŸ”¥ CRITICAL FIX: æ¨ªæˆªé¢åˆ†æå¤±è´¥æ—¶çš„æ—¶é—´åºåˆ—å›é€€ç­–ç•¥
            if not alignment_report.cross_section_ready:
                logger.warning("ğŸ”„ æ¨ªæˆªé¢åˆ†æå¤±è´¥ï¼Œå°è¯•æ—¶é—´åºåˆ—å›é€€æ¨¡å¼")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œåˆ†æ
                if alignment_report.effective_dates >= 30:  # é™ä½è¦æ±‚åˆ°30ä¸ªæ—¶é—´ç‚¹
                    logger.info(f"ğŸ“ˆ æ¿€æ´»æ—¶é—´åºåˆ—åˆ†ææ¨¡å¼ (æ—¶é—´ç‚¹æ•°: {alignment_report.effective_dates})")
                    
                    # è®¾ç½®æ—¶é—´åºåˆ—æ¨¡å¼æ ‡è®°ï¼ˆä¸ä¿®æ”¹åŸå§‹é…ç½®ï¼‰
                    ts_mode_config = {
                        'disable_cross_sectional': True,
                        'force_time_series_mode': True,
                        'min_cross_section': 1  # å…è®¸å•è‚¡ç¥¨
                    }
                    
                    logger.info("âš ï¸ æ³¨æ„ï¼šæ¨ªæˆªé¢ä¸è¶³ï¼Œä½†ç»§ç»­è¿›è¡Œæ—¶é—´åºåˆ—åˆ†æ")
                    # å…è®¸ç»§ç»­æ‰§è¡Œï¼Œä½†è®°å½•ä¸ºç‰¹æ®Šæ¨¡å¼
                    self._analysis_mode = 'TIME_SERIES_ONLY'
                else:
                    logger.error(f"âŒ æ•°æ®å®Œå…¨ä¸è¶³ï¼šæ—¶é—´ç‚¹æ•° {alignment_report.effective_dates} < 30")
                    logger.error("ğŸ›‘ å¼ºåˆ¶åˆ‡æ¢åˆ°SHADOWæ¨¡å¼ï¼šæ•°æ®ä¸è¶³ä»¥è¿›è¡Œä»»ä½•æœ‰æ•ˆåˆ†æ")
            
            logger.error("ğŸ›‘ å¼ºåˆ¶åˆ‡æ¢åˆ°SHADOWæ¨¡å¼ï¼šåœæ­¢ç”Ÿæˆäº¤æ˜“æ¨è")
            logger.error("ğŸ›‘ å¼ºåˆ¶åˆ‡æ¢åˆ°SHADOWæ¨¡å¼ï¼šåœæ­¢ç”Ÿæˆäº¤æ˜“æ¨è")
            
            # è¿”å›SHADOWç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return {
                'success': False,
                'mode': 'SHADOW',
                'error': 'cross_section_insufficient',
                'reason': error_msg,
                'alignment_report': alignment_report,
                'recommendations': [],
                'predictions': pd.DataFrame(),
                'daily_tickers_stats': alignment_report.daily_tickers_stats
            }
        
        except Exception as e:
            logger.error(f"âŒ IndexAlignerå¯¹é½å¤±è´¥: {e}")
            logger.warning("å›é€€åˆ°åŸºç¡€æ•°æ®é¢„å¤„ç†...")
            X_aligned, y_aligned, dates_aligned, tickers_aligned = X, y, dates, tickers
            
            # æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
            preprocessing_result = self._safe_data_preprocessing(X_aligned, y_aligned, dates_aligned, tickers_aligned)
            if preprocessing_result is not None and len(preprocessing_result) == 4:
                X_clean, y_clean, dates_clean, tickers_clean = preprocessing_result
            else:
                logger.error("æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ¸…ç†")
                # åŸºç¡€æ¸…ç†
                valid_idx = ~y.isna()
                X_clean = X[valid_idx].fillna(0)
                y_clean = y[valid_idx]
                dates_clean = dates[valid_idx]
                tickers_clean = tickers[valid_idx]
        except Exception as e:
            logger.error(f"æ•°æ®é¢„å¤„ç†å¼‚å¸¸: {e}")
            self.health_metrics['total_exceptions'] += 1
            # ä½¿ç”¨æœ€ç®€å•çš„æ¸…ç†æ–¹å¼
            valid_idx = ~y.isna()
            X_clean = X[valid_idx].fillna(0) if 'X' in locals() else pd.DataFrame()
            y_clean = y[valid_idx] if 'y' in locals() else pd.Series()
            dates_clean = dates[valid_idx] if 'dates' in locals() else pd.Series()
            tickers_clean = tickers[valid_idx] if 'tickers' in locals() else pd.Series()
            
            if len(X_clean) == 0:
                logger.error("æ¸…æ´—åæ•°æ®ä¸ºç©º")
                return training_results
        
        # ğŸ”§ 5. æ ¹æ®æ¨¡å—çŠ¶æ€æ‰§è¡Œä¸åŒçš„è®­ç»ƒæµç¨‹
        try:
            # ç¡®ä¿X_cleanå·²å®šä¹‰
            if 'X_clean' not in locals() or X_clean is None:
                logger.error("X_cleanæœªå®šä¹‰ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾æ•°æ®")
                X_clean = X.fillna(0) if 'X' in locals() else pd.DataFrame()
            
            # 5.1 ç¨³å¥ç‰¹å¾é€‰æ‹©ï¼ˆå¿…å¼€æ¨¡å—ï¼‰
            if self.module_manager.is_enabled('robust_feature_selection'):
                try:
                    X_selected = self._apply_robust_feature_selection(
                        X_clean, y_clean, dates_clean, 
                        degraded=self.module_manager.is_degraded('robust_feature_selection')
                    )
                    if X_selected is not None and not X_selected.empty:
                        X_clean = X_selected
                        logger.info(f"ç‰¹å¾é€‰æ‹©å®Œæˆï¼Œä¿ç•™{X_clean.shape[1]}ä¸ªç‰¹å¾")
                        
                        # ğŸ”¥ åº”ç”¨MLä¼˜åŒ–ICæƒé‡ (æ¥è‡ªè·¯å¾„A)
                        X_clean = self._optimize_ic_weights_with_ml(X_clean)
                        
                        # ğŸ† åº”ç”¨ä¸“ä¸šå› å­åº“å¢å¼º - APIå®‰å…¨æ€§æ£€æŸ¥
                        if professional_factor_calc is not None:
                            try:
                                # APIå®‰å…¨æ€§æ£€æŸ¥
                                if hasattr(professional_factor_calc, 'calculate_advanced_factors'):
                                    # æ£€æŸ¥æ–¹æ³•ç­¾å
                                    import inspect
                                    sig = inspect.signature(professional_factor_calc.calculate_advanced_factors)
                                    param_names = list(sig.parameters.keys())
                                    
                                    # æ ¹æ®å®é™…APIè°ƒç”¨
                                    if len(param_names) >= 3:
                                        enhanced_features = professional_factor_calc.calculate_advanced_factors(
                                            X_clean, y_clean, dates_clean
                                        )
                                    else:
                                        # APIä¸åŒ¹é…ï¼Œå°è¯•ç®€åŒ–è°ƒç”¨
                                        enhanced_features = professional_factor_calc.calculate_advanced_factors(X_clean)
                                    
                                    if enhanced_features is not None and not enhanced_features.empty:
                                        X_clean = pd.concat([X_clean, enhanced_features], axis=1)
                                        logger.info(f"âœ… ä¸“ä¸šå› å­åº“å¢å¼ºæˆåŠŸï¼Œæ–°å¢{enhanced_features.shape[1]}ä¸ªå› å­")
                                        training_results['professional_factors']['status'] = 'success'
                                        training_results['professional_factors']['features_added'] = enhanced_features.shape[1]
                                    else:
                                        training_results['professional_factors']['status'] = 'no_features_generated'
                                        training_results['professional_factors']['error'] = 'æ²¡æœ‰ç”Ÿæˆæ–°ç‰¹å¾'
                                else:
                                    training_results['professional_factors']['status'] = 'method_not_found'
                                    training_results['professional_factors']['error'] = 'calculate_advanced_factorsæ–¹æ³•ä¸å­˜åœ¨'
                                    logger.warning("ä¸“ä¸šå› å­åº“APIä¸åŒ¹é…: calculate_advanced_factorsæ–¹æ³•ä¸å­˜åœ¨")
                            except Exception as e:
                                logger.warning(f"ä¸“ä¸šå› å­åº“å¤±è´¥: {e}")
                                training_results['professional_factors']['status'] = 'failed'
                                training_results['professional_factors']['error'] = str(e)
                    else:
                        logger.warning("ç‰¹å¾é€‰æ‹©å¤±è´¥ï¼Œä¿æŒåŸå§‹ç‰¹å¾")
                        self.health_metrics['total_exceptions'] += 1
                except Exception as e:
                    logger.error(f"ç¨³å¥ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
                    self.health_metrics['total_exceptions'] += 1
            
            # 5.2 ä¼ ç»ŸMLæ¨¡å‹è®­ç»ƒï¼ˆå¿…å¼€æ¨¡å—ï¼‰
            if self.module_manager.is_enabled('traditional_ml'):
                with self.exception_handler.safe_execution("ä¼ ç»ŸMLè®­ç»ƒ"):
                    traditional_results = self._train_standard_models(
                        X_clean, y_clean, dates_clean, tickers_clean
                    )
                    training_results['traditional_models'] = traditional_results
                    
                    # ğŸ† ä½¿ç”¨ç»Ÿä¸€ICè®¡ç®—å™¨è¯„ä¼°æ¨¡å‹ - APIå®‰å…¨æ€§æ£€æŸ¥
                    if unified_ic_calc is not None and traditional_results:
                        try:
                            # è·å–æ¨¡å‹é¢„æµ‹
                            model_predictions = {}
                            for model_name, model_result in traditional_results.items():
                                if isinstance(model_result, dict) and 'oof_predictions' in model_result:
                                    model_predictions[model_name] = model_result['oof_predictions']
                            
                            if model_predictions:
                                # APIå®‰å…¨æ€§æ£€æŸ¥
                                if hasattr(unified_ic_calc, 'calculate_comprehensive_ic'):
                                    # æ£€æŸ¥æ–¹æ³•ç­¾å
                                    import inspect
                                    sig = inspect.signature(unified_ic_calc.calculate_comprehensive_ic)
                                    param_names = list(sig.parameters.keys())
                                    
                                    # æ ¹æ®å®é™…APIè°ƒç”¨
                                    try:
                                        if 'predictions_dict' in param_names:
                                            ic_metrics = unified_ic_calc.calculate_comprehensive_ic(
                                                predictions_dict=model_predictions,
                                                targets=y_clean,
                                                dates=dates_clean,
                                                tickers=tickers_clean
                                            )
                                        else:
                                            # å°è¯•æ›¿ä»£API
                                            ic_metrics = unified_ic_calc.calculate_comprehensive_ic(
                                                model_predictions, y_clean, dates_clean, tickers_clean
                                            )
                                        
                                        training_results['unified_ic_metrics'] = ic_metrics
                                        mean_ic = ic_metrics.get('mean_ic', ic_metrics.get('average_ic', 0)) if isinstance(ic_metrics, dict) else 0
                                        logger.info(f"âœ… ç»Ÿä¸€ICè®¡ç®—å®Œæˆï¼Œå¹³å‡IC: {mean_ic:.4f}")
                                    except TypeError as te:
                                        logger.warning(f"ç»Ÿä¸€ICè®¡ç®—APIä¸åŒ¹é…: {te}")
                                        training_results['unified_ic_metrics'] = {'error': f'APIä¸åŒ¹é…: {str(te)}'}
                                else:
                                    logger.warning("ç»Ÿä¸€ICè®¡ç®—å™¨APIä¸åŒ¹é…: calculate_comprehensive_icæ–¹æ³•ä¸å­˜åœ¨")
                                    training_results['unified_ic_metrics'] = {'error': 'calculate_comprehensive_icæ–¹æ³•ä¸å­˜åœ¨'}
                            else:
                                logger.warning("æ²¡æœ‰æ‰¾åˆ°OOFé¢„æµ‹ç»“æœï¼Œè·³è¿‡ICè®¡ç®—")
                                training_results['unified_ic_metrics'] = {'error': 'æ²¡æœ‰OOFé¢„æµ‹ç»“æœ'}
                        except Exception as e:
                            logger.warning(f"ç»Ÿä¸€ICè®¡ç®—å¤±è´¥: {e}")
                            training_results['unified_ic_metrics'] = {'error': str(e)}
            
            # 5.3 LTRè®­ç»ƒï¼ˆæ¡ä»¶å¯ç”¨ï¼‰
            if self.module_manager.is_enabled('ltr_ranking'):
                with self.exception_handler.safe_execution("LTRè®­ç»ƒ"):
                    ltr_results = self._train_ltr_models_modular(
                        X_clean, y_clean, dates_clean, tickers_clean
                    )
                    training_results['learning_to_rank'] = ltr_results
            
            # ğŸ† 5.35 åˆå§‹åŒ–å¹¶åº”ç”¨å…¶ä»–å…³é”®æ¨¡å—
            
            # Alphaé…ç½®å¢å¼º - ä½¿ç”¨åŸºç¡€é…ç½®ï¼ˆç®€åŒ–ï¼‰
            if EnhancedAlphaConfig is not None:
                try:
                    # ç›´æ¥ä½¿ç”¨åŸºç¡€é…ç½®ï¼Œä¸ä¾èµ–create_enhanced_configå‡½æ•°
                    enhanced_config = EnhancedAlphaConfig()
                    training_results['enhanced_alpha_config']['config'] = enhanced_config
                    training_results['enhanced_alpha_config']['status'] = 'success'
                    logger.info("âœ… Alphaé…ç½®ç³»ç»Ÿå¯ç”¨")
                except Exception as e:
                    logger.warning(f"Alphaé…ç½®åˆå§‹åŒ–å¤±è´¥: {e}")
                    training_results['enhanced_alpha_config']['status'] = 'failed'
                    training_results['enhanced_alpha_config']['error'] = str(e)
            
            # ICåŠ æƒå¤„ç†å™¨ - APIå®‰å…¨æ€§æ£€æŸ¥å’Œé…ç½®å‚æ•°åŒ–
            if ICWeightedAlphaProcessor is not None:
                try:
                    # ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°
                    ic_weighted_settings = self.config.get('ic_weighted_processing', {})
                    ic_config = ICWeightedConfig(**ic_weighted_settings) if ic_weighted_settings else ICWeightedConfig()
                    ic_processor = ICWeightedAlphaProcessor(ic_config)
                    
                    if 'traditional_models' in training_results and training_results['traditional_models']:
                        # APIå®‰å…¨æ€§æ£€æŸ¥
                        if hasattr(ic_processor, 'process_alpha_signals'):
                            import inspect
                            sig = inspect.signature(ic_processor.process_alpha_signals)
                            param_names = list(sig.parameters.keys())
                            
                            try:
                                if len(param_names) >= 3:
                                    processed_results = ic_processor.process_alpha_signals(
                                        training_results['traditional_models'], y_clean, dates_clean
                                    )
                                else:
                                    # ç®€åŒ–è°ƒç”¨
                                    processed_results = ic_processor.process_alpha_signals(
                                        training_results['traditional_models']
                                    )
                                
                                training_results['ic_weighted_processing']['processed_results'] = processed_results
                                training_results['ic_weighted_processing']['status'] = 'success'
                                logger.info("âœ… ICåŠ æƒå¤„ç†å™¨åº”ç”¨æˆåŠŸ")
                            except TypeError as te:
                                training_results['ic_weighted_processing']['status'] = 'api_mismatch'
                                training_results['ic_weighted_processing']['error'] = f'APIä¸åŒ¹é…: {str(te)}'
                                logger.warning(f"ICåŠ æƒå¤„ç†APIä¸åŒ¹é…: {te}")
                        else:
                            training_results['ic_weighted_processing']['status'] = 'method_not_found'
                            training_results['ic_weighted_processing']['error'] = 'process_alpha_signalsæ–¹æ³•ä¸å­˜åœ¨'
                            logger.warning("ICåŠ æƒå¤„ç†å™¨APIä¸åŒ¹é…: process_alpha_signalsæ–¹æ³•ä¸å­˜åœ¨")
                    else:
                        training_results['ic_weighted_processing']['status'] = 'no_models'
                        training_results['ic_weighted_processing']['error'] = 'æ²¡æœ‰ä¼ ç»ŸMLæ¨¡å‹ç»“æœ'
                except Exception as e:
                    logger.warning(f"ICåŠ æƒå¤„ç†å¤±è´¥: {e}")
                    training_results['ic_weighted_processing']['status'] = 'failed'
                    training_results['ic_weighted_processing']['error'] = str(e)
            
            # ğŸš¨ æ•°æ®æµä¿®å¤: æ—¥é¢‘ä¸­æ€§åŒ–åº”è¯¥åœ¨æ¨¡å‹è®­ç»ƒä¹‹å‰è¿›è¡Œ
            # æš‚æ—¶ç¦ç”¨ï¼Œå°†åœ¨æ­£ç¡®ä½ç½®é‡æ–°å¯ç”¨
            if False:  # DailyNeutralizationPipeline is not None:
                try:
                    # ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°
                    neutralization_config = self.config.get('neutralization', {})
                    neut_config = NeutralizationConfig(**neutralization_config)
                    neut_pipeline = DailyNeutralizationPipeline(neut_config)
                    
                    # æ³¨æ„: è¿™é‡Œåº”è¯¥åœ¨æ¨¡å‹è®­ç»ƒä¹‹å‰è°ƒç”¨
                    logger.warning("âš ï¸ æ—¥é¢‘ä¸­æ€§åŒ–åœ¨é”™è¯¯ä½ç½®è°ƒç”¨ï¼Œå·²ç¦ç”¨")
                    training_results['daily_neutralization']['status'] = 'disabled_wrong_position'
                    training_results['daily_neutralization']['error'] = 'æ•°æ®æµé”™è¯¯: åº”åœ¨æ¨¡å‹è®­ç»ƒå‰è¿›è¡Œ'
                except Exception as e:
                    logger.warning(f"æ—¥é¢‘ä¸­æ€§åŒ–å¤±è´¥: {e}")
                    training_results['daily_neutralization']['status'] = 'failed'
                    training_results['daily_neutralization']['error'] = str(e)
            else:
                training_results['daily_neutralization']['status'] = 'disabled_for_reordering'
                training_results['daily_neutralization']['error'] = 'éœ€è¦é‡æ–°æ’åºåˆ°æ­£ç¡®ä½ç½®'
            
            # 5.4 Regime-awareè®­ç»ƒï¼ˆæ¡ä»¶å¯ç”¨ï¼‰
            if self.module_manager.is_enabled('regime_aware'):
                with self.exception_handler.safe_execution("Regime-awareè®­ç»ƒ"):
                    if self.module_manager.is_degraded('regime_aware'):
                        # é™çº§æ¨¡å¼ï¼šæ ·æœ¬åŠ æƒ
                        regime_results = self._apply_regime_sample_weighting(
                            X_clean, y_clean, dates_clean, tickers_clean
                        )
                    else:
                            # å®Œæ•´æ¨¡å¼ï¼šå¤šæ¨¡å‹è®­ç»ƒ
                        # ğŸ”¥ ä½¿ç”¨å¢å¼ºåˆ¶åº¦æ„ŸçŸ¥è®­ç»ƒ (æ¥è‡ªè·¯å¾„A+Bèåˆ)
                        regime_results = self._train_enhanced_regime_aware_models(
                            X_clean, y_clean, dates_clean)
                        training_results['regime_aware'] = regime_results
            
            # 5.5 Stackingé›†æˆï¼ˆé»˜è®¤å…³é—­ï¼‰
            if self.module_manager.is_enabled('stacking'):
                with self.exception_handler.safe_execution("Stackingé›†æˆ"):
                    stacking_results = self._train_stacking_models_modular(
                        training_results, X_clean, y_clean
                    )
                    training_results['stacking'] = stacking_results
            else:
                # ä½¿ç”¨IC/IRæ— è®­ç»ƒåŠ æƒä½œä¸ºæ›¿ä»£
                ensemble_results = self._apply_icir_weighting(training_results)
                training_results['ensemble_weights'] = ensemble_results
            
            # ğŸ† 5.55 OOFé›†æˆç³»ç»Ÿæœ€ç»ˆé›†æˆ
            if oof_ensemble is not None:
                try:
                    # æ”¶é›†æ‰€æœ‰OOFé¢„æµ‹
                    oof_predictions = {}
                    for category in ['traditional_models', 'learning_to_rank', 'regime_aware']:
                        if category in training_results and training_results[category]:
                            models_data = training_results[category]
                            if isinstance(models_data, dict):
                                for model_name, model_result in models_data.items():
                                    if isinstance(model_result, dict) and 'oof_predictions' in model_result:
                                        oof_predictions[f"{category}_{model_name}"] = model_result['oof_predictions']
                    
                    if oof_predictions:
                        # ğŸ”¥ CRITICAL FIX: ä½¿ç”¨æ—¶é—´å®‰å…¨çš„BMAæƒé‡è®¡ç®—
                        try:
                            from time_safe_bma_weights import create_time_safe_bma_calculator
                            
                            # åˆ›å»ºæ—¶é—´å®‰å…¨çš„BMAæƒé‡è®¡ç®—å™¨
                            safe_calculator = create_time_safe_bma_calculator(
                                lookback_days=252,      # 1å¹´å†å²æ•°æ®
                                min_history_days=63,    # æœ€å°‘3ä¸ªæœˆæ•°æ®
                                rebalance_frequency=21  # æ¯æœˆé‡æ–°è®¡ç®—
                            )
                            
                            # ğŸš¨ CRITICAL FIX: ç¡®å®šå½“å‰æ—¥æœŸï¼ˆä¸¥æ ¼é˜²æ­¢æ—¶é—´æ³„éœ²ï¼‰
                            if dates_clean is not None and len(dates_clean) > 0:
                                # ä½¿ç”¨è®­ç»ƒæ•°æ®ä¸­çš„æœ€å¤§æ—¥æœŸï¼Œä½†ä¸èƒ½è¶…è¿‡ä»Šå¤©
                                training_end_date = pd.to_datetime(dates_clean.max())
                                today = pd.Timestamp.now().normalize()
                                
                                # æƒé‡è®¡ç®—çš„å½“å‰æ—¥æœŸåº”è¯¥æ˜¯è®­ç»ƒç»“æŸçš„ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œä½†ä¸è¶…è¿‡ä»Šå¤©
                                current_date = min(training_end_date + timedelta(days=1), today)
                                
                                if training_end_date >= today:
                                    logger.warning(f"âš ï¸ è®­ç»ƒæ•°æ®åŒ…å«ä»Šæ—¥æˆ–æœªæ¥æ•°æ® {training_end_date.strftime('%Y-%m-%d')} >= {today.strftime('%Y-%m-%d')}")
                                    current_date = today
                            else:
                                current_date = pd.Timestamp.now().normalize()
                            
                            logger.info(f"ğŸ•’ ä½¿ç”¨æ—¶é—´å®‰å…¨BMAæƒé‡è®¡ç®— (å½“å‰æ—¥æœŸ: {current_date.strftime('%Y-%m-%d')})")
                            
                            # è®¡ç®—æ—¶é—´å®‰å…¨çš„BMAæƒé‡
                            bma_weights = safe_calculator.calculate_time_safe_weights(
                                oof_predictions=oof_predictions,
                                targets=y_clean,
                                current_date=current_date,
                                force_rebalance=True  # è®­ç»ƒæ—¶å¼ºåˆ¶é‡æ–°è®¡ç®—
                            )
                            
                            # è®°å½•æƒé‡ç»Ÿè®¡ä¿¡æ¯
                            weight_stats = safe_calculator.get_weight_statistics()
                            logger.info(f"BMAæƒé‡ç»Ÿè®¡: {weight_stats}")
                            
                        except Exception as e:
                            logger.error(f"æ—¶é—´å®‰å…¨BMAæƒé‡è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•: {e}")
                            # é™çº§åˆ°åŸæœ‰æ–¹æ³•
                            try:
                                if hasattr(oof_ensemble, 'calculate_bma_weights'):
                                    import inspect
                                    sig = inspect.signature(oof_ensemble.calculate_bma_weights)
                                    param_names = list(sig.parameters.keys())
                                    
                                    if 'oof_predictions' in param_names:
                                        bma_weights = oof_ensemble.calculate_bma_weights(
                                            oof_predictions=oof_predictions,
                                            targets=y_clean,
                                            dates=dates_clean,
                                            tickers=tickers_clean
                                        )
                                    else:
                                        bma_weights = oof_ensemble.calculate_bma_weights(
                                            oof_predictions, y_clean, dates_clean, tickers_clean
                                        )
                                else:
                                    # ä½¿ç”¨ç®€å•å‡ç­‰æƒé‡
                                    bma_weights = {model_name: 1.0/len(oof_predictions) for model_name in oof_predictions.keys()}
                                    logger.warning("é™çº§ä½¿ç”¨å‡ç­‰æƒé‡ä½œä¸ºBMAæƒé‡")
                            except Exception as fallback_error:
                                logger.error(f"é™çº§æ–¹æ³•ä¹Ÿå¤±è´¥: {fallback_error}")
                                bma_weights = {model_name: 1.0/len(oof_predictions) for model_name in oof_predictions.keys()}
                            
                            # ç”Ÿæˆæœ€ç»ˆé›†æˆé¢„æµ‹
                            if hasattr(oof_ensemble, 'generate_ensemble_prediction'):
                                ensemble_prediction = oof_ensemble.generate_ensemble_prediction(
                                    oof_predictions, bma_weights
                                )
                            elif hasattr(oof_ensemble, 'ensemble_predict'):
                                ensemble_prediction = oof_ensemble.ensemble_predict(oof_predictions, bma_weights)
                            else:
                                # æ‰‹åŠ¨è®¡ç®—é›†æˆé¢„æµ‹
                                ensemble_prediction = pd.Series(0.0, index=list(oof_predictions.values())[0].index)
                                for model_name, pred in oof_predictions.items():
                                    weight = bma_weights.get(model_name, 0.0)
                                    ensemble_prediction += pred * weight
                                logger.warning("ä½¿ç”¨æ‰‹åŠ¨è®¡ç®—çš„é›†æˆé¢„æµ‹")
                            
                            training_results['oof_ensemble']['bma_weights'] = bma_weights
                            training_results['oof_ensemble']['ensemble_prediction'] = ensemble_prediction
                            training_results['oof_ensemble']['model_count'] = len(oof_predictions)
                            training_results['oof_ensemble']['status'] = 'success'
                            logger.info(f"âœ… OOFé›†æˆç³»ç»ŸæˆåŠŸé›†æˆ{len(oof_predictions)}ä¸ªæ¨¡å‹")
                                
                        except Exception as oof_error:
                            logger.warning(f"OOFé›†æˆç³»ç»Ÿå†…éƒ¨é”™è¯¯: {oof_error}")
                            training_results['oof_ensemble']['status'] = 'partial_success'
                            training_results['oof_ensemble']['error'] = str(oof_error)
                            training_results['oof_ensemble']['model_count'] = len(oof_predictions)
                            # ä½¿ç”¨å‡ç­‰æƒé‡ä½œä¸ºæ›¿ä»£
                            training_results['oof_ensemble']['bma_weights'] = {name: 1.0/len(oof_predictions) for name in oof_predictions.keys()}
                except Exception as e:
                    logger.warning(f"OOFé›†æˆç³»ç»Ÿå¤±è´¥: {e}")
                    training_results['oof_ensemble']['status'] = 'failed'
                    training_results['oof_ensemble']['error'] = str(e)
                
            # V5å¢å¼ºåŠŸèƒ½è°ƒç”¨å·²åˆ é™¤ - V6ç³»ç»Ÿæä¾›å®Œæ•´æ›¿ä»£
            
            # ğŸ† 5.6 åŠ¨æ€å› å­åŠ æƒç³»ç»Ÿ
            if DynamicFactorWeighting is not None:
                try:
                    weighting_config = WeightingConfig()
                    dynamic_weighter = DynamicFactorWeighting(weighting_config)
                    
                    # å¯¹æ‰€æœ‰æ¨¡å‹ç»“æœåº”ç”¨åŠ¨æ€åŠ æƒ
                    weighted_results = dynamic_weighter.apply_dynamic_weighting(
                        training_results, dates_clean, tickers_clean
                    )
                    training_results['dynamic_weighting'] = weighted_results
                    logger.info("âœ… åŠ¨æ€å› å­åŠ æƒåº”ç”¨æˆåŠŸ")
                except Exception as e:
                    logger.warning(f"åŠ¨æ€å› å­åŠ æƒå¤±è´¥: {e}")
            
            # ğŸ† 5.65 å®æ—¶æ€§èƒ½ç›‘æ§
            if RealtimePerformanceMonitor is not None:
                try:
                    alert_config = AlertThresholds()
                    perf_monitor = RealtimePerformanceMonitor(alert_config)
                    
                    # ç›‘æ§è®­ç»ƒç»“æœ
                    monitoring_result = perf_monitor.monitor_training_performance(
                        training_results, X_clean, y_clean
                    )
                    training_results['realtime_monitoring'] = monitoring_result
                    logger.info("âœ… å®æ—¶æ€§èƒ½ç›‘æ§å¯åŠ¨")
                except Exception as e:
                    logger.warning(f"å®æ—¶æ€§èƒ½ç›‘æ§å¤±è´¥: {e}")
            
            # ğŸ† 5.67 çœŸå®OOSç®¡ç†å™¨
            if RealOOSManager is not None:
                try:
                    oos_config = OOSConfig()
                    oos_manager = RealOOSManager(oos_config)
                    
                    # ç®¡ç†çœŸå®OOSæµ‹è¯•
                    oos_results = oos_manager.manage_real_oos_testing(
                        training_results, feature_data, y_clean
                    )
                    training_results['real_oos_results'] = oos_results
                    logger.info("âœ… çœŸå®OOSç®¡ç†å™¨éƒ¨ç½²æˆåŠŸ")
                except Exception as e:
                    logger.warning(f"çœŸå®OOSç®¡ç†å™¨å¤±è´¥: {e}")
            
            # ğŸ† 5.68 é«˜çº§Alphaç³»ç»Ÿé›†æˆ
            if AdvancedAlphaSystem is not None:
                try:
                    advanced_system = AdvancedAlphaSystem()
                    
                    # é›†æˆæ‰€æœ‰é«˜çº§åŠŸèƒ½
                    advanced_results = advanced_system.integrate_all_components(
                        training_results, X_clean, y_clean, dates_clean, tickers_clean
                    )
                    training_results['enhanced_alpha_system'] = advanced_results
                    logger.info("âœ… é«˜çº§Alphaç³»ç»Ÿé›†æˆæˆåŠŸ")
                except Exception as e:
                    logger.warning(f"é«˜çº§Alphaç³»ç»Ÿé›†æˆå¤±è´¥: {e}")
            
            # ğŸ”¥ 5.7 Enhanced OOS System Integration
            if self.enhanced_oos_system and len(X_clean) > 500:  # åªåœ¨æœ‰è¶³å¤Ÿæ•°æ®æ—¶è¿è¡Œ
                with self.exception_handler.safe_execution("Enhanced OOSéªŒè¯"):
                    try:
                        # æ”¶é›†è®­ç»ƒçš„æ¨¡å‹
                        trained_models = {}
                        
                        # ä»è®­ç»ƒç»“æœä¸­æå–æ¨¡å‹
                        for category in ['traditional_models', 'learning_to_rank', 'regime_aware']:
                            if category in training_results and training_results[category]:
                                models_data = training_results[category]
                                if isinstance(models_data, dict) and 'models' in models_data:
                                    for model_name, model in models_data['models'].items():
                                        if hasattr(model, 'predict'):
                                            trained_models[f"{category}_{model_name}"] = model
                        
                        logger.info(f"Enhanced OOSéªŒè¯: æ”¶é›†åˆ°{len(trained_models)}ä¸ªæ¨¡å‹")
                        
                        # é‡å»ºç‰¹å¾æ•°æ®åŒ…å«æ—¥æœŸä¿¡æ¯
                        oos_feature_data = feature_data[['date'] + feature_cols].copy()
                        
                        # é›†æˆOOSéªŒè¯
                        if trained_models and len(oos_feature_data) > 100:
                            oos_result = self.enhanced_oos_system.integrate_with_bma_cv(
                                feature_data=oos_feature_data,
                                target_data=y_clean,
                                models=trained_models,
                                bma_config=self.config or {}
                            )
                            
                            training_results['enhanced_oos'] = oos_result
                            
                            # å¦‚æœOOSéªŒè¯æˆåŠŸï¼Œä½¿ç”¨OOSæƒé‡æ›´æ–°BMA
                            if oos_result.get('success') and 'weight_update' in oos_result:
                                oos_weights = oos_result['weight_update'].get('weights', {})
                                if oos_weights:
                                    training_results['oos_optimized_weights'] = oos_weights
                                    logger.info("âœ… BMAæƒé‡å·²åŸºäºçœŸå®OOSæ€§èƒ½æ›´æ–°")
                        
                    except Exception as e:
                        logger.warning(f"Enhanced OOSé›†æˆå¤±è´¥: {e}")
                        training_results['enhanced_oos'] = {'success': False, 'error': str(e)}
            
            # 6. è®­ç»ƒç»Ÿè®¡å’Œæ€§èƒ½è¯„ä¼°
            training_results['training_metrics'] = self._calculate_training_metrics(
                training_results, X_clean, y_clean
            )
            
            # ğŸ”¥ åº”ç”¨çŸ¥è¯†ä¿æŒç³»ç»Ÿ (æ¥è‡ªè·¯å¾„A)
            training_results = self._apply_knowledge_retention(training_results)
            
            # ğŸ”¥ åº”ç”¨ç”Ÿäº§å°±ç»ªé—¨ç¦éªŒè¯ (æ¥è‡ªè·¯å¾„A)
            production_decision = self._apply_production_readiness_gates(training_results)
            training_results['production_decision'] = production_decision
            training_results['training_type'] = training_type if 'training_type' in locals() else 'FULL_REBUILD'
            
            logger.info("ğŸ‰ ç»Ÿä¸€è®­ç»ƒæµç¨‹å®Œæˆ (è·¯å¾„A+Bèåˆ)")
            return training_results
                    
        except Exception as e:
            logger.error(f"æ¨¡å—åŒ–è®­ç»ƒè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
            training_results['error_log'].append(str(e))
            return training_results
        
        # åˆ é™¤äº†æ—§ä»£ç ï¼Œç°åœ¨ä½¿ç”¨æ¨¡å—åŒ–æµç¨‹
        # æ¨¡å—åŒ–è®­ç»ƒæµç¨‹å·²å®Œæˆï¼Œè¿”å›ç»“æœ
        logger.info("ğŸ‰ BMA Ultra Enhanced æ¨¡å—åŒ–è®­ç»ƒå®Œæˆï¼ˆV6å¢å¼ºï¼‰")
        return training_results
    
    def _set_fundamental_nan_values(self, prepared: pd.DataFrame):
        """è®¾ç½®åŸºæœ¬é¢æ•°æ®ä¸ºNaNå€¼"""
        prepared['book_to_market'] = np.nan
        prepared['roe'] = np.nan
        prepared['debt_to_equity'] = np.nan
        prepared['earnings'] = np.nan
        prepared['pe_ratio'] = np.nan
        prepared['market_cap'] = np.nan
        prepared['revenue_growth'] = np.nan
        prepared['profit_margin'] = np.nan
    
    def _get_fundamental_data_fallback(self, prepared: pd.DataFrame, ticker: str, data: pd.DataFrame):
        """å›é€€çš„åŸºæœ¬é¢æ•°æ®è·å–æ–¹æ³•ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        try:
            from polygon_only_data_provider import PolygonOnlyDataProvider
            
            # åˆå§‹åŒ–Polygonæ•°æ®æä¾›å™¨
            if not hasattr(self, 'polygon_provider'):
                self.polygon_provider = PolygonOnlyDataProvider()
            
            # ä»Polygonè·å–çœŸå®åŸºæœ¬é¢æ•°æ®
            fund_df = self.polygon_provider.get_fundamentals(ticker, limit=1)
            
            if fund_df is not None and not fund_df.empty:
                # ä½¿ç”¨PolygonçœŸå®æ•°æ®
                latest = fund_df.iloc[0]
                
                # è®¡ç®—book to market (å¦‚æœæœ‰æ•°æ®)
                if latest.get('book_value_per_share') and data['close'].iloc[-1] > 0:
                    prepared['book_to_market'] = latest['book_value_per_share'] / data['close'].iloc[-1]
                else:
                    prepared['book_to_market'] = np.nan
                    
                prepared['roe'] = latest.get('roe', np.nan)
                prepared['debt_to_equity'] = latest.get('debt_to_equity', np.nan)
                prepared['earnings'] = latest.get('earnings_per_share', np.nan)
                prepared['pe_ratio'] = latest.get('pe_ratio', np.nan)
                
                # å¯¹äºå¢å¼ºæŒ‡æ ‡ï¼Œå¦‚æœä¸å¯ç”¨åˆ™è®¾ä¸ºNaN
                prepared['revenue_growth'] = np.nan
                prepared['profit_margin'] = np.nan
                
                logger.info(f"Using fallback Polygon API fundamental data for {ticker}")
            else:
                # å¦‚æœPolygonæ— æ•°æ®ï¼Œä½¿ç”¨NaN - ç»ä¸ç”Ÿæˆå‡æ•°æ®
                logger.warning(f"No fallback Polygon fundamental data for {ticker}, using NaN")
                self._set_fundamental_nan_values(prepared)
                
        except Exception as e:
            logger.warning(f"Fallback fundamental data failed for {ticker}: {e}")
            # å¤±è´¥æ—¶ä½¿ç”¨NaNï¼Œä¸ä½¿ç”¨éšæœºæ•°
            self._set_fundamental_nan_values(prepared)
    
    # ğŸ”§ ä»¥ä¸‹ä¿ç•™é‡è¦çš„è¾…åŠ©æ–¹æ³•
    
    def _create_fused_features(self, X_clean: pd.DataFrame,
                             alpha_summary_features: Optional[pd.DataFrame],
                             dates_clean: pd.Series,
                             tickers_clean: pd.Series) -> pd.DataFrame:
        """åˆ›å»ºèåˆç‰¹å¾ï¼ˆä¼ ç»Ÿç‰¹å¾ + Alphaæ‘˜è¦ç‰¹å¾ï¼‰"""
        try:
            X_fused = X_clean.copy()
            
            # å¦‚æœæœ‰Alphaæ‘˜è¦ç‰¹å¾ï¼Œæ·»åŠ åˆ°ç‰¹å¾çŸ©é˜µä¸­
            if alpha_summary_features is not None and not alpha_summary_features.empty:
                logger.info("èåˆAlphaæ‘˜è¦ç‰¹å¾...")
                
                # ç¡®ä¿alpha_summary_featuresçš„ç´¢å¼•ä¸X_cleanä¸€è‡´
                if isinstance(alpha_summary_features.index, pd.MultiIndex):
                    # å¦‚æœæ˜¯MultiIndexï¼Œå°è¯•å¯¹é½
                    alpha_df = alpha_summary_features.reset_index()
                    merge_df = pd.DataFrame({
                        'date': dates_clean,
                        'ticker': tickers_clean
                    }).reset_index()
                    
                    merged = merge_df.merge(alpha_df, on=['date', 'ticker'], how='left')
                    alpha_cols = [col for col in merged.columns if col.startswith('alpha_')]
                    
                    for col in alpha_cols:
                        X_fused[col] = merged[col].fillna(0)
            else:
                    # å¦‚æœç´¢å¼•åŒ¹é…ï¼Œç›´æ¥æ·»åŠ 
                    for col in alpha_summary_features.columns:
                        if col.startswith('alpha_'):
                            X_fused[col] = alpha_summary_features[col].reindex(X_clean.index).fillna(0)
                
            
            # æœ€ç»ˆNaNæ£€æŸ¥å’Œå¤„ç†
            if X_fused.isna().any().any():
                final_nan_count = X_fused.isna().sum().sum()
                if final_nan_count > 0:
                    logger.error(f"âš ï¸ è­¦å‘Šï¼šä»æœ‰ {final_nan_count} ä¸ªNaNå€¼æ— æ³•å¡«å……")
                    # æœ€åæ‰‹æ®µï¼šç”¨0å¡«å……
                    X_fused = X_fused.fillna(0)
                else:
                        logger.info(f"âœ… NaNå¡«å……å®Œæˆï¼Œæ‰€æœ‰NaNå€¼å·²å¤„ç†")
            else:
                logger.info("âœ… ç‰¹å¾èåˆåæ— NaNå€¼")
            
            return X_fused
            
        except Exception as e:
            logger.error(f"ç‰¹å¾èåˆå¤±è´¥: {e}")
            logger.info("å›é€€ä½¿ç”¨åŸå§‹ç‰¹å¾çŸ©é˜µ")
            return X_clean

    def run_complete_analysis(self, tickers: List[str], 
                             start_date: str, end_date: str,
                             top_n: int = 10) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´åˆ†ææµç¨‹ V6 - é›†æˆæ‰€æœ‰ç”Ÿäº§çº§å¢å¼º
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            top_n: è¿”å›æ¨èæ•°é‡
            
        Returns:
            å®Œæ•´åˆ†æç»“æœ
        """
        logger.info(f"å¼€å§‹å®Œæ•´åˆ†ææµç¨‹ V6 - ç”Ÿäº§çº§å¢å¼ºæ¨¡å¼")
        
        # ğŸš€ ç”Ÿäº§çº§ä¿®å¤ç³»ç»Ÿé¢„æ£€æŸ¥
        production_status = self.get_production_fixes_status()
        if production_status.get('available', False):
            logger.info("âœ… ç”Ÿäº§çº§ä¿®å¤ç³»ç»Ÿå·²æ¿€æ´»")
            
            # è®°å½•å…³é”®é…ç½®å‚æ•°
            timing_config = production_status.get('timing_config', {})
            # ğŸ”¥ CRITICAL FIX: å¼ºåˆ¶ä½¿ç”¨å…¨å±€ç»Ÿä¸€æ—¶é—´é…ç½®ï¼Œé˜²æ­¢é…ç½®å†²çªå¯¼è‡´æ•°æ®æ³„éœ²
            temporal_config = validate_temporal_configuration()
            cv_gap = temporal_config['cv_gap_days']         # å¼ºåˆ¶ä½¿ç”¨ç»Ÿä¸€é…ç½®
            cv_embargo = temporal_config['cv_embargo_days'] # å¼ºåˆ¶ä½¿ç”¨ç»Ÿä¸€é…ç½®
            weight_halflife = self.config.get('sample_weighting', {}).get('half_life_days', timing_config.get('sample_weight_half_life', 30))
            regime_smooth = self.config.get('regime', {}).get('enable_smoothing', timing_config.get('regime_enable_smoothing', True))
            
            logger.info(f"  - CVéš”ç¦»å‚æ•°: gap={cv_gap}å¤©, embargo={cv_embargo}å¤©")
            logger.info(f"  - æ ·æœ¬æƒé‡åŠè¡°æœŸ: {weight_halflife}å¤©")
            logger.info(f"  - Regimeå¹³æ»‘: {'ç¦ç”¨' if not regime_smooth else 'å¯ç”¨'}")
        else:
            logger.warning("âš ï¸ ç”Ÿäº§çº§ä¿®å¤ç³»ç»Ÿä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†é…ç½®")
        
        # ğŸš€ å¦‚æœå¯ç”¨V6å¢å¼ºç³»ç»Ÿï¼Œä½¿ç”¨æ–°çš„è®­ç»ƒæµç¨‹
        # V6ç³»ç»Ÿå¯¼å…¥é—®é¢˜å·²ä¿®å¤ï¼Œç°åœ¨å¯ä»¥æ­£å¸¸ä½¿ç”¨
            logger.info("âœ… ä½¿ç”¨V6å¢å¼ºè®­ç»ƒè·¯å¾„")
            return self._run_v6_enhanced_analysis(tickers, start_date, end_date, top_n)
        
        # å›é€€åˆ°ä¼ ç»Ÿæµç¨‹
        analysis_results = {
            'start_time': datetime.now(),
            'config': self.config,
            'tickers': tickers,
            'date_range': f"{start_date} to {end_date}",
            'production_fixes_status': production_status,
            'optimization_enabled': getattr(self, 'memory_optimized', False)
        }
        
        #  æ–°å¢åŠŸèƒ½ï¼šWalk-Forwardé‡è®­ç»ƒè¯„ä¼°
        if self.walk_forward_system:
            try:
                logger.info("è¯„ä¼°Walk-Forwardé‡è®­ç»ƒéœ€æ±‚...")
                
                # ä½¿ç”¨å®é™…ç‰¹å¾æ•°æ®è¿›è¡ŒWalk-Forwardåˆ†æ
                if feature_data is not None and not feature_data.empty and 'date' in feature_data.columns:
                    temp_data = feature_data[['date']].copy()
                    logger.info(f"ä½¿ç”¨å®é™…æ•°æ®è¿›è¡ŒWalk-Forwardåˆ†æï¼Œæ•°æ®ç‚¹: {len(temp_data)}")
                else:
                    # å¦‚æœæ²¡æœ‰å®é™…æ•°æ®ï¼Œåˆ›å»ºä¸´æ—¶æ•°æ®æ¡†
                    temp_data = pd.DataFrame({'date': pd.date_range(start_date, end_date, freq='D')})
                    logger.warning("ä½¿ç”¨ä¸´æ—¶æ—¥æœŸæ•°æ®è¿›è¡ŒWalk-Forwardåˆ†æ")
                
                # ç”Ÿæˆç®€åŒ–çš„ä»£ç å†…å®¹hashï¼ˆé¿å…è¯»å–å¤§æ–‡ä»¶ï¼‰
                code_content = f"bma_enhanced_v1.0_{datetime.now().strftime('%Y%m%d')}"
                config_dict = self.config.copy() if self.config else {}
                
                from walk_forward_retraining import integrate_walk_forward_to_bma
                wf_result = integrate_walk_forward_to_bma(
                    data=temp_data,
                    code_content=code_content,
                    config_dict=config_dict,
                    current_date=end_date
                )
                
                analysis_results['walk_forward'] = wf_result
                
                if wf_result.get('success') and wf_result.get('should_retrain'):
                    logger.info(f"[WF] Walk-Forwardå»ºè®®é‡è®­ç»ƒ: {wf_result.get('retrain_reason')}")
                    logger.info(f"[WF] è¿è¡ŒID: {wf_result.get('run_config', {}).get('run_id')}")
                else:
                    logger.info("[WF] Walk-Forwardè¯„ä¼°ï¼šæ— éœ€é‡è®­ç»ƒ")
                    
            except Exception as e:
                logger.warning(f"Walk-Forwardè¯„ä¼°å¤±è´¥: {e}")
                analysis_results['walk_forward'] = {'success': False, 'error': str(e)}
        
        # ä½¿ç”¨å…¨å±€ç»Ÿä¸€è®­ç»ƒæ¨¡å¼
        # if getattr(self, 'memory_optimized', False) and len(tickers) > self.batch_size:
        #     return self._run_optimized_analysis(tickers, start_date, end_date, top_n, analysis_results)
        
        logger.info(f"[BMA] ä½¿ç”¨å…¨å±€ç»Ÿä¸€è®­ç»ƒæ¨¡å¼ - æ‰€æœ‰ {len(tickers)} è‚¡ç¥¨å°†åœ¨åŒä¸€æ¨¡å‹ä¸­è®­ç»ƒ")
        
        # å­˜å‚¨è¯·æ±‚çš„è‚¡ç¥¨åˆ—è¡¨ï¼Œç¡®ä¿æœ€ç»ˆè¾“å‡ºåŒ…å«æ‰€æœ‰è¿™äº›è‚¡ç¥¨
        self.requested_tickers = tickers
        
        try:
            # 1. ä¸‹è½½æ•°æ®
            logger.info(f"[DEBUG] å¼€å§‹ä¸‹è½½è‚¡ç¥¨æ•°æ®: {tickers}, æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
            stock_data = self.download_stock_data(tickers, start_date, end_date)
            logger.info(f"[DEBUG] è‚¡ç¥¨æ•°æ®ä¸‹è½½å®Œæˆ: {len(stock_data) if stock_data else 0} åªè‚¡ç¥¨")
            if not stock_data:
                raise ValueError("æ— æ³•è·å–è‚¡ç¥¨æ•°æ®")
            
            analysis_results['data_download'] = {
                'success': True,
                'stocks_downloaded': len(stock_data)
            }
            
            # 2. åˆ›å»ºç‰¹å¾
            feature_data = self.create_traditional_features(stock_data)
            if len(feature_data) == 0:
                raise ValueError("ç‰¹å¾åˆ›å»ºå¤±è´¥")
            
            # ğŸ”¥ æ–°å¢ï¼šAlphaæ‘˜è¦ç‰¹å¾é›†æˆï¼ˆRoute A: Representation-levelï¼‰
            alpha_integration_success = False
            try:
                original_cols = feature_data.shape[1]
                alpha_result = self._integrate_alpha_summary_features(feature_data, stock_data)
                
                # ğŸ”§ CRITICAL FIX: ä¿®å¤Alphaé›†æˆçŠ¶æ€åˆ¤æ–­é€»è¾‘ï¼Œé¿å…çŸ›ç›¾æ—¥å¿—
                if alpha_result is not None and not alpha_result.empty:
                    result_cols = alpha_result.shape[1]
                    if result_cols > original_cols:
                        # æˆåŠŸæ·»åŠ äº†Alphaç‰¹å¾
                        feature_data = alpha_result
                        alpha_integration_success = True
                        added_features = result_cols - original_cols
                        logger.info(f"âœ… Alphaæ‘˜è¦ç‰¹å¾é›†æˆæˆåŠŸï¼Œç‰¹å¾ç»´åº¦: {feature_data.shape}")
                        logger.info(f"   - æ–°å¢Alphaç‰¹å¾: {added_features}ä¸ª")
                    else:
                        # åˆ—æ•°ç›¸åŒï¼Œè¯´æ˜æ²¡æœ‰æˆåŠŸæ·»åŠ Alphaç‰¹å¾
                        logger.warning("âš ï¸ Alphaæ‘˜è¦ç‰¹å¾æœªç”Ÿæˆæ–°ç‰¹å¾ï¼Œä½¿ç”¨ä¼ ç»Ÿç‰¹å¾")
                else:
                    # alpha_resultä¸ºNoneæˆ–emptyï¼Œæ˜ç¡®è¡¨ç¤ºAlphaé›†æˆå¤±è´¥
                    logger.warning("âš ï¸ Alphaæ‘˜è¦ç‰¹å¾é›†æˆå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿç‰¹å¾")
            except Exception as e:
                logger.warning(f"âš ï¸ Alphaæ‘˜è¦ç‰¹å¾é›†æˆå¼‚å¸¸: {e}ï¼Œä½¿ç”¨ä¼ ç»Ÿç‰¹å¾")
            
            analysis_results['feature_engineering'] = {
                'success': True,
                'feature_shape': feature_data.shape,
                'feature_columns': len([col for col in feature_data.columns 
                                      if col not in ['ticker', 'date', 'target']]),
                'alpha_integrated': 'alpha_pc1' in feature_data.columns or 'alpha_composite_orth1' in feature_data.columns
            }
            
            # 3. æ„å»ºMulti-factoré£é™©æ¨¡å‹ - ä½¿ç”¨å·²æœ‰æ•°æ®é¿å…é‡å¤ä¸‹è½½
            try:
                risk_model = self.build_risk_model(stock_data=stock_data, start_date=start_date, end_date=end_date)
                analysis_results['risk_model'] = {
                    'success': True,
                    'factor_count': len(risk_model['risk_factors'].columns),
                    'assets_covered': len(risk_model['factor_loadings'])
                }
                logger.info("é£é™©æ¨¡å‹æ„å»ºå®Œæˆ")
            except Exception as e:
                logger.warning(f"é£é™©æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
                analysis_results['risk_model'] = {'success': False, 'error': str(e)}
            
            # 4. æ£€æµ‹å¸‚åœºçŠ¶æ€ - ä½¿ç”¨å·²æœ‰æ•°æ®é¿å…é‡å¤ä¸‹è½½
            try:
                market_regime = self.detect_market_regime(stock_data=stock_data, start_date=start_date, end_date=end_date)
                analysis_results['market_regime'] = {
                    'success': True,
                    'regime': market_regime.name,
                    'probability': market_regime.probability,
                    'characteristics': market_regime.characteristics
                }
                logger.info(f"å¸‚åœºçŠ¶æ€æ£€æµ‹å®Œæˆ: {market_regime.name}")
            except Exception as e:
                logger.warning(f"å¸‚åœºçŠ¶æ€æ£€æµ‹å¤±è´¥: {e}")
                analysis_results['market_regime'] = {'success': False, 'error': str(e)}
                market_regime = MarketRegime(0, "Normal", 0.7, {'volatility': 0.15, 'trend': 0.0})
            
            # 5. è®­ç»ƒæ¨¡å‹
            training_results = self.train_enhanced_models(feature_data)
            analysis_results['model_training'] = training_results
            
            # 6. ç”Ÿæˆé¢„æµ‹ï¼ˆç»“åˆregime-awareæƒé‡ï¼‰
            ensemble_predictions = self.generate_enhanced_predictions(training_results, market_regime)
            
            # ğŸ”§ Enhanced debugging for prediction failure
            
            # ğŸ”¥ CRITICAL FIX: æ”¹è¿›é”™è¯¯å¤„ç†é€»è¾‘
            
            logger.info(f"é¢„æµ‹ç”Ÿæˆç»“æœç±»å‹: {type(ensemble_predictions)}")
            
            # æ£€æŸ¥é¢„æµ‹ç»“æœçš„æœ‰æ•ˆæ€§
            if ensemble_predictions is None:
                logger.error("é¢„æµ‹ç”Ÿæˆè¿”å›None")
            elif hasattr(ensemble_predictions, '__len__'):
                pred_len = len(ensemble_predictions)
                logger.info(f"é¢„æµ‹ç”Ÿæˆè¿”å›é•¿åº¦: {pred_len}")
                
                if pred_len == 0:
                    logger.error("é¢„æµ‹ç”Ÿæˆè¿”å›ç©ºç»“æœ")
                    # è¯¦ç»†è¯Šæ–­ä¿¡æ¯
                    logger.error("Training results keys: %s", list(training_results.keys()))
                    
                    # å°è¯•ä»alignment_reportè·å–ä¿¡æ¯
                    if 'alignment_report' in analysis_results:
                        ar = analysis_results['alignment_report']
                        logger.error(f"å¯¹é½æŠ¥å‘Š: æœ‰æ•ˆè‚¡ç¥¨={ar.effective_tickers}, æœ‰æ•ˆæ—¥æœŸ={ar.effective_dates}")
                        logger.error(f"æ¨ªæˆªé¢å°±ç»ª: {ar.cross_section_ready}")
                    
                    # å°è¯•ç”Ÿæˆfallbacké¢„æµ‹
                    logger.warning("å°è¯•ç”Ÿæˆå›é€€é¢„æµ‹...")
            
            if ensemble_predictions is None:
                logger.error("é¢„æµ‹ç”Ÿæˆè¿”å›None")
                raise ValueError("é¢„æµ‹ç”Ÿæˆå¤±è´¥: è¿”å›None")
            elif hasattr(ensemble_predictions, '__len__') and len(ensemble_predictions) == 0:
                logger.error(f"é¢„æµ‹ç”Ÿæˆè¿”å›ç©ºç»“æœï¼Œé•¿åº¦: {len(ensemble_predictions)}")
                logger.error(f"Training results keys: {list(training_results.keys()) if training_results else 'None'}")
                
                # ğŸ”§ Try to generate fallback predictions
                logger.warning("å°è¯•ç”Ÿæˆå›é€€é¢„æµ‹...")
                try:
                    fallback_predictions = self._generate_base_predictions(training_results)
                    if len(fallback_predictions) > 0:
                        logger.info(f"å›é€€é¢„æµ‹æˆåŠŸï¼Œé•¿åº¦: {len(fallback_predictions)}")
                        ensemble_predictions = fallback_predictions
                    else:
                        raise ValueError("é¢„æµ‹ç”Ÿæˆå¤±è´¥: é›†æˆé¢„æµ‹å’Œå¢å¼ºé¢„æµ‹å‡ä¸ºç©º")
                except Exception as fallback_error:
                    logger.error(f"å›é€€é¢„æµ‹ä¹Ÿå¤±è´¥: {fallback_error}")
                    raise ValueError("é¢„æµ‹ç”Ÿæˆå¤±è´¥: æ‰€æœ‰é¢„æµ‹æ–¹æ³•å‡å¤±è´¥")
            else:
                logger.info(f"é¢„æµ‹ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(ensemble_predictions) if hasattr(ensemble_predictions, '__len__') else 'N/A'}")
            
            # ğŸ”¥ CRITICAL FIX: ç”ŸæˆBMAæƒé‡æ˜ç»†ï¼Œä¾›éªŒè¯å™¨ä½¿ç”¨
            bma_weights = self._extract_bma_weights_from_training(training_results)
            self._last_weight_details = {
                'model_performance': self._extract_model_performance(training_results),
                'ensemble_weights': bma_weights,
                'diversity_metrics': self._calculate_ensemble_diversity(training_results),
                'oos_ready_models': [k for k, v in bma_weights.items() if v > 0.01],  # æœ‰æ•ˆæƒé‡æ¨¡å‹
                'weight_herfindahl': sum(w**2 for w in bma_weights.values()),  # é›†ä¸­åº¦æŒ‡æ ‡
                'timestamp': pd.Timestamp.now()
            }
            
            logger.info(f"BMAæƒé‡æ˜ç»†: {len(self._last_weight_details['oos_ready_models'])} ä¸ªæœ‰æ•ˆæ¨¡å‹ï¼ŒHerfindahl={self._last_weight_details['weight_herfindahl']:.3f}")
            
            analysis_results['prediction_generation'] = {
                'success': True,
                'predictions_count': len(ensemble_predictions),
                'prediction_stats': {
                    'mean': ensemble_predictions.mean(),
                    'std': ensemble_predictions.std(),
                    'min': ensemble_predictions.min(),
                    'max': ensemble_predictions.max()
                },
                'regime_adjusted': True,
                'bma_weights_summary': {
                    'active_models': len(self._last_weight_details['oos_ready_models']),
                    'concentration': self._last_weight_details['weight_herfindahl']
                },
                # CRITICAL FIX: æ·»åŠ é£é™©çº¦æŸéªŒè¯
                'risk_constraints_check': self._validate_portfolio_risk_constraints(ensemble_predictions)
            }
            
            # 7. è‚¡ç¥¨é€‰æ‹©å’Œæ’åï¼ˆå¸¦é£é™©åˆ†æï¼‰
            selection_result = self.generate_stock_ranking_with_risk_analysis(ensemble_predictions, feature_data)
            analysis_results['stock_selection'] = selection_result
            
            # 8. ç”Ÿæˆè‚¡ç¥¨æ¨è
            recommendations = self._generate_stock_recommendations(selection_result, top_n)
            analysis_results['recommendations'] = recommendations
            
            # 9. ä¿å­˜ç»“æœ
            result_file = self._save_results(recommendations, selection_result, analysis_results)
            analysis_results['result_file'] = result_file
            
            analysis_results['end_time'] = datetime.now()
            analysis_results['total_time'] = (analysis_results['end_time'] - analysis_results['start_time']).total_seconds()
            analysis_results['success'] = True
            
            # æ·»åŠ å¥åº·ç›‘æ§æŠ¥å‘Š
            analysis_results['health_report'] = self.get_health_report()
            
            # ğŸ”¥ ç”Ÿäº§å°±ç»ªæ€§éªŒè¯
            try:
                logger.info("å¼€å§‹ç”Ÿäº§å°±ç»ªæ€§éªŒè¯...")
                
                # ğŸ”¥ CRITICAL FIX: ä½¿ç”¨IndexAlignerå¯¹é½éªŒè¯æ•°æ®ï¼Œè§£å†³738 vs 748é—®é¢˜
                if hasattr(self, 'feature_data') and self.feature_data is not None and self.production_validator:
                    logger.info("ğŸ¯ ç”Ÿäº§éªŒè¯æ•°æ®IndexAlignerå¯¹é½å¼€å§‹...")
                    
                    # åŸå§‹æ•°æ®
                    raw_predictions = ensemble_predictions.values if hasattr(ensemble_predictions, 'values') else np.array(ensemble_predictions)
                    raw_labels = self.feature_data['target'].values
                    raw_dates = pd.Series(self.feature_data['date'])
                    
                    logger.info(f"ğŸ“Š éªŒè¯å‰ç»´åº¦: predictions={len(raw_predictions)}, labels={len(raw_labels)}, dates={len(raw_dates)}")
                    
                    # ä½¿ç”¨IndexAlignerç»Ÿä¸€å¯¹é½éªŒè¯æ•°æ®
                    try:
                        from index_aligner import create_index_aligner
                        # ğŸ”¥ CRITICAL FIX: éªŒè¯horizonå¿…é¡»ä¸è®­ç»ƒä¸€è‡´
                        validation_aligner = create_index_aligner(horizon=10, strict_mode=True)  # ä¸è®­ç»ƒT+10ä¸€è‡´ï¼Œé¿å…å‰è§†åå·®
                        
                        # æš‚æ—¶æ³¨é‡Šä¸å®Œæ•´çš„è¡Œ
                        # aligned_validation_data, validation_report = validation_
                        
                        # ğŸ”¥ CRITICAL DATA FORMAT VALIDATION
                        logger.info("ğŸ“Š IndexAlignerè¾“å…¥æ•°æ®æ ¼å¼éªŒè¯:")
                        
                        for data_name, data_obj in [('X', X), ('y', y), ('dates', dates), ('tickers', tickers)]:
                            if data_obj is not None:
                                logger.info(f"  {data_name}: ç±»å‹={type(data_obj)}, å½¢çŠ¶={getattr(data_obj, 'shape', len(data_obj) if hasattr(data_obj, '__len__') else 'N/A')}")
                
                                if hasattr(data_obj, 'index'):
                                    index_info = f"ç´¢å¼•ç±»å‹={type(data_obj.index)}"
                                    if isinstance(data_obj.index, pd.MultiIndex):
                                        unique_tickers = len(data_obj.index.get_level_values(1).unique()) if data_obj.index.nlevels >= 2 else 0
                                        unique_dates = len(data_obj.index.get_level_values(0).unique()) if data_obj.index.nlevels >= 1 else 0
                                        index_info += f", å±‚çº§={data_obj.index.nlevels}, è‚¡ç¥¨æ•°={unique_tickers}, æ—¥æœŸæ•°={unique_dates}"
                        
                                        # ğŸ”¥ CRITICAL: éªŒè¯æ•°æ®å®Œæ•´æ€§
                                        expected_length = unique_tickers * unique_dates
                                        actual_length = len(data_obj)
                                        if actual_length != expected_length:
                                            logger.warning(f"    âš ï¸ æ•°æ®é•¿åº¦ä¸åŒ¹é…: å®é™…{actual_length} vs é¢„æœŸ{expected_length}")
                                        else:
                                            logger.info(f"    âœ… MultiIndexæ•°æ®å®Œæ•´: {unique_tickers}è‚¡ç¥¨ Ã— {unique_dates}æ—¥æœŸ = {actual_length}")
                                    else:
                                        index_info += ", è‚¡ç¥¨æ•°=1 (å¯èƒ½æœ‰é—®é¢˜!)"
                                        if len(data_obj) > 1000:  # å¦‚æœæ•°æ®å¾ˆé•¿ä½†ä¸æ˜¯MultiIndex
                                            logger.error(f"    âŒ å¯ç–‘: {data_name}æœ‰{len(data_obj)}æ¡æ•°æ®ä½†ä¸æ˜¯MultiIndexæ ¼å¼!")
                                    
                                    logger.info(f"    {index_info}")
        
                        # ğŸ”¥ CRITICAL: å¦‚æœæ£€æµ‹åˆ°æ•°æ®æ ¼å¼é—®é¢˜ï¼Œå°è¯•ä¿®å¤
                        if X is not None and not isinstance(X.index, pd.MultiIndex) and len(X) > 1000:
                            logger.warning("ğŸš¨ æ£€æµ‹åˆ°å¯èƒ½çš„æ•°æ®æ ¼å¼é—®é¢˜ï¼Œå°è¯•ä¿®å¤...")
            
                            # å°è¯•ä»feature_dataé‡å»ºMultiIndex
                            if hasattr(self, 'feature_data') and self.feature_data is not None:
                                if 'ticker' in self.feature_data.columns and 'date' in self.feature_data.columns:
                                    logger.info("ğŸ”§ å°è¯•ä»feature_dataé‡å»ºMultiIndex...")
                        
                                    try:
                                        # é‡å»ºMultiIndex
                                        feature_data_copy = self.feature_data.copy()
                                        feature_data_copy['date'] = pd.to_datetime(feature_data_copy['date'])
                                        
                                        # è®¾ç½®MultiIndex
                                        feature_data_copy = feature_data_copy.set_index(['date', 'ticker']).sort_index()
                                        
                                        # æå–ç‰¹å¾åˆ—ï¼ˆæ’é™¤éæ•°å€¼åˆ—ï¼‰
                                        numeric_cols = feature_data_copy.select_dtypes(include=[float, int]).columns
                                        X_fixed = feature_data_copy[numeric_cols]
                                        
                                        # ç”Ÿæˆå¯¹åº”çš„yï¼ˆç®€å•ä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºç›®æ ‡ï¼Œå®é™…åº”è¯¥ç”¨çœŸå®ç›®æ ‡ï¼‰
                                        if len(numeric_cols) > 0:
                                            y_fixed = feature_data_copy[numeric_cols[0]]  # ä¸´æ—¶ä½¿ç”¨ç¬¬ä¸€åˆ—
                                            
                                            # æå–dateså’Œtickers
                                            dates_fixed = X_fixed.index.get_level_values(0)
                                            tickers_fixed = X_fixed.index.get_level_values(1)
                                            
                                            logger.info(f"ğŸ¯ æ•°æ®æ ¼å¼ä¿®å¤æˆåŠŸ!")
                                            logger.info(f"  ä¿®å¤åX: {X_fixed.shape}")
                                            logger.info(f"  ä¿®å¤åè‚¡ç¥¨æ•°: {len(X_fixed.index.get_level_values(1).unique())}")
                                            logger.info(f"  ä¿®å¤åæ—¥æœŸæ•°: {len(X_fixed.index.get_level_values(0).unique())}")
                                            
                                            # ä½¿ç”¨ä¿®å¤åçš„æ•°æ®
                                            X = X_fixed
                                            y = y_fixed
                                            dates = dates_fixed  
                                            tickers = tickers_fixed
                                    
                                    except Exception as fix_error:
                                        logger.error(f"âŒ æ•°æ®æ ¼å¼ä¿®å¤å¤±è´¥: {fix_error}")
                                        logger.warning("âš ï¸ ç»§ç»­ä½¿ç”¨åŸå§‹æ•°æ®ï¼Œä½†å¯èƒ½å½±å“ç»“æœ")

                        # ç»§ç»­ä¸»æµç¨‹ï¼šå¯¹é½éªŒè¯æ•°æ®
                        aligned_data, alignment_report = aligner.align_all_data(
                            oos_predictions=pd.Series(raw_predictions),
                            oos_true_labels=pd.Series(raw_labels), 
                            prediction_dates=raw_dates
                        )
                        
                        # ä½¿ç”¨å¯¹é½åçš„æ•°æ®
                        oos_predictions = aligned_data['oos_predictions'].values
                        oos_true_labels = aligned_data['oos_true_labels'].values
                        prediction_dates = aligned_data['prediction_dates']
                        
                        logger.info(f"âœ… éªŒè¯æ•°æ®å¯¹é½æˆåŠŸ: ç»Ÿä¸€é•¿åº¦={len(oos_predictions)}, è¦†ç›–ç‡={alignment_report.coverage_rate:.1%}")
                        
                    except Exception as align_e:
                        logger.error(f"âŒ éªŒè¯æ•°æ®å¯¹é½å¤±è´¥: {align_e}")
                        logger.warning("ä½¿ç”¨åŸå§‹æ•°æ®è¿›è¡ŒéªŒè¯ï¼ˆå¯èƒ½å­˜åœ¨ç»´åº¦ä¸åŒ¹é…ï¼‰")
                        # ç®€å•æˆªæ–­åˆ°æœ€å°é•¿åº¦ä½œä¸ºå›é€€
                        min_len = min(len(raw_predictions), len(raw_labels), len(raw_dates))
                        oos_predictions = raw_predictions[:min_len]
                        oos_true_labels = raw_labels[:min_len]
                        prediction_dates = raw_dates.iloc[:min_len]
                    
                    # ğŸ”¥ CRITICAL FIX: å›å½’ä»»åŠ¡æ ¡å‡†ï¼ˆä¸ä½¿ç”¨åˆ†ç±»Brier Scoreï¼‰
                    calibration_result = None
                    try:
                        # ä½¿ç”¨ç®€å•çš„çº¿æ€§ç¼©æ”¾æ ¡å‡†ï¼ˆå›å½’é€‚ç”¨ï¼‰
                        calibrated_preds, calibration_metrics = self._linear_regression_calibration(
                            oos_predictions, oos_true_labels
                        )
                        
                        calibration_result = {
                            'success': True,
                            'calibrated_predictions': calibrated_preds,
                            'calibration_metrics': calibration_metrics,
                            'original_predictions': oos_predictions
                        }
                        
                        logger.info(f"âœ… å›å½’æ ¡å‡†å®Œæˆ: RÂ² = {calibration_metrics.get('r2_score', 'N/A'):.4f}")
                        
                    except Exception as e:
                        logger.warning(f"å›å½’æ ¡å‡†å¤±è´¥: {e}")
                        calibration_result = None
                    
                    # ğŸ”¥ CRITICAL FIX: å•è‚¡ç¥¨æƒ…å†µä½¿ç”¨ä¸“ç”¨éªŒè¯å™¨
                    is_single_stock = False
                    if hasattr(self, 'feature_data') and self.feature_data is not None and 'ticker' in self.feature_data.columns:
                        unique_tickers = self.feature_data['ticker'].nunique()
                        is_single_stock = unique_tickers == 1
                        
                    if is_single_stock:
                        logger.info("ğŸ¯ æ£€æµ‹åˆ°å•è‚¡ç¥¨æƒ…å†µï¼Œä½¿ç”¨ä¸“ç”¨æ—¶é—´åºåˆ—éªŒè¯")
                        try:
                            from single_stock_validator import create_single_stock_validator
                            
                            single_validator = create_single_stock_validator()
                            single_result = single_validator.validate_single_stock_predictions(
                                oos_predictions, oos_true_labels, prediction_dates
                            )
                            
                            if single_result.get('success', False):
                                logger.info(f"âœ… å•è‚¡ç¥¨éªŒè¯: {'PASS' if single_result['passed'] else 'FAIL'}, å¾—åˆ†: {single_result['score']:.3f}")
                                logger.info(f"   ç›¸å…³æ€§: {single_result['metrics']['correlation']:.3f}")
                                logger.info(f"   å‘½ä¸­ç‡: {single_result['metrics']['hit_rate']:.3f}")
                                logger.info(f"   Sharpe: {single_result['metrics']['sharpe_ratio']:.3f}")
                                
                                # ç”¨å•è‚¡ç¥¨éªŒè¯ç»“æœè¦†ç›–é»˜è®¤éªŒè¯
                                analysis_results['single_stock_validation'] = single_result
                                
                                # å¦‚æœå•è‚¡ç¥¨éªŒè¯é€šè¿‡ï¼Œè®°å½•ä½†ç»§ç»­æ‰§è¡Œå®Œæ•´è®­ç»ƒæµç¨‹
                                if single_result['passed']:
                                    logger.info("âœ… å•è‚¡ç¥¨éªŒè¯é€šè¿‡ï¼Œä½†ç»§ç»­æ‰§è¡Œå®Œæ•´æœºå™¨å­¦ä¹ è®­ç»ƒæµç¨‹")
                                    analysis_results['single_stock_validation_passed'] = True
                                    analysis_results['single_stock_score'] = single_result['score']
                                    # ä¸è¦æå‰è¿”å›ï¼Œç»§ç»­æ‰§è¡Œåç»­çš„æœºå™¨å­¦ä¹ è®­ç»ƒ
                        except ImportError:
                            logger.warning("å•è‚¡ç¥¨éªŒè¯å™¨å¯¼å…¥å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†éªŒè¯")
                        except Exception as e:
                            logger.warning(f"å•è‚¡ç¥¨éªŒè¯å¤±è´¥: {e}")
                    
                    # ğŸš€ ä½¿ç”¨å¢å¼ºç”Ÿäº§é—¨ç¦ç³»ç»Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if PRODUCTION_FIXES_AVAILABLE and self.production_gate:
                        logger.info("ğŸ”§ ä½¿ç”¨å¢å¼ºç”Ÿäº§é—¨ç¦ç³»ç»Ÿè¿›è¡ŒéªŒè¯")
                        
                        # è®¡ç®—æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
                        from sklearn.metrics import mean_squared_error
                        import scipy.stats as stats
                        
                        # å‡†å¤‡éªŒè¯æŒ‡æ ‡
                        model_metrics = {}
                        if len(oos_predictions) > 0 and len(oos_true_labels) > 0:
                            # ğŸ”¥ CRITICAL FIX: æ¨ªæˆªé¢RankICè®¡ç®—
                            cross_sectional_ic, valid_days = self._calculate_cross_sectional_ic(
                                oos_predictions, oos_true_labels, prediction_dates
                            )
                            
                            if cross_sectional_ic is not None and valid_days > 0:
                                model_metrics['rank_ic_mean'] = cross_sectional_ic
                                model_metrics['rank_ic_t_stat'] = abs(cross_sectional_ic) * np.sqrt(valid_days)  # è¿‘ä¼¼tç»Ÿè®¡é‡
                                model_metrics['valid_cross_section_days'] = valid_days
                                logger.info(f"âœ… æ¨ªæˆªé¢IC: {cross_sectional_ic:.4f}, æœ‰æ•ˆå¤©æ•°: {valid_days}")
                            else:
                                model_metrics['rank_ic_mean'] = 0
                                model_metrics['rank_ic_t_stat'] = 0
                                model_metrics['valid_cross_section_days'] = 0
                                logger.warning("âŒ æ— æ³•è®¡ç®—æœ‰æ•ˆçš„æ¨ªæˆªé¢IC")
                            
                            # QLIKEè¯¯å·®è®¡ç®—ï¼ˆå¦‚æœæœ‰æ¦‚ç‡é¢„æµ‹ï¼‰
                            try:
                                qlike_error = mean_squared_error(oos_true_labels, oos_predictions)
                                model_metrics['qlike_error'] = qlike_error
                            except:
                                pass
                            
                            # è®¡ç®—è¦†ç›–æœˆæ•°
                            if prediction_dates is not None and len(prediction_dates) > 0:
                                date_range = pd.to_datetime(prediction_dates).max() - pd.to_datetime(prediction_dates).min()
                                coverage_months = date_range.days / 30.44  # å¹³å‡æœˆé•¿åº¦
                            else:
                                coverage_months = 0
                            
                            # è¿è¡Œå¢å¼ºç”Ÿäº§é—¨ç¦éªŒè¯
                            gate_result = self.production_gate.validate_for_production(
                                model_metrics=model_metrics,
                                baseline_metrics=None,  # å¯é€‰ï¼šä¼ å…¥åŸºå‡†æ¨¡å‹æŒ‡æ ‡è¿›è¡Œå¯¹æ¯”
                                coverage_months=coverage_months,
                                model_name="BMA_Enhanced"
                            )
                            
                            analysis_results['enhanced_production_gate'] = {
                                'passed': gate_result.passed,
                                'gate_type': gate_result.gate_type,
                                'score': gate_result.score,
                                'risk_level': gate_result.risk_level,
                                'recommendation': gate_result.recommendation,
                                'details': gate_result.details
                            }
                            
                            logger.info(f"ğŸ¯ å¢å¼ºç”Ÿäº§é—¨ç¦å†³ç­–: {'é€šè¿‡' if gate_result.passed else 'æœªé€šè¿‡'}")
                            logger.info(f"   éªŒè¯ç±»å‹: {gate_result.gate_type}")
                            logger.info(f"   ç»¼åˆå¾—åˆ†: {gate_result.score:.3f}")
                            logger.info(f"   é£é™©ç­‰çº§: {gate_result.risk_level}")
                            logger.info(f"   å»ºè®®: {gate_result.recommendation}")
                            
                        else:
                            logger.warning("âš ï¸ ç¼ºå°‘éªŒè¯æ•°æ®ï¼Œè·³è¿‡å¢å¼ºç”Ÿäº§é—¨ç¦éªŒè¯")
                    
                    # è¿è¡ŒåŸç”Ÿäº§å°±ç»ªæ€§éªŒè¯ï¼ˆä½œä¸ºè¡¥å……ï¼‰
                    if self.production_validator:
                        readiness_result = self.production_validator.validate_bma_production_readiness(
                            oos_predictions=oos_predictions,
                            oos_true_labels=oos_true_labels,
                            prediction_dates=prediction_dates,
                            calibration_results=calibration_result if calibration_result and calibration_result.get('success') else None,
                            weight_details=getattr(self, '_last_weight_details', None)
                        )
                        
                        analysis_results['production_readiness'] = {
                            'validation_result': readiness_result,
                            'calibration_result': calibration_result
                        }
                        
                        # è®°å½•Go/No-Goå†³ç­–
                        decision = readiness_result.go_no_go_decision
                        score = readiness_result.overall_score
                        logger.info(f"ğŸ“Š åŸç”Ÿäº§éªŒè¯å†³ç­–: {decision} (å¾—åˆ†: {score:.2f})")
                        
                        # æ˜¾ç¤ºè¯¦ç»†å»ºè®®
                        if readiness_result.recommendations:
                            logger.info("ğŸ“‹ éªŒè¯å»ºè®®:")
                            for rec in readiness_result.recommendations:
                                logger.info(f"  â€¢ {rec}")
                    
                else:
                    logger.warning("ç¼ºå°‘éªŒè¯æ‰€éœ€æ•°æ®ï¼Œè·³è¿‡ç”Ÿäº§å°±ç»ªæ€§éªŒè¯")
                    analysis_results['production_readiness'] = {'skipped': True, 'reason': 'ç¼ºå°‘æ•°æ®'}
                    
            except Exception as e:
                logger.warning(f"ç”Ÿäº§å°±ç»ªæ€§éªŒè¯å¤±è´¥: {e}")
                analysis_results['production_readiness'] = {'failed': True, 'error': str(e)}
            
            logger.info(f"å®Œæ•´åˆ†ææµç¨‹å®Œæˆï¼Œè€—æ—¶: {analysis_results['total_time']:.1f}ç§’")
            logger.info(f"ç³»ç»Ÿå¥åº·çŠ¶å†µ: {analysis_results['health_report']['risk_level']}, "
                       f"å¤±è´¥ç‡: {analysis_results['health_report']['failure_rate_percent']:.2f}%")
            
            return analysis_results
            
        except Exception as e:
            logger.error(f"åˆ†ææµç¨‹å¤±è´¥: {e}")
            analysis_results['error'] = str(e)
            analysis_results['success'] = False
            analysis_results['end_time'] = datetime.now()
            
            return analysis_results

    def run_analysis(self, tickers: List[str], 
                    start_date: str = "2021-01-01", 
                    end_date: str = "2024-12-31",
                    top_n: int = 10) -> Dict[str, Any]:
        """
        ä¸»åˆ†ææ–¹æ³• - æ™ºèƒ½é€‰æ‹©V6å¢å¼ºç³»ç»Ÿæˆ–å›é€€æœºåˆ¶
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            top_n: è¿”å›æ¨èæ•°é‡
            
        Returns:
            åˆ†æç»“æœ
        """
        logger.info(f"ğŸš€ å¯åŠ¨é‡åŒ–åˆ†ææµç¨‹ - V6å¢å¼º: {self.enable_v6_enhancements}")
        
        # V6å¢å¼ºç³»ç»Ÿå·²åˆ é™¤ - ä½¿ç”¨ç»Ÿä¸€è·¯å¾„
        
        # å›é€€åˆ°ä¼ ç»Ÿåˆ†ææ–¹æ³•
        logger.info("ğŸ“Š ä½¿ç”¨ä¼ ç»ŸBMAç³»ç»Ÿè¿›è¡Œåˆ†æ")
        return self._run_traditional_analysis(tickers, start_date, end_date, top_n)
        
    def _run_traditional_analysis(self, tickers: List[str], 
                                 start_date: str, end_date: str,
                                 top_n: int = 10) -> Dict[str, Any]:
        """
        ä¼ ç»Ÿåˆ†ææ–¹æ³• - å›é€€æœºåˆ¶
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            top_n: è¿”å›æ¨èæ•°é‡
            
        Returns:
            ä¼ ç»Ÿåˆ†æç»“æœ
        """
        traditional_start = datetime.now()
        
        try:
            # ä½¿ç”¨ç°æœ‰çš„æ‰¹é‡åˆ†ææ–¹æ³•
            logger.info("æ‰§è¡Œä¼ ç»Ÿæ‰¹é‡åˆ†æ...")
            
            # ç›´æ¥ä½¿ç”¨å·²æœ‰çš„ä¼˜åŒ–åˆ†ææ–¹æ³•
            results = self.run_complete_analysis(tickers, start_date, end_date, top_n)
            
            # æ·»åŠ ä¼ ç»Ÿåˆ†ææ ‡è¯†
            results['analysis_method'] = 'traditional_bma'
            results['v6_enhancements'] = 'not_used'
            results['execution_time'] = (datetime.now() - traditional_start).total_seconds()
            
            logger.info(f"âœ… ä¼ ç»Ÿåˆ†æå®Œæˆ: {results['execution_time']:.1f}s")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ä¼ ç»Ÿåˆ†æä¹Ÿå¤±è´¥: {e}")
            
            # æœ€å°å¯è¡Œåˆ†æç»“æœ
            return {
                'success': False,
                'error': f'æ‰€æœ‰åˆ†ææ–¹æ³•å‡å¤±è´¥: {str(e)}',
                'analysis_method': 'failed',
                'v6_enhancements': 'not_available',
                'execution_time': (datetime.now() - traditional_start).total_seconds(),
                'predictions': {},
                'recommendations': []
            }


def main():
    """ä¸»å‡½æ•°"""
    print("=== BMA Ultra Enhanced é‡åŒ–åˆ†ææ¨¡å‹ V4 ===")
    print("é›†æˆAlphaç­–ç•¥ã€Learning-to-Rankã€é«˜çº§æŠ•èµ„ç»„åˆä¼˜åŒ–")
    print(f"å¢å¼ºæ¨¡å—å¯ç”¨: {ENHANCED_MODULES_AVAILABLE}")
    print(f"é«˜çº§æ¨¡å‹: XGBoost={XGBOOST_AVAILABLE}, LightGBM={LIGHTGBM_AVAILABLE}")
    
    # è®¾ç½®å…¨å±€è¶…æ—¶ä¿æŠ¤
    start_time = time.time()
    MAX_EXECUTION_TIME = 300  # 5åˆ†é’Ÿè¶…æ—¶
    
    # å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='BMA Ultra Enhancedé‡åŒ–æ¨¡å‹V4')
    parser.add_argument('--start-date', type=str, default='2022-08-26', help='å¼€å§‹æ—¥æœŸ (3å¹´è®­ç»ƒæœŸ)')
    parser.add_argument('--end-date', type=str, default='2025-08-26', help='ç»“æŸæ—¥æœŸ (3å¹´è®­ç»ƒæœŸ)')
    parser.add_argument('--top-n', type=int, default=200, help='è¿”å›top Nä¸ªæ¨è')
    parser.add_argument('--config', type=str, default='alphas_config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--tickers', type=str, nargs='+', default=None, help='è‚¡ç¥¨ä»£ç åˆ—è¡¨')
    parser.add_argument('--tickers-file', type=str, default='filtered_stocks_20250817_002928', help='è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªä»£ç ï¼‰')
    parser.add_argument('--tickers-limit', type=int, default=0, help='å…ˆç”¨å‰Nåªåšå°æ ·æœ¬æµ‹è¯•ï¼Œå†å…¨é‡è®­ç»ƒï¼ˆ0è¡¨ç¤ºç›´æ¥å…¨é‡ï¼‰')
    
    args = parser.parse_args()
    
    # ç¡®å®šè‚¡ç¥¨åˆ—è¡¨
    if args.tickers:
        tickers = args.tickers
    else:
        tickers = load_universe_from_file(args.tickers_file) or load_universe_fallback()
    
    print(f"åˆ†æå‚æ•°:")
    print(f"  æ—¶é—´èŒƒå›´: {args.start_date} - {args.end_date}")
    print(f"  è‚¡ç¥¨æ•°é‡: {len(tickers)}")
    print(f"  æ¨èæ•°é‡: {args.top_n}")
    print(f"  é…ç½®æ–‡ä»¶: {args.config}")
    
    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¯ç”¨å†…å­˜ä¼˜åŒ–ï¼‰
    model = UltraEnhancedQuantitativeModel(config_path=args.config, enable_optimization=True)
    
   
    # è¿è¡Œå®Œæ•´åˆ†æ (å¸¦è¶…æ—¶ä¿æŠ¤)
    try:
        results = model.run_complete_analysis(
            tickers=tickers,
            start_date=args.start_date,
            end_date=args.end_date,
            top_n=args.top_n
        )
        
        # æ£€æŸ¥æ‰§è¡Œæ—¶é—´
        execution_time = time.time() - start_time
        if execution_time > MAX_EXECUTION_TIME:
            print(f"\n[WARNING] æ‰§è¡Œæ—¶é—´è¶…è¿‡{MAX_EXECUTION_TIME}ç§’ï¼Œä½†å·²å®Œæˆ")
            
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        results = {'success': False, 'error': 'ç”¨æˆ·ä¸­æ–­'}
    except Exception as e:
        execution_time = time.time() - start_time
        print(f"\nâŒ æ‰§è¡Œå¼‚å¸¸ (è€—æ—¶{execution_time:.1f}s): {e}")
        results = {'success': False, 'error': str(e)}
    
    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    print("\n" + "="*60)
    print("åˆ†æç»“æœæ‘˜è¦")
    print("="*60)
    
    if results.get('success', False):
        # é¿å…æ§åˆ¶å°ç¼–ç é”™è¯¯ï¼ˆGBKï¼‰
        print(f"åˆ†ææˆåŠŸå®Œæˆï¼Œè€—æ—¶: {results['total_time']:.1f}ç§’")
        
        if 'data_download' in results:
            print(f"æ•°æ®ä¸‹è½½: {results['data_download']['stocks_downloaded']}åªè‚¡ç¥¨")
        
        if 'feature_engineering' in results:
            fe_info = results['feature_engineering']
            print(f"ç‰¹å¾å·¥ç¨‹: {fe_info['feature_shape'][0]}æ ·æœ¬, {fe_info['feature_columns']}ç‰¹å¾")
        
        if 'prediction_generation' in results:
            pred_info = results['prediction_generation']
            stats = pred_info['prediction_stats']
            print(f"é¢„æµ‹ç”Ÿæˆ: {pred_info['predictions_count']}ä¸ªé¢„æµ‹ (å‡å€¼: {stats['mean']:.4f})")
        
        if 'stock_selection' in results and results['stock_selection'].get('success', False):
            selection_metrics = results['stock_selection']['portfolio_metrics']
            print(f"è‚¡ç¥¨é€‰æ‹©: å¹³å‡é¢„æµ‹{selection_metrics.get('avg_prediction', 0):.4f}, "
                  f"è´¨é‡è¯„åˆ†{selection_metrics.get('quality_score', 0):.4f}")
        
        if 'recommendations' in results:
            recommendations = results['recommendations']
            print(f"\næŠ•èµ„å»ºè®® (Top {len(recommendations)}):")
            for i, rec in enumerate(recommendations[:5], 1):
                print(f"  {i}. {rec['ticker']}: æƒé‡{rec['weight']:.3f}, "
                      f"ä¿¡å·{rec['prediction_signal']:.4f}")
        
        if 'result_file' in results:
            print(f"\nç»“æœå·²ä¿å­˜è‡³: {results['result_file']}")
    
    else:
        print(f"åˆ†æå¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    print("="*60)

    def _validate_portfolio_risk_constraints(self, predictions: pd.Series) -> Dict[str, Any]:
        """
        CRITICAL FIX: æŠ•èµ„ç»„åˆé£é™©çº¦æŸéªŒè¯
        éªŒè¯é¢„æµ‹ç»“æœæ˜¯å¦æ»¡è¶³é£é™©ç®¡ç†è¦æ±‚
        """
        try:
            risk_check = {
                'passed': True,
                'warnings': [],
                'violations': [],
                'metrics': {}
            }
            
            # 1. é¢„æµ‹åˆ†å¸ƒæ£€æŸ¥
            pred_std = predictions.std()
            pred_mean = predictions.mean()
            
            risk_check['metrics']['prediction_volatility'] = pred_std
            risk_check['metrics']['prediction_mean'] = pred_mean
            
            # å¼‚å¸¸æ³¢åŠ¨ç‡æ£€æŸ¥
            if pred_std > 0.5:  # 50%æ ‡å‡†å·®é˜ˆå€¼
                risk_check['violations'].append(f"é¢„æµ‹æ³¢åŠ¨ç‡è¿‡é«˜: {pred_std:.2%}")
                risk_check['passed'] = False
            elif pred_std > 0.3:  # 30%è­¦å‘Šé˜ˆå€¼
                risk_check['warnings'].append(f"é¢„æµ‹æ³¢åŠ¨ç‡è¾ƒé«˜: {pred_std:.2%}")
            
            # 2. æå€¼æ£€æŸ¥
            extreme_predictions = predictions[(predictions > 1.0) | (predictions < -1.0)]
            if len(extreme_predictions) > 0:
                extreme_ratio = len(extreme_predictions) / len(predictions)
                risk_check['metrics']['extreme_prediction_ratio'] = extreme_ratio
                
                if extreme_ratio > 0.05:  # 5%æå€¼æ¯”ä¾‹é˜ˆå€¼
                    risk_check['violations'].append(f"æå€¼é¢„æµ‹æ¯”ä¾‹è¿‡é«˜: {extreme_ratio:.1%}")
                    risk_check['passed'] = False
                elif extreme_ratio > 0.02:
                    risk_check['warnings'].append(f"å­˜åœ¨æå€¼é¢„æµ‹: {extreme_ratio:.1%}")
            
            # 3. é›†ä¸­åº¦æ£€æŸ¥ï¼ˆå¦‚æœæœ‰è‚¡ç¥¨æƒé‡ä¿¡æ¯ï¼‰
            if hasattr(self, '_last_weight_details') and self._last_weight_details:
                herfindahl = self._last_weight_details.get('weight_herfindahl', 0)
                risk_check['metrics']['weight_concentration'] = herfindahl
                
                if herfindahl > 0.5:  # HerfindahlæŒ‡æ•°è¿‡é«˜
                    risk_check['violations'].append(f"æƒé‡é›†ä¸­åº¦è¿‡é«˜: {herfindahl:.2f}")
                    risk_check['passed'] = False
                elif herfindahl > 0.3:
                    risk_check['warnings'].append(f"æƒé‡é›†ä¸­åº¦è¾ƒé«˜: {herfindahl:.2f}")
            
            # 4. å¸‚åœºä¸­æ€§æ£€æŸ¥
            abs_mean = abs(pred_mean)
            if abs_mean > 0.1:  # 10%ç³»ç»Ÿæ€§åå·®é˜ˆå€¼
                risk_check['violations'].append(f"é¢„æµ‹å­˜åœ¨ç³»ç»Ÿæ€§åå·®: {pred_mean:.2%}")
                risk_check['passed'] = False
            elif abs_mean > 0.05:
                risk_check['warnings'].append(f"é¢„æµ‹åå·®è¾ƒå¤§: {pred_mean:.2%}")
            
            # 5. æ ·æœ¬æ•°é‡æ£€æŸ¥
            valid_predictions = predictions.dropna()
            if len(valid_predictions) < 10:
                risk_check['violations'].append(f"æœ‰æ•ˆé¢„æµ‹æ•°é‡ä¸è¶³: {len(valid_predictions)}")
                risk_check['passed'] = False
            elif len(valid_predictions) < 20:
                risk_check['warnings'].append(f"æœ‰æ•ˆé¢„æµ‹æ•°é‡è¾ƒå°‘: {len(valid_predictions)}")
            
            risk_check['metrics']['valid_prediction_count'] = len(valid_predictions)
            risk_check['metrics']['total_prediction_count'] = len(predictions)
            
            # è®°å½•æ£€æŸ¥ç»“æœ
            if not risk_check['passed']:
                logger.error(f"é£é™©çº¦æŸéªŒè¯å¤±è´¥: {len(risk_check['violations'])}ä¸ªè¿è§„")
            elif risk_check['warnings']:
                logger.warning(f"é£é™©çº¦æŸéªŒè¯é€šè¿‡ä½†æœ‰è­¦å‘Š: {len(risk_check['warnings'])}ä¸ªè­¦å‘Š")
            else:
                logger.info("é£é™©çº¦æŸéªŒè¯å®Œå…¨é€šè¿‡")
            
            return risk_check
            
        except Exception as e:
            logger.error(f"é£é™©çº¦æŸéªŒè¯å¼‚å¸¸: {e}")
            return {
                'passed': False,
                'error': str(e),
                'warnings': [],
                'violations': [f"éªŒè¯è¿‡ç¨‹å¼‚å¸¸: {e}"],
                'metrics': {}
            }


if __name__ == "__main__":
    main()
