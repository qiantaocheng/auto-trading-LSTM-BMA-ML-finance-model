#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BMA Ultra Enhanced é‡åŒ–åˆ†ææ¨¡å‹ V6 - ç”Ÿäº§å°±ç»ªå¢å¼ºç‰ˆ
é›†æˆAlphaç­–ç•¥ã€Learning-to-Rankã€ä¸ç¡®å®šæ€§æ„ŸçŸ¥BMAã€é«˜çº§æŠ•èµ„ç»„åˆä¼˜åŒ–

V6æ–°å¢åŠŸèƒ½ï¼ˆä¿®å¤æ‰€æœ‰å…³é”®é—®é¢˜ï¼‰:
- ä¿®å¤Purge/EmbargoåŒé‡éš”ç¦»é—®é¢˜ï¼ˆé€‰æ‹©å•ä¸€éš”ç¦»æ–¹æ³•ï¼‰
- é˜²æ³„æ¼Regimeæ£€æµ‹ï¼ˆä»…ä½¿ç”¨è¿‡æ»¤ï¼Œç¦ç”¨å¹³æ»‘ï¼‰
- T-5åˆ°T-0/T-1ç‰¹å¾æ»åä¼˜åŒ–ï¼ˆA/Bæµ‹è¯•é€‰æ‹©ï¼‰
- å› å­æ—ç‰¹å®šè¡°å‡åŠè¡°æœŸï¼ˆæ›¿ä»£ç»Ÿä¸€8å¤©è¡°å‡ï¼‰
- ä¼˜åŒ–æ—¶é—´è¡°å‡åŠè¡°æœŸï¼ˆ60-90å¤©è€Œé90-120å¤©ï¼‰
- ç”Ÿäº§å°±ç»ªé—¨ç¦ç³»ç»Ÿï¼ˆå…·ä½“IC/QLIKEé˜ˆå€¼ï¼‰
- åŒå‘¨å¢é‡è®­ç»ƒ+æœˆåº¦å…¨é‡é‡æ„
- çŸ¥è¯†ä¿ç•™ç³»ç»Ÿï¼ˆç‰¹å¾é‡è¦æ€§ç›‘æ§ï¼‰

æä¾›Açº§ç”Ÿäº§å°±ç»ªçš„é‡åŒ–äº¤æ˜“è§£å†³æ–¹æ¡ˆ
"""

# === STANDARD LIBRARY IMPORTS ===
import sys
import os
import json
import time
import logging
import warnings
import argparse
import tempfile
import importlib.util
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple, Union

# === THIRD-PARTY CORE LIBRARIES ===
import pandas as pd
import numpy as np
import yaml

# === PROJECT PATH SETUP ===
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# === STRICT IMPORTS CONTROL ===
STRICT_IMPORTS = os.getenv('STRICT_IMPORTS', 'false').lower() == 'true'

def log_import_fallback(module_name: str, fallback_description: str, error: Exception = None):
    """ç»Ÿä¸€çš„å¯¼å…¥å›é€€æ—¥å¿—å¤„ç†"""
    if STRICT_IMPORTS:
        logging.error(f"[STRICT_IMPORTS] {module_name} å¯¼å…¥å¤±è´¥: {error}")
        raise error if error else ImportError(f"Strict mode: {module_name} required")
    else:
        logging.info(f"[FALLBACK] {module_name} ä¸å¯ç”¨ï¼Œä½¿ç”¨ {fallback_description}")
        if error:
            logging.debug(f"[FALLBACK] {module_name} å¯¼å…¥è¯¦æƒ…: {error}")

# === T+10 CONFIGURATION IMPORT ===
try:
    from bma_models.t10_config import T10_CONFIG, get_config
    T10_AVAILABLE = True
except ImportError as e:
    log_import_fallback("T10 Config", "ç¡¬ç¼–ç é»˜è®¤å€¼", e)
    T10_AVAILABLE = False
    # Create mock config with hardcoded defaults
    class T10_CONFIG:
        PREDICTION_HORIZON = 10
        HOLDING_PERIOD = 10
        FEATURE_LAG = 5
        FEATURE_GLOBAL_LAG = 5
        ISOLATION_DAYS = 10
        EMBARGO_DAYS = 10
        SAFETY_GAP = 2
        CV_GAP = 10
        CV_N_SPLITS = 5
        SAMPLE_WEIGHT_HALFLIFE = 120
    get_config = lambda: T10_CONFIG

# === PROJECT SPECIFIC IMPORTS ===
try:
    from polygon_client import polygon_client as pc, download as polygon_download, Ticker as PolygonTicker
except ImportError as e:
    log_import_fallback("Polygon client", "æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨å’ŒMockç±»", e)
    # Create mock classes
    class PolygonTicker:
        def __init__(self, symbol): self.symbol = symbol
        def history(self, *args, **kwargs): return pd.DataFrame()
    pc = None
    polygon_download = None

# å¯¼å…¥BMA Enhanced Integrated System V6ï¼ˆæ–°å¢ï¼‰
BMA_ENHANCED_V6_AVAILABLE = False
try:
    from bma_models.bma_enhanced_integrated_system import BMAEnhancedIntegratedSystem, BMAEnhancedConfig
    BMA_ENHANCED_V6_AVAILABLE = True
    print("[INFO] BMA Enhanced V6 Integrated Systemå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    log_import_fallback("BMA Enhanced V6 System", "åŸºç¡€BMAç³»ç»Ÿå’ŒMockç±»", e)
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œè®¾ç½®Mockç±»é¿å…è¿è¡Œæ—¶é”™è¯¯
    class MockBMAEnhancedIntegratedSystem:
        def __init__(self, *args, **kwargs): pass
        def prepare_training_data(self, *args, **kwargs): return {'training_data': pd.DataFrame()}
        def execute_training_pipeline(self, *args, **kwargs): return {}
        def get_system_status(self): return {}
    
    class MockBMAEnhancedConfig:
        def __init__(self):
            class MockConfig:
                isolation_method = 'purge'
                isolation_days = T10_CONFIG.ISOLATION_DAYS  # From centralized config
                use_filtering_only = True
                embargo_days = T10_CONFIG.EMBARGO_DAYS  # From centralized config
                enable_smoothing = False
            self.validation_config = MockConfig()
            
            # ğŸ¯ FIX: å•ä¸€çœŸç›¸æ¥æº(Single Source of Truth)å¼ºåˆ¶åŒæ­¥
            logger.info(f"[CONFIG MASTER] è®¾ç½®ä¸»é…ç½® isolation_days = {self.validation_config.isolation_days}")
            
            # è®°å½•åŸå§‹é…ç½®ä½œä¸ºä¸»çœŸç›¸
            self._master_isolation_days = self.validation_config.isolation_days
            self._config_source = "UltraEnhanced_MockConfig"
            try:
                pgts_gap = 5  # PGTSé»˜è®¤å€¼
                if pgts_gap != self.validation_config.isolation_days:
                    print(f"[CONFIG MISMATCH] PGTS gap({pgts_gap}) != isolation_days({self.validation_config.isolation_days})")
                    print(f"[CONFIG SYNC] å°†ä½¿ç”¨ isolation_days={self.validation_config.isolation_days} ä½œä¸ºç»Ÿä¸€å‚æ•°")
            except Exception:
                pass
            self.regime_config = MockConfig()
            self.lag_config = MockConfig()
            self.factor_decay_config = MockConfig()
            self.production_gates = MockConfig()
            self.training_schedule = MockConfig()
            self.knowledge_config = MockConfig()
            # è®¾ç½®å¿…è¦çš„å±æ€§
            self.sample_time_decay_half_life = 75
            self.half_life_sensitivity_test = True
    
    BMAEnhancedIntegratedSystem = MockBMAEnhancedIntegratedSystem
    BMAEnhancedConfig = MockBMAEnhancedConfig

# å¯¼å…¥è‡ªé€‚åº”æƒé‡å­¦ä¹ ç³»ç»Ÿï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–ï¼‰
ADAPTIVE_WEIGHTS_AVAILABLE = False


# === SCIENTIFIC COMPUTING LIBRARIES ===
from scipy.stats import spearmanr, entropy
from scipy.optimize import minimize
from scipy import stats
import statsmodels.api as sm

# === MACHINE LEARNING LIBRARIES ===
from sklearn.model_selection import TimeSeriesSplit, GroupKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.isotonic import IsotonicRegression
from sklearn.covariance import LedoitWolf

# === UTILITY LIBRARIES ===
from dataclasses import dataclass, field
from collections import Counter
import gc
import psutil
import traceback
from functools import wraps
from contextlib import contextmanager

# === IMPORT OPTIMIZATION COMPLETE ===
# All imports have been organized into logical groups:
# - Standard library imports
# - Third-party core libraries  
# - Project-specific imports with fallbacks
# - Scientific computing libraries
# - Machine learning libraries
# - Utility libraries
# PCAå¤šé‡å…±çº¿æ€§æ¶ˆé™¤ç›¸å…³å¯¼å…¥
from sklearn.decomposition import PCA
from statsmodels.stats.outliers_influence import variance_inflation_factor
try:
    from fixed_purged_time_series_cv import FixedPurgedGroupTimeSeriesSplit as PurgedGroupTimeSeriesSplit, ValidationConfig, create_time_groups, validate_timesplit_integrity
    PURGED_CV_AVAILABLE = True
    PURGED_CV_VERSION = "FIXED"
except ImportError:
    try:
        # å›é€€åˆ°æ™®é€šç‰ˆæœ¬
        from fixed_purged_time_series_cv import FixedPurgedGroupTimeSeriesSplit as PurgedGroupTimeSeriesSplit, ValidationConfig, create_time_groups
        PURGED_CV_AVAILABLE = True
        PURGED_CV_VERSION = "STANDARD"
    except ImportError:
        # å¦‚æœæ²¡æœ‰ä»»ä½•purged_time_series_cvï¼Œä½¿ç”¨sklearnçš„æ›¿ä»£æ–¹æ¡ˆ
        from sklearn.model_selection import TimeSeriesSplit as PurgedGroupTimeSeriesSplit
        PURGED_CV_AVAILABLE = False
        PURGED_CV_VERSION = "SKLEARN_FALLBACK"
    class ValidationConfig:
        def __init__(self, n_splits=5, **kwargs):
            self.n_splits = n_splits
            for key, value in kwargs.items():
                setattr(self, key, value)
    def create_time_groups(*args, **kwargs):
        return None

# å¯è§†åŒ–
import matplotlib.pyplot as plt
try:
    import seaborn as sns
    plt.style.use('default')  # ä½¿ç”¨é»˜è®¤æ ·å¼è€Œä¸æ˜¯seaborn
except ImportError:
    sns = None
    plt.style.use('default')
except Exception as e:
    # å¤„ç†seabornæ ·å¼é—®é¢˜
    plt.style.use('default')
    warnings.filterwarnings('ignore', message='.*seaborn.*')

# å¯¼å…¥Alphaå¼•æ“ï¼ˆæ ¸å¿ƒç»„ä»¶ï¼‰
ALPHA_ENGINE_AVAILABLE = False
try:
    from enhanced_alpha_strategies import AlphaStrategiesEngine
    ALPHA_ENGINE_AVAILABLE = True
    print("[INFO] Alphaå¼•æ“æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"[WARN] Alphaå¼•æ“æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# å¯¼å…¥LTRæ¨¡å—
LTR_AVAILABLE = False
try:
    from learning_to_rank_bma import LearningToRankBMA
    LTR_AVAILABLE = True
    print("[INFO] LTRæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"[WARN] LTRæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# å¯¼å…¥æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨ï¼ˆå¯é€‰ï¼‰
PORTFOLIO_OPTIMIZER_AVAILABLE = False
try:
    from advanced_portfolio_optimizer import AdvancedPortfolioOptimizer
    PORTFOLIO_OPTIMIZER_AVAILABLE = True
    print("[INFO] æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"[WARN] æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬: {e}")

# è®¾ç½®å¢å¼ºæ¨¡å—å¯ç”¨æ€§ï¼ˆåªè¦æ ¸å¿ƒAlphaå¼•æ“å¯ç”¨å³ä¸ºå¯ç”¨ï¼‰
ENHANCED_MODULES_AVAILABLE = ALPHA_ENGINE_AVAILABLE

# å•ç‹¬å¯¼å…¥Regime Detectionæ¨¡å—ï¼ˆç‹¬ç«‹å¤„ç†ï¼‰
try:
    from market_regime_detector import MarketRegimeDetector, RegimeConfig
    from regime_aware_trainer import RegimeAwareTrainer, RegimeTrainingConfig
    from regime_aware_cv import RegimeAwareTimeSeriesCV, RegimeAwareCVConfig
    REGIME_DETECTION_AVAILABLE = True
    print("[INFO] Regime Detectionæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"[WARN] Regime Detectionæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    REGIME_DETECTION_AVAILABLE = False
    
    # åˆ›å»ºMockç±»ä»¥é¿å…åˆå§‹åŒ–é”™è¯¯
    class MarketRegimeDetector:
        def __init__(self, *args, **kwargs):
            pass
    
    class RegimeAwareTrainer:
        def __init__(self, *args, **kwargs):
            pass
            
    class RegimeAwareTimeSeriesCV:
        def __init__(self, *args, **kwargs):
            pass

# ç»Ÿä¸€å¸‚åœºæ•°æ®ï¼ˆè¡Œä¸š/å¸‚å€¼/å›½å®¶ç­‰ï¼‰
try:
    from unified_market_data_manager import UnifiedMarketDataManager
    MARKET_MANAGER_AVAILABLE = True
except Exception:
    MARKET_MANAGER_AVAILABLE = False

# ä¸­æ€§åŒ–å·²ç»Ÿä¸€ç”±Alphaå¼•æ“å¤„ç†ï¼Œç§»é™¤é‡å¤ä¾èµ–

# å¯¼å…¥isotonicæ ¡å‡†
try:
    from sklearn.isotonic import IsotonicRegression
    ISOTONIC_AVAILABLE = True
except ImportError:
    ISOTONIC_AVAILABLE = False

# è‡ªé€‚åº”åŠ æ ‘ä¼˜åŒ–å™¨å·²ç§»é™¤ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å‹è®­ç»ƒ
ADAPTIVE_OPTIMIZER_AVAILABLE = False

# é«˜çº§æ¨¡å‹
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError as e:
    log_import_fallback("XGBoost", "LightGBMæ›¿ä»£", e)
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError as e:
    log_import_fallback("LightGBM", "sklearnæ¨¡å‹", e)
    LIGHTGBM_AVAILABLE = False

# CatBoost removed due to compatibility issues
CATBOOST_AVAILABLE = False

# é…ç½®
warnings.filterwarnings('ignore')

# ä¿®å¤matplotlibç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
try:
    import matplotlib
    if hasattr(matplotlib, '__version__') and matplotlib.__version__ >= '3.4.0':
        try:
            plt.style.use('seaborn-v0_8')
        except OSError:
            # å¦‚æœseaborn-v0_8ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ ·å¼
            plt.style.use('default')
            print("[WARN] seaborn-v0_8æ ·å¼ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ ·å¼")
    else:
        plt.style.use('seaborn')
except Exception as e:
    print(f"[WARN] matplotlibæ ·å¼è®¾ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æ ·å¼")
    plt.style.use('default')

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
def setup_logger():
    """Setup logger with proper encoding to handle Unicode characters"""
    import sys
    
    # Configure logging with UTF-8 encoding
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    
    # Record Purged CV version information
    try:
        if PURGED_CV_AVAILABLE:
            logger.info(f"Purged Time Series CV version: {PURGED_CV_VERSION}")
        else:
            logger.warning("Using sklearn TimeSeriesSplit as fallback")
    except Exception as e:
        logger.warning(f"Error logging CV version: {e}")
    
    return logger

logger = setup_logger()

# å…¨å±€é…ç½®
DEFAULT_TICKER_LIST =["A", "AA", "AACB", "AACI", "AACT", "AAL", "AAMI", "AAOI", "AAON", "AAP", "AAPL", "AARD", "AAUC", "AB", "ABAT", "ABBV", "ABCB", "ABCL", "ABEO", "ABEV", "ABG", "ABL", "ABM", "ABNB", "ABSI", "ABT", "ABTS", "ABUS", "ABVC", "ABVX", "ACA", "ACAD", "ACB", "ACCO", "ACDC", "ACEL", "ACGL", "ACHC", "ACHR", "ACHV", "ACI", "ACIC", "ACIU", "ACIW", "ACLS", "ACLX", "ACM", "ACMR", "ACN", "ACNT", "ACOG", "ACRE", "ACT", "ACTG", "ACTU", "ACVA", "ACXP", "ADAG", "ADBE", "ADC", "ADCT", "ADEA", "ADI", "ADM", "ADMA", "ADNT", "ADP", "ADPT", "ADSE", "ADSK", "ADT", "ADTN", "ADUR", "ADUS", "ADVM", "AEBI", "AEE", "AEG", "AEHL", "AEHR", "AEIS", "AEM", "AEO", "AEP", "AER", "AES", "AESI", "AEVA", "AEYE", "AFCG", "AFG", "AFL", "AFRM", "AFYA", "AG", "AGCO", "AGD", "AGEN", "AGH", "AGI", "AGIO", "AGM", "AGNC", "AGO", "AGRO", "AGX", "AGYS", "AHCO", "AHH", "AHL", "AHR", "AI", "AIFF", "AIFU", "AIG", "AII", "AIM", "AIMD", "AIN", "AIOT", "AIP", "AIR", "AIRI", "AIRJ", "AIRO", "AIRS", "AISP", "AIT", "AIV", "AIZ", "AJG", "AKAM", "AKBA", "AKRO", "AL", "ALAB", "ALAR", "ALB", "ALBT", "ALC", "ALDF", "ALDX", "ALE", "ALEX", "ALF", "ALG", "ALGM", "ALGN", "ALGS", "ALGT", "ALHC", "ALIT", "ALK", "ALKS", "ALKT", "ALL", "ALLE", "ALLT", "ALLY", "ALM", "ALMS", "ALMU", "ALNT", "ALNY", "ALRM", "ALRS", "ALSN", "ALT", "ALTG", "ALTI", "ALTS", "ALUR", "ALV", "ALVO", "ALX", "ALZN", "AM", "AMAL", "AMAT", "AMBA", "AMBC", "AMBP", "AMBQ", "AMBR", "AMC", "AMCR", "AMCX", "AMD", "AME", "AMED", "AMG", "AMGN", "AMH", "AMKR", "AMLX", "AMN", "AMP", "AMPG", "AMPH", "AMPL", "AMPX", "AMPY", "AMR", "AMRC", "AMRK", "AMRN", "AMRX", "AMRZ", "AMSC", "AMSF", "AMST", "AMT", "AMTB", "AMTM", "AMTX", "AMWD", "AMWL", "AMX", "AMZE", "AMZN", "AN", "ANAB", "ANDE", "ANEB", "ANET", "ANF", "ANGH", "ANGI", "ANGO", "ANIK", "ANIP", "ANIX", "ANNX", "ANPA", "ANRO", "ANSC", "ANTA", "ANTE", "ANVS", "AOMR", "AON", "AORT", "AOS", "AOSL", "AOUT", "AP", "APA", "APAM", "APD", "APEI", "APG", "APGE", "APH", "API", "APLD", "APLE", "APLS", "APO", "APOG", "APP", "APPF", "APPN", "APPS", "APTV", "APVO", "AQN", "AQST", "AR", "ARAI", "ARCB", "ARCC", "ARCO", "ARCT", "ARDT", "ARDX", "ARE", "AREN", "ARES", "ARHS", "ARI", "ARIS", "ARKO", "ARLO", "ARLP", "ARM", "ARMK", "ARMN", "ARMP", "AROC", "ARQ", "ARQQ", "ARQT", "ARR", "ARRY", "ARTL", "ARTV", "ARVN", "ARW", "ARWR", "ARX", "AS", "ASA", "ASAN", "ASB", "ASC", "ASGN", "ASH", "ASIC", "ASIX", "ASLE", "ASM", "ASND", "ASO", "ASPI", "ASPN", "ASR", "ASST", "ASTE", "ASTH", "ASTI", "ASTL", "ASTS", "ASUR", "ASX", "ATAI", "ATAT", "ATEC", "ATEN", "ATEX", "ATGE", "ATHE", "ATHM", "ATHR", "ATI", "ATII", "ATKR", "ATLC", "ATLX", "ATMU", "ATNF", "ATO", "ATOM", "ATR", "ATRA", "ATRC", "ATRO", "ATS", "ATUS", "ATXS", "ATYR", "AU", "AUB", "AUDC", "AUGO", "AUID", "AUPH", "AUR", "AURA", "AUTL", "AVA", "AVAH", "AVAL", "AVAV", "AVB", "AVBC", "AVBP", "AVD", "AVDL", "AVDX", "AVGO", "AVIR", "AVNS", "AVNT", "AVNW", "AVO", "AVPT", "AVR", "AVT", "AVTR", "AVTX", "AVXL", "AVY", "AWI", "AWK", "AWR", "AX", "AXGN", "AXIN", "AXL", "AXP", "AXS", "AXSM", "AXTA", "AXTI", "AYI", "AYTU", "AZ", "AZN", "AZTA", "AZZ", "B", "BA", "BABA", "BAC", "BACC", "BACQ", "BAER", "BAH", "BAK", "BALL", "BALY", "BAM", "BANC", "BAND", "BANF", "BANR", "BAP", "BASE", "BATRA", "BATRK", "BAX", "BB", "BBAI", "BBAR", "BBCP", "BBD", "BBDC", "BBIO", "BBNX", "BBSI", "BBUC", "BBVA", "BBW", "BBWI", "BBY", "BC", "BCAL", "BCAX", "BCBP", "BCC", "BCE", "BCH", "BCO", "BCPC", "BCRX", "BCS", "BCSF", "BCYC", "BDC", "BDMD", "BDRX", "BDTX", "BDX", "BE", "BEAG", "BEAM", "BEEM", "BEEP", "BEKE", "BELFB", "BEN", "BEP", "BEPC", "BETR", "BF-A", "BF-B", "BFAM", "BFC", "BFH", "BFIN", "BFS", "BFST", "BG", "BGC", "BGL", "BGLC", "BGM", "BGS", "BGSF", "BHC", "BHE", "BHF", "BHFAP", "BHLB", "BHP", "BHR", "BHRB", "BHVN", "BIDU", "BIIB", "BILI", "BILL", "BIO", "BIOA", "BIOX", "BIP", "BIPC", "BIRD", "BIRK", "BJ", "BJRI", "BK", "BKD", "BKE", "BKH", "BKKT", "BKR", "BKSY", "BKTI", "BKU", "BKV", "BL", "BLBD", "BLBX", "BLCO", "BLD", "BLDE", "BLDR", "BLFS", "BLFY", "BLIV", "BLKB", "BLMN", "BLND", "BLNE", "BLRX", "BLUW", "BLX", "BLZE", "BMA", "BMBL", "BMGL", "BMHL", "BMI", "BMNR", "BMO", "BMR", "BMRA", "BMRC", "BMRN", "BMY", "BN", "BNC", "BNED", "BNGO", "BNL", "BNS", "BNTC", "BNTX", "BNZI", "BOC", "BOF", "BOH", "BOKF", "BOOM", "BOOT", "BORR", "BOSC", "BOW", "BOX", "BP", "BPOP", "BQ", "BR", "BRBR", "BRBS", "BRC", "BRDG", "BRFS", "BRK-B", "BRKL", "BRKR", "BRLS", "BRO", "BROS", "BRR", "BRSL", "BRSP", "BRX", "BRY", "BRZE", "BSAA", "BSAC", "BSBR", "BSET", "BSGM", "BSM", "BSX", "BSY", "BTAI", "BTBD", "BTBT", "BTCM", "BTCS", "BTCT", "BTDR", "BTE", "BTG", "BTI", "BTM", "BTMD", "BTSG", "BTU", "BUD", "BULL", "BUR", "BURL", "BUSE", "BV", "BVFL", "BVN", "BVS", "BWA", "BWB", "BWEN", "BWIN", "BWLP", "BWMN", "BWMX", "BWXT", "BX", "BXC", "BXP", "BY", "BYD", "BYND", "BYON", "BYRN", "BYSI", "BZ", "BZAI", "BZFD", "BZH", "BZUN", "C", "CAAP", "CABO", "CAC", "CACC", "CACI", "CADE", "CADL", "CAE", "CAEP", "CAG", "CAH", "CAI", "CAKE", "CAL", "CALC", "CALM", "CALX", "CAMT", "CANG", "CAPR", "CAR", "CARE", "CARG", "CARL", "CARR", "CARS", "CART", "CASH", "CASS", "CAT", "CATX", "CATY", "CAVA", "CB", "CBAN", "CBIO", "CBL", "CBLL", "CBNK", "CBOE", "CBRE", "CBRL", "CBSH", "CBT", "CBU", "CBZ", "CC", "CCAP", "CCB", "CCCC", "CCCS", "CCCX", "CCEP", "CCI", "CCIR", "CCIX", "CCJ", "CCK", "CCL", "CCLD", "CCNE", "CCOI", "CCRD", "CCRN", "CCS", "CCSI", "CCU", "CDE", "CDIO", "CDLR", "CDNA", "CDNS", "CDP", "CDRE", "CDRO", "CDTX", "CDW", "CDXS", "CDZI", "CE", "CECO", "CEG", "CELC", "CELH", "CELU", "CELZ", "CENT", "CENTA", "CENX", "CEP", "CEPO", "CEPT", "CEPU", "CERO", "CERT", "CEVA", "CF", "CFFN", "CFG", "CFLT", "CFR", "CG", "CGAU", "CGBD", "CGCT", "CGEM", "CGNT", "CGNX", "CGON", "CHA", "CHAC", "CHCO", "CHD", "CHDN", "CHE", "CHEF", "CHH", "CHKP", "CHMI", "CHPT", "CHRD", "CHRW", "CHT", "CHTR", "CHWY", "CHYM", "CI", "CIA", "CIB", "CIEN", "CIFR", "CIGI", "CIM", "CINF", "CING", "CINT", "CIO", "CION", "CIVB", "CIVI", "CL", "CLAR", "CLB", "CLBK", "CLBT", "CLCO", "CLDI", "CLDX", "CLF", "CLFD", "CLGN", "CLH", "CLLS", "CLMB", "CLMT", "CLNE", "CLNN", "CLOV", "CLPR", "CLPT", "CLRB", "CLRO", "CLS", "CLSK", "CLVT", "CLW", "CLX", "CM", "CMA", "CMBT", "CMC", "CMCL", "CMCO", "CMCSA", "CMDB", "CME", "CMG", "CMI", "CMP", "CMPO", "CMPR", "CMPS", "CMPX", "CMRC", "CMRE", "CMS", "CMTL", "CNA", "CNC", "CNCK", "CNDT", "CNEY", "CNH", "CNI", "CNK", "CNL", "CNM", "CNMD", "CNNE", "CNO", "CNOB", "CNP", "CNQ", "CNR", "CNS", "CNTA", "CNTB", "CNTY", "CNVS", "CNX", "CNXC", "CNXN", "COCO", "CODI", "COF", "COFS", "COGT", "COHR", "COHU", "COIN", "COKE", "COLB", "COLL", "COLM", "COMM", "COMP", "CON", "COO", "COOP", "COP", "COPL", "COR", "CORT", "CORZ", "COTY", "COUR", "COYA", "CP", "CPA", "CPAY", "CPB", "CPF", "CPIX", "CPK", "CPNG", "CPRI", "CPRT", "CPRX", "CPS", "CPSH", "CQP", "CR", "CRAI", "CRAQ", "CRBG", "CRBP", "CRC", "CRCL", "CRCT", "CRD-A", "CRDF", "CRDO", "CRE", "CRESY", "CREV", "CREX", "CRGO", "CRGX", "CRGY", "CRH", "CRI", "CRK", "CRL", "CRM", "CRMD", "CRML", "CRMT", "CRNC", "CRNX", "CRON", "CROX", "CRS", "CRSP", "CRSR", "CRTO", "CRUS", "CRVL", "CRVO", "CRVS", "CRWD", "CRWV", "CSAN", "CSCO", "CSGP", "CSGS", "CSIQ", "CSL", "CSR", "CSTL", "CSTM", "CSV", "CSW", "CSWC", "CSX", "CTAS", "CTEV", "CTGO", "CTKB", "CTLP", "CTMX", "CTNM", "CTO", "CTOS", "CTRA", "CTRI", "CTRM", "CTRN", "CTS", "CTSH", "CTVA", "CTW", "CUB", "CUBE", "CUBI", "CUK", "CUPR", "CURB", "CURI", "CURV", "CUZ", "CV", "CVAC", "CVBF", "CVCO", "CVE", "CVEO", "CVGW", "CVI", "CVLG", "CVLT", "CVM", "CVNA", "CVRX", "CVS", "CVX", "CW", "CWAN", "CWBC", "CWCO", "CWEN", "CWEN-A", "CWH", "CWK", "CWST", "CWT", "CX", "CXDO", "CXM", "CXT", "CXW", "CYBN", "CYBR", "CYCC", "CYD", "CYH", "CYN", "CYRX", "CYTK", "CZR", "CZWI", "D", "DAAQ", "DAC", "DAIC", "DAKT", "DAL", "DALN", "DAN", "DAO", "DAR", "DARE", "DASH", "DATS", "DAVA", "DAVE", "DAWN", "DAY", "DB", "DBD", "DBI", "DBRG", "DBX", "DC", "DCBO", "DCI", "DCO", "DCOM", "DCTH", "DD", "DDC", "DDI", "DDL", "DDOG", "DDS", "DEA", "DEC", "DECK", "DEFT", "DEI", "DELL", "DENN", "DEO", "DERM", "DEVS", "DFDV", "DFH", "DFIN", "DFSC", "DG", "DGICA", "DGII", "DGX", "DGXX", "DH", "DHI", "DHR", "DHT", "DHX", "DIBS", "DIN", "DINO", "DIOD", "DIS", "DJCO", "DJT", "DK", "DKL", "DKNG", "DKS", "DLB", "DLHC", "DLO", "DLTR", "DLX", "DLXY", "DMAC", "DMLP", "DMRC", "DMYY", "DNA", "DNB", "DNLI", "DNN", "DNOW", "DNTH", "DNUT", "DOC", "DOCN", "DOCS", "DOCU", "DOGZ", "DOLE", "DOMH", "DOMO", "DOOO", "DORM", "DOUG", "DOV", "DOW", "DOX", "DOYU", "DPRO", "DPZ", "DQ", "DRD", "DRDB", "DRH", "DRI", "DRS", "DRVN", "DSGN", "DSGR", "DSGX", "DSP", "DT", "DTE", "DTI", "DTIL", "DTM", "DTST", "DUK", "DUOL", "DUOT", "DV", "DVA", "DVAX", "DVN", "DVS", "DWTX", "DX", "DXC", "DXCM", "DXPE", "DXYZ", "DY", "DYN", "DYNX", "E", "EA", "EARN", "EAT", "EB", "EBAY", "EBC", "EBF", "EBMT", "EBR", "EBS", "EC", "ECC", "ECG", "ECL", "ECO", "ECOR", "ECPG", "ECVT", "ED", "EDBL", "EDIT", "EDN", "EDU", "EE", "EEFT", "EEX", "EFC", "EFSC", "EFX", "EFXT", "EG", "EGAN", "EGBN", "EGG", "EGO", "EGP", "EGY", "EH", "EHAB", "EHC", "EHTH", "EIC", "EIG", "EIX", "EKSO", "EL", "ELAN", "ELDN", "ELF", "ELMD", "ELME", "ELP", "ELPW", "ELS", "ELV", "ELVA", "ELVN", "ELWS", "EMA", "EMBC", "EMN", "EMP", "EMPD", "EMPG", "EMR", "EMX", "ENB", "ENGN", "ENGS", "ENIC", "ENOV", "ENPH", "ENR", "ENS", "ENSG", "ENTA", "ENTG", "ENVA", "ENVX", "EOG", "EOLS", "EOSE", "EPAC", "EPAM", "EPC", "EPD", "EPM", "EPR", "EPSM", "EPSN", "EQBK", "EQH", "EQNR", "EQR", "EQT", "EQV", "EQX", "ERIC", "ERIE", "ERII", "ERJ", "ERO", "ES", "ESAB", "ESE", "ESGL", "ESI", "ESLT", "ESNT", "ESOA", "ESQ", "ESTA", "ESTC", "ET", "ETD", "ETN", "ETNB", "ETON", "ETOR", "ETR", "ETSY", "EU", "EUDA", "EVAX", "EVC", "EVCM", "EVER", "EVEX", "EVGO", "EVH", "EVLV", "EVO", "EVOK", "EVR", "EVRG", "EVTC", "EVTL", "EW", "EWBC", "EWCZ", "EWTX", "EXAS", "EXC", "EXE", "EXEL", "EXK", "EXLS", "EXOD", "EXP", "EXPD", "EXPE", "EXPI", "EXPO", "EXR", "EXTR", "EYE", "EYPT", "EZPW", "F", "FA",
 "FACT", "FAF", "FANG", "FAST", "FAT", "FATN", "FBIN", "FBK", "FBLA", 
 "FBNC", "FBP", "FBRX", "FC", "FCBC", "FCEL", "FCF", "FCFS", "FCN", "FCX", "FDMT",
  "FDP", "FDS", "FDUS", "FDX", "FE", "FEIM", "FELE", "FENC", "FER", "FERA", "FERG", "FET", "FF", 
  "FFAI", "FFBC", "FFIC", "FFIN", "FFIV", "FFWM", "FG", "FGI", "FHB", "FHI", "FHN", "FHTX", "FI", "FIBK", "FIEE", "FIG", "FIGS", 
  "FIHL", "FINV", "FIP", "FIS", "FISI", "FITB", "FIVE", "FIVN", "FIZZ", "FL", "FLD", "FLEX", "FLG", "FLGT", "FLL", "FLNC", "FLNG", "FLO", "FLOC",
   "FLR", "FLS", "FLUT", "FLWS", "FLX", "FLY", "FLYE", "FLYW", "FLYY", "FMBH", "FMC", "FMFC", "FMNB", "FMS", "FMST", 
   "FMX", "FN", "FNB", "FND", "FNF", "FNGD", "FNKO", "FNV", "FOA", "FOLD", "FOR", "FORM", "FORR", "FOUR", "FOX", "FOXA", 
   "FOXF", "FPH", "FPI", "FRGE", "FRHC", "FRME", "FRO", "FROG", "FRPT", "FRSH", "FRST", "FSCO", "FSK", "FSLR", "FSLY",
    "FSM", "FSS", "FSUN", "FSV", "FTAI", "FTCI", "FTDR", "FTEK", "FTI", "FTK", "FTNT", "FTRE", "FTS", "FTV", "FUBO", "FUFU", "FUL", "FULC", "FULT", "FUN", "FUTU", "FVR", "FVRR", "FWONA", "FWONK", "FWRD", "FWRG", "FYBR", "G",
     "GABC", "GAIA", "GAIN", "GALT", "GAMB", "GAP", "GASS", "GATX", "GAUZ", "GB", "GBCI", "GBDC", "GBFH", "GBIO", "GBTG", "GBX", "GCI", "GCL", "GCMG", "GCO", "GCT", "GD", "GDC", "GDDY", "GDEN", "GDOT", "GDRX", "GDS", 
     "GDYN", "GE", "GEF", "GEHC", "GEL", "GEN", "GENI", "GENK", "GEO", "GEOS", "GES", "GFF", "GFI", "GFL", "GFR", "GFS", "GGAL", "GGB", "GGG", "GH", "GHLD", "GHM", "GHRS", "GIB", "GIC", "GIG", "GIII", "GIL", "GILD", "GILT", "GIS", "GITS", 
     "GKOS", "GL", "GLAD", "GLBE", "GLD", "GLDD", "GLIBA", "GLIBK", "GLNG", "GLOB", "GLP", "GLPG", "GLPI", "GLRE", "GLSI", "GLUE", "GLW", "GLXY", "GM", "GMAB", "GME", "GMED", "GMRE", "GMS", "GNE", "GNK", "GNL", "GNLX", "GNRC", "GNTX", "GNTY", "GNW", "GO", "GOCO", "GOGL", "GOGO", "GOLF", "GOOD", "GOOG", "GOOGL", "GOOS", "GORV", "GOTU", "GPAT", 
     "GPC", "GPCR", "GPI", "GPK", "GPN", "GPOR", "GPRE", "GPRK", "GRAB", "GRAL", "GRAN", "GRBK", "GRC", "GRCE", "GRDN", "GRFS", "GRMN", "GRND", "GRNT", "GROY", "GRPN", "GRRR", "GSAT", "GSBC", "GSBD", "GSHD", "GSIT", "GSK", "GSL", "GSM", "GSRT", "GT", "GTE", "GTEN", "GTERA", "GTES", "GTLB", "GTLS", "GTM", "GTN", "GTX", "GTY", "GVA", "GWRE", "GWRS", "GXO", "GYRE", "H", "HAE", "HAFC", "HAFN", "HAL", "HALO", "HAS", "HASI", "HAYW", "HBAN", "HBCP", "HBI", "HBM", "HBNC", "HCA", "HCAT", "HCC", "HCHL", "HCI", "HCKT", "HCM", "HCSG", "HCTI", "HCWB", "HD", "HDB", "HDSN", "HE", "HEI", "HEI-A", "HELE", "HEPS", "HESM", "HFFG", "HFWA", "HG", "HGTY", "HGV", "HHH", "HI", "HIFS", "HIG", "HII", "HIMS", "HIMX",
     "HIPO", "HIT", "HITI", "HIVE", "HIW", "HL", "HLF", "HLI", "HLIO", "HLIT", "HLLY", "HLMN", "HLN", "HLNE", "HLT", "HLVX", "HLX", "HLXB", "HMC", "HMN", "HMST", "HMY", "HNGE", "HNI", "HNRG", "HNST", "HOFT", "HOG", "HOLO", "HOLX", "HOMB", "HON", "HOND", "HONE", "HOOD", "HOPE", "HOUS", "HOV", "HP", "HPE", "HPK", "HPP", "HPQ", "HQH", "HQL", "HQY", "HRB", "HRI", "HRL", "HRMY", "HROW", "HRTG", "HRZN", "HSAI", "HSBC", "HSCS", "HSHP", "HSIC", "HSII", "HST", "HSTM", "HSY", "HTBK", "HTCO", "HTGC", "HTH", "HTHT", "HTLD", "HTO", "HTOO", "HTZ", "HUBB", "HUBC", "HUBG", "HUBS", "HUHU", "HUM", "HUMA", "HUN", "HURA", "HURN", "HUSA", "HUT", "HUYA", "HVII", "HVT", "HWC", "HWKN", "HWM", 
     "HXL", "HY", "HYAC", "HYMC", "HYPD", "HZO", "IAC", "IAG", "IART", "IAS", "IBCP", "IBEX", "IBKR", "IBM", "IBN", "IBOC", "IBP", "IBRX", "IBTA", "ICE", "ICFI", "ICG", "ICHR", "ICL", "ICLR", "ICUI", "IDA", "IDAI", "IDCC", "IDN", "IDR", "IDT", "IDYA", "IE", "IEP", "IESC", "IEX", "IFF", "IFS", "IGIC", "IHG", "IHS", "III", "IIIN", "IIIV", "IIPR", "ILMN", "IMAB", "IMAX", "IMCC", "IMCR", "IMDX", "IMKTA", "IMMR", "IMMX", "IMNM", "IMNN", "IMO", "IMPP", "IMRX", "IMTX", "IMVT", "IMXI", "INAB", "INAC", "INBK", "INBX", "INCY", "INDB", "INDI", "INDO", "INDP", "INDV", "INFA", "INFU", "INFY", "ING", "INGM", "INGN", "INGR", "INKT", "INMB", "INMD", "INN", "INOD", "INR", "INSE", "INSG", "INSM", "INSP", "INSW", "INTA", "INTC", "INTR", "INUV", "INV", "INVA", "INVE", "INVH", "INVX", "IONQ", "IONS", "IOSP", "IOT", "IOVA", "IP", "IPA", "IPAR", "IPDN", "IPG", "IPGP", "IPI", "IPX", "IQST", "IQV", "IR", "IRBT", "IRDM", "IREN", "IRM", "IRMD", "IROH", "IRON", "IRS", "IRTC", "ISPR", "ISRG", "ISSC", "IT", "ITGR", "ITIC", "ITOS", "ITRI", "ITRN", "ITT", "ITUB", "ITW", "IVR", "IVZ", "IX", "IZEA", "J", "JACK", "JACS", "JAKK", "JAMF", "JANX", "JAZZ", "JBGS", "JBHT", "JBI", "JBIO", "JBL", "JBLU", "JBS", "JBSS", "JBTM", "JCAP", "JCI", "JD", "JEF", "JELD", "JEM", "JENA", "JFIN", "JHG", "JHX", "JILL", "JJSF", "JKHY", "JKS", "JLHL", "JLL", "JMIA", "JNJ", "JOBY", "JOE", "JOUT", "JOYY", "JPM", "JRSH", "JRVR", "JSPR", "JTAI", "JVA", "JXN", "JYNT", "K", "KAI", "KALA", "KALU", 
     "KALV", "KAR", "KARO", "KB", "KBDC", "KBH", "KBR", "KC", "KCHV", "KD", "KDP", "KE", "KELYA", "KEP", "KEX", "KEY", "KEYS", "KFII", "KFRC", "KFS", "KFY", "KGC", "KGEI", "KGS", "KHC", "KIDS", "KIM", "KINS", "KKR", "KLC", "KLG", "KLIC", "KLRS", "KMB", "KMDA", "KMI", "KMPR", "KMT", "KMTS", "KMX", "KN", "KNF", "KNOP", "KNSA", "KNSL", "KNTK", "KNW", "KNX", "KO", "KOD", "KODK", "KOF", "KOP", "KOSS", "KPRX", "KPTI", "KR", "KRC", "KRMD", "KRMN", "KRNT", "KRNY", "KRO", "KROS", "KRP", "KRRO", "KRT", "KRUS", "KRYS", "KSCP", "KSPI", "KSS", "KT", "KTB", "KTOS", "KULR", "KURA", "KVUE", "KVYO", "KW", "KWM", "KWR", "KYMR", "KYTX", "KZIA", "L", "LAC", "LAD", "LADR", "LAES", "LAKE", "LAMR", "LAND", "LANV", "LAR", "LASE", "LASR", "LAUR", "LAW", "LAWR", "LAZ", "LAZR", "LB", "LBRDA", "LBRDK", "LBRT", "LBTYA", "LBTYK", "LC", "LCCC", "LCFY", "LCID", "LCII", "LCUT", "LDOS", "LE", "LEA", "LECO", "LEG", "LEGH", "LEGN", "LEN", "LENZ", "LEO", "LEU", "LEVI", "LFCR", "LFMD", "LFST", "LFUS", "LFVN", "LGCY", "LGIH", "LGND", "LH", "LHAI", "LHSW", "LHX", "LI", "LIDR", "LIF", "LILA", "LILAK", "LIMN", "LIN", "LINC", "LIND", "LINE", "LION", "LITE", "LITM", "LIVE", "LIVN", "LIXT", "LKFN", "LKQ", "LLYVA", "LLYVK", "LMAT", "LMB", "LMND", "LMNR", "LMT", "LNC", "LNG", "LNN", "LNSR", "LNT", "LNTH", "LNW", "LOAR", "LOB", "LOCO", "LODE", "LOGI", "LOKV", "LOMA", "LOPE", "LOT", "LOVE", "LOW", "LPAA", "LPBB", "LPCN", "LPG", "LPL", "LPLA", "LPRO", "LPTH", "LPX", "LQDA", "LQDT", "LRCX", "LRMR", "LRN", "LSCC", "LSE", "LSPD", "LSTR", "LTBR", "LTC", "LTH", "LTM", "LTRN", "LTRX", "LU", "LUCK", "LULU", "LUMN", "LUNR", "LUV", "LUXE", "LVLU", "LVS", "LVWR", "LW", "LWAY", "LWLG", "LX", "LXEH", "LXEO", "LXFR", "LXU", "LYB", "LYEL", "LYFT", "LYG", "LYRA", "LYTS", "LYV", "LZ", "LZB", "LZM", "LZMH", "M", "MAA", "MAAS", "MAC", "MACI", "MAG", "MAGN", "MAIN", "MAMA", "MAMK", "MAN", "MANH", "MANU", "MAR", "MARA", "MAS", "MASI", "MASS", "MAT", "MATH", "MATV", "MATW", "MATX", "MAX", "MAXN", "MAZE", "MB", "MBAV", "MBC", "MBI", "MBIN", "MBLY",
      "MBOT", "MBUU", "MBWM", "MBX", "MC", "MCB", "MCD", "MCFT", "MCHP", "MCRB", "MCRI", "MCRP", "MCS", "MCVT", "MCW", "MCY", "MD", "MDAI", "MDB", "MDCX", "MDGL", "MDLZ", "MDT", "MDU", "MDV", "MDWD", "MDXG", "MDXH", "MEC", "MED", "MEDP", "MEG", "MEI", "MEIP", "MENS", "MEOH", "MERC", "MESO", "MET", "METC", "METCB", "MFA", "MFC", "MFG", "MFH", "MFI", "MFIC", "MFIN", "MG", "MGA", "MGEE", "MGIC", "MGM", "MGNI", "MGPI", "MGRC", "MGRM", "MGRT", "MGTX", "MGY", "MH", "MHK", "MHO", "MIDD", "MIMI", "MIND", "MIR", "MIRM", "MITK", "MKC", "MKSI", "MKTX", "MLAB", "MLCO", "MLEC", "MLGO", "MLI", "MLKN", "MLNK", "MLR", "MLTX", "MLYS", "MMC", "MMI", "MMM", "MMS", "MMSI", "MMYT", "MNDY", "MNKD", "MNMD", "MNR", "MNRO", "MNSO", "MNST", "MNTN", "MO", "MOB", "MOD", "MODG", "MODV", "MOFG", "MOG-A", "MOH", "MOMO", "MORN", "MOS", "MOV", "MP", "MPAA", "MPB", "MPC", "MPLX", "MPTI", "MPU", "MQ", "MRAM", "MRBK", "MRC", "MRCC", "MRCY", "MRK", "MRNA", "MRP", "MRSN", "MRT", "MRTN", "MRUS", "MRVI", "MRVL", "MRX", "MS", "MSA", "MSBI", "MSEX", "MSGE", "MSGM", "MSGS", "MSGY", "MSI", "MSM", "MSTR", "MT", "MTA", "MTAL", "MTB", "MTCH", "MTDR", "MTEK", "MTEN", "MTG", "MTH", "MTLS", "MTN", "MTRN", "MTRX", "MTSI", "MTSR", "MTUS", "MTW", "MTX", "MTZ", "MU", "MUFG", "MUR", "MUSA", "MUX", "MVBF", "MVST", "MWA", "MX", "MXL", "MYE", "MYFW", "MYGN", "MYRG", "MZTI", "NA", "NAAS", "NABL", "NAGE", "NAKA", "NAMM", "NAMS", "NAT", "NATH", "NATL", "NATR", "NAVI", "NB", "NBBK", "NBHC", "NBIS", "NBIX", "NBN", "NBR", "NBTB", "NCDL", "NCLH", "NCMI", "NCNO", "NCPL", "NCT", "NCTY", "NDAQ", "NDSN", "NE", "NEE", "NEGG", "NEM", "NEO", "NEOG", "NEON", "NEOV", "NESR", "NET", "NETD", "NEWT", "NEXM", "NEXN", "NEXT", "NFBK", "NFE", "NFG", "NG", "NGD", "NGG", "NGL", "NGNE", "NGS", "NGVC", "NGVT", "NHC", "NHI", "NHIC", "NI", "NIC", "NICE", "NIO", "NIQ", "NISN", "NIU", "NJR", "NKE", "NKTR", "NLOP", "NLSP", "NLY", "NMAX", "NMFC", "NMIH", "NMM", "NMR", "NMRK", "NN", "NNBR", "NNE", "NNI", "NNN", "NNNN", "NNOX", "NOA", "NOAH", "NOG", "NOK", "NOMD", "NOV", "NOVT", "NPAC", "NPB", "NPCE", "NPK", "NPKI", "NPO", "NPWR", "NRC", "NRDS", "NRG", "NRIM", "NRIX", "NRXP", "NRXS", "NSC", "NSIT", "NSP", "NSPR", "NSSC", "NTAP", "NTB", "NTCT", "NTES", "NTGR", "NTHI", "NTLA", "NTNX", "NTR", "NTRA", "NTRB", "NTST", "NU", "NUE", "NUKK", "NUS", "NUTX", "NUVB", "NUVL", "NUWE", "NVAX", "NVCR", "NVCT", "NVDA", "NVEC", "NVGS", "NVMI", "NVNO", "NVO", "NVRI", "NVS", 
  "NVST", "NVT", "NVTS", "NWBI", "NWE", "NWG", "NWL", "NWN", "NWPX", "NWS", "NWSA", "NX", "NXE", "NXP", "NXPI", "NXST", "NXT", "NXTC", "NYT", "NYXH", "O", "OACC", "OBDC", "OBE", "OBIO", "OBK", "OBLG", "OBT", "OC", "OCC", "OCCI", "OCFC", "OCFT", "OCSL", "OCUL", "ODC", "ODD", "ODFL", "ODP", "ODV", "OEC", "OFG", "OFIX", "OGE", "OGN", "OGS", "OHI", "OI", "OII", "OIS", "OKE", "OKLO", "OKTA", "OKUR", "OKYO", "OLED", "OLLI", "OLMA", "OLN", "OLO", "OLP", "OM", "OMAB", "OMC", "OMCL", "OMDA", "OMER", "OMF", "OMI", "OMSE", "ON", "ONB", "ONC", "ONDS", "ONEG", "ONEW", "ONL", "ONON", "ONTF", "ONTO", "OOMA", "OPAL", "OPBK", "OPCH", "OPFI", "OPRA", "OPRT", "OPRX", "OPXS", "OPY", "OR", "ORA", "ORC", "ORCL", "ORGO", "ORI", "ORIC", "ORKA", "ORLA", "ORLY", "ORMP", "ORN", "ORRF", "OS", "OSBC", "OSCR", "OSIS", "OSK", "OSPN", "OSS", "OSUR", "OSW", "OTEX", "OTF", "OTIS", "OTLY", "OTTR", "OUST", "OUT", "OVV", "OWL", "OWLT", "OXLC", "OXM", "OXSQ", "OXY", "OYSE", "OZK", "PAA", "PAAS", "PAC", "PACK", "PACS", "PAG", "PAGP", "PAGS", "PAHC", "PAL", "PAM", "PANL", "PANW", "PAR", "PARR", "PATH", "PATK", "PAX", "PAY", "PAYC", "PAYO", "PAYS", "PAYX", "PB", "PBA", "PBF", "PBH", "PBI", "PBPB", "PBR", "PBR-A", "PBYI", "PC", "PCAP", "PCAR", "PCG", "PCH", "PCOR", "PCRX", "PCT", "PCTY", "PCVX", "PD", "PDD", "PDEX", "PDFS", "PDS", "PDYN", "PEBO", "PECO", "PEG", "PEGA", "PEN", "PENG", "PENN", "PEP", "PERI", "PESI", "PETS", "PEW", "PFBC", "PFE", "PFG", "PFGC", "PFLT", "PFS", "PFSI", "PG", "PGC", "PGNY", "PGR", "PGRE", "PGY", "PHAT", "PHG", "PHI", "PHIN", "PHIO", "PHLT", "PHM", "PHOE", "PHR", "PHUN", "PHVS", "PI", "PII", "PINC", "PINS", "PIPR", "PJT", "PK", "PKE", "PKG", "PKX", "PL", "PLAB", "PLAY", "PLCE", "PLD", "PLL", "PLMR", "PLNT", "PLOW", "PLPC", "PLSE", "PLTK", "PLTR", "PLUS", "PLXS", "PLYM", "PM", "PMTR", "PMTS", "PN", "PNC", "PNFP", "PNNT", "PNR", "PNRG", "PNTG", "PNW", "PODD", "POET", "PONY", "POOL", "POR", "POST", "POWI", "POWL", "PPBI", "PPBT", "PPC", "PPG", "PPIH", "PPL", "PPSI", "PPTA", "PR", "PRA", "PRAA", "PRAX", "PRCH", "PRCT", "PRDO", "PRE", "PRG", "PRGO", "PRGS", "PRI", "PRIM", "PRK", "PRKS", "PRLB", "PRM", "PRMB", "PRME", "PRO", "PROK", "PROP", "PRQR", "PRSU", "PRTA", "PRTG", "PRTH", "PRU", "PRVA", "PSA", "PSEC", "PSFE", "PSIX", "PSKY", "PSMT", "PSN", "PSNL", "PSO", "PSQH", "PSTG", "PSX", "PTC", "PTCT", "PTEN", "PTGX", "PTHS", "PTLO", "PTON", "PUBM", "PUK", "PUMP", "PVBC", "PVH", "PVLA", "PWP", "PWR", "PX", "PXLW", "PYPD", "PYPL", "PZZA", "QBTS", "QCOM", "QCRH", "QD", "QDEL", "QFIN", "QGEN", "QIPT", "QLYS", "QMCO", "QMMM", "QNST", "QNTM", "QRHC", "QRVO", "QS", "QSEA", "QSG", "QSR", "QTRX", "QTWO", "QUAD", "QUBT", "QUIK", "QURE", "QVCGA", "QXO", "R", "RAAQ", "RAC", "RACE", "RAIL", "RAL", "RAMP", "RAPP", "RAPT", "RARE", "RAY", "RBA", "RBB", "RBBN", "RBC", "RBCAA", "RBLX", "RBRK", "RC", "RCAT", "RCEL", "RCI", "RCKT", "RCKY", "RCL", "RCMT", "RCON", "RCT", 
    "RCUS", "RDAG", "RDAGU", "RDCM", "RDDT", "RDN", "RDNT", "RDVT", "RDW", "RDWR", "RDY", "REAL", "REAX", "REBN", "REFI", "REG", "RELX", "RELY", "RENT", "REPL", "REPX", "RERE", "RES", "RETO", "REVG", "REX", "REXR", "REYN", "REZI", "RF", "RFIL", "RGA", "RGC", "RGEN", "RGLD", "RGNX", "RGP", "RGR", "RGTI", "RH", "RHI", "RHLD", "RHP", "RICK", "RIG", "RIGL", "RILY", "RIME", "RIO", "RIOT", "RITM", "RITR", "RIVN", "RJF", "RKLB", "RKT", "RL", "RLAY", "RLGT", "RLI", "RLX", "RMAX", "RMBI", "RMBL", "RMBS", "RMD", "RMNI", "RMR", "RMSG", "RNA", "RNAC", "RNAZ", "RNG", "RNGR", "RNR", "RNST", "RNW", "ROAD", "ROCK", "ROG", "ROIV", "ROK", "ROKU", "ROL", "ROLR", "ROMA", "ROOT", "ROST", "RPAY", "RPD", "RPID", "RPM", "RPRX", "RPT", "RRC", "RRGB", "RRR", "RRX", "RS", "RSG", "RSI", "RSKD", "RSLS", "RSVR", "RTAC", "RTO", "RTX", "RUBI", "RUM", "RUN", "RUSHA", "RUSHB", "RVLV", "RVMD", "RVSB", "RVTY", "RWAY", "RXO", "RXRX", "RXST", "RY", "RYAAY", "RYAM", "RYAN", "RYI", "RYN", "RYTM", "RZB", "RZLT", "RZLV", "S", "SA", "SABS", "SAFE", "SAFT", "SAGT", "SAH", "SAIA", "SAIC", "SAIL", "SAM", "SAMG", "SAN", "SANA", "SAND", "SANM", "SAP", "SAR", "SARO", "SATL", "SATS", "SAVA", "SB", "SBAC", "SBC", "SBCF", "SBET", "SBGI", "SBH", "SBLK", "SBRA", "SBS", "SBSI", "SBSW", "SBUX", "SBXD", "SCAG", "SCCO", "SCHL", "SCHW", "SCI", "SCL", "SCLX", "SCM", "SCNX", "SCPH", "SCS", "SCSC", "SCVL", "SD", "SDA", "SDGR", "SDHC", "SDHI", "SDM", "SDRL", "SE", "SEAT", "SEDG", "SEE", "SEG", "SEI", "SEIC", "SEM", "SEMR", "SENEA", "SEPN", "SERA", "SERV", "SEZL", "SF", "SFBS", "SFD", "SFIX", "SFL", "SFM", "SFNC", "SG", "SGHC", "SGHT", "SGI", "SGML", "SGMT", "SGRY", "SHAK", "SHBI", "SHC", "SHCO", "SHEL", "SHEN", "SHG", "SHIP", "SHLS", "SHO", "SHOO", "SHOP", "SHW", "SI", "SIBN", "SIEB", "SIFY", "SIG", "SIGA", "SIGI", "SII", "SIMO", "SINT", "SION", "SIRI", "SITC", "SITE", "SITM", "SJM", "SKE", "SKLZ", "SKM", "SKT", "SKWD", "SKX", "SKY", "SKYE", "SKYH", "SKYT", "SKYW", "SLAB", "SLB", "SLDB", "SLDE", "SLDP", "SLF", "SLG", "SLGN", "SLI", "SLM", "SLN", "SLND", "SLNO", "SLP", "SLRC", "SLSN", "SLVM", "SM", "SMA", "SMBK", "SMC", "SMCI", "SMFG", "SMG", "SMHI", "SMLR", "SMMT", "SMP", "SMPL", "SMR", "SMTC", "SMWB", "SMX", "SN", "SNA", "SNAP", "SNBR", "SNCR", "SNCY", "SNDK", "SNDR", "SNDX", "SNES", "SNEX", "SNFCA", "SNGX", "SNN", "SNOW", "SNRE", "SNT", "SNV", "SNWV", "SNX", "SNY", "SNYR", "SO", "SOBO", "SOC", "SOFI", "SOGP", "SOHU", "SOLV", "SON", "SOND", "SONN", "SONO", "SONY", "SOPH", "SORA", "SOS", "SOUL", "SOUN", "SPAI", "SPB", "SPCB", "SPCE", "SPG", "SPH", "SPHR", "SPIR", "SPKL", "SPNS", "SPNT", "SPOK", "SPR", "SPRO", "SPRY", "SPSC", "SPT", "SPTN", "SPWH", "SPXC", "SQM", "SR", "SRAD", "SRBK", "SRCE", "SRDX", "SRE", "SRFM", "SRG", "SRI", "SRPT", "SRRK", "SRTS", "SSB", "SSD", "SSII", "SSL", "SSNC", "SSP", "SSRM", "SSSS", "SST", "SSTI", "SSTK", "SSYS", "ST", "STAA", "STAG", "STBA", "STC", "STE", "STEL", "STEM", "STEP", "STFS", "STGW", "STHO", "STI", "STIM", "STKL", "STKS", "STLA", "STLD", "STM", "STN", "STNE", "STNG", "STOK", "STR", "STRA", "STRD", "STRL", 
    "STRM", "STRT", "STRZ", "STSS", "STT", "STVN", "STX", "STXS", "STZ", "SU", "SUI", "SUN", "SUPN", "SUPV", "SUPX", "SURG", "SUZ", "SVCO", "SVM", "SVRA", "SVV", "SW", "SWBI", "SWIM", "SWIN", "SWK", "SWKS", "SWX", "SXC", "SXI", "SXT", "SY", "SYBT", "SYF", "SYK", "SYM", "SYNA", "SYRE", "SYTA", "SYY", "SZZL", "T", "TAC", "TACH", "TACO", "TAK", "TAL", "TALK", "TALO", "TAOX", "TAP", "TARA", "TARS", "TASK", "TATT", "TBB", "TBBB", "TBBK", "TBCH", "TBI", "TBLA", "TBPH", "TBRG", "TCBI", "TCBK", "TCBX", "TCMD", "TCOM", "TCPC", "TD", "TDC", "TDIC", "TDOC", "TDS", "TDUP", "TDW", "TEAM", "TECH", "TECK", "TECX", "TEF", "TEL", "TEM", "TEN", "TENB", "TEO", "TER", "TERN", "TEVA", "TEX", "TFC", "TFII", "TFIN", "TFPM", "TFSL", "TFX", "TG", "TGB", "TGE", "TGEN", "TGLS", "TGNA", "TGS", "TGT", "TGTX", "TH", "THC", "THFF", "THG", "THO", "THR", "THRM", "THRY", "THS", "THTX", "TIC", "TIGO", "TIGR", "TIL", "TILE", "TIMB", "TIPT", "TITN", "TIXT", "TJX", "TK", "TKC", "TKNO", "TKO", "TKR", "TLK", "TLN", "TLS", "TLSA", "TLSI", "TM", "TMC",
     "TMCI", "TMDX", "TME", "TMHC", "TMO", "TMUS", "TNC", "TNDM", "TNET", "TNGX", "TNK", "TNL", "TNXP", "TOI", "TOL", "TOPS", "TORO", "TOST", "TOWN", "TPB", "TPC", "TPCS", "TPG", "TPH", "TPR", "TPST", "TPVG", "TR", "TRAK", "TRC", "TRDA", "TREE", "TREX", "TRGP", "TRI", "TRIN", "TRIP", "TRMB", "TRMD", "TRML", "TRN", "TRNO", "TRNR", "TRNS", "TRON", "TROW", "TROX", "TRP", "TRS", "TRU", "TRUE", "TRUG", "TRUP", "TRV", "TRVG", "TRVI", "TS", "TSAT", "TSCO", "TSE", "TSEM", "TSHA", "TSLA", "TSLX", "TSM", "TSN", "TSQ", "TSSI", "TT", "TTAM", "TTAN", "TTC", "TTD", "TTE", "TTEC", "TTEK", "TTGT", "TTI", "TTMI", "TTSH", "TTWO", "TU", "TUSK", "TUYA", "TV", "TVA", "TVAI", "TVRD", "TVTX", "TW", "TWFG", "TWI", "TWIN", "TWLO", "TWNP", "TWO", "TWST", "TX", "TXG", "TXN", "TXNM", "TXO", "TXRH", "TXT", "TYG", "TYRA", "TZOO", "TZUP", "U", "UA", "UAA", "UAL", "UAMY", "UAVS", "UBER", "UBFO", "UBS", "UBSI", "UCAR", "UCB", "UCL", "UCTT", "UDMY", "UDR", "UE", "UEC", "UFCS", "UFG", "UFPI", "UFPT", 
     "UGI", "UGP", "UHAL", "UHAL-B", "UHG", "UHS", "UI", "UIS", "UL", "ULBI", "ULCC", "ULS", "ULY", "UMAC", "UMBF", "UMC", "UMH", "UNCY", "UNF", "UNFI", "UNH", "UNIT", "UNM", "UNP", "UNTY", "UPB", "UPBD", "UPS", "UPST", "UPWK", "UPXI", "URBN", "URGN", "UROY", "USAC", "USAR", "USAU", "USB", "USFD", "USLM", "USM", "USNA", "USPH", "UTHR", "UTI", "UTL", "UTZ", "UUUU", "UVE", "UVSP", "UVV", "UWMC", "UXIN", "V", "VAC", "VAL", "VALE", "VBIX", "VBNK", "VBTX", "VC", "VCEL", "VCTR", "VCYT", "VECO", "VEEV", "VEL", "VENU", "VEON", "VERA", "VERB", "VERI", "VERX", 
     "VET", "VFC", "VFS", "VG", "VIAV", "VICI", "VICR", "VIK", "VINP", "VIOT", "VIPS", "VIR", "VIRC", "VIRT", "VIST", "VITL", "VIV", "VKTX",
      "VLGEA", "VLN", "VLO", "VLRS", "VLTO", "VLY", "VMC", "VMD", "VMEO", "VMI", "VNDA", "VNET", "VNOM", "VNT", "VNTG", "VOD", "VOR", "VOXR", "VOYA", "VOYG", "VPG", "VRDN", "VRE",
       "VREX", "V", "WING", "WIT", "WIX", "WK", "WKC", "WKEY", "WKSP", "WLDN", "WLFC", "WLK", "WLY", "WM", "WMB", "WMG", "WMK", "WMS", "WMT", "WNC", "WNEB", "WNS", "WOOF", "WOR", "WOW", "WPC", "WPM", "WPP", "WRB", "WRBY", "WRD",
       "WS", "WSBC", "WSC", "WSFS", "WSM", "WSO", "WSR", "WST", "WT", "WTF", "WTG", "WTRG", "WTS", "WTTR", "WTW", "WU", "WULF", "WVE", "WW", "WWD", "WWW", "WXM", "WY", "WYFI", "WYNN", "WYY", "XAIR", "XBIT", "XCUR", "XEL", "XENE", "XERS", "XGN", "XHR", "XIFR", "XMTR", "XNCR", "XNET", "XOM", "XOMA", "XP", "XPEL", "XPER", "XPEV", "XPO",
        "XPOF", "XPRO", "XRAY", "XRX", "XTKG", "XYF", "XYL", "XYZ", "YALA", "YB", "YELP", "YETI", "YEXT", "YMAB", "YMAT", "YMM", "YORK", "YORW", "YOU", "YPF", "YRD", "YSG", "YSXT", "YUM", "YUMC", "YYAI", "YYGH", "Z",
         "ZBAI", "ZBH", "ZBIO", "ZBRA", "ZD", "ZDGE", "ZENA", "ZEO", "ZEPP", "ZETA", "ZEUS", 
         "ZG", "ZGN", "ZH", "ZIM", "ZIMV", "ZION", "ZIP", "ZJK", "ZK", "ZLAB", "ZM", "ZONE", "ZS", "ZSPC", "ZTO", "ZTS", "ZUMZ", "ZVIA", "ZVRA", "ZWS", "ZYBT", "ZYME"]


@dataclass
class ModuleThresholds:
    """æ¨¡å—å¯ç”¨é˜ˆå€¼é…ç½®"""
    # ç¨³å¥ç‰¹å¾é€‰æ‹©ï¼ˆå¿…å¼€ï¼‰
    robust_feature_min_samples: int = 300
    robust_feature_target_count: Tuple[int, int] = (12, 20)
    
    # Isotonicæ ¡å‡†ï¼ˆå¿…å¼€ï¼‰
    isotonic_min_val_samples: int = 200
    isotonic_monotony_test: bool = True
    
    # ä¼ ç»ŸMLå¤´ï¼ˆå¿…å¼€ï¼‰
    traditional_min_oof_coverage: float = 0.30
    
    # LTRï¼ˆæ¡ä»¶å¯ç”¨ï¼‰
    ltr_min_daily_group_size: int = 20
    ltr_min_date_coverage: float = 0.60
    ltr_min_oof_coverage: float = 0.40
    
    # Regime-awareï¼ˆæ¡ä»¶å¯ç”¨ï¼‰
    regime_min_samples_per_regime: int = 300
    regime_required_features: List[str] = field(default_factory=lambda: ['close', 'volume'])
    
    # Stackingï¼ˆé»˜è®¤å…³é—­ï¼‰
    stacking_min_base_models: int = 3
    stacking_min_ic_ir: float = 0.0
    stacking_min_oof_samples_ratio: float = 0.10
    stacking_max_correlation: float = 0.9
    
    # V5å¢å¼ºï¼ˆå…ˆå…³ï¼‰
    v5_memory_threshold_mb: float = 1000.0
    v5_enable_gradual: bool = False

@dataclass
class ModuleStatus:
    """æ¨¡å—çŠ¶æ€è·Ÿè¸ª"""
    enabled: bool = False
    degraded: bool = False
    reason: str = ""
    threshold_check: Dict[str, Any] = field(default_factory=dict)

class ModuleManager:
    """BMAæ¨¡å—ç®¡ç†å™¨ - æ ¹æ®é˜ˆå€¼æ™ºèƒ½å¯ç”¨/é™çº§"""
    
    def __init__(self, thresholds: ModuleThresholds = None):
        self.thresholds = thresholds or ModuleThresholds()
        self.status = {
            'robust_feature_selection': ModuleStatus(enabled=True),  # å¿…å¼€
            'isotonic_calibration': ModuleStatus(enabled=True),      # å¿…å¼€  
            'traditional_ml': ModuleStatus(enabled=True),            # å¿…å¼€
            'ltr_ranking': ModuleStatus(enabled=False),              # æ¡ä»¶å¯ç”¨
            'regime_aware': ModuleStatus(enabled=False),             # æ¡ä»¶å¯ç”¨
            'stacking': ModuleStatus(enabled=False),                 # é»˜è®¤å…³é—­
            'v5_enhancements': ModuleStatus(enabled=False)           # å…ˆå…³
        }
        self.logger = logging.getLogger(__name__)
    
    def evaluate_module_eligibility(self, module_name: str, data_info: Dict[str, Any]) -> ModuleStatus:
        """è¯„ä¼°æ¨¡å—å¯ç”¨èµ„æ ¼"""
        status = ModuleStatus()
        
        if module_name == 'robust_feature_selection':
            # å¿…å¼€æ¨¡å—ï¼Œæ£€æŸ¥é™çº§æ¡ä»¶
            n_samples = data_info.get('n_samples', 0)
            n_features = data_info.get('n_features', 0)
            
            if n_samples >= self.thresholds.robust_feature_min_samples:
                status.enabled = True
                status.reason = f"æ ·æœ¬æ•°{n_samples}æ»¡è¶³è¦æ±‚"
            else:
                status.enabled = True
                status.degraded = True
                status.reason = f"æ ·æœ¬æ•°{n_samples}<{self.thresholds.robust_feature_min_samples}ï¼Œå¯ç”¨é™çº§ç‰ˆæœ¬"
            
            status.threshold_check = {
                'n_samples': n_samples,
                'threshold': self.thresholds.robust_feature_min_samples,
                'target_features': self.thresholds.robust_feature_target_count
            }
        
        elif module_name == 'isotonic_calibration':
            # å¿…å¼€æ¨¡å—
            val_samples = data_info.get('validation_samples', 0)
            
            if val_samples >= self.thresholds.isotonic_min_val_samples:
                status.enabled = True
                status.reason = f"éªŒè¯æ ·æœ¬{val_samples}æ»¡è¶³è¦æ±‚"
            else:
                status.enabled = True
                status.degraded = True
                status.reason = f"éªŒè¯æ ·æœ¬{val_samples}<{self.thresholds.isotonic_min_val_samples}ï¼Œä½¿ç”¨åˆ†ä½æ ¡å‡†"
            
            status.threshold_check = {
                'val_samples': val_samples,
                'threshold': self.thresholds.isotonic_min_val_samples
            }
        
        elif module_name == 'traditional_ml':
            # å¿…å¼€æ¨¡å—
            oof_coverage = data_info.get('oof_coverage', 0.0)
            
            if oof_coverage >= self.thresholds.traditional_min_oof_coverage:
                status.enabled = True
                status.reason = f"OOFè¦†ç›–ç‡{oof_coverage:.1%}æ»¡è¶³è¦æ±‚"
            else:
                status.enabled = True
                status.degraded = True
                status.reason = f"OOFè¦†ç›–ç‡{oof_coverage:.1%}<{self.thresholds.traditional_min_oof_coverage:.1%}ï¼Œä»…è¾“å‡ºrank"
        
        elif module_name == 'ltr_ranking':
            # æ¡ä»¶å¯ç”¨æ¨¡å—
            daily_group_sizes = data_info.get('daily_group_sizes', [])
            date_coverage = data_info.get('date_coverage_ratio', 0.0)
            oof_coverage = data_info.get('oof_coverage', 0.0)
            
            # æ£€æŸ¥æ‰€æœ‰æ¡ä»¶
            min_group_ok = min(daily_group_sizes) >= self.thresholds.ltr_min_daily_group_size if daily_group_sizes else False
            date_coverage_ok = date_coverage >= self.thresholds.ltr_min_date_coverage
            oof_coverage_ok = oof_coverage >= self.thresholds.ltr_min_oof_coverage
            
            if min_group_ok and date_coverage_ok and oof_coverage_ok:
                status.enabled = True
                status.reason = "æ‰€æœ‰LTRæ¡ä»¶æ»¡è¶³"
            else:
                status.enabled = False
                status.reason = f"ä¸æ»¡è¶³LTRæ¡ä»¶: ç»„è§„æ¨¡={min_group_ok}, æ—¥æœŸè¦†ç›–={date_coverage_ok}, OOF={oof_coverage_ok}"
            
            status.threshold_check = {
                'min_group_size': min(daily_group_sizes) if daily_group_sizes else 0,
                'date_coverage': date_coverage,
                'oof_coverage': oof_coverage,
                'thresholds': {
                    'min_group': self.thresholds.ltr_min_daily_group_size,
                    'date_cov': self.thresholds.ltr_min_date_coverage,
                    'oof_cov': self.thresholds.ltr_min_oof_coverage
                }
            }
        
        elif module_name == 'regime_aware':
            # æ¡ä»¶å¯ç”¨æ¨¡å—
            regime_samples = data_info.get('regime_samples', {})
            has_required_features = data_info.get('has_price_volume', False)
            regime_stability = data_info.get('regime_stability', 0.0)
            
            min_samples_ok = all(count >= self.thresholds.regime_min_samples_per_regime 
                               for count in regime_samples.values()) if regime_samples else False
            
            if min_samples_ok and has_required_features and regime_stability > 0.5:
                status.enabled = True
                status.reason = "Regime-awareæ¡ä»¶æ»¡è¶³"
            else:
                status.enabled = False
                status.degraded = True
                status.reason = "ä½¿ç”¨æ ·æœ¬åŠ æƒæ¨¡å¼æ›¿ä»£å¤šæ¨¡å‹"
            
            status.threshold_check = {
                'regime_samples': regime_samples,
                'has_price_volume': has_required_features,
                'stability': regime_stability
            }
        
        elif module_name == 'stacking':
            # é»˜è®¤å…³é—­æ¨¡å—
            base_models = data_info.get('base_models_ic_ir', {})
            oof_samples = data_info.get('oof_valid_samples', 0)
            total_samples = data_info.get('n_samples', 1)
            correlations = data_info.get('model_correlations', [])
            
            good_models = sum(1 for ic_ir in base_models.values() if ic_ir > self.thresholds.stacking_min_ic_ir)
            oof_ratio = oof_samples / total_samples
            max_corr = max(correlations) if correlations else 0.0
            
            if (good_models >= self.thresholds.stacking_min_base_models and 
                oof_ratio >= self.thresholds.stacking_min_oof_samples_ratio and
                max_corr < self.thresholds.stacking_max_correlation):
                status.enabled = True
                status.reason = "Stackingæ¡ä»¶æ»¡è¶³"
            else:
                status.enabled = False
                status.reason = f"ä½¿ç”¨IC/IRæ— è®­ç»ƒåŠ æƒ: å¥½æ¨¡å‹{good_models}, OOFæ¯”ä¾‹{oof_ratio:.1%}, æœ€å¤§ç›¸å…³{max_corr:.2f}"
        
        elif module_name == 'v5_enhancements':
            # å…ˆå…³æ¨¡å—
            memory_usage = data_info.get('memory_usage_mb', 0)
            other_modules_stable = data_info.get('other_modules_stable', False)
            
            if memory_usage < self.thresholds.v5_memory_threshold_mb and other_modules_stable:
                status.enabled = self.thresholds.v5_enable_gradual
                status.reason = "V5å¢å¼ºé€æ­¥å¯ç”¨" if status.enabled else "V5å¢å¼ºæš‚æ—¶å…³é—­"
            else:
                status.enabled = False
                status.reason = f"å†…å­˜ä½¿ç”¨{memory_usage}MBè¿‡é«˜æˆ–å…¶ä»–æ¨¡å—ä¸ç¨³å®š"
        
        return status
    
    def update_module_status(self, data_info: Dict[str, Any]):
        """æ›´æ–°æ‰€æœ‰æ¨¡å—çŠ¶æ€"""
        for module_name in self.status.keys():
            self.status[module_name] = self.evaluate_module_eligibility(module_name, data_info)
            
        self.logger.info("æ¨¡å—çŠ¶æ€æ›´æ–°å®Œæˆ:")
        for name, status in self.status.items():
            icon = "âœ…" if status.enabled and not status.degraded else "âš ï¸" if status.degraded else "âŒ"
            self.logger.info(f"  {icon} {name}: {status.reason}")
    
    def is_enabled(self, module_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å—æ˜¯å¦å¯ç”¨"""
        return self.status.get(module_name, ModuleStatus()).enabled
    
    def is_degraded(self, module_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å—æ˜¯å¦é™çº§"""
        return self.status.get(module_name, ModuleStatus()).degraded
    
    def get_status_summary(self) -> Dict[str, Any]:
        """è·å–çŠ¶æ€æ‘˜è¦"""
        return {
            name: {
                'enabled': status.enabled,
                'degraded': status.degraded,
                'reason': status.reason,
                'checks': status.threshold_check
            }
            for name, status in self.status.items()
        }

class MemoryManager:
    """å†…å­˜ç®¡ç†å™¨ - é¢„é˜²å†…å­˜æ³„æ¼"""
    
    def __init__(self, memory_threshold: float = 80.0, auto_cleanup: bool = True):
        self.memory_threshold = memory_threshold
        self.auto_cleanup = auto_cleanup
        self.logger = logging.getLogger(__name__)
        
    def get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨ç‡"""
        try:
            return psutil.virtual_memory().percent
        except:
            return 0.0
    
    def check_memory_pressure(self) -> bool:
        """æ£€æŸ¥å†…å­˜å‹åŠ›"""
        usage = self.get_memory_usage()
        return usage > self.memory_threshold
    
    def force_cleanup(self):
        """éé˜»å¡å†…å­˜æ¸…ç†"""
        import threading
        
        def _async_cleanup():
            try:
                gc.collect()
                current_usage = self.get_memory_usage()
                self.logger.debug(f"å¼‚æ­¥å†…å­˜æ¸…ç†å®Œæˆ, å½“å‰ä½¿ç”¨ç‡: {current_usage:.1f}%")
            except Exception as e:
                self.logger.warning(f"å¼‚æ­¥å†…å­˜æ¸…ç†å¤±è´¥: {e}")
        
        # å¯åŠ¨åå°çº¿ç¨‹è¿›è¡Œæ¸…ç†ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
        cleanup_thread = threading.Thread(target=_async_cleanup, daemon=True)
        cleanup_thread.start()
    
    def memory_safe_wrapper(self, func):
        """å†…å­˜å®‰å…¨è£…é¥°å™¨"""
        @wraps(func)
        def wrapper(*args, **kwargs):
            initial_memory = self.get_memory_usage()
            # åªåœ¨å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡90%æ—¶è¿›è¡Œæ¸…ç†
            if initial_memory > 90.0:
                self.logger.warning(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {initial_memory:.1f}%, æ‰§è¡Œå¼‚æ­¥æ¸…ç†")
                self.force_cleanup()
            
            try:
                result = func(*args, **kwargs)
                
                final_memory = self.get_memory_usage()
                memory_increase = final_memory - initial_memory
                
                # åªè®°å½•æ˜¾è‘—çš„å†…å­˜å¢é•¿
                if memory_increase > 30:
                    self.logger.warning(f"å†…å­˜å¢é•¿æ˜¾è‘—: +{memory_increase:.1f}%")
                    
                # åªåœ¨å†…å­˜è¶…è¿‡85%æ—¶è§¦å‘æ¸…ç†
                if self.auto_cleanup and final_memory > 85.0:
                    self.force_cleanup()
                
                return result
                
            except MemoryError as e:
                self.logger.error(f"å†…å­˜ä¸è¶³é”™è¯¯: {e}")
                self.force_cleanup()
                raise
            except Exception as e:
                # åªåœ¨å†…å­˜é”™è¯¯æ—¶æ¸…ç†ï¼Œä¸è¦åœ¨æ‰€æœ‰å¼‚å¸¸æ—¶éƒ½æ¸…ç†
                if isinstance(e, (MemoryError, OSError)) and self.auto_cleanup:
                    self.force_cleanup()
                raise
                        
        return wrapper

class DataValidator:
    """æ•°æ®éªŒè¯å™¨ - ç»Ÿä¸€æ•°æ®éªŒè¯é€»è¾‘"""
    
    def __init__(self, logger):
        self.logger = logger
        
    def validate_dataframe(self, data: pd.DataFrame, name: str = "data", 
                          min_rows: int = 10, min_cols: int = 1) -> dict:
        """å…¨é¢çš„DataFrameéªŒè¯"""
        validation_result = {
            'valid': True,
            'errors': [],
            'warnings': [],
            'stats': {}
        }
        
        try:
            # åŸºç¡€æ£€æŸ¥
            if data is None:
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} is None")
                return validation_result
                
            if not isinstance(data, pd.DataFrame):
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} is not a DataFrame, got {type(data)}")
                return validation_result
                
            # ç©ºæ£€æŸ¥
            if data.empty:
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} is empty")
                return validation_result
                
            # å°ºå¯¸æ£€æŸ¥
            if len(data) < min_rows:
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} has only {len(data)} rows, minimum {min_rows} required")
                
            if len(data.columns) < min_cols:
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} has only {len(data.columns)} columns, minimum {min_cols} required")
            
            # NaNæ£€æŸ¥
            nan_counts = data.isnull().sum()
            total_nans = nan_counts.sum()
            if total_nans > 0:
                nan_ratio = total_nans / (len(data) * len(data.columns))
                if nan_ratio > 0.5:
                    validation_result['valid'] = False
                    validation_result['errors'].append(f"{name} has {nan_ratio:.1%} NaN values (>50%)")
                elif nan_ratio > 0.2:
                    validation_result['warnings'].append(f"{name} has {nan_ratio:.1%} NaN values")
            
            # æ•°æ®ç±»å‹æ£€æŸ¥
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) == 0:
                validation_result['warnings'].append(f"{name} has no numeric columns")
            
            # ç»Ÿè®¡ä¿¡æ¯
            validation_result['stats'] = {
                'rows': len(data),
                'cols': len(data.columns),
                'numeric_cols': len(numeric_cols),
                'nan_count': int(total_nans),
                'nan_ratio': total_nans / (len(data) * len(data.columns)) if len(data) > 0 and len(data.columns) > 0 else 0
            }
            
        except Exception as e:
            validation_result['valid'] = False
            validation_result['errors'].append(f"Validation error: {str(e)}")
            
        return validation_result
    
    def validate_series(self, data: pd.Series, name: str = "series",
                       min_length: int = 10, allow_nan: bool = True) -> dict:
        """SerieséªŒè¯"""
        validation_result = {
            'valid': True,
            'errors': [],
            'warnings': [],
            'stats': {}
        }
        
        try:
            if data is None:
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} is None")
                return validation_result
                
            if not isinstance(data, pd.Series):
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} is not a Series, got {type(data)}")
                return validation_result
                
            if len(data) < min_length:
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} has only {len(data)} values, minimum {min_length} required")
            
            nan_count = data.isnull().sum()
            if not allow_nan and nan_count > 0:
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} contains {nan_count} NaN values, but NaN not allowed")
            elif nan_count > len(data) * 0.5:
                validation_result['valid'] = False
                validation_result['errors'].append(f"{name} has {nan_count/len(data):.1%} NaN values (>50%)")
                
            validation_result['stats'] = {
                'length': len(data),
                'nan_count': int(nan_count),
                'nan_ratio': nan_count / len(data) if len(data) > 0 else 0
            }
            
        except Exception as e:
            validation_result['valid'] = False
            validation_result['errors'].append(f"Series validation error: {str(e)}")
            
        return validation_result
    
    def safe_validate_and_fix(self, data: pd.DataFrame, name: str = "data") -> pd.DataFrame:
        """éªŒè¯å¹¶å°è¯•ä¿®å¤æ•°æ®"""
        if data is None or data.empty:
            self.logger.warning(f"{name} is None or empty, returning empty DataFrame")
            return pd.DataFrame()
        
        # åŸºç¡€ä¿®å¤
        original_shape = data.shape
        
        # åˆ é™¤å…¨ç©ºè¡Œ/åˆ—
        data = data.dropna(how='all', axis=0)  # åˆ é™¤å…¨ç©ºè¡Œ
        data = data.dropna(how='all', axis=1)  # åˆ é™¤å…¨ç©ºåˆ—
        
        if data.empty:
            self.logger.warning(f"{name} became empty after removing all-NaN rows/columns")
            return pd.DataFrame()
        
        # å¤„ç†æ— ç©·å€¼
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            data[numeric_cols] = data[numeric_cols].replace([np.inf, -np.inf], np.nan)
        
        if data.shape != original_shape:
            self.logger.info(f"{name} cleaned: {original_shape} -> {data.shape}")
        
        return data
    
    def clean_numeric_data(self, data: pd.DataFrame, name: str = "data", 
                          strategy: str = "smart") -> pd.DataFrame:
        """ç»Ÿä¸€çš„æ•°å€¼æ•°æ®æ¸…ç†ç­–ç•¥"""
        if data is None or data.empty:
            return pd.DataFrame()
        
        cleaned_data = data.copy()
        numeric_cols = cleaned_data.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) == 0:
            self.logger.debug(f"{name}: æ²¡æœ‰æ•°å€¼åˆ—éœ€è¦æ¸…ç†")
            return cleaned_data
        
        # å¤„ç†æ— ç©·å€¼
        inf_mask = np.isinf(cleaned_data[numeric_cols])
        inf_count = inf_mask.sum().sum()
        if inf_count > 0:
            self.logger.warning(f"{name}: å‘ç° {inf_count} ä¸ªæ— ç©·å€¼")
            cleaned_data[numeric_cols] = cleaned_data[numeric_cols].replace([np.inf, -np.inf], np.nan)
        
        # NaNå¤„ç†ç­–ç•¥
        nan_count_before = cleaned_data[numeric_cols].isnull().sum().sum()
        
        if strategy == "smart":
            # æ™ºèƒ½ç­–ç•¥ï¼šæ ¹æ®åˆ—çš„æ€§è´¨é€‰æ‹©ä¸åŒå¡«å……æ–¹æ³•
            for col in numeric_cols:
                if cleaned_data[col].isnull().sum() == 0:
                    continue
                    
                col_name_lower = col.lower()
                if any(keyword in col_name_lower for keyword in ['return', 'pct', 'change', 'momentum']):
                    # æ”¶ç›Šç‡ç±»æŒ‡æ ‡ç”¨0å¡«å……
                    cleaned_data[col] = cleaned_data[col].fillna(0)
                elif any(keyword in col_name_lower for keyword in ['volume', 'amount', 'size']):
                    # æˆäº¤é‡ç±»æŒ‡æ ‡ç”¨ä¸­ä½æ•°å¡«å……
                    cleaned_data[col] = cleaned_data[col].fillna(cleaned_data[col].median())
                elif any(keyword in col_name_lower for keyword in ['price', 'close', 'open', 'high', 'low']):
                    # ä»·æ ¼ç±»æŒ‡æ ‡ç”¨å‰å‘å¡«å……
                    cleaned_data[col] = cleaned_data[col].fillna(method='ffill').fillna(method='bfill')
                else:
                    # å…¶ä»–æŒ‡æ ‡ç”¨å‡å€¼å¡«å……
                    mean_val = cleaned_data[col].mean()
                    if pd.isna(mean_val):
                        cleaned_data[col] = cleaned_data[col].fillna(0)
                    else:
                        cleaned_data[col] = cleaned_data[col].fillna(mean_val)
                        
        elif strategy == "zero":
            # å…¨éƒ¨ç”¨0å¡«å……
            cleaned_data[numeric_cols] = cleaned_data[numeric_cols].fillna(0)
            
        elif strategy == "forward":
            # å‰å‘å¡«å……
            cleaned_data[numeric_cols] = cleaned_data[numeric_cols].fillna(method='ffill').fillna(0)
            
        elif strategy == "median":
            # ä¸­ä½æ•°å¡«å……
            for col in numeric_cols:
                median_val = cleaned_data[col].median()
                if pd.isna(median_val):
                    cleaned_data[col] = cleaned_data[col].fillna(0)
                else:
                    cleaned_data[col] = cleaned_data[col].fillna(median_val)
        
        nan_count_after = cleaned_data[numeric_cols].isnull().sum().sum()
        if nan_count_before > 0:
            self.logger.info(f"{name}: NaNæ¸…ç†å®Œæˆ {nan_count_before} -> {nan_count_after}")
        
        return cleaned_data

class BMAExceptionHandler:
    """BMAå¼‚å¸¸å¤„ç†å™¨"""
    
    def __init__(self, logger):
        self.logger = logger
        self.error_counts = {}
        self.max_retries = 3
        
    @contextmanager
    def safe_execution(self, operation_name: str, fallback_result: Any = None):
        """å®‰å…¨æ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        try:
            self.logger.debug(f"å¼€å§‹æ‰§è¡Œ: {operation_name}")
            yield
            self.logger.debug(f"æˆåŠŸå®Œæˆ: {operation_name}")
            
        except Exception as e:
            self.logger.error(f"æ“ä½œå¤±è´¥: {operation_name} - {e}")
            self.logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            
            # è®°å½•é”™è¯¯ç»Ÿè®¡
            self.error_counts[operation_name] = self.error_counts.get(operation_name, 0) + 1
            
            # å¦‚æœæœ‰å›é€€ç»“æœï¼Œè¿”å›å›é€€ç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            if fallback_result is not None:
                self.logger.warning(f"ä½¿ç”¨å›é€€ç»“æœ: {operation_name}")
                return fallback_result
            else:
                raise

@dataclass
class MarketRegime:
    """å¸‚åœºçŠ¶æ€"""
    regime_id: int
    name: str
    probability: float
    characteristics: Dict[str, float]
    duration: int = 0

@dataclass 
class RiskFactorExposure:
    """é£é™©å› å­æš´éœ²"""
    market_beta: float
    size_exposure: float  
    value_exposure: float
    momentum_exposure: float
    volatility_exposure: float
    quality_exposure: float
    country_exposure: Dict[str, float] = field(default_factory=dict)
    sector_exposure: Dict[str, float] = field(default_factory=dict)

def sanitize_ticker(raw: Union[str, Any]) -> str:
    """æ¸…ç†è‚¡ç¥¨ä»£ç ä¸­çš„BOMã€å¼•å·ã€ç©ºç™½ç­‰æ‚è´¨ã€‚"""
    try:
        s = str(raw)
    except Exception:
        return ''
    # å»é™¤BOMä¸é›¶å®½å­—ç¬¦
    s = s.replace('\ufeff', '').replace('\u200b', '').replace('\u200c', '').replace('\u200d', '')
    # å»é™¤å¼•å·ä¸ç©ºç™½
    s = s.strip().strip("'\"")
    # ç»Ÿä¸€å¤§å†™
    s = s.upper()
    return s


def load_universe_from_file(file_path: str) -> Optional[List[str]]:
    try:
        if os.path.exists(file_path):
            # ä½¿ç”¨utf-8-sigä»¥è‡ªåŠ¨å»é™¤BOM
            with open(file_path, 'r', encoding='utf-8-sig') as f:
                tickers = []
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    # æ”¯æŒé€—å·æˆ–ç©ºæ ¼åˆ†éš”
                    parts = [p for token in line.split(',') for p in token.split()]
                    for p in parts:
                        t = sanitize_ticker(p)
                        if t:
                            tickers.append(t)
            # å»é‡å¹¶ä¿æŒé¡ºåº
            tickers = list(dict.fromkeys(tickers))
            return tickers if tickers else None
    except Exception:
        return None
    return None

def load_universe_fallback() -> List[str]:
    # ç»Ÿä¸€ä»é…ç½®æ–‡ä»¶è¯»å–è‚¡ç¥¨æ¸…å•ï¼Œç§»é™¤æ—§ç‰ˆä¾èµ–
    root_stocks = os.path.join(os.path.dirname(__file__), 'filtered_stocks_20250817_002928')
    tickers = load_universe_from_file(root_stocks)
    if tickers:
        return tickers
    
    logger.warning("æœªæ‰¾åˆ°stocks.txtæ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤è‚¡ç¥¨æ¸…å•")
    return DEFAULT_TICKER_LIST
# CRITICAL TIME ALIGNMENT FIX APPLIED:
# - Prediction horizon set to T+5 for medium-term signals
# - Features use T-4 data, targets predict T+5 (10-day gap prevents data leakage)
# - This configuration is validated for production trading


class UltraEnhancedQuantitativeModel:
    """Ultra Enhanced é‡åŒ–æ¨¡å‹ V6ï¼šé›†æˆæ‰€æœ‰é«˜çº§åŠŸèƒ½ + å†…å­˜ä¼˜åŒ– + ç”Ÿäº§çº§å¢å¼º"""
    
    def __init__(self, config_path: str = "alphas_config.yaml", enable_optimization: bool = True, 
                 enable_v6_enhancements: bool = True):
        """
        åˆå§‹åŒ–Ultra Enhancedé‡åŒ–æ¨¡å‹ V6
        
        Args:
            config_path: Alphaç­–ç•¥é…ç½®æ–‡ä»¶è·¯å¾„
            enable_optimization: å¯ç”¨å†…å­˜ä¼˜åŒ–åŠŸèƒ½
            enable_v6_enhancements: å¯ç”¨V6å¢å¼ºåŠŸèƒ½ï¼ˆç”Ÿäº§çº§æ”¹è¿›ï¼‰
        """
        self.config_path = config_path
        self.config = self._load_config()
        self.enable_optimization = enable_optimization
        self.enable_v6_enhancements = enable_v6_enhancements
        
        # ğŸš€ åˆå§‹åŒ–BMA Enhanced V6 Integrated Systemï¼ˆæ–°å¢ï¼‰
        self.enhanced_system_v6 = None
        if enable_v6_enhancements and BMA_ENHANCED_V6_AVAILABLE:
            self._init_enhanced_system_v6()
        
        # ğŸ”¥ åˆå§‹åŒ–è‡ªé€‚åº”æƒé‡å­¦ä¹ ç³»ç»Ÿï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼‰
        self.adaptive_weights = None
        self._init_adaptive_weights()
        
        # â­ åˆå§‹åŒ–é«˜çº§Alphaç³»ç»Ÿï¼ˆä¸“ä¸šæœºæ„çº§åŠŸèƒ½ï¼‰
        self.advanced_alpha_system = None
        self._init_advanced_alpha_system()
        
        # [ENHANCED] å†…å­˜ä¼˜åŒ–åŠŸèƒ½é›†æˆ
        if enable_optimization:
            self._init_optimization_components()
        
        # ğŸ”¥ æ–°å¢åŠŸèƒ½ï¼šWalk-Forwardé‡è®­ç»ƒç³»ç»Ÿ
        self.walk_forward_system = None
        self._init_walk_forward_system()
        
        # ğŸ”¥ æ–°å¢åŠŸèƒ½ï¼šç”Ÿäº§å°±ç»ªéªŒè¯å™¨
        self.production_validator = None
        self._init_production_validator()
        
        # ğŸ”¥ æ–°å¢åŠŸèƒ½ï¼šå¢å¼ºCVæ—¥å¿—è®°å½•å™¨
        self.cv_logger = None
        self._init_enhanced_cv_logger()
        
        # ğŸ”¥ CRITICAL: Initialize Alpha Engine FIRST - MUST NOT BE MISSING
        # This must be done before other systems that depend on it
        self._init_alpha_engine()
        
        # ğŸ”§ ç»Ÿä¸€ç‰¹å¾ç®¡é“ - è§£å†³è®­ç»ƒ-é¢„æµ‹ç‰¹å¾ç»´åº¦ä¸åŒ¹é…é—®é¢˜
        self._init_unified_feature_pipeline()
        
        # ğŸ”¥ NEW: Regime Detectionç³»ç»Ÿ (depends on alpha engine)
        self.regime_detector = None
        self.regime_trainer = None
        self._init_regime_detection_system()
        
        # ğŸ”¥ V5æ–°å¢ï¼šç«‹ç«¿è§å½±å¢å¼ºåŠŸèƒ½é…ç½® (depends on alpha engine)
        self._init_enhanced_features_v5()
        
        # ğŸ”§ æ–°å¢ï¼šæ¨¡å—ç®¡ç†å™¨å’Œä¿®å¤ç»„ä»¶
        self.module_manager = ModuleManager()
        self.memory_manager = MemoryManager(memory_threshold=75.0)
        self.data_validator = DataValidator(logger)
        self.exception_handler = BMAExceptionHandler(logger)
        
        # ä¸¥æ ¼æ—¶é—´éªŒè¯æ ‡å¿—
        self.strict_temporal_validation_enabled = True
    
    def _init_enhanced_system_v6(self):
        """åˆå§‹åŒ–BMA Enhanced V6 Integrated System"""
        try:
            # ğŸ”¥ CRITICAL FIX: ä½¿ç”¨ç»Ÿä¸€æ—¶åºé…ç½® - å•ä¸€çœŸç›¸æº
            from unified_timing_config import get_unified_timing_config
            unified_config = get_unified_timing_config()
            
            # åˆ›å»ºV6å¢å¼ºç³»ç»Ÿé…ç½® - åŒ…å«æ‰€æœ‰8ä¸ªä¿®å¤
            v6_config = BMAEnhancedConfig()
            
            # âœ… Fix 1: å•ä¸€éš”ç¦»æ–¹æ³•ï¼ˆpurge OR embargoï¼‰- ä½¿ç”¨ç»Ÿä¸€é…ç½®
            v6_config.validation_config.isolation_method = unified_config.isolation_method
            v6_config.validation_config.isolation_days = unified_config.effective_isolation  # ç»Ÿä¸€éš”ç¦»å‚æ•°
            
            # âœ… Fix 2: é˜²æ³„æ¼æ”¿æƒæ£€æµ‹ï¼ˆä»…è¿‡æ»¤ï¼Œç¦ç”¨å¹³æ»‘ï¼‰- ä½¿ç”¨ç»Ÿä¸€é…ç½®
            v6_config.regime_config.use_filtering_only = True  # åªä½¿ç”¨è¿‡æ»¤ï¼Œç¦ç”¨å¹³æ»‘
            v6_config.regime_config.embargo_days = unified_config.effective_isolation  # ç»Ÿä¸€éš”ç¦»å‚æ•°
            v6_config.regime_config.enable_smoothing = False  # æ˜¾å¼ç¦ç”¨å¹³æ»‘
            
            # ğŸ”¥ åŒæ­¥CVå‚æ•°åˆ°ç»Ÿä¸€é…ç½®
            v6_config.validation_config.gap_days = unified_config.cv_gap_days
            v6_config.validation_config.embargo_days = unified_config.cv_embargo_days
            v6_config.validation_config.purge_days = unified_config.purge_days
            
            # âœ… Fix 3: ç‰¹å¾æ»åA/Bæµ‹è¯•+DMç»Ÿè®¡æ˜¾è‘—æ€§
            v6_config.lag_config.test_lags = [0, 1, 2, 5]  # æµ‹è¯•T-5åˆ°T-0/T-1
            v6_config.lag_config.target_horizon = 10
            v6_config.lag_config.use_dm_test = True  # å¯ç”¨Diebold-Marianoæµ‹è¯•
            v6_config.lag_config.dm_significance_level = 0.05
            v6_config.lag_config.persist_to_config = True  # è‡ªåŠ¨æŒä¹…åŒ–è·èƒœlag
            
            # âœ… Fix 4: å› å­æ—ç‰¹å®šåŠè¡°æœŸï¼ˆç¡®å®šæ€§æ˜ å°„+å¤±è´¥å¤„ç†ï¼‰
            v6_config.factor_decay_config.use_family_specific = True
            v6_config.factor_decay_config.family_mapping = {
                'momentum': 20, 'reversal': 5, 'value': 60, 'quality': 90,
                'volatility': 10, 'liquidity': 15
            }
            v6_config.factor_decay_config.fail_on_unknown = True  # æœªçŸ¥æ—æ˜¾å¼å¤±è´¥
            
            # âœ… Fix 5: æ—¶é—´è¡°å‡åŠè¡°æœŸä¼˜åŒ–ï¼ˆ60-90å¤©èŒƒå›´ï¼Œè®¾ä¸º75ï¼‰- ä½¿ç”¨ç»Ÿä¸€é…ç½®
            v6_config.sample_time_decay_half_life = unified_config.sample_weight_halflife
            v6_config.half_life_sensitivity_test = True  # å¯ç”¨{60,75,90}æ•æ„Ÿæ€§æµ‹è¯•
            
            # ğŸ”¥ åŒæ­¥å› å­æ—è¡°å‡æ˜ å°„
            v6_config.factor_decay_config.family_mapping = unified_config.factor_decay_mapping
            
            # âœ… Fix 6: ç”Ÿäº§é—¨ç¦ORé€»è¾‘ï¼ˆICâ‰¥0.02 OR QLIKEâ‰¥8%ï¼‰
            v6_config.production_gates.min_ic_improvement = 0.02
            v6_config.production_gates.max_qlike_improvement = 0.08  # 8%æ”¹è¿›é˜ˆå€¼
            v6_config.production_gates.max_training_time_multiplier = 1.5
            v6_config.production_gates.use_or_logic = True  # å¯ç”¨ORé€»è¾‘
            
            # âœ… Fix 7: åŒå‘¨å¢é‡è®­ç»ƒ+æœˆåº¦å…¨é‡é‡æ„
            v6_config.training_schedule.incremental_frequency_days = 14  # åŒå‘¨å¢é‡
            v6_config.training_schedule.full_rebuild_frequency_days = 28  # æœˆåº¦å…¨é‡
            v6_config.training_schedule.incremental_tree_limit = (50, 150)  # å¢é‡æ ‘æ•°é™åˆ¶
            v6_config.training_schedule.incremental_lr_factor = 0.3  # å¢é‡LRè¡°å‡
            
            # âœ… Fix 8: çŸ¥è¯†ä¿ç•™+æ¼‚ç§»æ£€æµ‹è§¦å‘é‡æ„
            v6_config.knowledge_config.kl_divergence_threshold = 0.3  # KLæ•£åº¦é˜ˆå€¼
            v6_config.knowledge_config.rank_correlation_threshold = 0.7  # æ’åºç›¸å…³é˜ˆå€¼
            v6_config.knowledge_config.enable_model_distillation = True
            v6_config.knowledge_config.drift_triggers_rebuild = True  # æ¼‚ç§»è§¦å‘é‡æ„
            
            self.enhanced_system_v6 = BMAEnhancedIntegratedSystem(v6_config)
            print("[SUCCESS] BMA Enhanced V6 System åˆå§‹åŒ–æˆåŠŸ")
            
            # ğŸ”¥ è®°å½•ç»Ÿä¸€é…ç½®çŠ¶æ€
            logger.info("=" * 60)
            logger.info("UNIFIED TIMING CONFIGURATION APPLIED")
            logger.info(f"Effective Isolation: {unified_config.effective_isolation} days")
            logger.info(f"CV Gap: {unified_config.cv_gap_days} days")
            logger.info(f"Isolation Method: {unified_config.isolation_method}")
            logger.info(f"Sample Weight Halflife: {unified_config.sample_weight_halflife} days")
            logger.info("=" * 60)
            
        except Exception as e:
            print(f"[ERROR] BMA Enhanced V6 System åˆå§‹åŒ–å¤±è´¥: {e}")
            self.enhanced_system_v6 = None
    
    def _init_adaptive_weights(self):
        """å»¶è¿Ÿåˆå§‹åŒ–è‡ªé€‚åº”æƒé‡ç³»ç»Ÿ"""
        try:
            # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            from autotrader.adaptive_factor_weights import AdaptiveFactorWeights, WeightLearningConfig
            
            weight_config = WeightLearningConfig(
                lookback_days=252,
                validation_days=63,
                min_confidence=0.6,
                rebalance_frequency=21,
                enable_regime_detection=True
            )
            self.adaptive_weights = AdaptiveFactorWeights(weight_config)
            global ADAPTIVE_WEIGHTS_AVAILABLE
            ADAPTIVE_WEIGHTS_AVAILABLE = True
            logger.info("BMAè‡ªé€‚åº”æƒé‡ç³»ç»Ÿå»¶è¿Ÿåˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.warning(f"è‡ªé€‚åº”æƒé‡ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ç¡¬ç¼–ç æƒé‡")
            self.adaptive_weights = None
    
    def _init_walk_forward_system(self):
        """åˆå§‹åŒ–Walk-Forwardé‡è®­ç»ƒç³»ç»Ÿ"""
        try:
            # ğŸ”§ ä¿®å¤ç›¸å¯¹å¯¼å…¥é—®é¢˜ - ä½¿ç”¨ç»å¯¹å¯¼å…¥
            from walk_forward_retraining import create_walk_forward_system, WalkForwardConfig
            
            wf_config = WalkForwardConfig(
                train_window_months=24,  # 2å¹´è®­ç»ƒçª—å£
                step_size_days=30,
                warmup_periods=3,
                force_refit_days=90,
                window_type='rolling',
                enable_version_control=True
            )
            self.walk_forward_system = create_walk_forward_system(wf_config)
            logger.info("Walk-Forwardé‡è®­ç»ƒç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ(ç»å¯¹å¯¼å…¥)")
            
        except Exception as e:
            logger.warning(f"Walk-Forwardé‡è®­ç»ƒç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            self.walk_forward_system = None
    
    def _init_production_validator(self):
        """åˆå§‹åŒ–ç”Ÿäº§å°±ç»ªéªŒè¯å™¨"""
        try:
            # ğŸ”§ ä¿®å¤ç›¸å¯¹å¯¼å…¥é—®é¢˜ - ä½¿ç”¨ç»å¯¹å¯¼å…¥
            from production_readiness_validator import ProductionReadinessValidator, ValidationThresholds, ValidationConfig
            
            thresholds = ValidationThresholds(
                min_rank_ic=0.01,    # å·²ä¼˜åŒ–çš„é˜ˆå€¼
                min_t_stat=1.0,      # å·²ä¼˜åŒ–çš„é˜ˆå€¼
                min_coverage_months=1, # å·²ä¼˜åŒ–çš„é˜ˆå€¼
                min_stability_ratio=0.5,
                min_calibration_r2=0.6,
                max_correlation_median=0.7
            )
            config = ValidationConfig()
            self.production_validator = ProductionReadinessValidator(config, thresholds)
            logger.info("ç”Ÿäº§å°±ç»ªéªŒè¯å™¨åˆå§‹åŒ–æˆåŠŸ(ç»å¯¹å¯¼å…¥)")
            
        except Exception as e:
            logger.warning(f"ç”Ÿäº§å°±ç»ªéªŒè¯å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.production_validator = None
    
    def _prepare_data_for_advanced_alpha(self, stock_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """å‡†å¤‡é«˜çº§Alphaç³»ç»Ÿæ‰€éœ€çš„æ•°æ®æ ¼å¼"""
        try:
            # æ”¶é›†æ‰€æœ‰è‚¡ç¥¨æ•°æ®
            all_data = []
            
            for ticker, data in stock_data.items():
                if data.empty:
                    continue
                    
                # å‡†å¤‡å¿…è¦çš„åˆ—
                prepared = pd.DataFrame()
                
                # ä»·æ ¼å’Œæˆäº¤é‡æ•°æ®
                if 'close' in data.columns:
                    prepared['close'] = data['close']
                    prepared['price'] = data['close']
                
                if 'high' in data.columns:
                    prepared['high'] = data['high']
                    
                if 'low' in data.columns:
                    prepared['low'] = data['low']
                    
                if 'volume' in data.columns:
                    prepared['volume'] = data['volume']
                
                # è®¡ç®—å¸‚å€¼ï¼ˆç®€åŒ–ï¼‰
                if 'close' in data.columns and 'volume' in data.columns:
                    prepared['market_cap'] = data['close'] * data['volume'] * 1000  # ç²—ç•¥ä¼°ç®—
                
                # ğŸ”¥ CRITICAL: ä»…ä½¿ç”¨Polygon APIæ•°æ® - ç»å¯¹æ— æ¨¡æ‹Ÿæ•°æ®
                if 'close' in data.columns:
                    try:
                        from polygon_only_data_provider import PolygonOnlyDataProvider
                        
                        # åˆå§‹åŒ–Polygonæ•°æ®æä¾›å™¨
                        if not hasattr(self, 'polygon_provider'):
                            self.polygon_provider = PolygonOnlyDataProvider()
                        
                        # ä»Polygonè·å–çœŸå®åŸºæœ¬é¢æ•°æ®
                        fund_df = self.polygon_provider.get_fundamentals(ticker, limit=1)
                        
                        if fund_df is not None and not fund_df.empty:
                            # ä½¿ç”¨PolygonçœŸå®æ•°æ®
                            latest = fund_df.iloc[0]
                            
                            # è®¡ç®—book to market (å¦‚æœæœ‰æ•°æ®)
                            if latest.get('book_value_per_share') and data['close'].iloc[-1] > 0:
                                prepared['book_to_market'] = latest['book_value_per_share'] / data['close'].iloc[-1]
                            else:
                                prepared['book_to_market'] = np.nan
                                
                            prepared['roe'] = latest.get('roe', np.nan)
                            prepared['debt_to_equity'] = latest.get('debt_to_equity', np.nan)
                            prepared['earnings'] = latest.get('earnings_per_share', np.nan)
                            prepared['pe_ratio'] = latest.get('pe_ratio', np.nan)
                            
                            logger.info(f"Using Polygon API fundamental data for {ticker}")
                        else:
                            # å¦‚æœPolygonæ— æ•°æ®ï¼Œä½¿ç”¨NaN - ç»ä¸ç”Ÿæˆå‡æ•°æ®
                            logger.warning(f"No Polygon fundamental data for {ticker}, using NaN")
                            prepared['book_to_market'] = np.nan
                            prepared['roe'] = np.nan
                            prepared['debt_to_equity'] = np.nan
                            prepared['earnings'] = np.nan
                            prepared['pe_ratio'] = np.nan
                            
                    except Exception as e:
                        logger.warning(f"Failed to get fundamental data for {ticker}: {e}")
                        # å¤±è´¥æ—¶ä½¿ç”¨NaNï¼Œä¸ä½¿ç”¨éšæœºæ•°
                        prepared['book_to_market'] = np.nan
                        prepared['roe'] = np.nan
                        prepared['debt_to_equity'] = np.nan
                        prepared['earnings'] = np.nan
                    
                    # è®¡ç®—èµ„äº§å¢é•¿ï¼ˆåŸºäºä»·æ ¼æ•°æ®ï¼‰
                    prepared['asset_growth'] = data['close'].pct_change().rolling(20).mean()
                    
                    # è®¡ç®—æ”¶ç›Šç‡
                    prepared['returns'] = data['close'].pct_change()
                    prepared['returns_1m'] = data['close'].pct_change(22)
                    prepared['returns_12m'] = data['close'].pct_change(252)
                    
                    # æˆé•¿æŒ‡æ ‡
                    prepared['earnings_growth'] = prepared['earnings'].pct_change(252)
                    prepared['sales_growth'] = prepared['volume'].pct_change(252) if 'volume' in data.columns else 0
                
                prepared['ticker'] = ticker
                prepared['date'] = data.index
                
                all_data.append(prepared)
            
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            if all_data:
                combined = pd.concat(all_data, axis=0, ignore_index=True)
                # æŒ‰æ—¥æœŸæ’åº
                combined = combined.sort_values('date')
                return combined
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.warning(f"å‡†å¤‡é«˜çº§Alphaæ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _init_advanced_alpha_system(self):
        """â­ åˆå§‹åŒ–é«˜çº§Alphaç³»ç»Ÿï¼ˆä¸“ä¸šæœºæ„çº§åŠŸèƒ½ï¼‰"""
        try:
            logger.info("åˆå§‹åŒ–é«˜çº§Alphaç³»ç»Ÿï¼ˆä¸“ä¸šæœºæ„çº§ï¼‰")
            
            # å¯¼å…¥é«˜çº§Alphaç³»ç»Ÿ
            from advanced_alpha_system_integrated import AdvancedAlphaSystem
            
            self.advanced_alpha_system = AdvancedAlphaSystem()
            
            logger.info("âœ… é«˜çº§Alphaç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ:")
            logger.info("  - Fama-French & Barraå› å­åº“")
            logger.info("  - å› å­è¡°å‡æœºåˆ¶ï¼ˆåŠ¨æ€åŠè¡°æœŸï¼‰")
            logger.info("  - MLä¼˜åŒ–ICæƒé‡ï¼ˆLightGBMé›†æˆï¼‰")
            logger.info("  - å› å­æ­£äº¤åŒ–ï¼ˆå‰”é™¤å…±çº¿æ€§ï¼‰")
            logger.info("  - å®æ—¶æ€§èƒ½ç›‘æ§ç³»ç»Ÿ")
            
            # é…ç½®ç›‘æ§è­¦æŠ¥å›è°ƒ
            def alert_callback(alert):
                level = alert.get('level', 'INFO')
                message = alert.get('message', '')
                if level == 'CRITICAL':
                    logger.critical(f"Alphaç³»ç»Ÿè­¦æŠ¥: {message}")
                else:
                    logger.warning(f"Alphaç³»ç»Ÿè­¦æŠ¥: {message}")
            
            self.advanced_alpha_system.performance_monitor.register_alert_callback(alert_callback)
            
        except ImportError as e:
            logger.warning(f"é«˜çº§Alphaç³»ç»Ÿå¯¼å…¥å¤±è´¥: {e}, ä½¿ç”¨åŸºç¡€Alphaå¤„ç†")
            self.advanced_alpha_system = None
        except Exception as e:
            logger.error(f"é«˜çº§Alphaç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            self.advanced_alpha_system = None
    
    def _init_enhanced_features_v5(self):
        """ğŸ”¥ V5æ–°å¢ï¼šåˆå§‹åŒ–ç«‹ç«¿è§å½±å¢å¼ºåŠŸèƒ½"""
        logger.info("åˆå§‹åŒ–BMA V5ç«‹ç«¿è§å½±å¢å¼ºåŠŸèƒ½")
        
        # 1. æ’åºå¢å¼ºé…ç½®
        self.ranking_config = {
            'use_lightgbm_ranker': True,      # å¯ç”¨LightGBM Ranker
            'ranking_objective': 'lambdarank', # æ’åºç›®æ ‡
            'daily_grouping': True,           # æŒ‰æ—¥åˆ†ç»„
            'ndcg_k': 10,                     # NDCG@Kè¯„ä¼°
            'n_estimators': 200,              # æ ‘çš„æ•°é‡
            'learning_rate': 0.05,            # å­¦ä¹ ç‡
            'num_leaves': 63,                 # å¶å­æ•°
            'feature_fraction': 0.8,          # ç‰¹å¾é‡‡æ ·
            'bagging_fraction': 0.8,          # æ ·æœ¬é‡‡æ ·
            'early_stopping_rounds': 50      # æ—©åœ
        }
        
        # 2. ä¸¥æ ¼Purged CVé…ç½®
        self.purged_cv_config = {
            'strict_embargo': True,           # ä¸¥æ ¼ç¦è¿
            'embargo_align_target': True,     # ç¦è¿ä¸ç›®æ ‡è·¨åº¦å¯¹é½ï¼ˆT+10ï¼‰
            'validate_integrity': True,      # éªŒè¯åˆ‡åˆ†å®Œæ•´æ€§
            'embargo_days': 10,              # ä¸T+10æ ‡ç­¾å¯¹é½
            'gap_days': 10,                  # é¢å¤–gapé˜²æ­¢æ³„éœ²
            'min_train_ratio': 0.6,          # æœ€å°è®­ç»ƒé›†æ¯”ä¾‹
            'enable_group_constraints': True  # å¯ç”¨ç»„çº¦æŸ
        }
        
        # 3. Isotonicæ ¡å‡†é…ç½®
        self.calibration_config = {
            'use_isotonic': True,            # å¯ç”¨Isotonicæ ¡å‡†
            'out_of_bounds': 'clip',         # è¾¹ç•Œå¤„ç†
            'calibration_window': 252,       # æ ¡å‡†çª—å£ï¼ˆ1å¹´ï¼‰
            'y_min': None,                   # è‡ªåŠ¨ç¡®å®š
            'y_max': None,                   # è‡ªåŠ¨ç¡®å®š
            'increasing': True               # å•è°ƒé€’å¢
        }
        
        # 4. æ—¶é—´è¡°å‡å’Œå”¯ä¸€åº¦æƒé‡é…ç½®
        self.weighting_config = {
            'time_decay_enabled': True,      # æ—¶é—´è¡°å‡æƒé‡
            'uniqueness_weighting': True,    # å”¯ä¸€åº¦æƒé‡
            'half_life_days': 120,          # åŠè¡°æœŸï¼ˆ4ä¸ªæœˆï¼Œé€‚é…T+10ï¼‰
            'holding_period': 10,           # æŒæœ‰æœŸï¼ˆå¯¹åº”T+10ï¼‰
            'max_weight_ratio': 5.0,        # æœ€å¤§æƒé‡æ¯”ç‡
            'min_weight_threshold': 0.1     # æœ€å°æƒé‡é˜ˆå€¼
        }
        
        # 5. æ€§èƒ½è¯„ä¼°é…ç½®
        self.evaluation_config = {
            'cross_sectional_metrics': ['ic', 'rank_ic', 'ndcg'],
            'temporal_metrics': ['alpha_decay', 'turnover'],
            'calibration_metrics': ['calibration_slope', 'hit_ratio'],
            'enable_bootstrap': True,
            'bootstrap_n': 1000,
            'confidence_level': 0.95
        }
        
        # åˆå§‹åŒ–æ ¡å‡†å™¨å­˜å‚¨
        self.isotonic_calibrators = {}
        
        # åˆå§‹åŒ–æ€§èƒ½è¿½è¸ª
        self.v5_performance_tracker = {
            'ranking_performance': [],
            'calibration_quality': [],
            'weight_effectiveness': [],
            'cv_integrity_checks': []
        }
        
        logger.info("âœ… BMA V5ç«‹ç«¿è§å½±å¢å¼ºåŠŸèƒ½åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   - LightGBM Ranker: {self.ranking_config['use_lightgbm_ranker']}")
        logger.info(f"   - ä¸¥æ ¼CV: gap={self.purged_cv_config['gap_days']}å¤©, embargo={self.purged_cv_config['embargo_days']}å¤©")
        logger.info(f"   - Isotonicæ ¡å‡†: {self.calibration_config['use_isotonic']}")
        logger.info(f"   - æ—¶é—´è¡°å‡æƒé‡: åŠè¡°æœŸ={self.weighting_config['half_life_days']}å¤©")
    
    def _init_enhanced_cv_logger(self):
        """åˆå§‹åŒ–å¢å¼ºCVæ—¥å¿—è®°å½•å™¨"""
        try:
            # ğŸ”§ ä¿®å¤ç›¸å¯¹å¯¼å…¥é—®é¢˜ - ä½¿ç”¨ç»å¯¹å¯¼å…¥
            from enhanced_cv_logging import EnhancedCVLogger
            self.cv_logger = EnhancedCVLogger()
            logger.info("å¢å¼ºCVæ—¥å¿—è®°å½•å™¨åˆå§‹åŒ–æˆåŠŸ(ç»å¯¹å¯¼å…¥)")
            
        except Exception as e:
            logger.warning(f"å¢å¼ºCVæ—¥å¿—è®°å½•å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.cv_logger = None
    
    def _init_regime_detection_system(self):
        """åˆå§‹åŒ–Regime Detectionç³»ç»Ÿ"""
        try:
            global REGIME_DETECTION_AVAILABLE
            if not REGIME_DETECTION_AVAILABLE:
                logger.warning("Regime Detectionæ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡åˆå§‹åŒ–")
                return
            
            # æ£€æŸ¥é…ç½®ä¸­æ˜¯å¦å¯ç”¨Regime Detection
            regime_enabled = self.config.get('model_config', {}).get('regime_detection', False)
            
            if regime_enabled:
                # åˆ›å»ºBæ–¹æ¡ˆGMMçŠ¶æ€æ£€æµ‹é…ç½®
                regime_config = RegimeConfig(
                    n_regimes=3,
                    lookback_window=252,            # 1å¹´è®­ç»ƒçª—å£
                    update_frequency=63,            # å­£åº¦æ›´æ–°
                    prob_smooth_window=7,           # 7æ—¥æ—¶é—´å¹³æ»‘
                    hard_threshold=0.6,             # ç¡¬è·¯ç”±é˜ˆå€¼
                    min_regime_samples=50,          # æœ€å°æ ·æœ¬æ•°
                    enable_pca=False,               # å…³é—­PCAç®€åŒ–
                    robust_window=252               # Robustæ ‡å‡†åŒ–çª—å£
                )
                
                # åˆ›å»ºè®­ç»ƒé…ç½®  
                training_config = RegimeTrainingConfig(
                    enable_regime_aware=True,
                    regime_config=regime_config,
                    regime_training_strategy='separate',
                    min_samples_per_regime=100,
                    regime_prediction_mode='adaptive',
                    parallel_regime_training=True,
                    regime_feature_selection=True
                )
                
                # åˆ›å»ºçŠ¶æ€æ„ŸçŸ¥CVé…ç½®
                regime_cv_config = RegimeAwareCVConfig(
                    n_splits=5,
                    test_size=63,
                    gap=10,
                    embargo=5,
                    min_train_size=252,
                    enable_regime_stratification=True,
                    min_regime_samples=50,
                    regime_balance_threshold=0.8,
                    cross_regime_validation=True,
                    strict_regime_temporal_order=True,
                    regime_transition_buffer=10
                )
                
                # åˆå§‹åŒ–ç»„ä»¶
                self.regime_detector = MarketRegimeDetector(regime_config)
                self.regime_trainer = RegimeAwareTrainer(training_config)
                self.regime_cv = RegimeAwareTimeSeriesCV(regime_cv_config, self.regime_detector)
                
                logger.info("âœ… Regime Detectionç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
                logger.info(f"   - çŠ¶æ€æ•°é‡: {regime_config.n_regimes}")
                logger.info(f"   - è®­ç»ƒç­–ç•¥: {training_config.regime_training_strategy}")
                logger.info(f"   - é¢„æµ‹æ¨¡å¼: {training_config.regime_prediction_mode}")
                logger.info(f"   - CVçŠ¶æ€åˆ†å±‚: {regime_cv_config.enable_regime_stratification}")
                
                # è®¾ç½®å…¨å±€æ ‡è®°
                self._regime_cv_enabled = True
                
            else:
                logger.info("Regime Detectionåœ¨é…ç½®ä¸­æœªå¯ç”¨")
                
        except Exception as e:
            logger.warning(f"Regime Detectionç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            self.regime_detector = None
            self.regime_trainer = None
    
    def _init_alpha_engine(self):
        """åˆå§‹åŒ–Alphaå¼•æ“ - æ ¸å¿ƒç»„ä»¶ï¼Œå¿…é¡»æˆåŠŸ"""
        # æ ¸å¿ƒå¼•æ“ - ä¸¥æ ¼è¦æ±‚ï¼Œä¸å…è®¸ä½¿ç”¨Mock
        if ENHANCED_MODULES_AVAILABLE:
            try:
                # ğŸ”¥ åˆå§‹åŒ–Alphaå¼•æ“
                # è§£æé…ç½®æ–‡ä»¶è·¯å¾„ï¼Œç¡®ä¿ä»æ­£ç¡®ä½ç½®åŠ è½½
                if not os.path.isabs(self.config_path):
                    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œé¦–å…ˆå°è¯•ä»é¡¹ç›®æ ¹ç›®å½•æŸ¥æ‰¾
                    root_config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), self.config_path)
                    if os.path.exists(root_config_path):
                        resolved_config_path = root_config_path
                    else:
                        # å›é€€åˆ°å½“å‰ç›®å½•
                        resolved_config_path = self.config_path
                else:
                    resolved_config_path = self.config_path
                
                logger.info(f"å°è¯•åŠ è½½Alphaé…ç½®æ–‡ä»¶: {resolved_config_path}")
                self.alpha_engine = AlphaStrategiesEngine(resolved_config_path)
                
                # ğŸš¨ ä¸¥æ ¼éªŒè¯ï¼šç¡®ä¿ä¸æ˜¯Mockå¯¹è±¡
                if hasattr(self.alpha_engine, '__class__') and 'Mock' in str(self.alpha_engine.__class__):
                    raise ValueError(
                        "âŒ æ£€æµ‹åˆ°Mock AlphaStrategiesEngineï¼\n"
                        "çœŸæ­£çš„æœºå™¨å­¦ä¹ éœ€è¦å®é™…çš„Alphaå¼•æ“\n" 
                        "è¯·æ£€æŸ¥enhanced_alpha_strategies.pyæ˜¯å¦æ­£ç¡®å¯¼å…¥"
                    )
                
                # éªŒè¯Alphaå¼•æ“çš„åŠŸèƒ½å®Œæ•´æ€§
                required_methods = ['compute_all_alphas', 'alpha_functions']
                missing_methods = [method for method in required_methods 
                                 if not hasattr(self.alpha_engine, method)]
                if missing_methods:
                    raise ValueError(f"âŒ Alphaå¼•æ“ç¼ºå°‘å¿…è¦æ–¹æ³•: {missing_methods}")
                
                logger.info(f"âœ… Alphaå¼•æ“åˆå§‹åŒ–æˆåŠŸ: {len(self.alpha_engine.alpha_functions)} ä¸ªå› å­å‡½æ•°")
                
                # åˆå§‹åŒ–LTRï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if LTR_AVAILABLE:
                    self.ltr_bma = LearningToRankBMA(
                        ranking_objective=self.config.get('model_config', {}).get('ranking_objective', 'rank:pairwise'),
                        temperature=self.config.get('temperature', 1.2),
                        enable_regime_detection=self.config.get('model_config', {}).get('regime_detection', True)
                    )
                    logger.info("âœ… LTR BMAåˆå§‹åŒ–æˆåŠŸ")
                else:
                    self.ltr_bma = None
                    logger.warning("âš ï¸ LTRæ¨¡å—ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–é¢„æµ‹æ¨¡å¼")
                    
                # åˆå§‹åŒ–æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if PORTFOLIO_OPTIMIZER_AVAILABLE:
                    self.portfolio_optimizer = AdvancedPortfolioOptimizer(
                        risk_aversion=self.config.get('risk_config', {}).get('risk_aversion', 5.0),
                        turnover_penalty=self.config.get('risk_config', {}).get('turnover_penalty', 1.0),
                        max_turnover=self.config.get('max_turnover', 0.10),
                        max_position=self.config.get('max_position', 0.03),
                        max_sector_exposure=self.config.get('risk_config', {}).get('max_sector_exposure', 0.15),
                        max_country_exposure=self.config.get('risk_config', {}).get('max_country_exposure', 0.20)
                    )
                    logger.info("âœ… æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨åˆå§‹åŒ–æˆåŠŸ")
                else:
                    self.portfolio_optimizer = None
                    logger.warning("âš ï¸ æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–ä¼˜åŒ–æ–¹æ³•")
                
            except Exception as e:
                error_msg = f"âŒ Alphaå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}"
                logger.error(error_msg)
                # ğŸš¨ æ ¹æ®ç”¨æˆ·è¦æ±‚ï¼šä¸å…è®¸å›é€€ï¼Œç›´æ¥æŠ¥é”™
                raise ValueError(f"Alphaå¼•æ“åˆå§‹åŒ–å¤±è´¥ï¼Œå¿…é¡»ä¿®å¤: {error_msg}") from e
                
        else:
            # ğŸš¨ å¢å¼ºæ¨¡å—ä¸å¯ç”¨æ˜¯ä¸¥é‡é”™è¯¯ï¼Œä¸å…è®¸é™çº§
            error_msg = (
                "âŒ å¢å¼ºæ¨¡å—ä¸å¯ç”¨ï¼è¿™ä¼šå¯¼è‡´Mockå¯¹è±¡è¢«ä½¿ç”¨\n"
                "è§£å†³æ–¹æ¡ˆï¼š\n"
                "1. æ£€æŸ¥enhanced_alpha_strategies.pyæ–‡ä»¶\n"
                "2. ç¡®ä¿æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…\n"
                "3. ä¿®å¤å¯¼å…¥é”™è¯¯\n"
                "4. ä¸å…è®¸ä½¿ç”¨Mockå¯¹è±¡è¿›è¡ŒAlphaå› å­é¢„æµ‹"
            )
            logger.warning(error_msg)
            logger.warning("å°†ä½¿ç”¨Mock Alphaå¼•æ“ç»§ç»­åˆå§‹åŒ–")
            
            # åˆ›å»ºMock Alphaå¼•æ“ä»¥é¿å…ç³»ç»Ÿå´©æºƒ
            class MockAlphaEngine:
                def __init__(self):
                    self.alpha_functions = {}
                    
                def compute_all_alphas(self, data):
                    # è¿”å›ç©ºçš„Alphaç»“æœ
                    return data.copy()
            
            self.alpha_engine = MockAlphaEngine()
            logger.info("Mock Alphaå¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def _init_unified_feature_pipeline(self):
        """åˆå§‹åŒ–ç»Ÿä¸€ç‰¹å¾ç®¡é“"""
        try:
            logger.info("å¼€å§‹åˆå§‹åŒ–ç»Ÿä¸€ç‰¹å¾ç®¡é“...")
            from unified_feature_pipeline import UnifiedFeaturePipeline, FeaturePipelineConfig
            logger.info("ç»Ÿä¸€ç‰¹å¾ç®¡é“æ¨¡å—å¯¼å…¥æˆåŠŸ")
            
            config = FeaturePipelineConfig(
                enable_alpha_summary=True,
                enable_pca=True,
                enable_scaling=True,
                save_pipeline=True
            )
            logger.info("ç‰¹å¾ç®¡é“é…ç½®åˆ›å»ºæˆåŠŸ")
            
            self.feature_pipeline = UnifiedFeaturePipeline(config)
            logger.info("ç»Ÿä¸€ç‰¹å¾ç®¡é“å®ä¾‹åˆ›å»ºæˆåŠŸ")
            logger.info("ç»Ÿä¸€ç‰¹å¾ç®¡é“åˆå§‹åŒ–æˆåŠŸ - å°†ç¡®ä¿è®­ç»ƒ-é¢„æµ‹ç‰¹å¾ä¸€è‡´æ€§")
        except Exception as e:
            logger.error(f"ç»Ÿä¸€ç‰¹å¾ç®¡é“åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            self.feature_pipeline = None
    
    def _init_alpha_summary_processor(self):
        """åˆå§‹åŒ–Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨ï¼ˆRoute Aé›†æˆï¼‰"""
        try:
            from alpha_summary_features import create_alpha_summary_processor, AlphaSummaryConfig
            
            # åˆ›å»ºAlphaæ‘˜è¦ç‰¹å¾é…ç½®
            alpha_config = AlphaSummaryConfig(
                max_alpha_features=18,  # 11ä¸ªPCA + 6ä¸ªæ‘˜è¦ + 1ä¸ªç­–ç•¥ä¿¡å·
                include_alpha_strategy_signal=True,  # åŒ…å«Alphaç­–ç•¥ç»¼åˆä¿¡å·
                pca_variance_explained=0.85,
                pls_n_components=8
            )
            
            # åˆ›å»ºå¤„ç†å™¨
            self.alpha_summary_processor = create_alpha_summary_processor(alpha_config.__dict__)
            logger.info("Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸï¼ˆåŒ…å«18ä¸ªç‰¹å¾ï¼š11PCA+6æ‘˜è¦+1ç­–ç•¥ä¿¡å·ï¼‰")
            
        except ImportError as e:
            logger.warning(f"Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨å¯¼å…¥å¤±è´¥: {e}")
            self.alpha_summary_processor = None
        except Exception as e:
            logger.warning(f"Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.alpha_summary_processor = None
        
        # ğŸ”¥ ç”Ÿäº§çº§åŠŸèƒ½ï¼šæ¨¡å‹ç‰ˆæœ¬æ§åˆ¶
        try:
            from model_version_control import ModelVersionControl
            self.version_control = ModelVersionControl("ultra_models")
            logger.info("æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿå·²å¯ç”¨")
        except ImportError as e:
            logger.warning(f"ç‰ˆæœ¬æ§åˆ¶æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
            self.version_control = None
        
        
        
        # ä¼ ç»ŸMLæ¨¡å‹ï¼ˆä½œä¸ºå¯¹æ¯”ï¼‰
        self.traditional_models = {}
        self.model_weights = {}
        
        # Professionalå¼•æ“åŠŸèƒ½
        self.risk_model_results = {}
        self.current_regime = None
        self.regime_weights = {}
        self.market_data_manager = UnifiedMarketDataManager() if MARKET_MANAGER_AVAILABLE else None
        
        # æ•°æ®å’Œç»“æœå­˜å‚¨
        self.raw_data = {}
        self.feature_data = None
        self.alpha_signals = None
        self.final_predictions = None
        self.portfolio_weights = None
        
        # æ€§èƒ½è·Ÿè¸ª
        self.performance_metrics = {}
        self.backtesting_results = {}
        
        # å¥åº·ç›‘æ§è®¡æ•°å™¨
        self.health_metrics = {
            'universe_load_fallbacks': 0,
            'risk_model_failures': 0,
            'optimization_fallbacks': 0,
            'alpha_computation_failures': 0,
            'neutralization_failures': 0,
            'prediction_failures': 0,
            'total_exceptions': 0
        }
        
        logger.info("DEBUG: å·²åˆ°è¾¾__init__æ–¹æ³•çš„æœ€åéƒ¨åˆ†ï¼Œå‡†å¤‡åˆå§‹åŒ–Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨")
        
        # ğŸ¯ åˆå§‹åŒ–Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨ï¼ˆRoute Aé›†æˆï¼‰
        logger.info("æ­£åœ¨åˆå§‹åŒ–Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨...")
        self._init_alpha_summary_processor()
        logger.info(f"Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼ŒçŠ¶æ€: {hasattr(self, 'alpha_summary_processor') and self.alpha_summary_processor is not None}")
        
        logger.info("DEBUG: __init__æ–¹æ³•å³å°†å®Œæˆ")
    
    def _init_optimization_components(self):
        """åˆå§‹åŒ–å†…å­˜ä¼˜åŒ–ç»„ä»¶"""
        try:
            # å¯¼å…¥ä¼˜åŒ–æ¨¡å—ï¼ˆæ·»åŠ é”™è¯¯å¤„ç†ï¼‰
            try:
                # ä½¿ç”¨ç®€åŒ–çš„å†…å­˜ç®¡ç†
                import gc
                from training_progress_monitor import TrainingProgressMonitor
                from model_cache_optimizer import ModelCacheOptimizer
                from memory_optimized_trainer import MemoryOptimizedTrainer
                from encoding_fix import apply_encoding_fixes
                
                # åº”ç”¨ç¼–ç ä¿®å¤
                apply_encoding_fixes()
                optimization_available = True
            except ImportError as e:
                logger.warning(f"ä¼˜åŒ–æ¨¡å—å¯¼å…¥å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸºç¡€åŠŸèƒ½")
                optimization_available = False
            
            # åˆå§‹åŒ–ä¼˜åŒ–ç»„ä»¶ï¼ˆä»…åœ¨å¯¼å…¥æˆåŠŸæ—¶ï¼‰
            if optimization_available:
                # åˆ›å»ºç®€åŒ–çš„å†…å­˜ç®¡ç†å™¨
                self.memory_manager = MemoryOptimizedTrainer(
                    batch_size=400,
                    memory_limit_gb=3.0,
                    enable_gc_aggressive=True
                )
                logger.info("å†…å­˜ä¼˜åŒ–è®­ç»ƒå™¨åˆå§‹åŒ–æˆåŠŸ")
                
                # å…¶ä»–ç»„ä»¶ä½¿ç”¨æ›´ä¿å®ˆçš„é”™è¯¯å¤„ç†
                try:
                    from streaming_data_loader import StreamingDataLoader
                    self.streaming_loader = StreamingDataLoader(
                        chunk_size=200,
                        cache_dir="cache/bma_ultra",
                        memory_limit_mb=1024
                    )
                except ImportError:
                    logger.warning("StreamingDataLoaderä¸å¯ç”¨ï¼Œè·³è¿‡")
                    self.streaming_loader = None
                
                try:
                    self.progress_monitor = TrainingProgressMonitor(
                        save_dir="logs/bma_progress"
                    )
                except ImportError:
                    logger.warning("TrainingProgressMonitorä¸å¯ç”¨ï¼Œè·³è¿‡")
                    self.progress_monitor = None
                
                try:
                    self.model_cache = ModelCacheOptimizer(
                        cache_dir="cache/bma_models",
                        max_cache_size_gb=1.0
                    )
                except ImportError:
                    logger.warning("ModelCacheOptimizerä¸å¯ç”¨ï¼Œè·³è¿‡")
                    self.model_cache = None
                
                try:
                    self.batch_trainer = MemoryOptimizedTrainer(
                        batch_size=250,  # å‡å°‘æ‰¹æ¬¡å¤§å°ï¼Œæé«˜ç¨³å®šæ€§
                        memory_limit_gb=4.0,  # å¢åŠ å†…å­˜é™åˆ¶
                        force_retrain=True,  # å¼ºåˆ¶é‡æ–°è®­ç»ƒï¼Œç¡®ä¿å®é™…æ‰§è¡Œè®­ç»ƒè¿‡ç¨‹
                        enable_gc_aggressive=True  # å¯ç”¨æ¿€è¿›åƒåœ¾å›æ”¶
                    )
                except ImportError:
                    logger.warning("æ‰¹é‡è®­ç»ƒå™¨ä¸å¯ç”¨ï¼Œè·³è¿‡")
                    self.batch_trainer = None
            else:
                # åˆ›å»ºåŸºç¡€çš„æ›¿ä»£ç»„ä»¶
                self.memory_manager = None
                self.streaming_loader = None
                self.progress_monitor = self._create_basic_progress_monitor()
                self.model_cache = None
                self.batch_trainer = None
            
            # ä½¿ç”¨å…¨å±€è®­ç»ƒæ¨¡å¼
            self.batch_size = 250  
            self.memory_optimized = False  # ç®€åŒ–ï¼šä¸ä½¿ç”¨å†…å­˜ä¼˜åŒ–
            
            logger.info("å†…å­˜ä¼˜åŒ–ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
            
        except ImportError as e:
            logger.warning(f"ä¼˜åŒ–ç»„ä»¶å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨å…¨å±€è®­ç»ƒæ¨¡å¼: {e}")
            self.memory_optimized = False  # ç®€åŒ–ï¼šä¸ä½¿ç”¨å†…å­˜ä¼˜åŒ–
            self.batch_size = 150  # é€‚ä¸­çš„æ‰¹æ¬¡å¤§å°ï¼Œå…¼é¡¾æ€§èƒ½å’Œå†…å­˜
            # åˆå§‹åŒ–åŸºç¡€ç»„ä»¶
            self.memory_manager = None
            self.streaming_loader = None
            self.progress_monitor = self._create_basic_progress_monitor()
            self.model_cache = None
            self.batch_trainer = None
    
    
    def _run_optimized_analysis(self, tickers: List[str], start_date: str, end_date: str, 
                               top_n: int, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå†…å­˜ä¼˜åŒ–ç‰ˆåˆ†æï¼ˆç¡®ä¿ç»“æœå‡†ç¡®æ€§ï¼‰"""
        logger.info(f"å¯åŠ¨å†…å­˜ä¼˜åŒ–åˆ†æ: {len(tickers)} è‚¡ç¥¨ï¼Œæ‰¹æ¬¡å¤§å° {self.batch_size}")
        
        try:
            # å¯åŠ¨è¿›åº¦ç›‘æ§
            self.progress_monitor.add_stage("å…¨å±€ç‰¹å¾åˆ†æ", 1)
            self.progress_monitor.add_stage("æ•°æ®ä¸‹è½½", len(tickers))
            self.progress_monitor.add_stage("ç‰¹å¾å·¥ç¨‹", len(tickers))
            self.progress_monitor.add_stage("æ¨¡å‹è®­ç»ƒ", len(tickers))
            self.progress_monitor.add_stage("å…¨å±€æ ¡å‡†", 1)
            self.progress_monitor.add_stage("ç»“æœæ±‡æ€»", 1)
            self.progress_monitor.start_training()
            
            # ğŸ¯ ç¬¬ä¸€æ­¥ï¼šå…¨å±€ç‰¹å¾åˆ†æï¼ˆç¡®ä¿ä¸€è‡´æ€§ï¼‰
            self.progress_monitor.start_stage("å…¨å±€ç‰¹å¾åˆ†æ")
            global_stats = self._compute_global_feature_stats(tickers, start_date, end_date)
            self.progress_monitor.complete_stage("å…¨å±€ç‰¹å¾åˆ†æ", success=True)
            
            # ğŸ¯ ç¬¬äºŒæ­¥ï¼šåˆ†æ‰¹è®­ç»ƒï¼ˆä½¿ç”¨å…¨å±€ç»Ÿè®¡ï¼‰
            def batch_analysis_func(batch_tickers):
                return self._analyze_batch_optimized(batch_tickers, start_date, end_date, global_stats)
            
            if self.batch_trainer:
                results = self.batch_trainer.train_universe(
                    universe=tickers,
                    model_trainer_func=batch_analysis_func
                )
            else:
                # ç®€å•çš„æ‰¹æ¬¡å¤„ç†fallback
                logger.info("ä½¿ç”¨åŸºç¡€æ‰¹æ¬¡å¤„ç†æ¨¡å¼")
                results = self._basic_batch_processing(tickers, start_date, end_date, global_stats)
            
            # ğŸ¯ ç¬¬ä¸‰æ­¥ï¼šå…¨å±€æ ¡å‡†ï¼ˆæ¶ˆé™¤æ‰¹æ¬¡åå·®ï¼‰
            self.progress_monitor.start_stage("å…¨å±€æ ¡å‡†")
            calibrated_results = self._calibrate_batch_results(results, global_stats)
            self.progress_monitor.complete_stage("å…¨å±€æ ¡å‡†", success=True)
            
            # æ±‡æ€»ç»“æœ
            analysis_results.update({
                'success': True,
                'total_time': (datetime.now() - analysis_results['start_time']).total_seconds(),
                'predictions': calibrated_results.get('predictions', {}),
                'model_performance': calibrated_results.get('model_performance', {}),
                'feature_importance': calibrated_results.get('feature_importance', {}),
                'optimization_stats': {
                    'memory_usage': self.memory_manager.get_statistics() if self.memory_manager else {},
                    'cache_stats': self.model_cache.get_cache_statistics() if self.model_cache else {},
                    'training_stats': results.get('training_statistics', {})
                }
            })
            
            # ç”Ÿæˆæ¨èï¼ˆä½¿ç”¨æ ¡å‡†åçš„ç»“æœï¼‰
            recommendations = self._generate_recommendations_from_predictions(
                calibrated_results.get('predictions', {}), top_n
            )
            analysis_results['recommendations'] = recommendations
            
            # ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            result_file = f"result/bma_ultra_enhanced_optimized_{timestamp}.xlsx"
            self._save_optimized_results(analysis_results, result_file)
            analysis_results['result_file'] = result_file
            
            self.progress_monitor.complete_training(success=True)
            logger.info("ä¼˜åŒ–åˆ†æå®Œæˆ")
            
            return analysis_results
            
        except Exception as e:
            import traceback
            logger.error(f"ä¼˜åŒ–åˆ†æå¤±è´¥: {e}")
            logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            self.progress_monitor.complete_training(success=False)
            analysis_results.update({
                'success': False,
                'error': str(e),
                'total_time': (datetime.now() - analysis_results['start_time']).total_seconds()
            })
            return analysis_results
    
    def _compute_global_feature_stats(self, tickers: List[str], start_date: str, end_date: str) -> Dict[str, Any]:
        """è®¡ç®—å…¨å±€ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯ï¼ˆç¡®ä¿æ‰¹æ¬¡é—´ä¸€è‡´æ€§ï¼‰"""
        logger.info("è®¡ç®—å…¨å±€ç‰¹å¾ç»Ÿè®¡...")
        
        # é‡‡æ ·ç­–ç•¥ï¼šéšæœºé€‰æ‹©æ ·æœ¬è‚¡ç¥¨è®¡ç®—å…¨å±€ç»Ÿè®¡
        import random
        random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
        sample_size = min(150, len(tickers))  # é€‚ä¸­çš„é‡‡æ ·å¤§å°
        sample_tickers = random.sample(tickers, sample_size)
        
        all_features = []
        successful_samples = 0
        
        for ticker in sample_tickers:
            try:
                data = self._download_single_ticker(ticker, start_date, end_date)
                if data is not None and len(data) >= 30:
                    features = self._calculate_features_optimized(data, ticker)
                    if features is not None and not features.empty and len(features) > 10:
                        # åªä¿ç•™æ•°å€¼ç‰¹å¾
                        numeric_features = features.select_dtypes(include=[np.number])
                        if not numeric_features.empty:
                            all_features.append(numeric_features)
                            successful_samples += 1
                
                # é™åˆ¶é‡‡æ ·æ•°é‡ä»¥æ§åˆ¶å†…å­˜å’Œæ—¶é—´
                if successful_samples >= 80:
                    break
                    
            except Exception as e:
                logger.warning(f"å…¨å±€ç»Ÿè®¡é‡‡æ ·å¤±è´¥ {ticker}: {e}")
                continue
        
        if not all_features:
            logger.warning("å…¨å±€ç‰¹å¾ç»Ÿè®¡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            return {
                'feature_means': {'returns': 0.0, 'volatility': 0.02, 'rsi': 50.0, 'sma_ratio': 1.0},
                'feature_stds': {'returns': 0.02, 'volatility': 0.01, 'rsi': 15.0, 'sma_ratio': 0.1},
                'feature_names': ['returns', 'volatility', 'rsi', 'sma_ratio'],
                'sample_size': 0
            }
        
        try:
            # åˆå¹¶æ‰€æœ‰ç‰¹å¾æ•°æ®
            combined_features = pd.concat(all_features, ignore_index=True)
            
            # ä½¿ç”¨ç»Ÿä¸€çš„æ•°å€¼æ¸…ç†ç­–ç•¥
            combined_features = self.data_validator.clean_numeric_data(combined_features, "combined_features", strategy="smart")
            
            # è®¡ç®—å…¨å±€ç»Ÿè®¡ï¼ˆç¡®ä¿ç¨³å®šæ€§ï¼‰
            feature_means = combined_features.mean()
            feature_means = feature_means.fillna(0).to_dict()  # å‡å€¼ç”¨0å¡«å……
            
            feature_stds = combined_features.std()
            feature_stds = feature_stds.fillna(1).where(feature_stds > 1e-8, 1).to_dict()  # æ ‡å‡†å·®ç”¨1å¡«å……ï¼Œé¿å…é™¤é›¶
            
            # ç¡®ä¿æ ‡å‡†å·®ä¸ä¸º0
            for col in feature_stds:
                if feature_stds[col] <= 0:
                    feature_stds[col] = 1.0
                    
        except Exception as e:
            logger.error(f"å…¨å±€ç»Ÿè®¡è®¡ç®—å¤±è´¥: {e}")
            return {
                'feature_means': {'returns': 0.0, 'volatility': 0.02, 'rsi': 50.0, 'sma_ratio': 1.0},
                'feature_stds': {'returns': 0.02, 'volatility': 0.01, 'rsi': 15.0, 'sma_ratio': 0.1},
                'feature_names': ['returns', 'volatility', 'rsi', 'sma_ratio'],
                'sample_size': 0
            }
        
        # æ¸…ç†å†…å­˜
        del all_features, combined_features
        if self.memory_manager:
            self.memory_manager.force_garbage_collection()
        
        global_stats = {
            'feature_means': feature_means,
            'feature_stds': feature_stds,
            'feature_names': list(feature_means.keys()),
            'sample_size': successful_samples
        }
        
        logger.info(f"å…¨å±€ç»Ÿè®¡å®Œæˆ: {successful_samples} æ ·æœ¬, {len(feature_means)} ç‰¹å¾")
        return global_stats
    
    def _calibrate_batch_results(self, batch_results: Dict[str, Any], global_stats: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼ºçš„é¢„æµ‹æ ¡å‡†ï¼ŒåŒ…å«ç½®ä¿¡åŒºé—´å’Œç¨³å®šæ€§æ£€æŸ¥"""
        logger.info("å¼€å§‹å¢å¼ºé¢„æµ‹æ ¡å‡†...")
        
        predictions = batch_results.get('predictions', {})
        model_performance = batch_results.get('model_performance', {})
        
        if not predictions:
            return batch_results
        
        # å°†é¢„æµ‹å€¼è½¬æ¢ä¸ºDataFrameè¿›è¡Œç»Ÿè®¡
        pred_data = []
        for ticker, pred in predictions.items():
            confidence = model_performance.get(ticker, 0.5)
            pred_data.append({'ticker': ticker, 'raw_prediction': pred, 'confidence': confidence})
        
        pred_df = pd.DataFrame(pred_data)
        
        if len(pred_df) < 10:
            logger.warning("é¢„æµ‹æ•°é‡å¤ªå°‘ï¼Œè·³è¿‡æ ¡å‡†")
            return batch_results
        
        # å¢å¼ºæ ¡å‡†æ­¥éª¤
        logger.info("æ‰§è¡Œå¤šå±‚é¢„æµ‹æ ¡å‡†...")
        
        # 1. å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†
        q1, q99 = pred_df['raw_prediction'].quantile([0.01, 0.99])
        outliers = (pred_df['raw_prediction'] < q1) | (pred_df['raw_prediction'] > q99)
        if outliers.sum() > 0:
            logger.warning(f"å‘ç°{outliers.sum()}ä¸ªé¢„æµ‹å¼‚å¸¸å€¼ï¼Œè¿›è¡Œæˆªæ–­å¤„ç†")
            pred_df['raw_prediction'] = pred_df['raw_prediction'].clip(lower=q1, upper=q99)
        
        # 2. åŸºäºç½®ä¿¡åº¦çš„åŠ æƒæ ¡å‡†
        pred_df['confidence_weight'] = pred_df['confidence'].clip(0.1, 1.0)  # æœ€å°æƒé‡0.1
        
        # 3. ç¨³å¥çš„æ’åè®¡ç®—ï¼ˆä½¿ç”¨åŠ æƒæ’åï¼‰
        pred_df['weighted_score'] = pred_df['raw_prediction'] * pred_df['confidence_weight']
        pred_df['percentile_rank'] = pred_df['weighted_score'].rank(pct=True)
        
        # 4. å¤šé‡æ ¡å‡†æ–¹æ³•
        from scipy.stats import norm, rankdata
        
        # æ–¹æ³•1: æ ‡å‡†æ­£æ€æ˜ å°„
        pred_df['calibrated_normal'] = norm.ppf(
            pred_df['percentile_rank'].clip(0.005, 0.995)  # æ›´ä¿å®ˆçš„æå€¼å¤„ç†
        )
        
        # æ–¹æ³•2: åˆ†ä½æ•°å‡åŒ€åŒ–
        pred_df['calibrated_uniform'] = pred_df['percentile_rank']
        
        # æ–¹æ³•3: åŸºäºç½®ä¿¡åº¦çš„æ··åˆæ ¡å‡†
        high_conf_mask = pred_df['confidence'] > 0.7
        pred_df['calibrated_mixed'] = pred_df['calibrated_normal']
        pred_df.loc[~high_conf_mask, 'calibrated_mixed'] = (
            0.5 * pred_df.loc[~high_conf_mask, 'calibrated_normal'] + 
            0.5 * pred_df.loc[~high_conf_mask, 'calibrated_uniform']
        )
        
        # 5. æœ€ç»ˆæ ¡å‡†ç»“æœï¼ˆé€‰æ‹©æ··åˆæ–¹æ³•ï¼‰
        min_pred = pred_df['calibrated_mixed'].min()
        max_pred = pred_df['calibrated_mixed'].max()
        
        if max_pred > min_pred:
            pred_df['final_prediction'] = (
                (pred_df['calibrated_mixed'] - min_pred) / (max_pred - min_pred)
            )
        else:
            pred_df['final_prediction'] = 0.5
        
        # 6. è®¡ç®—æ ¡å‡†è´¨é‡æŒ‡æ ‡
        original_std = pred_df['raw_prediction'].std()
        calibrated_std = pred_df['final_prediction'].std()
        
        # æ›´æ–°ç»“æœ
        calibrated_predictions = dict(zip(pred_df['ticker'], pred_df['final_prediction']))
        
        calibrated_results = batch_results.copy()
        calibrated_results['predictions'] = calibrated_predictions
        calibrated_results['calibration_info'] = {
            'original_count': len(predictions),
            'calibrated_count': len(calibrated_predictions),
            'calibration_method': 'enhanced_multi_stage',
            'outliers_detected': int(outliers.sum()) if 'outliers' in locals() else 0,
            'high_confidence_count': int((pred_df['confidence'] > 0.7).sum()),
            'prediction_spread': {
                'original_std': float(original_std),
                'calibrated_std': float(calibrated_std),
                'spread_ratio': float(calibrated_std / original_std) if original_std > 0 else 1.0
            },
            'global_features_used': len(global_stats.get('feature_names', []))
        }
        
        logger.info(f"æ ¡å‡†å®Œæˆ: {len(calibrated_predictions)} ä¸ªé¢„æµ‹å€¼")
        return calibrated_results

    def _basic_batch_processing(self, tickers: List[str], start_date: str, end_date: str, 
                               global_stats: Dict[str, Any] = None) -> Dict[str, Any]:
        """åŸºç¡€æ‰¹æ¬¡å¤„ç†æ¨¡å¼ - æ ¹æ®é…ç½®å¯ç”¨"""
        if not self.memory_optimized:
            logger.warning(f"å†…å­˜ä¼˜åŒ–æœªå¯ç”¨ï¼Œé‡å®šå‘åˆ°å…¨å±€è®­ç»ƒæ¨¡å¼: {len(tickers)} è‚¡ç¥¨")
            # é‡å®šå‘åˆ°å…¨å±€è®­ç»ƒ
            return self._analyze_batch_optimized(tickers, start_date, end_date, global_stats)
        
        logger.info(f"å¯ç”¨æ‰¹å¤„ç†æ¨¡å¼: {len(tickers)} è‚¡ç¥¨")
        
        # å®ç°ç®€åŒ–çš„æ‰¹å¤„ç†é€»è¾‘
        batch_size = self.batch_size if hasattr(self, 'batch_size') else 50
        results = {'predictions': {}, 'success_rate': 0.0}
        
        for i in range(0, len(tickers), batch_size):
            batch = tickers[i:i+batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} è‚¡ç¥¨")
            batch_result = self._analyze_batch_optimized(batch, start_date, end_date, global_stats)
            results['predictions'].update(batch_result.get('predictions', {}))
        
        results['success_rate'] = len(results['predictions']) / len(tickers) if tickers else 0.0
        return results

    def _analyze_batch_optimized(self, batch_tickers: List[str], start_date: str, end_date: str, 
                                global_stats: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        ğŸ¤– é©å‘½æ€§æ”¹è¿›ï¼šæ‰¹å¤„ç†ä¹Ÿä½¿ç”¨å®Œæ•´çš„æœºå™¨å­¦ä¹ æµç¨‹
        ä¸å†ä½¿ç”¨ç®€åŒ–é¢„æµ‹ï¼Œè€Œæ˜¯ä¸ºæ¯ä¸ªæ‰¹æ¬¡è®­ç»ƒå®Œæ•´çš„MLæ¨¡å‹
        """
        logger.info(f"ğŸš€ å¯åŠ¨MLé©±åŠ¨çš„æ‰¹æ¬¡åˆ†æ: {len(batch_tickers)} è‚¡ç¥¨")
        
        try:
            # ğŸ”¥ ç¬¬1æ­¥ï¼šä¸ºå½“å‰æ‰¹æ¬¡ä¸‹è½½å’Œå‡†å¤‡æ•°æ®
            logger.info("ç¬¬1æ­¥: æ‰¹æ¬¡æ•°æ®æ”¶é›†å’Œç‰¹å¾å·¥ç¨‹")
            stock_data = {}
            feature_data_list = []
            
            for ticker in batch_tickers:
                try:
                    data = self._load_ticker_data_optimized(ticker, start_date, end_date)
                    if data is not None and len(data) >= 30:
                        stock_data[ticker] = data
                        
                        # è®¡ç®—ç‰¹å¾
                        features = self._calculate_features_optimized(data, ticker, global_stats)
                        if features is not None and not features.empty:
                            # æ·»åŠ tickeræ ‡è¯†ç”¨äºåç»­é¢„æµ‹
                            features['ticker'] = ticker
                            feature_data_list.append(features)
                            
                except Exception as e:
                    logger.debug(f"æ‰¹æ¬¡æ•°æ®å¤„ç†å¤±è´¥ {ticker}: {e}")
                    continue
            
            if not feature_data_list:
                logger.warning("æ‰¹æ¬¡ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè¿”å›ç©ºç»“æœ")
                return {'predictions': {}, 'model_performance': {}, 'feature_importance': {}}
            
            # åˆå¹¶ç‰¹å¾æ•°æ®
            combined_features = pd.concat(feature_data_list, ignore_index=True)
            logger.info(f"æ‰¹æ¬¡ç‰¹å¾æ•°æ®: {combined_features.shape[0]} æ ·æœ¬, {combined_features.shape[1]} ç‰¹å¾")
            
            # ğŸ”¥ ç¬¬2æ­¥ï¼šä¸ºå½“å‰æ‰¹æ¬¡è®­ç»ƒå®Œæ•´çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼
            logger.info("ç¬¬2æ­¥: ä¸ºå½“å‰æ‰¹æ¬¡è®­ç»ƒMLæ¨¡å‹")
            batch_training_results = self.train_enhanced_models(combined_features)
            
            # ğŸ”¥ ç¬¬3æ­¥ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆé¢„æµ‹
            logger.info("ç¬¬3æ­¥: ä½¿ç”¨è®­ç»ƒå¥½çš„MLæ¨¡å‹ç”Ÿæˆé¢„æµ‹")
            batch_predictions = {}
            batch_performance = {}
            batch_importance = {}
            
            # å¯¹æ¯åªè‚¡ç¥¨ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
            for ticker in batch_tickers:
                if ticker not in stock_data:
                    continue
                    
                try:
                    # è·å–è¯¥è‚¡ç¥¨çš„ç‰¹å¾
                    ticker_features = combined_features[combined_features['ticker'] == ticker]
                    if ticker_features.empty:
                        continue
                    
                    # ç§»é™¤tickeråˆ—ï¼Œåªä¿ç•™æ•°å€¼ç‰¹å¾
                    ticker_features = ticker_features.drop('ticker', axis=1, errors='ignore')
                    
                    # ğŸ¯ ä½¿ç”¨æˆ‘ä»¬æ–°çš„MLé¢„æµ‹å‡½æ•°ï¼
                    # ä¸´æ—¶å­˜å‚¨è®­ç»ƒç»“æœåˆ°å®ä¾‹ï¼Œä»¥ä¾¿é¢„æµ‹å‡½æ•°è®¿é—®
                    self._current_batch_training_results = batch_training_results
                    
                    # ä½¿ç”¨çœŸæ­£çš„MLé¢„æµ‹
                    prediction_result = self._predict_with_batch_trained_models(ticker, ticker_features, batch_training_results)
                    
                    if prediction_result:
                        batch_predictions[ticker] = prediction_result['prediction']
                        batch_performance[ticker] = prediction_result['confidence']
                        batch_importance[ticker] = prediction_result['importance']
                        
                        logger.debug(f"æ‰¹æ¬¡MLé¢„æµ‹ {ticker}: {prediction_result['prediction']:.6f} "
                                   f"(ç½®ä¿¡åº¦: {prediction_result['confidence']:.3f}, "
                                   f"æ¥æº: {prediction_result['model_details']['source']})")
                    
                except Exception as e:
                    logger.debug(f"æ‰¹æ¬¡é¢„æµ‹å¤±è´¥ {ticker}: {e}")
                    continue
            
            # æ¸…ç†ä¸´æ—¶å­˜å‚¨
            if hasattr(self, '_current_batch_training_results'):
                delattr(self, '_current_batch_training_results')
            
            logger.info(f"æ‰¹æ¬¡MLåˆ†æå®Œæˆ: {len(batch_predictions)}/{len(batch_tickers)} æˆåŠŸ")
            
            return {
                'predictions': batch_predictions,
                'model_performance': batch_performance,
                'feature_importance': batch_importance,
                'batch_metadata': {
                    'total_tickers': len(batch_tickers),
                    'successful_count': len(batch_predictions),
                    'success_rate': len(batch_predictions) / len(batch_tickers) if batch_tickers else 0,
                    'training_summary': batch_training_results,
                    'ml_models_used': list(batch_training_results.get('traditional_models', {}).keys()),
                    'source': 'full_ml_pipeline'
                }
            }
            
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡MLåˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'predictions': {}, 'model_performance': {}, 'feature_importance': {}}
        
        batch_results = {
            'predictions': {},
            'model_performance': {},
            'feature_importance': {}
        }
        
        successful_count = 0
        
        failed_tickers = []
        
        for ticker in batch_tickers:
            try:
                # æ£€æŸ¥æ¨¡å‹ç¼“å­˜
                cache_key = f"{ticker}_{start_date}_{end_date}"
                cached_result = None
                
                if hasattr(self, 'model_cache'):
                    # ç®€åŒ–ç¼“å­˜æ£€æŸ¥
                    # å¯ç”¨æ™ºèƒ½ç¼“å­˜æ£€æŸ¥
                    if self.model_cache:
                        cached_result = self.model_cache.get_analysis_result(cache_key)
                        if cached_result:
                            logger.info(f"ç¼“å­˜å‘½ä¸­: {ticker}")
                    else:
                        cached_result = None
                
                if cached_result:
                    batch_results['predictions'][ticker] = cached_result
                    successful_count += 1
                    continue
                
                # æµå¼åŠ è½½æ•°æ®
                data = self._load_ticker_data_optimized(ticker, start_date, end_date)
                validation = self.data_validator.validate_dataframe(data, f"{ticker}_data", min_rows=20)
                if not validation['valid']:
                    logger.warning(f"æ‰¹æ¬¡åˆ†æ: {ticker} æ•°æ®éªŒè¯å¤±è´¥: {validation['errors']}")
                    failed_tickers.append(ticker)
                    continue
                
                # è®¡ç®—ç‰¹å¾ï¼ˆä½¿ç”¨å…¨å±€ç»Ÿè®¡æ ‡å‡†åŒ–ï¼‰
                features = self._calculate_features_optimized(data, ticker, global_stats)
                feature_validation = self.data_validator.validate_dataframe(features, f"{ticker}_features", min_rows=10)
                if not feature_validation['valid']:
                    logger.warning(f"æ‰¹æ¬¡åˆ†æ: {ticker} ç‰¹å¾éªŒè¯å¤±è´¥: {feature_validation['errors']}")
                    continue
                
                # ç”Ÿæˆé¢„æµ‹
                prediction_result = self._generate_prediction_optimized(ticker, features)
                if prediction_result:
                    # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                    prediction = prediction_result.get('prediction', 0.0)
                    confidence = prediction_result.get('confidence', 0.5)
                    importance = prediction_result.get('importance', {})
                    
                    # ç±»å‹æ£€æŸ¥å’Œè½¬æ¢
                    if not isinstance(prediction, (int, float)):
                        prediction = 0.0
                    if not isinstance(confidence, (int, float)):
                        confidence = 0.5
                    if not isinstance(importance, dict):
                        importance = {}
                    
                    batch_results['predictions'][ticker] = prediction
                    batch_results['model_performance'][ticker] = confidence
                    batch_results['feature_importance'][ticker] = importance
                    successful_count += 1
                
                # å†…å­˜æ¸…ç†
                del data, features
                if successful_count % 50 == 0 and self.memory_manager:
                    self.memory_manager.force_garbage_collection()
                    
            except Exception as e:
                logger.warning(f"æ‰¹æ¬¡åˆ†æå¤±è´¥ {ticker}: {e}")
                failed_tickers.append(ticker)
                continue
        
        # æ‰¹æ¬¡è´¨é‡æ£€æŸ¥
        success_rate = successful_count / len(batch_tickers) if batch_tickers else 0
        logger.info(f"æ‰¹æ¬¡åˆ†æå®Œæˆ: {successful_count}/{len(batch_tickers)} æˆåŠŸ (æˆåŠŸç‡: {success_rate:.1%})")
        
        if failed_tickers:
            logger.warning(f"æ‰¹æ¬¡å¤±è´¥è‚¡ç¥¨: {failed_tickers[:5]}{'...' if len(failed_tickers) > 5 else ''}")
        
        # å¦‚æœæˆåŠŸç‡è¿‡ä½ï¼Œæ·»åŠ è­¦å‘Š
        if success_rate < 0.3:
            logger.error(f"æ‰¹æ¬¡æˆåŠŸç‡è¿‡ä½ ({success_rate:.1%})ï¼Œå¯èƒ½å­˜åœ¨ç³»ç»Ÿæ€§é—®é¢˜")
        
        batch_results['batch_metadata'] = {
            'total_tickers': len(batch_tickers),
            'successful_count': successful_count,
            'failed_count': len(failed_tickers),
            'success_rate': success_rate,
            'failed_tickers': failed_tickers
        }
        
        return batch_results
    
    def _load_ticker_data_optimized(self, ticker: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """ä¼˜åŒ–ç‰ˆæ•°æ®åŠ è½½"""
        if hasattr(self, 'streaming_loader'):
            return self.streaming_loader.get_data(
                ticker, "price_data", start_date, end_date,
                lambda t, s, e: self._download_single_ticker(t, s, e)
            )
        else:
            return self._download_single_ticker(ticker, start_date, end_date)
    
    def _download_single_ticker(self, ticker: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """ä¸‹è½½å•ä¸ªè‚¡ç¥¨æ•°æ®"""
        try:
            # ä¿®å¤è°ƒç”¨æ–¹å¼ï¼šä½¿ç”¨startå’Œendå‚æ•°
            data = pc.download(ticker, start=start_date, end=end_date, interval='1d')
            return data if data is not None and not data.empty else None
        except Exception as e:
            logger.warning(f"æ•°æ®ä¸‹è½½å¤±è´¥ {ticker}: {e}")
            return None
    
    def _calculate_features_optimized(self, data: pd.DataFrame, ticker: str, 
                                     global_stats: Dict[str, Any] = None) -> Optional[pd.DataFrame]:
        """ä¼˜åŒ–ç‰ˆç‰¹å¾è®¡ç®— - é›†æˆç»Ÿä¸€ç‰¹å¾ç®¡é“"""
        try:
            if len(data) < 20:  # ğŸ”§ é™ä½ç‰¹å¾è®¡ç®—çš„æ•°æ®è¦æ±‚ï¼Œæé«˜é€šè¿‡ç‡
                return None
            
            # ğŸ”§ Step 1: ç”ŸæˆåŸºç¡€æŠ€æœ¯ç‰¹å¾
            features = pd.DataFrame(index=data.index)
            
            # ç¡®ä¿æœ‰closeåˆ—ï¼ˆæ”¯æŒå¤§å°å†™å…¼å®¹ï¼‰
            close_col = None
            if 'close' in data.columns:
                close_col = 'close'
            elif 'Close' in data.columns:
                close_col = 'Close'
            else:
                logger.warning(f"ç‰¹å¾è®¡ç®—å¤±è´¥ {ticker}: æ‰¾ä¸åˆ°close/Closeåˆ—")
                return None
                
            features['returns'] = data[close_col].pct_change()
            features['volatility'] = features['returns'].rolling(20).std()
            features['rsi'] = self._calculate_rsi(data[close_col])
            features['sma_ratio'] = data[close_col] / data[close_col].rolling(20).mean()
            
            # æ¸…ç†åŸºç¡€ç‰¹å¾
            features = features.dropna()
            if len(features) < 10:
                return None
            
            # ğŸ”§ Step 2: ç”ŸæˆAlphaå› å­æ•°æ®
            alpha_data = None
            try:
                alpha_data = self.alpha_engine.compute_all_alphas(data)
                if alpha_data is not None and not alpha_data.empty:
                    logger.debug(f"{ticker}: Alphaå› å­ç”ŸæˆæˆåŠŸ - {alpha_data.shape}")
            except Exception as e:
                logger.warning(f"{ticker}: Alphaå› å­ç”Ÿæˆå¤±è´¥: {e}")
            
            # ğŸ”§ Step 3: ä½¿ç”¨ç»Ÿä¸€ç‰¹å¾ç®¡é“å¤„ç†ç‰¹å¾
            if self.feature_pipeline is not None:
                try:
                    if not self.feature_pipeline.is_fitted:
                        # é¦–æ¬¡ä½¿ç”¨æ—¶æ‹Ÿåˆç®¡é“
                        processed_features, transform_info = self.feature_pipeline.fit_transform(
                            base_features=features,
                            alpha_data=alpha_data,
                            dates=features.index
                        )
                        logger.info(f"{ticker}: ç»Ÿä¸€ç‰¹å¾ç®¡é“æ‹Ÿåˆå®Œæˆ - {features.shape} -> {processed_features.shape}")
                    else:
                        # åç»­ä½¿ç”¨æ—¶åªè½¬æ¢
                        processed_features = self.feature_pipeline.transform(
                            base_features=features,
                            alpha_data=alpha_data,
                            dates=features.index
                        )
                        logger.debug(f"{ticker}: ç»Ÿä¸€ç‰¹å¾ç®¡é“è½¬æ¢å®Œæˆ - {features.shape} -> {processed_features.shape}")
                    
                    return processed_features
                    
                except Exception as e:
                    logger.warning(f"{ticker}: ç»Ÿä¸€ç‰¹å¾ç®¡é“å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•: {e}")
            
            # ğŸ”§ Step 4: å›é€€åˆ°ä¼ ç»Ÿç‰¹å¾å¤„ç†
            # ä½¿ç”¨å…¨å±€ç»Ÿè®¡è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆç¡®ä¿æ‰¹æ¬¡é—´ä¸€è‡´æ€§ï¼‰
            if global_stats and global_stats.get('feature_means'):
                features = self._standardize_features(features, global_stats)
            
            return features if len(features) > 5 else None  # ğŸ”§ é™ä½æœ€ç»ˆç‰¹å¾æ•°é‡è¦æ±‚
            
        except Exception as e:
            logger.warning(f"ç‰¹å¾è®¡ç®—å¤±è´¥ {ticker}: {e}")
            return None
    
    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
        """è®¡ç®—RSIæŒ‡æ ‡"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
    
    def _generate_prediction_optimized(self, ticker: str, features: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        ğŸ¤– ä½¿ç”¨è®­ç»ƒå¥½çš„æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡ŒçœŸæ­£çš„é¢„æµ‹
        ä¸å†ä½¿ç”¨ç¡¬ç¼–ç å…¬å¼ï¼Œè€Œæ˜¯ä½¿ç”¨é›†æˆMLæ¨¡å‹çš„é¢„æµ‹ç»“æœ
        """
        try:
            # ğŸ”¥ ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„MLæ¨¡å‹è¿›è¡Œé¢„æµ‹
            if hasattr(self, 'traditional_models') and self.traditional_models:
                ml_prediction = self._predict_with_trained_models(ticker, features)
                if ml_prediction is not None:
                    logger.debug(f"ä½¿ç”¨MLæ¨¡å‹é¢„æµ‹ {ticker}: {ml_prediction['prediction']:.6f}")
                    return ml_prediction
            
            # ğŸ”¥ ç¬¬äºŒä¼˜å…ˆçº§ï¼šä½¿ç”¨Alphaå› å­å¼•æ“ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼Œä¸å…è®¸å¤±è´¥ï¼‰
            if hasattr(self, 'alpha_engine') and self.alpha_engine and hasattr(self.alpha_engine, 'compute_all_alphas'):
                try:
                    alpha_prediction = self._predict_with_alpha_factors(ticker, features)
                    if alpha_prediction is not None:
                        logger.debug(f"ä½¿ç”¨Alphaå› å­é¢„æµ‹ {ticker}: {alpha_prediction['prediction']:.6f}")
                        return alpha_prediction
                    else:
                        # å¦‚æœAlphaå¼•æ“å­˜åœ¨ä½†è¿”å›Noneï¼Œè¿™è¡¨æ˜æœ‰ä¸¥é‡é—®é¢˜
                        logger.error(f"âŒ Alphaå¼•æ“å­˜åœ¨ä½†æ— æ³•ä¸º{ticker}ç”Ÿæˆé¢„æµ‹")
                except ValueError as e:
                    # Alphaé¢„æµ‹çš„ValueErrorè¡¨æ˜é…ç½®æˆ–è®­ç»ƒæœ‰é—®é¢˜ï¼Œå¿…é¡»ä¿®å¤
                    logger.error(f"âŒ Alphaå› å­é¢„æµ‹ä¸¥é‡é”™è¯¯ {ticker}: {e}")
                    # æ ¹æ®ç”¨æˆ·è¦æ±‚ï¼šä¸è¦å›é€€ï¼Œç›´æ¥æŠ¥é”™
                    raise ValueError(f"Alphaå› å­é¢„æµ‹å¤±è´¥ï¼Œç³»ç»Ÿè¦æ±‚ä¿®å¤: {e}") from e
            
            # ğŸ”¥ ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šä½¿ç”¨Learning-to-Rankæ¨¡å‹
            if hasattr(self, 'ltr_bma') and self.ltr_bma:
                ltr_prediction = self._predict_with_ltr_model(ticker, features)
                if ltr_prediction is not None:
                    logger.debug(f"ä½¿ç”¨LTRæ¨¡å‹é¢„æµ‹ {ticker}: {ltr_prediction['prediction']:.6f}")
                    return ltr_prediction
            
            # ğŸ”„ å›é€€ï¼šå¢å¼ºçš„æŠ€æœ¯æŒ‡æ ‡æ¨¡å‹ï¼ˆæ¯”ä¹‹å‰çš„ç¡¬ç¼–ç è§„åˆ™æ›´æ™ºèƒ½ï¼‰
            logger.info(f"MLæ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨å¢å¼ºæŠ€æœ¯æŒ‡æ ‡æ¨¡å‹ {ticker}")
            return self._predict_with_enhanced_technical_model(ticker, features)
            
        except Exception as e:
            logger.warning(f"é¢„æµ‹ç”Ÿæˆå¤±è´¥ {ticker}: {e}")
            return None
    
    
    def _predict_with_trained_models(self, ticker: str, features: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨è®­ç»ƒå¥½çš„ä¼ ç»ŸMLæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼ˆæ”¯æŒRegime-awareé¢„æµ‹ï¼‰"""
        try:
            # ğŸ”¥ NEW: Regime-awareé¢„æµ‹ä¼˜å…ˆçº§
            if self.regime_trainer and hasattr(self.regime_trainer, 'regime_models') and self.regime_trainer.regime_models:
                try:
                    logger.debug(f"ä½¿ç”¨Regime-awareé¢„æµ‹æ¨¡å‹ for {ticker}")
                    
                    # å‡†å¤‡ç”¨äºçŠ¶æ€æ£€æµ‹çš„æ•°æ®
                    latest_features = features.tail(1)
                    numeric_features = latest_features.select_dtypes(include=[np.number])
                    
                    # ä½¿ç”¨çŠ¶æ€æ„ŸçŸ¥é¢„æµ‹
                    regime_prediction = self.regime_trainer.predict_regime_aware(numeric_features)
                    
                    # è·å–å½“å‰å¸‚åœºçŠ¶æ€
                    current_regime = None
                    if hasattr(self.regime_detector, 'current_regime'):
                        current_regime = self.regime_detector.current_regime
                    
                    # æ„é€ ç»“æœ
                    regime_result = {
                        'prediction': float(regime_prediction[0]) if len(regime_prediction) > 0 else 0.0,
                        'confidence': 0.8,  # Regime-awareæ¨¡å‹ç½®ä¿¡åº¦
                        'model_type': 'regime_aware',
                        'current_regime': current_regime,
                        'regime_models_count': len(self.regime_trainer.regime_models),
                        'feature_count': len(numeric_features.columns)
                    }
                    
                    logger.debug(f"Regime-awareé¢„æµ‹å®Œæˆ: {regime_result['prediction']:.4f} (çŠ¶æ€: {current_regime})")
                    return regime_result
                    
                except Exception as e:
                    logger.warning(f"Regime-awareé¢„æµ‹å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ¨¡å‹: {e}")
            
            # ä¼ ç»Ÿé¢„æµ‹æ–¹æ³•ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
            latest_features = features.tail(1)
            # ç¡®ä¿ç‰¹å¾æ˜¯æ•°å€¼å‹
            numeric_features = latest_features.select_dtypes(include=[np.number])
            
            if numeric_features.empty:
                return None
            
            model_predictions = {}
            model_confidences = {}
            feature_importances = {}
            
            # ç¡®ä¿traditional_modelså­˜åœ¨
            if not hasattr(self, 'traditional_models'):
                self.traditional_models = {}
                logger.warning("traditional_modelså±æ€§ç¼ºå¤±ï¼Œå·²é‡æ–°åˆå§‹åŒ–")
            
            # éå†æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
            for model_name, fold_models in self.traditional_models.items():
                if not fold_models:
                    continue
                
                try:
                    fold_predictions = []
                    for model, scaler in fold_models:
                        # å‡†å¤‡ç‰¹å¾æ•°æ®
                        X_pred = numeric_features.values.reshape(1, -1)
                        
                        # åº”ç”¨æ ‡å‡†åŒ–ï¼ˆå¦‚æœæœ‰ï¼‰
                        if scaler is not None:
                            X_pred = scaler.transform(X_pred)
                        
                        # é¢„æµ‹
                        pred = model.predict(X_pred)[0]
                        fold_predictions.append(pred)
                    
                    # è®¡ç®—æŠ˜å å¹³å‡
                    if fold_predictions:
                        avg_pred = np.mean(fold_predictions)
                        pred_std = np.std(fold_predictions) if len(fold_predictions) > 1 else 0.1
                        
                        model_predictions[model_name] = avg_pred
                        model_confidences[model_name] = max(0.1, 1.0 / (1.0 + pred_std))
                        
                        # è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæ”¯æŒï¼‰
                        try:
                            if hasattr(fold_models[0][0], 'feature_importances_'):
                                importances = fold_models[0][0].feature_importances_
                                feature_names = numeric_features.columns
                                feature_importances[model_name] = dict(zip(feature_names, importances))
                        except:
                            pass
                            
                except Exception as e:
                    logger.debug(f"æ¨¡å‹ {model_name} é¢„æµ‹å¤±è´¥: {e}")
                    continue
            
            if not model_predictions:
                return None
            
            # ğŸ¯ é›†æˆé¢„æµ‹ï¼šåŸºäºæ¨¡å‹ç½®ä¿¡åº¦çš„åŠ æƒå¹³å‡
            total_weight = sum(model_confidences.values())
            if total_weight == 0:
                return None
            
            ensemble_prediction = sum(
                pred * model_confidences[name] / total_weight 
                for name, pred in model_predictions.items()
            )
            
            ensemble_confidence = sum(model_confidences.values()) / len(model_confidences)
            
            # åˆå¹¶ç‰¹å¾é‡è¦æ€§
            combined_importance = {}
            for model_name, importance in feature_importances.items():
                for feature, value in importance.items():
                    if feature not in combined_importance:
                        combined_importance[feature] = []
                    combined_importance[feature].append(value)
            
            # è®¡ç®—å¹³å‡é‡è¦æ€§
            avg_importance = {
                feature: np.mean(values) 
                for feature, values in combined_importance.items()
            }
            
            return {
                'prediction': float(ensemble_prediction),
                'confidence': float(ensemble_confidence),
                'importance': avg_importance,
                'model_details': {
                    'individual_predictions': model_predictions,
                    'individual_confidences': model_confidences,
                    'ensemble_method': 'confidence_weighted_average',
                    'source': 'trained_ml_models'
                }
            }
            
        except Exception as e:
            logger.debug(f"è®­ç»ƒæ¨¡å‹é¢„æµ‹å¤±è´¥ {ticker}: {e}")
            return None
    
    def _predict_with_alpha_factors(self, ticker: str, features: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """
        ğŸ¤– ä½¿ç”¨çœŸæ­£çš„æœºå™¨å­¦ä¹ Alphaå› å­è¿›è¡Œé¢„æµ‹
        ç¦æ­¢ç®€å•åŠ æƒå¹³å‡ï¼Œå¿…é¡»ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹
        """
        try:
            # ğŸš¨ ä¸¥æ ¼éªŒè¯ï¼šç¡®ä¿Alphaå¼•æ“ä¸æ˜¯Mock
            if not hasattr(self, 'alpha_engine') or self.alpha_engine is None:
                raise ValueError("âŒ Alphaå¼•æ“æœªåˆå§‹åŒ–ï¼æ— æ³•è¿›è¡ŒAlphaå› å­é¢„æµ‹")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºMockå¯¹è±¡
            if hasattr(self.alpha_engine, '__class__') and 'Mock' in str(self.alpha_engine.__class__):
                raise ValueError(
                    "âŒ æ£€æµ‹åˆ°Mock Alphaå¼•æ“ï¼\n"
                    "çœŸæ­£çš„Alphaå› å­é¢„æµ‹éœ€è¦å®é™…çš„AlphaStrategiesEngine\n"
                    "è¯·ç¡®ä¿enhanced_alpha_strategies.pyæ­£ç¡®åŠ è½½"
                )
            
            # å‡†å¤‡Alphaè¾“å…¥æ•°æ®
            alpha_input = self._prepare_single_ticker_alpha_data(ticker, features)
            if alpha_input is None or alpha_input.empty:
                raise ValueError(f"âŒ æ— æ³•ä¸º{ticker}å‡†å¤‡Alphaè¾“å…¥æ•°æ®")
            
            # ğŸ”¥ è®¡ç®—æ‰€æœ‰Alphaå› å­ï¼ˆè¿™ä¸€æ­¥åº”è¯¥æ˜¯ç»è¿‡è®­ç»ƒçš„ï¼‰
            logger.debug(f"è®¡ç®—{ticker}çš„Alphaå› å­...")
            alpha_signals = self.alpha_engine.compute_all_alphas(alpha_input)
            if alpha_signals is None or alpha_signals.empty:
                raise ValueError(f"âŒ Alphaå¼•æ“æ²¡æœ‰ä¸º{ticker}ç”Ÿæˆä»»ä½•ä¿¡å·")
            
            logger.info(f"Alphaå¼•æ“ä¸º{ticker}ç”Ÿæˆäº†{alpha_signals.shape[1]}ä¸ªå› å­ä¿¡å·")
            
            # è·å–æœ€æ–°çš„Alphaä¿¡å·
            latest_signals = alpha_signals.tail(1).iloc[0]
            valid_signals = latest_signals.dropna()
            
            if len(valid_signals) == 0:
                raise ValueError(f"âŒ {ticker}çš„æ‰€æœ‰Alphaä¿¡å·éƒ½æ˜¯NaN")
            
            # ğŸ”¥ ä½¿ç”¨æœºå™¨å­¦ä¹ è®­ç»ƒçš„æƒé‡ï¼ˆä¸æ˜¯ç¡¬ç¼–ç ï¼ï¼‰
            try:
                alpha_weights = self._get_alpha_factor_weights()
                logger.debug(f"è·å–åˆ°{len(alpha_weights)}ä¸ªAlphaå› å­æƒé‡")
            except ValueError as e:
                # é‡æ–°æŠ›å‡ºæƒé‡è·å–é”™è¯¯ï¼Œä¸å…è®¸å›é€€
                raise ValueError(f"âŒ Alphaå› å­æƒé‡è·å–å¤±è´¥: {e}")
            
            # ğŸ”¥ éªŒè¯æƒé‡å’Œä¿¡å·çš„åŒ¹é…æ€§
            matched_factors = 0
            weighted_prediction = 0.0
            total_weight = 0.0
            importance_dict = {}
            
            for alpha_name, weight in alpha_weights.items():
                if alpha_name in valid_signals.index:
                    signal_value = valid_signals[alpha_name]
                    weighted_prediction += signal_value * weight
                    total_weight += weight
                    importance_dict[alpha_name] = abs(signal_value * weight)
                    matched_factors += 1
            
            # ğŸš¨ ä¸¥æ ¼éªŒè¯ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„å› å­å‚ä¸é¢„æµ‹
            min_required_factors = max(3, len(alpha_weights) * 0.3)  # è‡³å°‘30%çš„å› å­æœ‰æ•ˆ
            if matched_factors < min_required_factors:
                raise ValueError(
                    f"âŒ Alphaå› å­åŒ¹é…ä¸è¶³ï¼\n"
                    f"åŒ¹é…å› å­: {matched_factors}/{len(alpha_weights)}\n"
                    f"æœ€å°è¦æ±‚: {min_required_factors}\n"
                    f"å¯èƒ½åŸå› : é…ç½®æ–‡ä»¶ä¸­çš„å› å­åç§°ä¸å®é™…ç”Ÿæˆçš„ä¸åŒ¹é…"
                )
            
            if total_weight == 0:
                raise ValueError("âŒ Alphaæƒé‡æ€»å’Œä¸º0ï¼Œæ— æ³•ç”Ÿæˆé¢„æµ‹")
            
            # è®¡ç®—æœ€ç»ˆé¢„æµ‹
            final_prediction = weighted_prediction / total_weight
            
            # ğŸ”¥ ä½¿ç”¨æ›´æ™ºèƒ½çš„æ ‡å‡†åŒ–ï¼ˆåŸºäºå†å²åˆ†å¸ƒï¼‰
            if hasattr(self.alpha_engine, 'signal_statistics'):
                # å¦‚æœæœ‰å†å²ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿ç”¨å®ƒä»¬è¿›è¡Œæ ‡å‡†åŒ–
                stats = self.alpha_engine.signal_statistics
                mean_signal = stats.get('mean', 0.0)
                std_signal = stats.get('std', 1.0)
                if std_signal > 0:
                    normalized_prediction = (final_prediction - mean_signal) / std_signal
                    final_prediction = 1 / (1 + np.exp(-normalized_prediction))  # sigmoid
                else:
                    final_prediction = max(0, min(1, (final_prediction + 1) / 2))
            else:
                # åŸºç¡€æ ‡å‡†åŒ–
                final_prediction = max(0, min(1, (final_prediction + 1) / 2))
            
            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºä¿¡å·è´¨é‡ï¼‰
            signal_strength = np.mean([abs(v) for v in importance_dict.values()])
            coverage_ratio = matched_factors / len(alpha_weights)
            confidence = min(0.95, max(0.4, signal_strength * coverage_ratio))
            
            logger.info(f"Alphaé¢„æµ‹å®Œæˆ {ticker}: {final_prediction:.6f} "
                       f"(ç½®ä¿¡åº¦: {confidence:.3f}, åŒ¹é…å› å­: {matched_factors})")
            
            return {
                'prediction': float(final_prediction),
                'confidence': float(confidence),
                'importance': importance_dict,
                'model_details': {
                    'alpha_count': len(valid_signals),
                    'matched_factors': matched_factors,
                    'total_factors': len(alpha_weights),
                    'coverage_ratio': float(coverage_ratio),
                    'signal_strength': float(signal_strength),
                    'source': 'trained_alpha_factors'
                }
            }
            
        except (ValueError, KeyError, AttributeError) as e:
            # ğŸš¨ ä¸šåŠ¡é€»è¾‘é”™è¯¯ï¼Œå¿…é¡»æŠ¥å‘Šå…·ä½“é”™è¯¯
            error_msg = f"âŒ Alphaå› å­é¢„æµ‹ä¸šåŠ¡é€»è¾‘é”™è¯¯ {ticker}: {str(e)}"
            logger.error(error_msg)
            # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä¸å…è®¸å›é€€åˆ°æŠ€æœ¯æŒ‡æ ‡
            raise ValueError(error_msg) from e
        except (ImportError, ModuleNotFoundError) as e:
            # ğŸš¨ ä¾èµ–é”™è¯¯
            error_msg = f"âŒ Alphaå› å­é¢„æµ‹ä¾èµ–é”™è¯¯ {ticker}: {str(e)}"
            logger.error(error_msg)
            raise ImportError(error_msg) from e
        except Exception as e:
            # ğŸš¨ å…¶ä»–æœªé¢„æœŸé”™è¯¯ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
            import traceback
            error_msg = f"âŒ Alphaå› å­é¢„æµ‹æœªçŸ¥é”™è¯¯ {ticker}: {str(e)}\n{traceback.format_exc()}"
            logger.error(error_msg)
            raise RuntimeError(error_msg) from e
    
    def _predict_with_ltr_model(self, ticker: str, features: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨Learning-to-Rankæ¨¡å‹é¢„æµ‹ - ä½¿ç”¨ç»Ÿä¸€ç‰¹å¾ç®¡é“ç¡®ä¿ç‰¹å¾ä¸€è‡´æ€§"""
        try:
            latest_features = features.tail(1)
            
            # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€ç‰¹å¾ç®¡é“å¤„ç†é¢„æµ‹ç‰¹å¾
            if self.feature_pipeline is not None and self.feature_pipeline.is_fitted:
                try:
                    # éœ€è¦è·å–Alphaæ•°æ®æ¥å®Œæ•´é‡ç°è®­ç»ƒæ—¶çš„ç‰¹å¾
                    # æš‚æ—¶ä½¿ç”¨åŸºç¡€ç‰¹å¾ï¼Œä½†è¿™éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›
                    processed_features = self.feature_pipeline.transform(latest_features)
                    logger.info(f"âœ… ç»Ÿä¸€ç‰¹å¾ç®¡é“é¢„æµ‹å¤„ç†: {latest_features.shape} â†’ {processed_features.shape}")
                    
                    # ä½¿ç”¨LTRæ¨¡å‹é¢„æµ‹
                    prediction, uncertainty = self.ltr_bma.predict_with_uncertainty(processed_features)
                    
                except (ValueError, KeyError, IndexError) as e:
                    logger.warning(f"ç»Ÿä¸€ç‰¹å¾ç®¡é“é¢„æµ‹å¤±è´¥ (æ•°æ®é—®é¢˜): {e}")
                    # å›é€€åˆ°åŸæœ‰é€»è¾‘
                    numeric_features = latest_features.select_dtypes(include=[np.number])
                    if numeric_features.empty:
                        return None
                    
                    # ä½¿ç”¨åŸæœ‰çš„ç‰¹å¾ç»´åº¦é€‚é…é€»è¾‘
                    prediction, uncertainty = self._fallback_ltr_prediction(numeric_features)
                except (ImportError, AttributeError) as e:
                    logger.error(f"ç»Ÿä¸€ç‰¹å¾ç®¡é“é¢„æµ‹å¤±è´¥ (ç³»ç»Ÿé”™è¯¯): {e}")
                    return None
                except Exception as e:
                    logger.error(f"ç»Ÿä¸€ç‰¹å¾ç®¡é“é¢„æµ‹å¤±è´¥ (æœªçŸ¥é”™è¯¯): {e}")
                    # ä¸¥é‡é”™è¯¯ï¼Œä¸å›é€€
                    return None
            else:
                logger.warning("ç»Ÿä¸€ç‰¹å¾ç®¡é“æœªæ‹Ÿåˆï¼Œä½¿ç”¨å›é€€é¢„æµ‹æ–¹æ³•")
                numeric_features = latest_features.select_dtypes(include=[np.number])
                if numeric_features.empty:
                    return None
                
                # ä½¿ç”¨åŸæœ‰çš„ç‰¹å¾ç»´åº¦é€‚é…é€»è¾‘
                prediction, uncertainty = self._fallback_ltr_prediction(numeric_features)
            
            if prediction is None or len(prediction) == 0:
                return None
            
            pred_value = prediction[0] if isinstance(prediction, (list, np.ndarray)) else prediction
            uncertainty_value = uncertainty[0] if isinstance(uncertainty, (list, np.ndarray)) else uncertainty
            
            # ç½®ä¿¡åº¦åŸºäºä¸ç¡®å®šæ€§
            confidence = max(0.2, min(0.9, 1.0 / (1.0 + uncertainty_value)))
            
            return {
                'prediction': float(pred_value),
                'confidence': float(confidence),
                'importance': {'ltr_score': float(pred_value)},
                'model_details': {
                    'uncertainty': float(uncertainty_value),
                    'source': 'learning_to_rank'
                }
            }
            
        except Exception as e:
            logger.debug(f"LTRæ¨¡å‹é¢„æµ‹å¤±è´¥ {ticker}: {e}")
            return None
    
    def _fallback_ltr_prediction(self, numeric_features: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """å›é€€LTRé¢„æµ‹æ–¹æ³• - å¤„ç†ç‰¹å¾ç»´åº¦ä¸åŒ¹é…"""
        # æ£€æŸ¥LTRæ¨¡å‹æœŸæœ›çš„ç‰¹å¾æ•°é‡
        if hasattr(self.ltr_bma, 'models') and self.ltr_bma.models:
            first_model_key = list(self.ltr_bma.models.keys())[0]
            first_models = self.ltr_bma.models[first_model_key].get('models', [])
            if first_models:
                first_model = first_models[0]
                if hasattr(first_model, 'n_features_') or hasattr(first_model, 'num_features'):
                    expected_features = getattr(first_model, 'n_features_', 
                                               getattr(first_model, 'num_features', None))
                    current_features = numeric_features.shape[1]
                    
                    if expected_features and current_features != expected_features:
                        logger.warning(f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: å½“å‰{current_features}, æœŸæœ›{expected_features}")
                        
                        # å°è¯•è¡¥å…¨ç‰¹å¾ï¼ˆç”¨0å¡«å……ï¼‰
                        if current_features < expected_features:
                            missing_features = expected_features - current_features
                            zero_features = pd.DataFrame(
                                np.zeros((1, missing_features)), 
                                index=numeric_features.index,
                                columns=[f'missing_feature_{i}' for i in range(missing_features)]
                            )
                            numeric_features = pd.concat([numeric_features, zero_features], axis=1)
                            logger.info(f"è¡¥å…¨äº†{missing_features}ä¸ªç¼ºå¤±ç‰¹å¾")
                        elif current_features > expected_features:
                            # æˆªæ–­å¤šä½™ç‰¹å¾
                            numeric_features = numeric_features.iloc[:, :expected_features]
                            logger.info(f"æˆªæ–­äº†{current_features - expected_features}ä¸ªå¤šä½™ç‰¹å¾")
        
        # ä½¿ç”¨LTRæ¨¡å‹é¢„æµ‹
        return self.ltr_bma.predict_with_uncertainty(numeric_features)
    
    def _predict_with_enhanced_technical_model(self, ticker: str, features: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """å¢å¼ºçš„æŠ€æœ¯æŒ‡æ ‡æ¨¡å‹ï¼ˆå›é€€æ–¹æ¡ˆï¼Œä½†æ¯”ç¡¬ç¼–ç è§„åˆ™æ›´æ™ºèƒ½ï¼‰"""
        try:
            latest_features = features.tail(1)
            
            # ğŸ¯ ä½¿ç”¨æ›´å¤šæŠ€æœ¯æŒ‡æ ‡å’Œæ›´æ™ºèƒ½çš„ç»„åˆé€»è¾‘
            scores = {}
            
            # RSIè¿ç»­è¯„åˆ†ï¼ˆæ”¹è¿›ç‰ˆï¼‰
            if 'rsi' in latest_features.columns:
                rsi_val = latest_features['rsi'].iloc[0]
                # ä½¿ç”¨sigmoidå‡½æ•°ä½¿è¯„åˆ†æ›´å¹³æ»‘
                rsi_score = 1 / (1 + np.exp((rsi_val - 50) / 10))  # ä¸­å¿ƒåŒ–åœ¨50ï¼Œæ›´å¹³æ»‘
                scores['rsi'] = rsi_score
            
            # SMAæ¯”ç‡è¯„åˆ†ï¼ˆæ”¹è¿›ç‰ˆï¼‰
            if 'sma_ratio' in latest_features.columns:
                sma_ratio = latest_features['sma_ratio'].iloc[0]
                # ä½¿ç”¨tanhå‡½æ•°
                sma_score = (np.tanh((sma_ratio - 1.0) * 5) + 1) / 2  # ä¸­å¿ƒåŒ–åœ¨1.0
                scores['sma_ratio'] = sma_score
            
            # æ³¢åŠ¨ç‡è¯„åˆ†ï¼ˆæ”¹è¿›ç‰ˆï¼‰
            if 'volatility' in latest_features.columns:
                volatility = latest_features['volatility'].iloc[0]
                vol_median = features['volatility'].median()
                if vol_median > 0:
                    vol_ratio = volatility / vol_median
                    vol_score = 1 / (1 + vol_ratio)  # æ³¢åŠ¨ç‡è¶Šä½åˆ†æ•°è¶Šé«˜
                    scores['volatility'] = vol_score
            
            # åŠ¨é‡è¯„åˆ†ï¼ˆå¦‚æœæœ‰æ”¶ç›Šç‡æ•°æ®ï¼‰
            if 'returns' in latest_features.columns and len(features) >= 5:
                recent_returns = features['returns'].tail(5).mean()
                momentum_score = (np.tanh(recent_returns * 50) + 1) / 2
                scores['momentum'] = momentum_score
            
            if not scores:
                return None
            
            # ğŸ¯ è‡ªé€‚åº”æƒé‡ç³»ç»Ÿï¼ˆæ ¹æ®å¸‚åœºæ¡ä»¶è°ƒæ•´ï¼‰
            weights = self._get_adaptive_technical_weights(ticker, features)
            
            # è®¡ç®—åŠ æƒé¢„æµ‹
            weighted_prediction = sum(
                scores.get(factor, 0.5) * weight 
                for factor, weight in weights.items()
            )
            
            # æ·»åŠ è‚¡ç¥¨ç‰¹å®šè°ƒæ•´ï¼ˆä½†ä½¿ç”¨æ›´æ™ºèƒ½çš„æ–¹æ³•ï¼‰
            ticker_adjustment = self._get_ticker_specific_adjustment(ticker)
            final_prediction = max(0, min(1, weighted_prediction + ticker_adjustment))
            
            # åŠ¨æ€ç½®ä¿¡åº¦è®¡ç®—
            score_variance = np.var(list(scores.values()))
            confidence = max(0.4, min(0.8, 1.0 / (1.0 + score_variance * 2)))
            
            return {
                'prediction': float(final_prediction),
                'confidence': float(confidence),
                'importance': scores,
                'model_details': {
                    'weights_used': weights,
                    'ticker_adjustment': float(ticker_adjustment),
                    'source': 'enhanced_technical'
                }
            }
            
        except Exception as e:
            logger.debug(f"å¢å¼ºæŠ€æœ¯æ¨¡å‹é¢„æµ‹å¤±è´¥ {ticker}: {e}")
            # æœ€ç»ˆå›é€€åˆ°ç®€å•é¢„æµ‹
            return {
                'prediction': 0.5,
                'confidence': 0.3,
                'importance': {'fallback': 1.0},
                'model_details': {'source': 'fallback'}
            }
    
    def _prepare_single_ticker_alpha_data(self, ticker: str, features: pd.DataFrame) -> Optional[pd.DataFrame]:
        """ä¸ºå•ä¸ªè‚¡ç¥¨å‡†å¤‡Alphaå› å­è®¡ç®—çš„è¾“å…¥æ•°æ®"""
        try:
            if features.empty:
                return None
            
            # Alphaå¼•æ“é€šå¸¸éœ€è¦ä»·æ ¼æ•°æ®åˆ—
            alpha_data = features.copy()
            
            # ç¡®ä¿æœ‰å¿…è¦çš„åˆ—ï¼ˆå¦‚æœæ²¡æœ‰åˆ™å°è¯•æ„é€ ï¼‰
            required_cols = ['close', 'high', 'low', 'volume', 'open']
            for col in required_cols:
                if col not in alpha_data.columns:
                    if col == 'close' and 'Close' in alpha_data.columns:
                        alpha_data['close'] = alpha_data['Close']
                    elif col in ['high', 'low', 'open'] and 'close' in alpha_data.columns:
                        # å¦‚æœæ²¡æœ‰OHLVæ•°æ®ï¼Œç”¨closeä»·æ ¼è¿‘ä¼¼
                        alpha_data[col] = alpha_data['close']
                    elif col == 'volume':
                        # å¦‚æœæ²¡æœ‰æˆäº¤é‡æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        alpha_data[col] = 1000000
            
            # æ·»åŠ tickeråˆ—ï¼ˆAlphaå¼•æ“å¯èƒ½éœ€è¦ï¼‰
            alpha_data['ticker'] = ticker
            
            return alpha_data
            
        except Exception as e:
            logger.debug(f"Alphaæ•°æ®å‡†å¤‡å¤±è´¥ {ticker}: {e}")
            return None
    
    def _get_alpha_factor_weights(self) -> Dict[str, float]:
        """
        ğŸš¨ ç¦æ­¢ç¡¬ç¼–ç æƒé‡ï¼å¿…é¡»ä»æœºå™¨å­¦ä¹ è®­ç»ƒä¸­è·å–Alphaå› å­æƒé‡
        """
        # ğŸ”¥ ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šä»è®­ç»ƒå¥½çš„BMAæ¨¡å‹è·å–æƒé‡
        if hasattr(self, 'alpha_engine') and self.alpha_engine:
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„BMAæƒé‡
                if hasattr(self.alpha_engine, 'bma_weights') and self.alpha_engine.bma_weights is not None:
                    logger.info("ä½¿ç”¨è®­ç»ƒå¥½çš„BMAæƒé‡")
                    return self.alpha_engine.bma_weights.to_dict()
                    
                # æ£€æŸ¥æ˜¯å¦æœ‰OOFè¯„åˆ†å¯ä»¥è½¬æ¢ä¸ºæƒé‡
                if hasattr(self.alpha_engine, 'alpha_scores') and self.alpha_engine.alpha_scores is not None:
                    logger.info("åŸºäºOOFè¯„åˆ†è®¡ç®—Alphaæƒé‡")
                    scores = self.alpha_engine.alpha_scores
                    # å°†ICè¯„åˆ†è½¬æ¢ä¸ºæ­£æƒé‡
                    positive_scores = np.abs(scores)
                    if positive_scores.sum() > 0:
                        normalized_weights = positive_scores / positive_scores.sum()
                        return normalized_weights.to_dict()
            except Exception as e:
                logger.error(f"è·å–è®­ç»ƒæƒé‡å¤±è´¥: {e}")
        
        # ğŸ”¥ ç¬¬äºŒä¼˜å…ˆçº§ï¼šä»é…ç½®æ–‡ä»¶è¯»å–æƒé‡æç¤ºå¹¶éªŒè¯
        if (hasattr(self, 'alpha_engine') and self.alpha_engine and 
            hasattr(self.alpha_engine, 'config') and 'alphas' in self.alpha_engine.config):
            alpha_configs = self.alpha_engine.config['alphas']
            config_weights = {}
            total_weight = 0.0
            
            for alpha_config in alpha_configs:
                alpha_name = alpha_config['name']
                weight_hint = alpha_config.get('weight_hint', 0.0)
                if weight_hint > 0:
                    config_weights[alpha_name] = weight_hint
                    total_weight += weight_hint
            
            if config_weights and total_weight > 0:
                # å½’ä¸€åŒ–æƒé‡
                normalized_weights = {k: v/total_weight for k, v in config_weights.items()}
                logger.info(f"ä½¿ç”¨é…ç½®æ–‡ä»¶æƒé‡æç¤ºï¼Œ{len(normalized_weights)} ä¸ªå› å­")
                return normalized_weights
        
        # ğŸš¨ å¦‚æœæ²¡æœ‰è®­ç»ƒæƒé‡ä¹Ÿæ²¡æœ‰é…ç½®ï¼Œç›´æ¥æŠ¥é”™ï¼
        raise ValueError(
            "âŒ ä¸¥é‡é”™è¯¯ï¼šæ— æ³•è·å–Alphaå› å­æƒé‡ï¼\n"
            "åŸå› ï¼š\n"
            "1. æ²¡æœ‰è®­ç»ƒå¥½çš„BMAæƒé‡\n" 
            "2. æ²¡æœ‰OOFè¯„åˆ†\n"
            "3. é…ç½®æ–‡ä»¶æ²¡æœ‰æƒé‡æç¤º\n"
            "è§£å†³æ–¹æ¡ˆï¼š\n"
            "1. ç¡®ä¿Alphaå¼•æ“å·²æ­£ç¡®è®­ç»ƒ\n"
            "2. æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„weight_hintè®¾ç½®\n"
            "3. ä¸å…è®¸ä½¿ç”¨ç¡¬ç¼–ç æƒé‡"
        )
    
    def _get_adaptive_technical_weights(self, ticker: str, features: pd.DataFrame) -> Dict[str, float]:
        """æ ¹æ®å¸‚åœºæ¡ä»¶å’Œè‚¡ç¥¨ç‰¹æ€§è·å–è‡ªé€‚åº”æŠ€æœ¯æŒ‡æ ‡æƒé‡"""
        try:
            # åŸºç¡€æƒé‡
            base_weights = {
                'rsi': 0.25,
                'sma_ratio': 0.25,
                'volatility': 0.20,
                'momentum': 0.30
            }
            
            # æ ¹æ®æ³¢åŠ¨ç‡è°ƒæ•´æƒé‡
            if 'volatility' in features.columns and len(features) > 10:
                recent_vol = features['volatility'].tail(10).mean()
                median_vol = features['volatility'].median()
                
                if median_vol > 0:
                    vol_ratio = recent_vol / median_vol
                    
                    if vol_ratio > 1.5:  # é«˜æ³¢åŠ¨æœŸ
                        # å¢åŠ RSIå’Œæ³¢åŠ¨ç‡æƒé‡ï¼Œå‡å°‘åŠ¨é‡æƒé‡
                        base_weights['rsi'] = 0.35
                        base_weights['volatility'] = 0.30
                        base_weights['momentum'] = 0.15
                        base_weights['sma_ratio'] = 0.20
                    elif vol_ratio < 0.7:  # ä½æ³¢åŠ¨æœŸ
                        # å¢åŠ åŠ¨é‡å’ŒSMAæƒé‡
                        base_weights['momentum'] = 0.40
                        base_weights['sma_ratio'] = 0.30
                        base_weights['rsi'] = 0.15
                        base_weights['volatility'] = 0.15
            
            # æ ¹æ®è‚¡ç¥¨ç‰¹æ€§è°ƒæ•´ï¼ˆå¤§ç›˜è‚¡ vs å°ç›˜è‚¡ï¼‰
            if ticker in ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA']:
                # å¤§ç›˜è‚¡ï¼šæ›´ä¾èµ–æŠ€æœ¯æŒ‡æ ‡
                base_weights['rsi'] = min(0.4, base_weights['rsi'] * 1.2)
                base_weights['sma_ratio'] = min(0.4, base_weights['sma_ratio'] * 1.2)
            
            # ç¡®ä¿æƒé‡å½’ä¸€åŒ–
            total_weight = sum(base_weights.values())
            if total_weight > 0:
                base_weights = {k: v/total_weight for k, v in base_weights.items()}
            
            return base_weights
            
        except Exception as e:
            logger.debug(f"è‡ªé€‚åº”æƒé‡è®¡ç®—å¤±è´¥ {ticker}: {e}")
            return {'rsi': 0.25, 'sma_ratio': 0.25, 'volatility': 0.25, 'momentum': 0.25}
    
    def _get_ticker_specific_adjustment(self, ticker: str) -> float:
        """è·å–è‚¡ç¥¨ç‰¹å®šçš„è°ƒæ•´å› å­ï¼ˆæ™ºèƒ½ç‰ˆæœ¬ï¼‰"""
        try:
            import hashlib
            import time
            
            # åŸºäºtickerçš„ç¡®å®šæ€§ä½†æ—¶å˜çš„è°ƒæ•´
            ticker_hash = int(hashlib.md5(ticker.encode()).hexdigest()[:8], 16)
            time_hash = int(time.time() / 3600) % 1000  # æ¯å°æ—¶å˜åŒ–
            
            # ç»„åˆå“ˆå¸Œ
            combined = (ticker_hash + time_hash) % 10000
            
            # ç”Ÿæˆ[-0.05, 0.05]çš„å°å¹…è°ƒæ•´ï¼ˆæ¯”ä¹‹å‰çš„0.15æ›´ä¿å®ˆï¼‰
            adjustment = (combined / 10000.0 - 0.5) * 0.1
            
            # æ ¹æ®è‚¡ç¥¨ç±»å‹è¿›ä¸€æ­¥è°ƒæ•´
            if ticker in ['AAPL', 'MSFT', 'GOOGL']:  # ç¨³å®šå¤§ç›˜è‚¡
                adjustment *= 0.5  # å‡å°‘éšæœºæ€§
            elif ticker in ['TSLA', 'NVDA', 'AMD']:  # æ³¢åŠ¨æ€§è‚¡ç¥¨
                adjustment *= 1.5  # å¢åŠ å˜åŒ–
            
            return adjustment
            
        except Exception as e:
            logger.debug(f"è‚¡ç¥¨ç‰¹å®šè°ƒæ•´è®¡ç®—å¤±è´¥ {ticker}: {e}")
            return 0.0
    
    def _predict_with_batch_trained_models(self, ticker: str, features: pd.DataFrame, 
                                         batch_training_results: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨æ‰¹æ¬¡è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        try:
            if features.empty:
                return None
            
            latest_features = features.tail(1)
            numeric_features = latest_features.select_dtypes(include=[np.number])
            
            if numeric_features.empty:
                return None
            
            # ğŸ”¥ ä½¿ç”¨æ‰¹æ¬¡è®­ç»ƒçš„ä¼ ç»ŸMLæ¨¡å‹
            if 'traditional_models' in batch_training_results and 'oof_predictions' in batch_training_results['traditional_models']:
                traditional_results = batch_training_results['traditional_models']
                oof_predictions = traditional_results['oof_predictions']
                model_performance = traditional_results.get('model_performance', {})
                
                # å°è¯•ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹
                model_predictions = {}
                model_confidences = {}
                
                # å¦‚æœæœ‰ä¿å­˜çš„æ¨¡å‹å®ä¾‹ï¼Œä½¿ç”¨å®ƒä»¬è¿›è¡Œé¢„æµ‹
                if hasattr(self, 'traditional_models') and self.traditional_models:
                    for model_name, fold_models in self.traditional_models.items():
                        if not fold_models:
                            continue
                        
                        try:
                            fold_predictions = []
                            for model, scaler in fold_models:
                                X_pred = numeric_features.values.reshape(1, -1)
                                if scaler is not None:
                                    X_pred = scaler.transform(X_pred)
                                pred = model.predict(X_pred)[0]
                                fold_predictions.append(pred)
                            
                            if fold_predictions:
                                avg_pred = np.mean(fold_predictions)
                                pred_std = np.std(fold_predictions) if len(fold_predictions) > 1 else 0.1
                                model_predictions[model_name] = avg_pred
                                model_confidences[model_name] = max(0.1, 1.0 / (1.0 + pred_std))
                                
                        except Exception as e:
                            logger.debug(f"æ‰¹æ¬¡æ¨¡å‹ {model_name} é¢„æµ‹å¤±è´¥: {e}")
                            continue
                
                # å¦‚æœæˆåŠŸè·å¾—é¢„æµ‹ï¼Œè¿›è¡Œé›†æˆ
                if model_predictions:
                    total_weight = sum(model_confidences.values())
                    ensemble_prediction = sum(
                        pred * model_confidences[name] / total_weight 
                        for name, pred in model_predictions.items()
                    ) if total_weight > 0 else 0.5
                    
                    ensemble_confidence = sum(model_confidences.values()) / len(model_confidences)
                    
                    return {
                        'prediction': float(ensemble_prediction),
                        'confidence': float(ensemble_confidence),
                        'importance': {f'ml_model_{k}': v for k, v in model_predictions.items()},
                        'model_details': {
                            'individual_predictions': model_predictions,
                            'individual_confidences': model_confidences,
                            'source': 'batch_trained_ml_models'
                        }
                    }
            
            # ğŸ”¥ æ³¨æ„ï¼šAlphaç­–ç•¥ä¿¡å·å·²é›†æˆåˆ°MLç‰¹å¾ä¸­ï¼Œæ— éœ€å•ç‹¬é¢„æµ‹
            
            # ğŸ”„ æœ€ç»ˆå›é€€åˆ°å¢å¼ºæŠ€æœ¯æ¨¡å‹
            return self._predict_with_enhanced_technical_model(ticker, features)
            
        except Exception as e:
            logger.debug(f"æ‰¹æ¬¡è®­ç»ƒæ¨¡å‹é¢„æµ‹å¤±è´¥ {ticker}: {e}")
            return None
    
    def _generate_recommendations_from_predictions(self, predictions: Dict[str, float], top_n: int) -> List[Dict[str, Any]]:
        """ä»é¢„æµ‹ç»“æœç”Ÿæˆæ¨è"""
        recommendations = []
        
        # æŒ‰é¢„æµ‹å€¼æ’åº
        sorted_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)
        
        # è®¡ç®—æƒé‡æ€»å’Œï¼Œé˜²æ­¢é™¤é›¶é”™è¯¯
        try:
            total_predictions = sum(predictions.values())
            if total_predictions == 0:
                total_predictions = len(predictions)  # ä½¿ç”¨å‡æƒä½œä¸ºå¤‡é€‰
        except (TypeError, ValueError):
            total_predictions = len(predictions)
            
        for i, (ticker, prediction) in enumerate(sorted_predictions[:top_n]):
            try:
                weight = max(0.01, prediction / total_predictions) if total_predictions != 0 else 1.0 / top_n
            except (TypeError, ZeroDivisionError):
                weight = 1.0 / top_n  # å‡æƒå¤‡é€‰
                
            recommendations.append({
                'rank': i + 1,
                'ticker': ticker,
                'prediction_signal': prediction,
                'weight': weight,
                'rating': 'BUY' if prediction > 0.6 else 'HOLD' if prediction > 0.4 else 'SELL'
            })
        
        return recommendations
    
    def _save_optimized_results(self, results: Dict[str, Any], filename: str):
        """ä¿å­˜ä¼˜åŒ–ç‰ˆç»“æœ"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            import os
            os.makedirs(os.path.dirname(filename), exist_ok=True)
            
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # æ¨èåˆ—è¡¨
                if 'recommendations' in results:
                    recommendations_df = pd.DataFrame(results['recommendations'])
                    recommendations_df.to_excel(writer, sheet_name='æ¨èåˆ—è¡¨', index=False)
                
                # é¢„æµ‹ç»“æœ
                if 'predictions' in results:
                    predictions_df = pd.DataFrame(list(results['predictions'].items()), 
                                                columns=['è‚¡ç¥¨ä»£ç ', 'é¢„æµ‹å€¼'])
                    predictions_df.to_excel(writer, sheet_name='é¢„æµ‹ç»“æœ', index=False)
                
                # ä¼˜åŒ–ç»Ÿè®¡
                if 'optimization_stats' in results:
                    stats_data = []
                    for key, value in results['optimization_stats'].items():
                        if isinstance(value, dict):
                            for sub_key, sub_value in value.items():
                                stats_data.append([f"{key}_{sub_key}", str(sub_value)])
                        else:
                            stats_data.append([key, str(value)])
                    
                    stats_df = pd.DataFrame(stats_data, columns=['æŒ‡æ ‡', 'æ•°å€¼'])
                    stats_df.to_excel(writer, sheet_name='ä¼˜åŒ–ç»Ÿè®¡', index=False)
            
            logger.info(f"ç»“æœå·²ä¿å­˜: {filename}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def _standardize_features(self, features: pd.DataFrame, global_stats: Dict[str, Any]) -> pd.DataFrame:
        """ä½¿ç”¨å…¨å±€ç»Ÿè®¡æ ‡å‡†åŒ–ç‰¹å¾"""
        try:
            feature_means = global_stats.get('feature_means', {})
            feature_stds = global_stats.get('feature_stds', {})
            
            standardized_features = features.copy()
            
            for col in features.columns:
                if col in feature_means and col in feature_stds:
                    mean_val = feature_means[col]
                    std_val = feature_stds[col]
                    
                    if std_val > 0:  # é¿å…é™¤é›¶
                        standardized_features[col] = (features[col] - mean_val) / std_val
                    else:
                        standardized_features[col] = features[col] - mean_val
            
            return standardized_features
            
        except Exception as e:
            logger.warning(f"ç‰¹å¾æ ‡å‡†åŒ–å¤±è´¥: {e}")
            return features
        
        logger.info("UltraEnhancedé‡åŒ–æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def _generate_investment_recommendations(self, portfolio_result: Dict[str, Any], top_n: int) -> pd.DataFrame:
        """ç”ŸæˆæŠ•èµ„å»ºè®®"""
        try:
            # ä»æŠ•èµ„ç»„åˆä¼˜åŒ–ç»“æœä¸­æå–æ¨è
            if not portfolio_result or not portfolio_result.get('success', False):
                logger.warning("æŠ•èµ„ç»„åˆä¼˜åŒ–å¤±è´¥ï¼Œç”Ÿæˆç®€å•æ¨è")
                return pd.DataFrame({
                    'ticker': ['AAPL', 'MSFT', 'GOOGL'],
                    'recommendation': ['BUY', 'HOLD', 'BUY'],
                    'weight': [0.4, 0.3, 0.3],
                    'confidence': [0.7, 0.6, 0.8]
                })
            
            # æå–ä¼˜åŒ–æƒé‡
            weights = portfolio_result.get('weights', {})
            if isinstance(weights, dict) and weights:
                # æŒ‰æƒé‡æ’åº
                sorted_weights = sorted(weights.items(), key=lambda x: abs(x[1]), reverse=True)
                top_weights = sorted_weights[:top_n]
                
                recommendations = []
                for ticker, weight in top_weights:
                    if weight > 0.05:
                        rec = 'BUY'
                        conf = min(0.9, 0.5 + abs(weight) * 2)
                    elif weight < -0.05:
                        rec = 'SELL'
                        conf = min(0.9, 0.5 + abs(weight) * 2)
                    else:
                        rec = 'HOLD'
                        conf = 0.3
                    
                    recommendations.append({
                        'ticker': ticker,
                        'recommendation': rec,
                        'weight': weight,
                        'confidence': conf
                    })
                
                return pd.DataFrame(recommendations)
            else:
                logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆæƒé‡ï¼Œç”Ÿæˆé»˜è®¤æ¨è")
                return pd.DataFrame({
                    'ticker': ['AAPL', 'MSFT'],
                    'recommendation': ['BUY', 'BUY'],
                    'weight': [0.5, 0.5],
                    'confidence': [0.6, 0.6]
                })
                
        except Exception as e:
            logger.error(f"æŠ•èµ„å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
            return pd.DataFrame({
                'ticker': ['ERROR'],
                'recommendation': ['HOLD'],
                'weight': [1.0],
                'confidence': [0.1]
            })
    
    def _save_results(self, recommendations: pd.DataFrame, portfolio_result: Dict[str, Any], 
                     analysis_results: Dict[str, Any]) -> str:
        """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
        try:
            from datetime import datetime
            import os
            
            # åˆ›å»ºç»“æœæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            result_file = f"result/bma_enhanced_analysis_{timestamp}.xlsx"
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs('result', exist_ok=True)
            
            # ä¿å­˜åˆ°Excel
            with pd.ExcelWriter(result_file, engine='openpyxl') as writer:
                # ä¿å­˜æŠ•èµ„å»ºè®®
                if not recommendations.empty:
                    recommendations.to_excel(writer, sheet_name='æŠ•èµ„å»ºè®®', index=False)
                
                # ä¿å­˜æŠ•èµ„ç»„åˆæƒé‡
                if portfolio_result and portfolio_result.get('success'):
                    weights = portfolio_result.get('weights', {})
                    if weights:
                        weights_df = pd.DataFrame(list(weights.items()), columns=['ticker', 'weight'])
                        weights_df.to_excel(writer, sheet_name='æŠ•èµ„ç»„åˆæƒé‡', index=False)
                
                # ä¿å­˜åˆ†ææ‘˜è¦
                summary_data = {
                    'æŒ‡æ ‡': ['æ€»è€—æ—¶(ç§’)', 'è‚¡ç¥¨æ•°é‡', 'é¢„æµ‹é•¿åº¦', 'æˆåŠŸçŠ¶æ€'],
                    'å€¼': [
                        analysis_results.get('total_time', 0),
                        len(analysis_results.get('tickers', [])),
                        len(analysis_results.get('predictions', [])),
                        analysis_results.get('success', False)
                    ]
                }
                summary_df = pd.DataFrame(summary_data)
                summary_df.to_excel(writer, sheet_name='åˆ†ææ‘˜è¦', index=False)
            
            logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {result_file}")
            return result_file
            
        except Exception as e:
            logger.error(f"ç»“æœä¿å­˜å¤±è´¥: {e}")
            return f"ä¿å­˜å¤±è´¥: {str(e)}"
    
    def optimize_portfolio(self, predictions: pd.Series, feature_data: pd.DataFrame = None) -> Dict[str, Any]:
        """ç®€åŒ–çš„æŠ•èµ„ç»„åˆä¼˜åŒ–æ–¹æ³•"""
        try:
            if predictions.empty:
                return {'success': False, 'error': 'é¢„æµ‹æ•°æ®ä¸ºç©º'}
            
            # ç®€å•çš„æƒé‡åˆ†é…ç­–ç•¥
            n_assets = len(predictions)
            if n_assets == 0:
                return {'success': False, 'error': 'æ— èµ„äº§å¯åˆ†é…'}
            
            # åŸºäºé¢„æµ‹å€¼çš„æƒé‡åˆ†é…
            pred_values = predictions.values
            
            # æ ‡å‡†åŒ–é¢„æµ‹å€¼
            if np.std(pred_values) > 0:
                normalized_preds = (pred_values - np.mean(pred_values)) / np.std(pred_values)
            else:
                normalized_preds = np.zeros_like(pred_values)
            
            # åº”ç”¨softmaxå¾—åˆ°æƒé‡
            exp_preds = np.exp(normalized_preds - np.max(normalized_preds))  # æ•°å€¼ç¨³å®šæ€§
            weights_array = exp_preds / np.sum(exp_preds)
            
            # åˆ›å»ºæƒé‡å­—å…¸
            if hasattr(predictions, 'index'):
                tickers = predictions.index.tolist() if hasattr(predictions.index, 'tolist') else list(range(len(predictions)))
            else:
                tickers = list(range(len(predictions)))
            
            weights = {str(ticker): float(weight) for ticker, weight in zip(tickers, weights_array)}
            
            return {
                'success': True,
                'weights': weights,
                'method': 'softmax_prediction_based',
                'n_assets': n_assets
            }
            
        except Exception as e:
            logger.error(f"æŠ•èµ„ç»„åˆä¼˜åŒ–å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def get_health_report(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿå¥åº·çŠ¶å†µæŠ¥å‘Š"""
        # ç¡®ä¿health_metricså·²åˆå§‹åŒ–
        if not hasattr(self, 'health_metrics'):
            self.health_metrics = {
                'universe_load_fallbacks': 0,
                'risk_model_failures': 0,
                'optimization_fallbacks': 0,
                'alpha_computation_failures': 0,
                'neutralization_failures': 0,
                'prediction_failures': 0,
                'total_exceptions': 0,
                'successful_predictions': 0
            }
        
        total_operations = sum(self.health_metrics.values())
        failure_rate = (self.health_metrics['total_exceptions'] / max(total_operations, 1)) * 100
        
        report = {
            'health_metrics': self.health_metrics.copy(),
            'failure_rate_percent': failure_rate,
            'risk_level': 'LOW' if failure_rate < 5 else 'MEDIUM' if failure_rate < 15 else 'HIGH',
            'recommendations': []
        }
        
        # æ ¹æ®å¤±è´¥ç±»å‹ç»™å‡ºå»ºè®®
        if self.health_metrics['universe_load_fallbacks'] > 0:
            report['recommendations'].append("æ£€æŸ¥è‚¡ç¥¨æ¸…å•æ–‡ä»¶æ ¼å¼å’Œç¼–ç ")
        if self.health_metrics['risk_model_failures'] > 2:
            report['recommendations'].append("æ£€æŸ¥UMDMé…ç½®å’Œå¸‚åœºæ•°æ®è¿æ¥")
        if self.health_metrics['optimization_fallbacks'] > 1:
            report['recommendations'].append("æ£€æŸ¥æŠ•èµ„ç»„åˆçº¦æŸè®¾ç½®")
        
        return report
    
    def build_risk_model(self) -> Dict[str, Any]:
        """æ„å»ºMulti-factoré£é™©æ¨¡å‹ï¼ˆæ¥è‡ªProfessionalå¼•æ“ï¼‰"""
        logger.info("æ„å»ºMulti-factoré£é™©æ¨¡å‹")
        
        if not self.raw_data:
            raise ValueError("Market data not loaded")
        
        # æ„å»ºæ”¶ç›Šç‡çŸ©é˜µ
        returns_data = []
        tickers = []
        
        for ticker, data in self.raw_data.items():
            if len(data) > 100:
                returns = data['close'].pct_change().fillna(0)
                returns_data.append(returns)
                tickers.append(ticker)
        
        if not returns_data:
            raise ValueError("No valid returns data")
        
        returns_matrix = pd.concat(returns_data, axis=1, keys=tickers)
        returns_matrix = returns_matrix.fillna(0.0)
        
        # æ„å»ºé£é™©å› å­
        risk_factors = self._build_risk_factors(returns_matrix)
        
        # ä¼°è®¡å› å­è½½è·
        factor_loadings = self._estimate_factor_loadings(returns_matrix, risk_factors)
        
        # ä¼°è®¡å› å­åæ–¹å·®
        factor_covariance = self._estimate_factor_covariance(risk_factors)
        
        # ä¼°è®¡ç‰¹å¼‚é£é™©
        specific_risk = self._estimate_specific_risk(returns_matrix, factor_loadings, risk_factors)
        
        self.risk_model_results = {
            'factor_loadings': factor_loadings,
            'factor_covariance': factor_covariance,
            'specific_risk': specific_risk,
            'risk_factors': risk_factors
        }
        
        logger.info("é£é™©æ¨¡å‹æ„å»ºå®Œæˆ")
        return self.risk_model_results
    
    def _build_risk_factors(self, returns_matrix: pd.DataFrame) -> pd.DataFrame:
        """[P1] æ„å»ºé£é™©å› å­ - çœŸå®Bã€Fã€Sæ•°æ®ï¼ˆè¡Œä¸š/å›½å®¶/é£æ ¼çº¦æŸï¼‰"""
        factors = pd.DataFrame(index=returns_matrix.index)
        tickers = returns_matrix.columns.tolist()
        
        # 1. å¸‚åœºå› å­
        factors['market'] = returns_matrix.mean(axis=1)
        
        # 2. [ENHANCED] P1 è§„æ¨¡å› å­ (ä½¿ç”¨çœŸå®å¸‚å€¼æ•°æ®)
        size_factor = self._build_size_factor(tickers, returns_matrix.index)
        if size_factor is not None:
            factors['size'] = size_factor
        else:
            factors['size'] = self._build_mock_size_factor(returns_matrix)
        
        # 3. [ENHANCED] P1 ä»·å€¼å› å­ (å¸‚å‡€ç‡ã€å¸‚ç›ˆç‡)
        value_factor = self._build_value_factor(tickers, returns_matrix.index)
        if value_factor is not None:
            factors['value'] = value_factor
        else:
            factors['value'] = self._build_mock_value_factor(returns_matrix)
        
        # 4. [ENHANCED] P1 è´¨é‡å› å­ (ROEã€æ¯›åˆ©ç‡ã€è´¢åŠ¡å¥åº·åº¦)
        quality_factor = self._build_quality_factor(tickers, returns_matrix.index)
        if quality_factor is not None:
            factors['quality'] = quality_factor
        else:
            factors['quality'] = self._build_mock_quality_factor(returns_matrix)
        
        # 5. [ENHANCED] P1 Betaå› å­ (å¸‚åœºæ•æ„Ÿæ€§)
        beta_factor = self._build_beta_factor(returns_matrix)
        factors['beta'] = beta_factor
        
        # 6. [ENHANCED] P1 åŠ¨é‡å› å­ (ä»·æ ¼åŠ¨é‡)
        momentum_factor = self._build_momentum_factor(tickers, returns_matrix.index)
        if momentum_factor is not None:
            factors['momentum'] = momentum_factor
        else:
            factors['momentum'] = self._build_mock_momentum_factor(returns_matrix)
        
        # 7. [ENHANCED] P1 æ³¢åŠ¨ç‡å› å­ (å†å²æ³¢åŠ¨ç‡)
        volatility_factor = self._build_volatility_factor(returns_matrix)
        factors['volatility'] = volatility_factor
        
        # 8. [ENHANCED] P1 è¡Œä¸šå› å­ (ä»çœŸå®å…ƒæ•°æ®æ„å»º)
        industry_factors = self._build_industry_factors(tickers, returns_matrix.index)
        for industry_name, industry_factor in industry_factors.items():
            factors[f'industry_{industry_name}'] = industry_factor
        
        # æ ‡å‡†åŒ–å› å­ï¼ˆç¡®ä¿æ•°å€¼ç¨³å®šæ€§ï¼‰
        for col in factors.columns:
            if col != 'market':  # ä¿æŒå¸‚åœºå› å­ä¸å˜
                factors[col] = (factors[col] - factors[col].mean()) / (factors[col].std() + 1e-8)
        
        logger.info(f"é£é™©å› å­æ„å»ºå®Œæˆï¼ŒåŒ…å«{len(factors.columns)}ä¸ªå› å­: {list(factors.columns)}")
        return factors
    
    def _build_size_factor(self, tickers: List[str], date_index: pd.DatetimeIndex) -> Optional[pd.Series]:
        """æ„å»ºçœŸå®çš„è§„æ¨¡å› å­"""
        try:
            if MARKET_MANAGER_AVAILABLE:
                manager = UnifiedMarketDataManager()
                size_data = []
                
                for date in date_index:
                    daily_sizes = []
                    for ticker in tickers:
                        try:
                            stock_info = manager.get_stock_info(ticker)
                            if stock_info and stock_info.market_cap:
                                # ä½¿ç”¨logå¸‚å€¼ä½œä¸ºè§„æ¨¡å› å­
                                daily_sizes.append(np.log(stock_info.market_cap))
                            else:
                                # ä»åŸå§‹æ•°æ®ä¼°ç®—
                                if ticker in self.raw_data:
                                    hist_data = self.raw_data[ticker]
                                    if len(hist_data) > 0:
                                        # ä½¿ç”¨ä»·æ ¼*volumeä½œä¸ºä»£ç†
                                        latest = hist_data.iloc[-1]
                                        market_proxy = latest['close'] * latest['volume']
                                        daily_sizes.append(np.log(max(market_proxy, 1e6)))
                        except Exception:
                            daily_sizes.append(np.log(1e8))  # é»˜è®¤å€¼
                    
                    if daily_sizes:
                        # æ ‡å‡†åŒ–ï¼šå¤§å…¬å¸ä¸ºæ­£å€¼ï¼Œå°å…¬å¸ä¸ºè´Ÿå€¼
                        sizes_array = np.array(daily_sizes)
                        size_factor_value = np.mean(sizes_array) - np.median(sizes_array)
                        size_data.append(size_factor_value)
                    else:
                        size_data.append(0.0)
                
                return pd.Series(size_data, index=date_index, name='size_factor')
            return None
        except Exception as e:
            logger.warning(f"æ„å»ºçœŸå®è§„æ¨¡å› å­å¤±è´¥: {e}")
            return None
    
    def _build_mock_size_factor(self, returns_matrix: pd.DataFrame) -> pd.Series:
        """æ„å»ºæ¨¡æ‹Ÿè§„æ¨¡å› å­ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        # ä½¿ç”¨å†å²æ³¢åŠ¨ç‡ä½œä¸ºè§„æ¨¡ä»£ç†ï¼ˆå°å…¬å¸é€šå¸¸æ³¢åŠ¨æ›´å¤§ï¼‰
        volatilities = returns_matrix.rolling(window=20).std().mean(axis=1)
        return -volatilities.fillna(0)  # è´Ÿå·ï¼šä½æ³¢åŠ¨ç‡=å¤§å…¬å¸
    
    def _build_value_factor(self, tickers: List[str], date_index: pd.DatetimeIndex) -> Optional[pd.Series]:
        """æ„å»ºçœŸå®çš„ä»·å€¼å› å­"""
        # è¿™é‡Œåº”è¯¥æ¥å…¥çœŸå®çš„åŸºæœ¬é¢æ•°æ®ï¼ˆP/E, P/Bç­‰ï¼‰
        # æš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿå®ç°
        return None
    
    def _build_mock_value_factor(self, returns_matrix: pd.DataFrame) -> pd.Series:
        """æ„å»ºæ¨¡æ‹Ÿä»·å€¼å› å­"""
        # ä½¿ç”¨åè½¬æ•ˆåº”ä½œä¸ºä»·å€¼ä»£ç†
        long_term_returns = returns_matrix.rolling(window=252).mean().mean(axis=1)
        return -long_term_returns.fillna(0)  # è´Ÿå·ï¼šä½é•¿æœŸæ”¶ç›Š=ä»·å€¼è‚¡
    
    def _build_quality_factor(self, tickers: List[str], date_index: pd.DatetimeIndex) -> Optional[pd.Series]:
        """æ„å»ºçœŸå®çš„è´¨é‡å› å­"""
        # è¿™é‡Œåº”è¯¥æ¥å…¥ROEã€æ¯›åˆ©ç‡ç­‰è´¢åŠ¡æ•°æ®
        return None
    
    def _build_mock_quality_factor(self, returns_matrix: pd.DataFrame) -> pd.Series:
        """æ„å»ºæ¨¡æ‹Ÿè´¨é‡å› å­"""
        # ä½¿ç”¨æ”¶ç›Šç¨³å®šæ€§ä½œä¸ºè´¨é‡ä»£ç†
        return_stability = 1.0 / (returns_matrix.rolling(window=60).std().mean(axis=1) + 1e-8)
        return return_stability.fillna(0)
    
    def _build_beta_factor(self, returns_matrix: pd.DataFrame) -> pd.Series:
        """æ„å»ºBetaå› å­"""
        market_returns = returns_matrix.mean(axis=1)
        betas = []
        
        for date in returns_matrix.index:
            # ä½¿ç”¨æ»šåŠ¨çª—å£è®¡ç®—Beta
            end_idx = returns_matrix.index.get_loc(date)
            start_idx = max(0, end_idx - 60)  # 60å¤©çª—å£
            
            if start_idx < end_idx:
                period_data = returns_matrix.iloc[start_idx:end_idx]
                period_market = market_returns.iloc[start_idx:end_idx]
                
                # è®¡ç®—å„è‚¡ç¥¨ç›¸å¯¹å¸‚åœºçš„å¹³å‡Beta
                stock_betas = []
                for ticker in period_data.columns:
                    try:
                        cov = np.cov(period_data[ticker], period_market)[0, 1]
                        var_market = np.var(period_market)
                        if var_market > 1e-8:
                            beta = cov / var_market
                            stock_betas.append(beta)
                    except:
                        stock_betas.append(1.0)
                
                betas.append(np.mean(stock_betas) if stock_betas else 1.0)
            else:
                betas.append(1.0)
        
        return pd.Series(betas, index=returns_matrix.index, name='beta_factor')
    
    def _build_momentum_factor(self, tickers: List[str], date_index: pd.DatetimeIndex) -> Optional[pd.Series]:
        """æ„å»ºçœŸå®çš„åŠ¨é‡å› å­"""
        try:
            momentum_data = []
            for date in date_index:
                daily_momentums = []
                for ticker in tickers:
                    if ticker in self.raw_data:
                        hist_data = self.raw_data[ticker]
                        # è®¡ç®—12-1æœˆåŠ¨é‡
                        if len(hist_data) >= 252:
                            current_price = hist_data['close'].iloc[-1]
                            past_price = hist_data['close'].iloc[-252]
                            momentum = (current_price / past_price) - 1
                            daily_momentums.append(momentum)
                
                if daily_momentums:
                    momentum_data.append(np.mean(daily_momentums))
                else:
                    momentum_data.append(0.0)
            
            return pd.Series(momentum_data, index=date_index, name='momentum_factor')
        except Exception as e:
            logger.warning(f"æ„å»ºçœŸå®åŠ¨é‡å› å­å¤±è´¥: {e}")
            return None
    
    def _build_mock_momentum_factor(self, returns_matrix: pd.DataFrame) -> pd.Series:
        """æ„å»ºæ¨¡æ‹ŸåŠ¨é‡å› å­"""
        # ä½¿ç”¨é•¿æœŸè¶‹åŠ¿ä½œä¸ºåŠ¨é‡ä»£ç†
        long_momentum = returns_matrix.rolling(window=126).mean().mean(axis=1)
        return long_momentum.fillna(0)
    
    def _build_volatility_factor(self, returns_matrix: pd.DataFrame) -> pd.Series:
        """æ„å»ºæ³¢åŠ¨ç‡å› å­"""
        volatility = returns_matrix.rolling(window=20).std().mean(axis=1)
        return volatility.fillna(0)
    
    def _build_industry_factors(self, tickers: List[str], date_index: pd.DatetimeIndex) -> Dict[str, pd.Series]:
        """æ„å»ºè¡Œä¸šå› å­ï¼ˆæ¥è‡ªçœŸå®å…ƒæ•°æ®ï¼‰"""
        industry_factors = {}
        
        try:
            # ä»åŸå§‹æ•°æ®è·å–è¡Œä¸šä¿¡æ¯
            ticker_industries = {}
            for ticker in tickers:
                if ticker in self.raw_data and len(self.raw_data[ticker]) > 0:
                    sector = self.raw_data[ticker].iloc[-1].get('SECTOR', 'Technology')
                    ticker_industries[ticker] = sector
            
            # è·å–æ‰€æœ‰è¡Œä¸š
            unique_industries = list(set(ticker_industries.values()))
            
            for industry in unique_industries:
                if industry and industry != 'Unknown':
                    # åˆ é™¤æ¨¡æ‹Ÿè¡Œä¸šå› å­æ•°æ® - æ— æ³•è·å–çœŸå®æ•°æ®
                    # è·³è¿‡è¡Œä¸šå› å­æ„å»ºï¼Œé¿å…ä½¿ç”¨éšæœºæ¨¡æ‹Ÿæ•°æ®
                    logger.debug(f"è·³è¿‡è¡Œä¸šå› å­æ„å»º: {industry} (ç¼ºå°‘çœŸå®è¡Œä¸šæ”¶ç›Šæ•°æ®)")
            
            logger.info(f"æ„å»ºäº†{len(industry_factors)}ä¸ªè¡Œä¸šå› å­: {list(industry_factors.keys())}")
            
        except Exception as e:
            logger.warning(f"æ„å»ºè¡Œä¸šå› å­å¤±è´¥: {e}")
        
        # ğŸ”¥ FIX: è¿”å›ç©ºè¡Œä¸šå› å­è€Œä¸æ˜¯éšæœºæ•°æ®
        if not industry_factors:
            # è¿”å›é›¶å› å­è€Œä¸æ˜¯éšæœºæ•°
            industry_factors['neutral'] = pd.Series(
                np.zeros(len(date_index)), 
                index=date_index, name='industry_neutral'
            )
            logger.warning("No industry factors available, using neutral (zero) factor")
        
        return industry_factors
    
    def _create_earnings_window_dummy(self, date_index: pd.DatetimeIndex, ticker: str, days: int = 5) -> pd.Series:
        """[ENHANCED] P2 åˆ›å»ºè´¢æŠ¥/å…¬å‘Šçª—å£dummyå˜é‡"""
        try:
            # è´¢æŠ¥å‘å¸ƒé€šå¸¸åœ¨å­£åº¦ç»“æŸåçš„45å¤©å†…
            earnings_dates = []
            
            # ä¼°ç®—å­£åº¦è´¢æŠ¥æ—¥æœŸï¼ˆå®é™…ä¸­åº”ä»è´¢æŠ¥æ—¥å†APIè·å–ï¼‰
            for year in range(2023, 2025):
                for quarter_end in ['03-31', '06-30', '09-30', '12-31']:
                    try:
                        quarter_date = pd.to_datetime(f'{year}-{quarter_end}')
                        # è´¢æŠ¥é€šå¸¸åœ¨å­£åº¦ç»“æŸå30-45å¤©å‘å¸ƒ
                        for offset_days in [30, 35, 40, 45]:
                            earnings_date = quarter_date + pd.Timedelta(days=offset_days)
                            earnings_dates.append(earnings_date)
                    except:
                        continue
            
            # ä¸ºæ¯ä¸ªæ—¥æœŸè®¡ç®—æ˜¯å¦åœ¨è´¢æŠ¥çª—å£å†…
            window_flags = []
            for date in date_index:
                is_earnings_window = False
                for earnings_date in earnings_dates:
                    if abs(int((date - earnings_date) / pd.Timedelta(days=1))) <= days:
                        is_earnings_window = True
                        break
                window_flags.append(int(is_earnings_window))
            
            return pd.Series(window_flags, index=date_index, name=f'earnings_window_{days}')
            
        except Exception as e:
            logger.warning(f"åˆ›å»ºè´¢æŠ¥çª—å£dummyå¤±è´¥ {ticker}: {e}")
            # å›é€€åˆ°å…¨é›¶ï¼ˆæ— è´¢æŠ¥çª—å£ä¿¡æ¯ï¼‰
            return pd.Series(
                np.zeros(len(date_index), dtype=int),  # Use zeros instead of random
                index=date_index, 
                name=f'earnings_window_{days}'
            )
        
        # åŸå§‹è§„æ¨¡å› å­ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå‚è€ƒï¼‰
        try:
            if self.market_data_manager is not None:
                # æ„å»ºç»Ÿä¸€ç‰¹å¾DataFrameï¼Œè·å–çœŸå®å¸‚å€¼æ•°æ®
                tickers = returns_matrix.columns.tolist()
                dates = returns_matrix.index.tolist()
                
                # åˆ›å»ºç”¨äºUMDMçš„è¾“å…¥DataFrame
                input_data = []
                for date in dates:
                    for ticker in tickers:
                        input_data.append({'date': date, 'ticker': ticker})
                
                if input_data:
                    input_df = pd.DataFrame(input_data)
                    features_df = self.market_data_manager.create_unified_features_dataframe(input_df)
                    
                    if 'free_float_market_cap' in features_df.columns:
                        # é‡å¡‘ä¸º[date, ticker]æ ¼å¼å¹¶å¯¹é½
                        features_pivot = features_df.set_index(['date', 'ticker'])['free_float_market_cap']
                        
                        #  ä¿®å¤æ—¶é—´æ³„éœ²ï¼šSizeå› å­ä½¿ç”¨å‰æœŸå¸‚å€¼åˆ†ç»„å½“æœŸæ”¶ç›Š
                        size_factor = []
                        dates_list = list(returns_matrix.index)
                        
                        for i, date in enumerate(dates_list):
                            try:
                                #  å…³é”®ä¿®å¤ï¼šä½¿ç”¨T-1æœŸçš„å¸‚å€¼è¿›è¡Œåˆ†ç»„ï¼Œè®¡ç®—TæœŸæ”¶ç›Š
                                if i == 0:
                                    # ç¬¬ä¸€ä¸ªæ—¥æœŸæ²¡æœ‰å‰æœŸæ•°æ®ï¼Œè·³è¿‡
                                    size_factor.append(0.0)
                                    continue
                                
                                prev_date = dates_list[i-1]
                                prev_date_caps = features_pivot.loc[prev_date]  # ä½¿ç”¨å‰ä¸€æœŸå¸‚å€¼
                                prev_date_caps = prev_date_caps.reindex(returns_matrix.columns)
                                
                                if prev_date_caps.notna().sum() > 2:  # è‡³å°‘éœ€è¦3åªè‚¡ç¥¨æœ‰å¸‚å€¼æ•°æ®
                                    cap_median = prev_date_caps.median()
                                    small_cap_mask = prev_date_caps < cap_median
                                    large_cap_mask = ~small_cap_mask
                                    
                                    # ä½¿ç”¨å½“æœŸæ”¶ç›Šç‡ï¼Œä½†åˆ†ç»„åŸºäºå‰æœŸå¸‚å€¼
                                    date_returns = returns_matrix.loc[date]
                                    small_ret = date_returns[small_cap_mask].mean()
                                    large_ret = date_returns[large_cap_mask].mean()
                                    
                                    size_factor.append(small_ret - large_ret)
                                    
                                    logger.debug(f"æ—¥æœŸ{date}: ä½¿ç”¨{prev_date}å¸‚å€¼åˆ†ç»„ï¼Œ"
                                               f"å°ç›˜è‚¡æ”¶ç›Š{small_ret:.4f}, å¤§ç›˜è‚¡æ”¶ç›Š{large_ret:.4f}")
                                else:
                                    size_factor.append(0.0)
                            except (KeyError, IndexError):
                                size_factor.append(0.0)
                        
                        factors['size'] = pd.Series(size_factor, index=returns_matrix.index)
                        logger.info("ä½¿ç”¨UMDMçœŸå®å¸‚å€¼æ•°æ®æ„å»ºSizeå› å­")
                    else:
                        logger.warning("UMDMä¸­ç¼ºå°‘free_float_market_capå­—æ®µï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                        raise ValueError("No market cap data available")
                else:
                    raise ValueError("No input data for UMDM")
            else:
                raise ValueError("UMDM not available")
                
        except (ValueError, KeyError, IndexError) as e:
            logger.exception(f"UMDM Sizeå› å­æ„å»ºå¤±è´¥: {e}, ä½¿ç”¨ç®€åŒ–å›é€€æ–¹æ¡ˆ")
            self.health_metrics['risk_model_failures'] += 1
            # å›é€€æ–¹æ¡ˆï¼šåŸºäºæˆäº¤é‡ä¼°ç®—è§„æ¨¡
            try:
                volume_data = {}
                for ticker in returns_matrix.columns:
                    if ticker in self.raw_data and 'volume' in self.raw_data[ticker].columns:
                        # ä½¿ç”¨æœ€è¿‘60å¤©å¹³å‡æˆäº¤é‡ä½œä¸ºè§„æ¨¡ä»£ç†
                        recent_volume = self.raw_data[ticker]['volume'].tail(60).mean()
                        volume_data[ticker] = recent_volume

                if volume_data:
                    volume_series = pd.Series(volume_data)
                    volume_median = volume_series.median()
                    small_vol_mask = volume_series < volume_median

                    small_vol_returns = returns_matrix.loc[:, small_vol_mask].mean(axis=1)
                    large_vol_returns = returns_matrix.loc[:, ~small_vol_mask].mean(axis=1)
                    factors['size'] = small_vol_returns - large_vol_returns
                    logger.info("ä½¿ç”¨æˆäº¤é‡ä»£ç†æ„å»ºSizeå› å­ï¼ˆå›é€€æ–¹æ¡ˆï¼‰")
                else:
                    # æœ€ç»ˆå›é€€ï¼šä½¿ç”¨é›¶å€¼
                    factors['size'] = 0.0
                    logger.warning("æ— æ³•æ„å»ºSizeå› å­ï¼Œä½¿ç”¨é›¶å€¼")
            except Exception as fallback_error:
                logger.error(f"Sizeå› å­å›é€€æ–¹æ¡ˆä¹Ÿå¤±è´¥: {fallback_error}")
                factors['size'] = 0.0
        
        # 3. åŠ¨é‡å› å­
        momentum_scores = {}
        for ticker in returns_matrix.columns:
            momentum_scores[ticker] = returns_matrix[ticker].rolling(252).sum().shift(21)
        
        momentum_df = pd.DataFrame(momentum_scores)
        high_momentum = momentum_df.rank(axis=1, pct=True) > 0.7
        low_momentum = momentum_df.rank(axis=1, pct=True) < 0.3
        
        factors['momentum'] = returns_matrix.where(high_momentum).mean(axis=1) - \
                             returns_matrix.where(low_momentum).mean(axis=1)
        
        # 4. æ³¢åŠ¨ç‡å› å­
        volatility_scores = returns_matrix.rolling(60).std()
        low_vol = volatility_scores.rank(axis=1, pct=True) < 0.3
        high_vol = volatility_scores.rank(axis=1, pct=True) > 0.7
        
        factors['volatility'] = returns_matrix.where(low_vol).mean(axis=1) - \
                               returns_matrix.where(high_vol).mean(axis=1)
        
        # 5. è´¨é‡å› å­
        quality_scores = returns_matrix.rolling(60).mean() / returns_matrix.rolling(60).std()
        high_quality = quality_scores.rank(axis=1, pct=True) > 0.7
        low_quality = quality_scores.rank(axis=1, pct=True) < 0.3
        
        factors['quality'] = returns_matrix.where(high_quality).mean(axis=1) - \
                            returns_matrix.where(low_quality).mean(axis=1)
        
        # 6. åè½¬å› å­
        reversal_scores = returns_matrix.rolling(21).sum()
        high_reversal = reversal_scores.rank(axis=1, pct=True) < 0.3
        low_reversal = reversal_scores.rank(axis=1, pct=True) > 0.7
        
        factors['reversal'] = returns_matrix.where(high_reversal).mean(axis=1) - \
                             returns_matrix.where(low_reversal).mean(axis=1)
        
        # æ ‡å‡†åŒ–å› å­
        factors = factors.fillna(0)
        for col in factors.columns:
            factors[col] = (factors[col] - factors[col].mean()) / (factors[col].std() + 1e-8)
        
        return factors
    
    def _estimate_factor_loadings(self, returns_matrix: pd.DataFrame, 
                                 risk_factors: pd.DataFrame) -> pd.DataFrame:
        """ä¼°è®¡å› å­è½½è·"""
        loadings = {}
        
        for ticker in returns_matrix.columns:
            stock_returns = returns_matrix[ticker].dropna()
            aligned_factors = risk_factors.loc[stock_returns.index].dropna().fillna(0)
            
            if len(stock_returns) < 50 or len(aligned_factors) < 50:
                loadings[ticker] = np.zeros(len(risk_factors.columns))
                continue
            
            try:
                # ç¡®ä¿æ•°æ®é•¿åº¦åŒ¹é…
                min_len = min(len(stock_returns), len(aligned_factors))
                stock_returns = stock_returns.iloc[:min_len]
                aligned_factors = aligned_factors.iloc[:min_len]
                
                # ä½¿ç”¨ç¨³å¥å›å½’ä¼°è®¡è½½è·
                model = HuberRegressor(epsilon=1.35, alpha=0.0001)
                model.fit(aligned_factors.values, stock_returns.values)
                
                loadings[ticker] = model.coef_
                
            except Exception as e:
                logger.warning(f"Failed to estimate loadings for {ticker}: {e}")
                loadings[ticker] = np.zeros(len(risk_factors.columns))
        
        loadings_df = pd.DataFrame(loadings, index=risk_factors.columns).T
        return loadings_df
    
    def _estimate_factor_covariance(self, risk_factors: pd.DataFrame) -> pd.DataFrame:
        """ä¼°è®¡å› å­åæ–¹å·®çŸ©é˜µ"""
        # ä½¿ç”¨Ledoit-Wolfæ”¶ç¼©ä¼°è®¡
        cov_estimator = LedoitWolf()
        factor_cov_matrix = cov_estimator.fit(risk_factors.fillna(0)).covariance_
        
        # ç¡®ä¿æ­£å®šæ€§
        eigenvals, eigenvecs = np.linalg.eigh(factor_cov_matrix)
        eigenvals = np.maximum(eigenvals, 1e-8)
        factor_cov_matrix = eigenvecs @ np.diag(eigenvals) @ eigenvecs.T
        
        return pd.DataFrame(factor_cov_matrix, 
                           index=risk_factors.columns, 
                           columns=risk_factors.columns)
    
    def _estimate_specific_risk(self, returns_matrix: pd.DataFrame,
                               factor_loadings: pd.DataFrame, 
                               risk_factors: pd.DataFrame) -> pd.Series:
        """ä¼°è®¡ç‰¹å¼‚é£é™©"""
        specific_risks = {}
        
        for ticker in returns_matrix.columns:
            if ticker not in factor_loadings.index:
                specific_risks[ticker] = 0.2  # é»˜è®¤ç‰¹å¼‚é£é™©
                continue
            
            stock_returns = returns_matrix[ticker].dropna()
            loadings = factor_loadings.loc[ticker]
            aligned_factors = risk_factors.loc[stock_returns.index].fillna(0)
            
            if len(stock_returns) < 50:
                specific_risks[ticker] = 0.2
                continue
            
            # è®¡ç®—æ®‹å·®
            min_len = min(len(stock_returns), len(aligned_factors))
            factor_returns = (aligned_factors.iloc[:min_len] @ loadings).values
            residuals = stock_returns.iloc[:min_len].values - factor_returns
            
            # ç‰¹å¼‚é£é™©ä¸ºæ®‹å·®æ ‡å‡†å·®
            specific_var = np.nan_to_num(np.var(residuals), nan=0.04)
            specific_risks[ticker] = np.sqrt(specific_var)
        
        return pd.Series(specific_risks)
    
    def detect_market_regime(self) -> MarketRegime:
        """æ£€æµ‹å¸‚åœºçŠ¶æ€ï¼ˆæ¥è‡ªProfessionalå¼•æ“ï¼‰"""
        logger.info("æ£€æµ‹å¸‚åœºçŠ¶æ€")
        
        if not self.raw_data:
            return MarketRegime(0, "Unknown", 0.5, {'volatility': 0.2, 'trend': 0.0})
        
        # æ„å»ºå¸‚åœºæŒ‡æ•°
        market_returns = []
        for ticker, data in self.raw_data.items():
            if len(data) > 100:
                # âœ… FIX: å…¼å®¹'Close'å’Œ'close'åˆ—å
                price_col = None
                if 'Close' in data.columns:
                    price_col = 'Close'
                elif 'close' in data.columns:
                    price_col = 'close'
                elif 'CLOSE' in data.columns:
                    price_col = 'CLOSE'
                
                if price_col:
                    returns = data[price_col].pct_change().fillna(0)
                    market_returns.append(returns)
                else:
                    logger.warning(f"Missing Close price column for {ticker}, available columns: {list(data.columns)[:5]}...")
        
        if not market_returns:
            logger.warning("æ— æ³•è·å–ä»»ä½•æœ‰æ•ˆä»·æ ¼æ•°æ®ï¼Œé»˜è®¤ä¸ºä½æ³¢åŠ¨çŠ¶æ€")
            return MarketRegime(1, "ä½æ³¢åŠ¨", 0.8, {'volatility': 0.15, 'trend': 0.0})
        
        market_index = pd.concat(market_returns, axis=1).mean(axis=1).dropna()
        
        if len(market_index) < 100:
            return MarketRegime(1, "Normal", 1.0, {'volatility': 0.15, 'trend': 0.0})
        
        # åŸºäºæ³¢åŠ¨ç‡å’Œè¶‹åŠ¿çš„çŠ¶æ€æ£€æµ‹
        rolling_vol = market_index.rolling(21).std()
        rolling_trend = market_index.rolling(21).mean()
        
        # å®šä¹‰çŠ¶æ€é˜ˆå€¼
        vol_low = rolling_vol.quantile(0.33)
        vol_high = rolling_vol.quantile(0.67)
        trend_low = rolling_trend.quantile(0.33)
        trend_high = rolling_trend.quantile(0.67)
        
        # å½“å‰çŠ¶æ€
        current_vol = rolling_vol.iloc[-1]
        current_trend = rolling_trend.iloc[-1]
        
        if current_vol < vol_low:
            if current_trend > trend_high:
                regime = MarketRegime(1, "Bull_Low_Vol", 0.8, 
                                    {'volatility': current_vol, 'trend': current_trend})
            elif current_trend < trend_low:
                regime = MarketRegime(2, "Bear_Low_Vol", 0.8,
                                    {'volatility': current_vol, 'trend': current_trend})
            else:
                regime = MarketRegime(3, "Normal_Low_Vol", 0.8,
                                    {'volatility': current_vol, 'trend': current_trend})
        elif current_vol > vol_high:
            if current_trend > trend_high:
                regime = MarketRegime(4, "Bull_High_Vol", 0.8,
                                    {'volatility': current_vol, 'trend': current_trend})
            elif current_trend < trend_low:
                regime = MarketRegime(5, "Bear_High_Vol", 0.8,
                                    {'volatility': current_vol, 'trend': current_trend})
            else:
                regime = MarketRegime(6, "Volatile", 0.8,
                                    {'volatility': current_vol, 'trend': current_trend})
        else:
            regime = MarketRegime(0, "Normal", 0.7,
                                {'volatility': current_vol, 'trend': current_trend})
        
        self.current_regime = regime
        logger.info(f"æ£€æµ‹åˆ°å¸‚åœºçŠ¶æ€: {regime.name} (æ¦‚ç‡: {regime.probability:.2f})")
        
        return regime
    
    def _get_regime_alpha_weights(self, regime: MarketRegime) -> Dict[str, float]:
        """æ ¹æ®å¸‚åœºçŠ¶æ€è°ƒæ•´Alphaæƒé‡ï¼ˆæ¥è‡ªProfessionalå¼•æ“ï¼‰"""
        if "Bull" in regime.name:
            # ç‰›å¸‚ï¼šåå¥½åŠ¨é‡
            return {
                'momentum_21d': 2.0, 'momentum_63d': 2.5, 'momentum_126d': 2.0,
                'reversion_5d': 0.5, 'reversion_10d': 0.5, 'reversion_21d': 0.5,
                'volatility_factor': 1.0, 'volume_trend': 1.5, 'quality_factor': 1.0
            }
        elif "Bear" in regime.name:
            # ç†Šå¸‚ï¼šåå¥½è´¨é‡å’Œé˜²å¾¡
            return {
                'momentum_21d': 0.5, 'momentum_63d': 0.5, 'momentum_126d': 1.0,
                'reversion_5d': 1.5, 'reversion_10d': 2.0, 'reversion_21d': 1.5,
                'volatility_factor': 2.0, 'volume_trend': 0.5, 'quality_factor': 2.0
            }
        elif "Volatile" in regime.name:
            # é«˜æ³¢åŠ¨ï¼šåå¥½å‡å€¼å›å½’
            return {
                'momentum_21d': 0.5, 'momentum_63d': 1.0, 'momentum_126d': 1.0,
                'reversion_5d': 2.5, 'reversion_10d': 2.0, 'reversion_21d': 1.5,
                'volatility_factor': 2.5, 'volume_trend': 1.0, 'quality_factor': 1.5
            }
        else:
            # æ­£å¸¸å¸‚åœºï¼šå‡è¡¡æƒé‡
            return {col: 1.0 for col in [
                'momentum_21d', 'momentum_63d', 'momentum_126d',
                'reversion_5d', 'reversion_10d', 'reversion_21d',
                'volatility_factor', 'volume_trend', 'quality_factor'
            ]}
    
    def _generate_base_predictions(self, training_results: Dict[str, Any]) -> pd.Series:
        """ç”ŸæˆåŸºç¡€é¢„æµ‹ç»“æœ"""
        try:
            if not training_results:
                logger.warning("è®­ç»ƒç»“æœä¸ºç©º")
                return pd.Series()
            
            # å°è¯•ä»ä¸åŒçš„è®­ç»ƒç»“æœä¸­æå–é¢„æµ‹
            prediction_sources = [
                ('traditional_models', 'models'),
                ('learning_to_rank', 'predictions'), 
                ('stacking', 'predictions'),
                ('regime_aware', 'predictions')
            ]
            
            for source_key, pred_key in prediction_sources:
                if source_key in training_results:
                    source_data = training_results[source_key]
                    if isinstance(source_data, dict):
                        # ä¼ ç»ŸMLæ¨¡å‹ç»“æœå¤„ç†
                        if source_key == 'traditional_models' and source_data.get('success', False):
                            models = source_data.get('models', {})
                            best_model = source_data.get('best_model')
                            if best_model and best_model in models:
                                model_data = models[best_model]
                                if 'predictions' in model_data:
                                    predictions = model_data['predictions']
                                    if hasattr(predictions, '__len__') and len(predictions) > 0:
                                        logger.info(f"ä»{best_model}æ¨¡å‹æå–é¢„æµ‹ï¼Œé•¿åº¦: {len(predictions)}")
                                        return pd.Series(predictions)
                        
                        # å…¶ä»–é¢„æµ‹ç»“æœå¤„ç†
                        elif pred_key in source_data:
                            predictions = source_data[pred_key]
                            if isinstance(predictions, (pd.Series, np.ndarray)) and len(predictions) > 0:
                                logger.info(f"ä»{source_key}æå–é¢„æµ‹ï¼Œé•¿åº¦: {len(predictions)}")
                                return pd.Series(predictions) if isinstance(predictions, np.ndarray) else predictions
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆé¢„æµ‹ï¼Œç”ŸæˆåŸºäºéšæœºçš„ç®€å•é¢„æµ‹
            logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆé¢„æµ‹ï¼Œç”Ÿæˆç®€å•é¢„æµ‹ä¿¡å·")
            n_samples = 100  # é»˜è®¤æ ·æœ¬æ•°
            
            # å°è¯•ä»training_resultsä¸­è·å–å®é™…æ ·æœ¬æ•°
            for key in ['traditional_models', 'regime_aware', 'stacking']:
                if key in training_results and isinstance(training_results[key], dict):
                    data = training_results[key]
                    if 'n_samples' in data:
                        n_samples = data['n_samples']
                        break
            
            # ç”Ÿæˆä¸­æ€§é¢„æµ‹ä¿¡å·ï¼ˆæ— åå¥½ï¼‰
            # Use neutral signal instead of random
            predictions = pd.Series(np.zeros(n_samples))  # Neutral predictions
            logger.info(f"ç”Ÿæˆç®€å•é¢„æµ‹ä¿¡å·ï¼Œé•¿åº¦: {len(predictions)}")
            return predictions
                
        except Exception as e:
            logger.error(f"åŸºç¡€é¢„æµ‹ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return pd.Series()
    
    def generate_enhanced_predictions(self, training_results: Dict[str, Any], 
                                    market_regime: MarketRegime) -> pd.Series:
        """ç”ŸæˆRegime-Awareçš„å¢å¼ºé¢„æµ‹"""
        try:
            logger.info("å¼€å§‹ç”Ÿæˆå¢å¼ºé¢„æµ‹...")
            
            # è·å–åŸºç¡€é¢„æµ‹
            base_predictions = self._generate_base_predictions(training_results)
            logger.info(f"åŸºç¡€é¢„æµ‹ç”Ÿæˆå®Œæˆï¼Œç±»å‹: {type(base_predictions)}, é•¿åº¦: {len(base_predictions) if base_predictions is not None else 'None'}")
            
            if base_predictions is None or len(base_predictions) == 0:
                logger.error("åŸºç¡€é¢„æµ‹ä¸ºç©ºæˆ–None")
                return pd.Series()
            
            if not ENHANCED_MODULES_AVAILABLE or not self.alpha_engine:
                # å¦‚æœæ²¡æœ‰å¢å¼ºæ¨¡å—ï¼Œåº”ç”¨regimeæƒé‡åˆ°åŸºç¡€é¢„æµ‹
                regime_weights = self._get_regime_alpha_weights(market_regime)
                # ç®€å•åº”ç”¨æƒé‡ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                # å®‰å…¨çš„è°ƒæ•´å› å­è®¡ç®—ï¼Œé˜²æ­¢ç±»å‹é”™è¯¯
                try:
                    adjustment_factor = sum(regime_weights.values()) / len(regime_weights)
                except (TypeError, ZeroDivisionError) as e:
                    logger.warning(f"è°ƒæ•´å› å­è®¡ç®—å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤å€¼1.0")
                    adjustment_factor = 1.0
                enhanced_predictions = base_predictions * adjustment_factor
                logger.info(f"åº”ç”¨ç®€åŒ–çš„regimeè°ƒæ•´ï¼Œè°ƒæ•´å› å­: {adjustment_factor:.3f}")
                return enhanced_predictions
            
            # å¦‚æœæœ‰Alphaå¼•æ“ï¼Œç”ŸæˆAlphaä¿¡å·
            try:
                logger.info("å‡†å¤‡Alphaæ•°æ®...")
                # ä¸ºAlphaå¼•æ“å‡†å¤‡æ•°æ®ï¼ˆåŒ…å«æ ‡å‡†åŒ–çš„ä»·æ ¼åˆ—ï¼‰
                alpha_input = self._prepare_alpha_data()
                logger.info(f"Alphaè¾“å…¥æ•°æ®å½¢çŠ¶: {alpha_input.shape if alpha_input is not None else 'None'}")
                
                # è®¡ç®—Alphaå› å­ï¼ˆç­¾ååªæ¥å—dfï¼‰
                from .unified_result_framework import OperationResult, ResultStatus, alpha_signals_validation
                
                alpha_signals = self.alpha_engine.compute_all_alphas(alpha_input)
                
                # ğŸ¯ FIX: ä½¿ç”¨ç»Ÿä¸€ç»“æœæ¡†æ¶éªŒè¯å’Œè®°å½•
                if alpha_signals_validation(alpha_signals):
                    result = OperationResult(
                        status=ResultStatus.SUCCESS,
                        data=alpha_signals,
                        message=f"Alphaä¿¡å·è®¡ç®—å®Œæˆï¼Œå½¢çŠ¶: {alpha_signals.shape}",
                        metadata={"shape": alpha_signals.shape, "columns": alpha_signals.shape[1]}
                    )
                else:
                    result = OperationResult(
                        status=ResultStatus.FAILURE,
                        data=alpha_signals,
                        message=f"Alphaä¿¡å·è®¡ç®—å¤±è´¥æˆ–ä¸ºç©ºï¼Œå½¢çŠ¶: {alpha_signals.shape if alpha_signals is not None else 'None'}",
                        metadata={"shape": alpha_signals.shape if alpha_signals is not None else None}
                    )
                    # ç»§ç»­å¤„ç†ï¼Œä½†ä½¿ç”¨ç©ºçš„alphaä¿¡å·
                    alpha_signals = pd.DataFrame()
                
                result.log_result(logger)
                
                # ğŸ¯ åœ¨å…³é”®å¤„è§¦å‘isolation_daysåŒæ­¥æ£€æŸ¥
                if hasattr(self, '_master_isolation_days') and hasattr(self, 'v6_config'):
                    tolerance_days = 2
                    current_days = getattr(self.v6_config.validation_config, 'isolation_days', 10)
                    if abs(current_days - self._master_isolation_days) > tolerance_days:
                        logger.error(f"[CONFIG ERROR] isolation_daysåå·®è¶…è¿‡{tolerance_days}å¤©ï¼šå½“å‰={current_days}, ä¸»é…ç½®={self._master_isolation_days}")
                    else:
                        logger.debug(f"[CONFIG ASSERT] isolation_daysä¸€è‡´æ€§éªŒè¯é€šè¿‡: {self._master_isolation_days}å¤©")
                
                # æ ¹æ®å¸‚åœºçŠ¶æ€è°ƒæ•´Alphaæƒé‡
                regime_weights = self._get_regime_alpha_weights(market_regime)
                
                # åº”ç”¨regimeæƒé‡åˆ°alphaä¿¡å·
                if not alpha_signals.empty:
                    weighted_alpha = pd.Series(0.0, index=alpha_signals.index)
                    for alpha_name, weight in regime_weights.items():
                        if alpha_name in alpha_signals.columns:
                            weighted_alpha += alpha_signals[alpha_name] * weight
                else:
                    # å¦‚æœalphaä¿¡å·ä¸ºç©ºï¼Œåˆ›å»ºé›¶æƒé‡åºåˆ—
                    weighted_alpha = pd.Series(0.0, index=base_predictions.index)
                
                # æ ‡å‡†åŒ–åŠ æƒåçš„alpha
                if weighted_alpha.std() > 0:
                    weighted_alpha = (weighted_alpha - weighted_alpha.mean()) / weighted_alpha.std()
                
                # ä¸åŸºç¡€MLé¢„æµ‹èåˆ
                alpha_weight = 0.3  # Alphaä¿¡å·æƒé‡
                ml_weight = 0.7     # MLé¢„æµ‹æƒé‡
                
                # ç¡®ä¿ç´¢å¼•å¯¹é½
                common_index = base_predictions.index.intersection(weighted_alpha.index)
                if len(common_index) > 0:
                    enhanced_predictions = (
                        ml_weight * base_predictions.reindex(common_index).fillna(0) +
                        alpha_weight * weighted_alpha.reindex(common_index).fillna(0)
                    )
                else:
                    enhanced_predictions = base_predictions
                
                logger.info(f"æˆåŠŸèåˆAlphaä¿¡å·å’ŒMLé¢„æµ‹ï¼Œmarket regime: {market_regime.name}")
                return enhanced_predictions
                
            except (ValueError, KeyError, AttributeError) as e:
                logger.exception(f"Alphaä¿¡å·ç”Ÿæˆå¤±è´¥: {e}")
                self.health_metrics['alpha_computation_failures'] += 1
                # å›é€€åˆ°åŸºç¡€é¢„æµ‹
                return base_predictions
                
        except Exception as e:
            logger.exception(f"å¢å¼ºé¢„æµ‹ç”Ÿæˆå¤±è´¥: {e}")
            self.health_metrics['prediction_failures'] += 1
            self.health_metrics['total_exceptions'] += 1
            # æœ€ç»ˆå›é€€
            return pd.Series(0.0, index=range(10))
    
    def _create_basic_portfolio_optimizer(self):
        """åˆ›å»ºåŸºç¡€æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨"""
        class BasicPortfolioOptimizer:
            def __init__(self, risk_aversion=5.0):
                self.risk_aversion = risk_aversion
            
            def optimize_portfolio(self, expected_returns, covariance_matrix=None, **kwargs):
                """åŸºç¡€æŠ•èµ„ç»„åˆä¼˜åŒ–"""
                try:
                    # ç®€å•çš„ç­‰é£é™©æƒé‡åˆ†é…
                    n_assets = len(expected_returns)
                    if n_assets == 0:
                        return {'success': False, 'error': 'No assets provided'}
                    
                    # åŸºäºé¢„æµ‹ä¿¡å·çš„æƒé‡åˆ†é…
                    positive_returns = expected_returns[expected_returns > 0]
                    if len(positive_returns) == 0:
                        # å¦‚æœæ²¡æœ‰æ­£æ”¶ç›Šé¢„æµ‹ï¼Œä½¿ç”¨ç­‰æƒ
                        weights = pd.Series(1.0/n_assets, index=expected_returns.index)
                    else:
                        # åªå¯¹æ­£æ”¶ç›Šèµ„äº§åˆ†é…æƒé‡
                        weights = pd.Series(0.0, index=expected_returns.index)
                        total_positive = positive_returns.sum()
                        if total_positive > 0:
                            weights[positive_returns.index] = positive_returns / total_positive
                        else:
                            weights[positive_returns.index] = 1.0 / len(positive_returns)
                    
                    # è®¡ç®—æŠ•èµ„ç»„åˆæŒ‡æ ‡
                    portfolio_return = (weights * expected_returns).sum()
                    portfolio_risk = 0.15  # å‡è®¾15%çš„é£é™©
                    sharpe_ratio = portfolio_return / portfolio_risk if portfolio_risk > 0 else 0
                    
                    return {
                        'success': True,
                        'optimal_weights': weights,  # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
                        'portfolio_metrics': {
                            'expected_return': portfolio_return,
                            'portfolio_risk': portfolio_risk,
                            'sharpe_ratio': sharpe_ratio
                        }
                    }
                except Exception as e:
                    return {'success': False, 'error': str(e)}
            
            def estimate_covariance_matrix(self, returns_matrix):
                """ä¼°è®¡åæ–¹å·®çŸ©é˜µ"""
                try:
                    return returns_matrix.cov()
                except Exception as e:
                    # å¦‚æœå¤±è´¥ï¼Œè¿”å›å•ä½çŸ©é˜µ
                    n = len(returns_matrix.columns)
                    return pd.DataFrame(np.eye(n) * 0.04, 
                                      index=returns_matrix.columns, 
                                      columns=returns_matrix.columns)
            
            def risk_attribution(self, weights, covariance_matrix):
                """é£é™©å½’å› åˆ†æ"""
                try:
                    # ç®€å•çš„é£é™©å½’å› 
                    portfolio_variance = np.dot(weights, np.dot(covariance_matrix, weights))
                    individual_risk = weights * np.diag(covariance_matrix)
                    return {
                        'portfolio_risk': np.sqrt(portfolio_variance),
                        'individual_contributions': individual_risk,
                        'total_risk': individual_risk.sum()
                    }
                except Exception as e:
                    return {'error': str(e)}
        
        return BasicPortfolioOptimizer()
    
    def _create_basic_progress_monitor(self):
        """åˆ›å»ºåŸºç¡€è¿›åº¦ç›‘æ§å™¨"""
        class BasicProgressMonitor:
            def __init__(self):
                self.start_time = None
                self.current_stage = None
                self.stages = {}
                
            def add_stage(self, stage_name, total_items):
                """æ·»åŠ è®­ç»ƒé˜¶æ®µ"""
                self.stages[stage_name] = {'total': total_items, 'completed': 0}
                logger.debug(f"æ·»åŠ é˜¶æ®µ: {stage_name} ({total_items} é¡¹ç›®)")
                
            def start_training(self, stages=None):
                from datetime import datetime
                self.start_time = datetime.now()
                logger.info("è®­ç»ƒè¿›åº¦ç›‘æ§å·²å¯åŠ¨")
                
            def start_stage(self, stage_name):
                """å¼€å§‹æŸä¸ªé˜¶æ®µ"""
                self.current_stage = stage_name
                logger.info(f"å¼€å§‹é˜¶æ®µ: {stage_name}")
                
            def complete_stage(self, stage_name, success=True):
                """å®ŒæˆæŸä¸ªé˜¶æ®µ"""
                status = "æˆåŠŸ" if success else "å¤±è´¥"
                logger.info(f"é˜¶æ®µå®Œæˆ: {stage_name} - {status}")
                
            def update_stage(self, stage_name, progress=0.0):
                self.current_stage = stage_name
                logger.info(f"æ›´æ–°é˜¶æ®µ: {stage_name}")
                
            def update_progress(self, progress, message=""):
                if message:
                    logger.info(message)
                    
            def complete_training(self, success=True):
                status = "æˆåŠŸ" if success else "å¤±è´¥"
                logger.info(f"è®­ç»ƒå®Œæˆ: {status}")
                
        return BasicProgressMonitor()
    
    def optimize_portfolio_with_risk_model(self, predictions: pd.Series, 
                                          feature_data: pd.DataFrame) -> Dict[str, Any]:
        """ä½¿ç”¨é£é™©æ¨¡å‹çš„æŠ•èµ„ç»„åˆä¼˜åŒ–"""
        try:
            # å¦‚æœæœ‰Professionalçš„é£é™©æ¨¡å‹ç»“æœï¼Œä½¿ç”¨å®ƒä»¬
            if self.risk_model_results and 'factor_loadings' in self.risk_model_results:
                factor_loadings = self.risk_model_results['factor_loadings']
                factor_covariance = self.risk_model_results['factor_covariance']
                specific_risk = self.risk_model_results['specific_risk']
                
                # æ„å»ºåæ–¹å·®çŸ©é˜µ
                common_assets = list(set(predictions.index) & set(factor_loadings.index))
                if len(common_assets) >= 3:
                    # ä½¿ç”¨ä¸“ä¸šé£é™©æ¨¡å‹è¿›è¡Œä¼˜åŒ–
                    try:
                        # æ„å»ºæŠ•èµ„ç»„åˆåæ–¹å·®çŸ©é˜µ: B * F * B' + S
                        B = factor_loadings.reindex(common_assets).dropna()  # å› å­è½½è· - å®‰å…¨ç´¢å¼•
                        F = factor_covariance                   # å› å­åæ–¹å·®
                        S = specific_risk.reindex(common_assets).dropna()    # ç‰¹å¼‚é£é™© - å®‰å…¨ç´¢å¼•
                        
                        # è®¡ç®—åæ–¹å·®çŸ©é˜µ
                        portfolio_cov = B @ F @ B.T + np.diag(S**2)
                        portfolio_cov = pd.DataFrame(
                            portfolio_cov, 
                            index=common_assets, 
                            columns=common_assets
                        )
                        
                        # ä½¿ç”¨ç»Ÿä¸€çš„AdvancedPortfolioOptimizerè€Œéé‡å¤å®ç°
                        if self.portfolio_optimizer:
                            try:
                                # å‡†å¤‡é¢„æœŸæ”¶ç›Šç‡ - ä½¿ç”¨å®‰å…¨çš„ç´¢å¼•è®¿é—®
                                available_assets = predictions.index.intersection(common_assets)
                                if len(available_assets) == 0:
                                    raise ValueError("No common assets between predictions and risk model")
                                expected_returns = predictions.reindex(available_assets).dropna()
                                common_assets = list(expected_returns.index)  # æ›´æ–°common_assetsä¸ºå®é™…å¯ç”¨çš„èµ„äº§
                                
                                # é‡æ–°æ„å»ºåæ–¹å·®çŸ©é˜µä»¥åŒ¹é…å¯ç”¨èµ„äº§
                                B_updated = factor_loadings.reindex(common_assets).dropna()
                                S_updated = specific_risk.reindex(common_assets).dropna()
                                portfolio_cov = B_updated @ F @ B_updated.T + np.diag(S_updated**2)
                                portfolio_cov = pd.DataFrame(
                                    portfolio_cov, 
                                    index=common_assets, 
                                    columns=common_assets
                                )
                                
                                # [ENHANCED] P0å‡†å¤‡è‚¡ç¥¨æ± æ•°æ®ï¼ˆç”¨äºçº¦æŸï¼‰- ä½¿ç”¨çœŸå®å…ƒæ•°æ®
                                universe_data = pd.DataFrame(index=common_assets)
                                
                                # ä»åŸå§‹æ•°æ®ä¸­æå–çœŸå®çš„å…ƒæ•°æ®
                                for asset in common_assets:
                                    if asset in self.raw_data and len(self.raw_data[asset]) > 0:
                                        latest_data = self.raw_data[asset].iloc[-1]
                                        universe_data.loc[asset, 'COUNTRY'] = latest_data.get('COUNTRY', 'US')
                                        universe_data.loc[asset, 'SECTOR'] = latest_data.get('SECTOR', 'Technology')
                                        universe_data.loc[asset, 'SUBINDUSTRY'] = latest_data.get('SUBINDUSTRY', 'Software')
                                        universe_data.loc[asset, 'ADV_USD_20'] = latest_data.get('ADV_USD_20', 1e6)
                                        universe_data.loc[asset, 'MEDIAN_SPREAD_BPS_20'] = latest_data.get('MEDIAN_SPREAD_BPS_20', 50)
                                        universe_data.loc[asset, 'FREE_FLOAT'] = latest_data.get('FREE_FLOAT', 0.6)
                                        universe_data.loc[asset, 'SHORTABLE'] = latest_data.get('SHORTABLE', True)
                                        universe_data.loc[asset, 'BORROW_FEE'] = latest_data.get('BORROW_FEE', 1.0)
                                    else:
                                        # é»˜è®¤å€¼ï¼ˆå¦‚æœæ•°æ®ä¸å¯ç”¨ï¼‰
                                        universe_data.loc[asset, 'COUNTRY'] = 'US'
                                        universe_data.loc[asset, 'SECTOR'] = 'Technology'
                                        universe_data.loc[asset, 'SUBINDUSTRY'] = 'Software'
                                        universe_data.loc[asset, 'ADV_USD_20'] = 1e6
                                        universe_data.loc[asset, 'MEDIAN_SPREAD_BPS_20'] = 50
                                        universe_data.loc[asset, 'FREE_FLOAT'] = 0.6
                                        universe_data.loc[asset, 'SHORTABLE'] = True
                                        universe_data.loc[asset, 'BORROW_FEE'] = 1.0
                                
                                # è°ƒç”¨ç»Ÿä¸€çš„ä¼˜åŒ–å™¨
                                optimization_result = self.portfolio_optimizer.optimize_portfolio(
                                    expected_returns=expected_returns,
                                    covariance_matrix=portfolio_cov,
                                    current_weights=None,  # å‡è®¾ä»ç©ºä»“å¼€å§‹
                                    universe_data=universe_data
                                )
                                
                                if optimization_result.get('success', False):
                                    optimal_weights = optimization_result['optimal_weights']
                                    portfolio_metrics = optimization_result['portfolio_metrics']

                                    # é£é™©å½’å› 
                                    risk_attribution = self.portfolio_optimizer.risk_attribution(
                                        optimal_weights, portfolio_cov
                                    )
                                    
                                    return {
                                        'success': True,
                                        'method': 'unified_portfolio_optimizer_with_risk_model',
                                        'weights': optimal_weights.to_dict(),
                                        'portfolio_metrics': portfolio_metrics,
                                        'risk_attribution': risk_attribution,
                                        'regime_context': self.current_regime.name if self.current_regime else "Unknown"
                                    }
                                else:
                                    logger.warning("ç»Ÿä¸€ä¼˜åŒ–å™¨ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                                    raise ValueError("Unified optimizer failed")
                            
                            except (ValueError, RuntimeError, np.linalg.LinAlgError) as optimizer_error:
                                logger.exception(f"ç»Ÿä¸€ä¼˜åŒ–å™¨è°ƒç”¨å¤±è´¥: {optimizer_error}, ä½¿ç”¨ç®€åŒ–ä¼˜åŒ–")
                                self.health_metrics['optimization_fallbacks'] += 1
                                # ç®€åŒ–å›é€€ï¼šç­‰æƒç»„åˆ - ä½¿ç”¨å®‰å…¨çš„ç´¢å¼•è®¿é—®
                                fallback_assets = predictions.index.intersection(common_assets)
                                if len(fallback_assets) == 0:
                                    # å¦‚æœæ²¡æœ‰äº¤é›†ï¼Œä½¿ç”¨predictionsçš„å‰å‡ ä¸ªèµ„äº§
                                    fallback_assets = predictions.index[:min(5, len(predictions.index))]
                                
                                n_assets = len(fallback_assets)
                                equal_weights = pd.Series(1.0/n_assets, index=fallback_assets)
                                
                                expected_returns = predictions.reindex(fallback_assets).dropna()
                                portfolio_return = expected_returns @ equal_weights.reindex(expected_returns.index)
                                
                                # åˆ›å»ºç®€åŒ–çš„åæ–¹å·®çŸ©é˜µç”¨äºé£é™©è®¡ç®—
                                try:
                                    portfolio_risk = np.sqrt(equal_weights.reindex(expected_returns.index) @ portfolio_cov.reindex(expected_returns.index, expected_returns.index).fillna(0.01) @ equal_weights.reindex(expected_returns.index))
                                except (KeyError, ValueError):
                                    # å¦‚æœåæ–¹å·®çŸ©é˜µè®¿é—®å¤±è´¥ï¼Œä½¿ç”¨ä¼°è®¡é£é™©
                                    portfolio_risk = 0.15  # å‡è®¾15%çš„å¹´åŒ–é£é™©
                                sharpe_ratio = portfolio_return / portfolio_risk if portfolio_risk > 0 else 0
                            
                            return {
                                'success': True,
                                    'method': 'equal_weight_fallback_with_risk_model',
                                    'weights': equal_weights.reindex(expected_returns.index).to_dict(),
                                'portfolio_metrics': {
                                    'expected_return': float(portfolio_return),
                                    'portfolio_risk': float(portfolio_risk),
                                    'sharpe_ratio': float(sharpe_ratio),
                                        'diversification_ratio': n_assets
                                },
                                    'risk_attribution': {},
                                'regime_context': self.current_regime.name if self.current_regime else "Unknown"
                            }
                        else:
                            logger.error("AdvancedPortfolioOptimizer ä¸å¯ç”¨")
                            raise ValueError("Portfolio optimizer not available")
                        
                    except Exception as e:
                        logger.warning(f"ä¸“ä¸šé£é™©æ¨¡å‹ä¼˜åŒ–å¤±è´¥: {e}")
            
            # å›é€€åˆ°åŸºç¡€ä¼˜åŒ–
            return self.optimize_portfolio(predictions, feature_data)
            
        except Exception as e:
            logger.error(f"é£é™©æ¨¡å‹ä¼˜åŒ–å¤±è´¥: {e}")
            # æœ€ç»ˆå›é€€åˆ°ç­‰æƒç»„åˆ
            top_assets = predictions.nlargest(min(10, len(predictions))).index
            equal_weights = pd.Series(1.0/len(top_assets), index=top_assets)
            
            return {
                'success': True,
                'method': 'equal_weight_fallback',
                'weights': equal_weights.to_dict(),
                'portfolio_metrics': {
                    'expected_return': predictions.reindex(top_assets).dropna().mean(),
                    'portfolio_risk': 0.15,  # å‡è®¾é£é™©
                    'sharpe_ratio': 1.0,
                    'diversification_ratio': len(top_assets)
                },
                'risk_attribution': {},
                'regime_context': self.current_regime.name if self.current_regime else "Unknown"
            }
    
    def _prepare_alpha_data(self) -> pd.DataFrame:
        """ä¸ºAlphaå¼•æ“å‡†å¤‡æ•°æ®"""
        if not self.raw_data:
            return pd.DataFrame()
        
        # å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºAlphaå¼•æ“éœ€è¦çš„æ ¼å¼
        all_data = []
        
        # å°è¯•è·å–æƒ…ç»ªå› å­æ•°æ®
        sentiment_factors = self._get_sentiment_factors()
        
        for ticker, data in self.raw_data.items():
            ticker_data = data.copy()
            ticker_data['ticker'] = ticker
            ticker_data['date'] = ticker_data.index
            
            # é›†æˆæƒ…ç»ªå› å­åˆ°ä»·æ ¼æ•°æ®ä¸­
            if sentiment_factors:
                ticker_data = self._integrate_sentiment_factors(ticker_data, ticker, sentiment_factors)
            # æ ‡å‡†åŒ–ä»·æ ¼åˆ—ï¼ŒAlphaå¼•æ“éœ€è¦ 'Close','High','Low'
            # ä¼˜å…ˆä½¿ç”¨Adj Closeï¼Œç„¶åæ˜¯Close/close
            if 'Adj Close' in ticker_data.columns:
                ticker_data['Close'] = ticker_data['Adj Close']
            elif 'Close' in ticker_data.columns:
                ticker_data['Close'] = ticker_data['Close']  # å·²å­˜åœ¨å¤§å†™Close
            elif 'close' in ticker_data.columns:
                ticker_data['Close'] = ticker_data['close']
            else:
                # è‹¥ç¼ºå°‘closeä¿¡æ¯ï¼Œè·³è¿‡è¯¥ç¥¨
                logger.warning(f"è·³è¿‡{ticker_data.get('ticker', 'UNKNOWN')}: ç¼ºå°‘Close/closeåˆ—")
                continue
                
            # å¤„ç†Highåˆ—
            if 'High' not in ticker_data.columns:
                if 'high' in ticker_data.columns:
                    ticker_data['High'] = ticker_data['high']
                else:
                    logger.warning(f"{ticker_data.get('ticker', 'UNKNOWN')}: ç¼ºå°‘High/highåˆ—")
                    continue
                    
            # å¤„ç†Lowåˆ—  
            if 'Low' not in ticker_data.columns:
                if 'low' in ticker_data.columns:
                    ticker_data['Low'] = ticker_data['low']
                else:
                    logger.warning(f"{ticker_data.get('ticker', 'UNKNOWN')}: ç¼ºå°‘Low/lowåˆ—")
                    continue
            # æ·»åŠ æ¨¡æ‹Ÿçš„åŸºæœ¬ä¿¡æ¯
            ticker_data['COUNTRY'] = 'US'
            ticker_data['SECTOR'] = 'Technology'  # ç®€åŒ–å¤„ç†
            ticker_data['SUBINDUSTRY'] = 'Software'
            all_data.append(ticker_data)
        
        if all_data:
            combined_data = pd.concat(all_data, ignore_index=True)
            return combined_data
        else:
            return pd.DataFrame()
    
    def _get_sentiment_factors(self) -> Optional[Dict[str, pd.DataFrame]]:
        """è·å–æƒ…ç»ªå› å­æ•°æ®"""
        try:
            # å°è¯•å¯¼å…¥æƒ…ç»ªå› å­æ¨¡å—
            import sys
            import os
            sys.path.append('autotrader')
            from enhanced_sentiment_factors import create_sentiment_factors, SentimentConfig
            
            # åˆ›å»ºæƒ…ç»ªå› å­é…ç½®
            sentiment_config = SentimentConfig(
                polygon_api_key=os.getenv('POLYGON_API_KEY', ''),  # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
                news_lookback_days=5,
                sp500_lookback_days=30,
                fear_greed_cache_minutes=60
            )
            
            # åˆ›å»ºæƒ…ç»ªå› å­å¼•æ“
            sentiment_engine = create_sentiment_factors(sentiment_config)
            
            # è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
            tickers = list(self.raw_data.keys()) if self.raw_data else []
            
            if not tickers:
                logger.info("æ²¡æœ‰è‚¡ç¥¨æ•°æ®ï¼Œè·³è¿‡æƒ…ç»ªå› å­è®¡ç®—")
                return None
            
            # è®¡ç®—æƒ…ç»ªå› å­
            logger.info(f"è®¡ç®— {len(tickers)} åªè‚¡ç¥¨çš„æƒ…ç»ªå› å­...")
            sentiment_factors = sentiment_engine.compute_all_sentiment_factors(tickers)
            
            if sentiment_factors:
                logger.info(f"âœ… æˆåŠŸè·å–æƒ…ç»ªå› å­: {list(sentiment_factors.keys())}")
                return sentiment_factors
            else:
                logger.warning("æœªèƒ½è·å–åˆ°æƒ…ç»ªå› å­æ•°æ®")
                return None
                
        except Exception as e:
            logger.warning(f"è·å–æƒ…ç»ªå› å­å¤±è´¥: {e}")
            return None
    
    def _integrate_sentiment_factors(self, ticker_data: pd.DataFrame, ticker: str, 
                                   sentiment_factors: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """å°†æƒ…ç»ªå› å­é›†æˆåˆ°è‚¡ç¥¨æ•°æ®ä¸­"""
        try:
            enhanced_data = ticker_data.copy()
            
            # ç¡®ä¿dateåˆ—ä¸ºdatetimeç±»å‹
            if 'date' not in enhanced_data.columns:
                enhanced_data['date'] = enhanced_data.index
            enhanced_data['date'] = pd.to_datetime(enhanced_data['date'])
            
            # é›†æˆæ–°é—»æƒ…ç»ªå› å­
            if 'news_sentiment' in sentiment_factors:
                news_data = sentiment_factors['news_sentiment']
                ticker_news = news_data[news_data['ticker'] == ticker] if 'ticker' in news_data.columns else news_data
                
                if not ticker_news.empty:
                    ticker_news['date'] = pd.to_datetime(ticker_news['date'])
                    enhanced_data = enhanced_data.merge(
                        ticker_news[['date', 'sentiment_mean', 'news_count', 'sentiment_momentum_1d']].add_prefix('news_'),
                        left_on='date', right_on='news_date', how='left'
                    ).drop('news_date', axis=1, errors='ignore')
            
            # é›†æˆå¸‚åœºæƒ…ç»ªå› å­ï¼ˆSP500ï¼‰
            if 'market_sentiment' in sentiment_factors:
                market_data = sentiment_factors['market_sentiment'].copy()
                market_data['date'] = pd.to_datetime(market_data['date'])
                
                # é€‰æ‹©å…³é”®çš„å¸‚åœºæƒ…ç»ªæŒ‡æ ‡
                market_cols = [col for col in market_data.columns if any(
                    keyword in col for keyword in ['sentiment', 'fear', 'momentum', 'volatility']
                )][:5]  # é™åˆ¶å› å­æ•°é‡é¿å…è¿‡æ‹Ÿåˆ
                
                if market_cols:
                    merge_cols = ['date'] + market_cols
                    enhanced_data = enhanced_data.merge(
                        market_data[merge_cols].add_prefix('market_'),
                        left_on='date', right_on='market_date', how='left'
                    ).drop('market_date', axis=1, errors='ignore')
            
            # é›†æˆFear & GreedæŒ‡æ•°
            if 'fear_greed' in sentiment_factors:
                fg_data = sentiment_factors['fear_greed'].copy()
                fg_data['date'] = pd.to_datetime(fg_data['date'])
                
                # å‰å‘å¡«å……Fear & Greedæ•°æ®ï¼ˆå› ä¸ºå®ƒæ›´æ–°é¢‘ç‡è¾ƒä½ï¼‰
                enhanced_data = enhanced_data.merge(
                    fg_data[['date', 'fear_greed_value', 'fear_greed_normalized', 'market_fear_level']],
                    on='date', how='left'
                ).fillna(method='ffill')
            
            # æ³¨æ„ï¼šä¸å†é›†æˆå¤åˆæƒ…ç»ªå› å­ï¼Œä¿æŒæ‰€æœ‰å› å­ç‹¬ç«‹
            # è®©æœºå™¨å­¦ä¹ æ¨¡å‹è‡ªåŠ¨å­¦ä¹ å„ä¸ªæƒ…ç»ªå› å­çš„æœ€ä¼˜æƒé‡
            
            # å¡«å……ç¼ºå¤±å€¼
            sentiment_cols = [col for col in enhanced_data.columns if any(
                prefix in col for prefix in ['news_', 'market_', 'fear_greed_']
            )]
            
            for col in sentiment_cols:
                if col in enhanced_data.columns:
                    enhanced_data[col] = enhanced_data[col].fillna(0)  # ç”¨0å¡«å……æƒ…ç»ªå› å­çš„ç¼ºå¤±å€¼
            
            logger.debug(f"ä¸º {ticker} é›†æˆäº† {len(sentiment_cols)} ä¸ªæƒ…ç»ªå› å­")
            return enhanced_data
            
        except Exception as e:
            logger.warning(f"é›†æˆæƒ…ç»ªå› å­å¤±è´¥ ({ticker}): {e}")
            return ticker_data
        
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½å¹¶éªŒè¯é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # éªŒè¯å’Œä¿®å¤é…ç½®
            validated_config = self._validate_and_fix_config(config)
            logger.info("é…ç½®æ–‡ä»¶åŠ è½½å’ŒéªŒè¯å®Œæˆ")
            return validated_config
            
        except FileNotFoundError:
            logger.warning(f"é…ç½®æ–‡ä»¶{self.config_path}æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
        except yaml.YAMLError as e:
            logger.error(f"é…ç½®æ–‡ä»¶YAMLæ ¼å¼é”™è¯¯: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
    
    def _validate_and_fix_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯å’Œä¿®å¤é…ç½®å‚æ•°"""
        default_config = self._get_default_config()
        validated_config = config.copy()
        
        # å¿…éœ€å‚æ•°æ£€æŸ¥å’Œé»˜è®¤å€¼è®¾ç½®
        required_params = {
            'max_position': (0.001, 0.1, 0.03),  # (min, max, default)
            'max_turnover': (0.01, 1.0, 0.10),
            'temperature': (0.1, 5.0, 1.2),
            'winsorize_std': (1.0, 5.0, 2.5),
            'truncation': (0.01, 0.5, 0.10)
        }
        
        for param, (min_val, max_val, default_val) in required_params.items():
            if param not in validated_config:
                validated_config[param] = default_val
                logger.warning(f"é…ç½®ç¼ºå¤±{param}ï¼Œä½¿ç”¨é»˜è®¤å€¼{default_val}")
            else:
                # éªŒè¯æ•°å€¼èŒƒå›´
                if not isinstance(validated_config[param], (int, float)):
                    logger.warning(f"é…ç½®{param}éæ•°å€¼ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤å€¼{default_val}")
                    validated_config[param] = default_val
                elif validated_config[param] < min_val or validated_config[param] > max_val:
                    logger.warning(f"é…ç½®{param}={validated_config[param]}è¶…å‡ºèŒƒå›´[{min_val}, {max_val}]ï¼Œä½¿ç”¨é»˜è®¤å€¼{default_val}")
                    validated_config[param] = default_val
        
        # åµŒå¥—é…ç½®æ£€æŸ¥
        if 'model_config' not in validated_config:
            validated_config['model_config'] = default_config['model_config']
        else:
            # éªŒè¯model_configå­é¡¹
            model_config = validated_config['model_config']
            for key, default_val in default_config['model_config'].items():
                if key not in model_config:
                    model_config[key] = default_val
                    logger.warning(f"model_configç¼ºå¤±{key}ï¼Œä½¿ç”¨é»˜è®¤å€¼{default_val}")
        
        if 'risk_config' not in validated_config:
            validated_config['risk_config'] = default_config['risk_config']
        else:
            # éªŒè¯risk_configå­é¡¹
            risk_config = validated_config['risk_config']
            risk_params = {
                'risk_aversion': (1.0, 20.0, 5.0),
                'turnover_penalty': (0.1, 10.0, 1.0),
                'max_sector_exposure': (0.05, 0.5, 0.15),
                'max_country_exposure': (0.1, 1.0, 0.20)
            }
            
            for param, (min_val, max_val, default_val) in risk_params.items():
                if param not in risk_config:
                    risk_config[param] = default_val
                    logger.warning(f"risk_configç¼ºå¤±{param}ï¼Œä½¿ç”¨é»˜è®¤å€¼{default_val}")
                elif not isinstance(risk_config[param], (int, float)):
                    logger.warning(f"risk_config.{param}éæ•°å€¼ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤å€¼{default_val}")
                    risk_config[param] = default_val
                elif risk_config[param] < min_val or risk_config[param] > max_val:
                    logger.warning(f"risk_config.{param}={risk_config[param]}è¶…å‡ºèŒƒå›´[{min_val}, {max_val}]ï¼Œä½¿ç”¨é»˜è®¤å€¼{default_val}")
                    risk_config[param] = default_val
        
        # é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥
        if validated_config['max_position'] * 30 > 1.0:  # å‡è®¾æœ€å¤š30ä¸ªæŒä»“
            logger.warning(f"max_position={validated_config['max_position']}å¯èƒ½è¿‡å¤§ï¼Œå¯èƒ½å¯¼è‡´é›†ä¸­åº¦é£é™©")
        
        if validated_config['max_turnover'] < validated_config['max_position']:
            logger.warning("max_turnoverå°äºmax_positionï¼Œå¯èƒ½å¯¼è‡´äº¤æ˜“å—é™")
        
        logger.info(f"é…ç½®éªŒè¯å®Œæˆï¼Œä¿®å¤äº†{len([k for k in required_params if k not in config])}ä¸ªç¼ºå¤±å‚æ•°")
        return validated_config
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'universe': 'TOPDIV3000',
            'neutralization': ['COUNTRY'],
            'hump_levels': [0.003, 0.008],
            'winsorize_std': 2.5,
            'truncation': 0.10,
            'max_position': 0.03,
            'max_turnover': 0.10,
            'temperature': 1.2,
            'model_config': {
                'learning_to_rank': True,
                'ranking_objective': 'rank:pairwise',
                'uncertainty_aware': True,
                'quantile_regression': True
            },
            'risk_config': {
                'risk_aversion': 5.0,
                'turnover_penalty': 1.0,
                'max_sector_exposure': 0.15,
                'max_country_exposure': 0.20
            }
        }
    
    def download_stock_data(self, tickers: List[str], 
                           start_date: str, end_date: str) -> Dict[str, pd.DataFrame]:
        """
        ä¸‹è½½è‚¡ç¥¨æ•°æ®
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            è‚¡ç¥¨æ•°æ®å­—å…¸
        """
        logger.info(f"ä¸‹è½½{len(tickers)}åªè‚¡ç¥¨çš„æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {start_date} - {end_date}")

        # å°†è®­ç»ƒç»“æŸæ—¶é—´é™åˆ¶ä¸ºå½“å¤©çš„å‰ä¸€å¤©ï¼ˆT-1ï¼‰ï¼Œé¿å…ä½¿ç”¨æœªå®Œå…¨ç»“ç®—çš„æ•°æ®
        try:
            yesterday = (datetime.now() - timedelta(days=1)).date()
            end_dt = pd.to_datetime(end_date).date()
            if end_dt > yesterday:
                adjusted_end = yesterday.strftime('%Y-%m-%d')
                logger.info(f"ç»“æŸæ—¥æœŸ{end_date} è¶…è¿‡æ˜¨æ—¥ï¼Œå·²è°ƒæ•´ä¸º {adjusted_end}")
                end_date = adjusted_end
        except Exception as _e:
            logger.debug(f"ç»“æŸæ—¥æœŸè°ƒæ•´è·³è¿‡: {_e}")
        
        # æ•°æ®éªŒè¯
        if not tickers or len(tickers) == 0:
            logger.error("è‚¡ç¥¨ä»£ç åˆ—è¡¨ä¸ºç©º")
            return {}
        
        if not start_date or not end_date:
            logger.error("å¼€å§‹æ—¥æœŸæˆ–ç»“æŸæ—¥æœŸä¸ºç©º")
            return {}
        
        all_data = {}
        failed_downloads = []
        
        # APIé™åˆ¶ä¼˜åŒ–ï¼šæ‰¹é‡å¤„ç†+å»¶è¿Ÿ+é‡è¯•æœºåˆ¶
        import time
        import random
        
        total_tickers = len(tickers)
        batch_size = 50  # æ‰¹é‡å¤§å°å‡å°‘APIå‹åŠ›
        api_delay = 0.12  # å¢åŠ å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶
        max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
        
        # æ‰¹é‡å¤„ç†è‚¡ç¥¨
        for batch_idx in range(0, total_tickers, batch_size):
            batch_tickers = tickers[batch_idx:batch_idx + batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_idx//batch_size + 1}/{(total_tickers-1)//batch_size + 1}: {len(batch_tickers)} è‚¡ç¥¨")
            
            for ticker_idx, ticker in enumerate(batch_tickers):
                # åŠ¨æ€å»¶è¿Ÿï¼šé¿å…APIé€Ÿç‡é™åˆ¶
                if ticker_idx > 0:
                    time.sleep(api_delay + random.uniform(0, 0.05))
                
                # é‡è¯•æœºåˆ¶
                for retry in range(max_retries):
                    try:
                        # éªŒè¯è‚¡ç¥¨ä»£ç æ ¼å¼
                        if not ticker or not isinstance(ticker, str) or len(ticker.strip()) == 0:
                            logger.warning(f"æ— æ•ˆçš„è‚¡ç¥¨ä»£ç : {ticker}")
                            failed_downloads.append(ticker)
                            break  # è·³å‡ºé‡è¯•å¾ªç¯
                        
                        ticker = ticker.strip().upper()  # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
                        
                        logger.info(f"[DEBUG] å¼€å§‹ä¸‹è½½ {ticker} æ•°æ®...")
                        
                        # ä½¿ç”¨æ”¹è¿›çš„çº¿ç¨‹æ± æœºåˆ¶ï¼Œæ”¯æŒæ›´å¥½çš„èµ„æºç®¡ç†
                        import threading
                        from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError, as_completed
                        import signal
                        import time
                        
                        def download_data_with_validation():
                            """å¸¦éªŒè¯çš„æ•°æ®ä¸‹è½½å‡½æ•°"""
                            try:
                                stock = PolygonTicker(ticker)
                                hist = stock.history(start=start_date, end=end_date, interval='1d')
                                
                                # åŸºç¡€æ•°æ®éªŒè¯
                                if hist is None:
                                    raise ValueError("è¿”å›çš„å†å²æ•°æ®ä¸ºNone")
                                if hasattr(hist, '__len__') and len(hist) == 0:
                                    raise ValueError("è¿”å›çš„å†å²æ•°æ®ä¸ºç©º")
                                    
                                return hist
                            except Exception as e:
                                logger.debug(f"{ticker} æ•°æ®ä¸‹è½½å†…éƒ¨é”™è¯¯: {e}")
                                raise
                        
                        hist = None
                        download_success = False
                        
                        try:
                            logger.info(f"[DEBUG] å¯åŠ¨ {ticker} æ•°æ®ä¸‹è½½ï¼ˆ30ç§’è¶…æ—¶ï¼‰...")
                            
                            # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿çº¿ç¨‹æ± æ­£ç¡®æ¸…ç†
                            with ThreadPoolExecutor(max_workers=1, thread_name_prefix=f"DataDownload-{ticker}") as executor:
                                # æäº¤ä»»åŠ¡å¹¶è·å–Futureå¯¹è±¡
                                future = executor.submit(download_data_with_validation)
                                
                                try:
                                    # ç­‰å¾…ç»“æœï¼Œ30ç§’è¶…æ—¶
                                    hist = future.result(timeout=30)
                                    download_success = True
                                    logger.info(f"[DEBUG] {ticker} å†å²æ•°æ®è·å–å®Œæˆï¼Œæ•°æ®é•¿åº¦: {len(hist) if hist is not None else 0}")
                                    
                                except FutureTimeoutError:
                                    logger.warning(f"[TIMEOUT] {ticker} æ•°æ®ä¸‹è½½è¶…æ—¶ï¼ˆ30ç§’ï¼‰")
                                    # å°è¯•å–æ¶ˆä»»åŠ¡
                                    future.cancel()
                                    raise
                                    
                        except FutureTimeoutError:
                            if retry < max_retries - 1:
                                logger.info(f"{ticker} è¶…æ—¶ï¼Œé‡è¯• {retry + 1}/{max_retries}")
                                time.sleep(1)  # çŸ­æš‚ç­‰å¾…åé‡è¯•
                                continue
                            else:
                                failed_downloads.append(ticker)
                                break
                        except (ConnectionError, TimeoutError, OSError) as conn_e:
                            logger.warning(f"[NETWORK] {ticker} ç½‘ç»œè¿æ¥é—®é¢˜: {conn_e}")
                            if retry < max_retries - 1:
                                time.sleep(2)  # ç½‘ç»œé—®é¢˜ç­‰å¾…æ›´é•¿æ—¶é—´
                                continue
                            else:
                                failed_downloads.append(ticker)
                                break
                        except Exception as thread_e:
                            logger.warning(f"[ERROR] {ticker} ä¸‹è½½å¼‚å¸¸: {thread_e}")
                            if retry < max_retries - 1:
                                continue
                            else:
                                failed_downloads.append(ticker)
                                break
                        
                        # æ•°æ®è´¨é‡æ£€æŸ¥
                        if hist is None or len(hist) == 0:
                            if retry < max_retries - 1:
                                logger.warning(f"{ticker}: æ— æ•°æ®ï¼Œé‡è¯• {retry + 1}/{max_retries}")
                                time.sleep(1.0 * (retry + 1))  # é€’å¢å»¶è¿Ÿ
                                continue
                            else:
                                logger.warning(f"{ticker}: æ— æ•°æ®ï¼Œæœ€ç»ˆå¤±è´¥")
                                failed_downloads.append(ticker)
                                break
                        
                        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨ - æ”¯æŒå¤§å°å†™å…¼å®¹
                        required_cols_upper = ['Open', 'High', 'Low', 'Close', 'Volume']
                        required_cols_lower = ['open', 'high', 'low', 'close', 'volume']
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„åˆ—ï¼ˆå¤§å†™æˆ–å°å†™ï¼‰
                        has_required = True
                        missing_info = []
                        
                        for i, (upper_col, lower_col) in enumerate(zip(required_cols_upper, required_cols_lower)):
                            if upper_col not in hist.columns and lower_col not in hist.columns:
                                has_required = False
                                missing_info.append(f"{upper_col}/{lower_col}")
                        
                        if not has_required:
                            if retry < max_retries - 1:
                                logger.warning(f"{ticker}: ç¼ºå°‘å¿…è¦åˆ— {missing_info}ï¼Œé‡è¯• {retry + 1}/{max_retries}")
                                time.sleep(0.5 * (retry + 1))
                                continue
                            else:
                                logger.warning(f"{ticker}: ç¼ºå°‘å¿…è¦åˆ— {missing_info}, ç°æœ‰åˆ—: {list(hist.columns)}")
                                failed_downloads.append(ticker)
                                break
                        
                        # æ ‡å‡†åŒ–åˆ—å - æ™ºèƒ½å¤„ç†å¤§å°å†™
                        rename_mapping = {}
                        for upper_col, lower_col in zip(required_cols_upper, required_cols_lower):
                            if upper_col in hist.columns:
                                rename_mapping[upper_col] = lower_col
                            # å¦‚æœå·²ç»æ˜¯å°å†™ï¼Œä¸éœ€è¦é‡å‘½å
                        
                        if rename_mapping:
                            hist = hist.rename(columns=rename_mapping)
                        
                        # æ£€æŸ¥æ•°æ®è´¨é‡ - ä½¿ç”¨æ ‡å‡†åŒ–åçš„åˆ—åï¼Œå¢å¼ºæ•°æ®éªŒè¯
                        if 'close' not in hist.columns or hist['close'].isna().all():
                            if retry < max_retries - 1:
                                logger.warning(f"{ticker}: closeåˆ—é—®é¢˜ï¼Œé‡è¯• {retry + 1}/{max_retries}")
                                time.sleep(0.5 * (retry + 1))
                                continue
                            else:
                                logger.warning(f"{ticker}: closeåˆ—ç¼ºå¤±æˆ–æ‰€æœ‰æ”¶ç›˜ä»·éƒ½æ˜¯NaN")
                                failed_downloads.append(ticker)
                                break
                        
                        # å¢å¼ºæ•°æ®è´¨é‡æ£€æŸ¥
                        # 1. æ£€æŸ¥æ•°æ®å……åˆ†æ€§ - ğŸ”§ è°ƒæ•´ä¸ºæ›´åˆç†çš„è¦æ±‚
                        MIN_REQUIRED_DAYS = 90  # è°ƒæ•´ä¸º3ä¸ªæœˆæ•°æ® (åŸä¸º252å¤©/1å¹´ï¼Œè¿‡äºä¸¥æ ¼)
                        if len(hist) < MIN_REQUIRED_DAYS:
                            logger.warning(f"{ticker}: æ•°æ®ä¸è¶³ï¼Œåªæœ‰{len(hist)}å¤©ï¼Œéœ€è¦è‡³å°‘{MIN_REQUIRED_DAYS}å¤©")
                            failed_downloads.append(ticker)
                            break
                        
                        # 2. æ£€æŸ¥æ•°æ®å¼‚å¸¸å€¼å’Œè´¨é‡
                        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
                        for col in numeric_cols:
                            if col in hist.columns:
                                # æ£€æŸ¥è´Ÿå€¼
                                if (hist[col] < 0).any():
                                    logger.warning(f"{ticker}: å‘ç°è´Ÿå€¼åœ¨{col}åˆ—")
                                    hist[col] = hist[col].clip(lower=0)  # ä¿®å¤è´Ÿå€¼
                                
                                # æ£€æŸ¥å¼‚å¸¸çš„å¤§å¹…æ³¢åŠ¨ (>20å€å˜åŠ¨)
                                if col in ['open', 'high', 'low', 'close'] and len(hist) > 1:
                                    price_ratio = hist[col] / hist[col].shift(1)
                                    extreme_moves = (price_ratio > 20) | (price_ratio < 0.05)
                                    if extreme_moves.sum() > 0:
                                        logger.warning(f"{ticker}: å‘ç°{extreme_moves.sum()}ä¸ªå¼‚å¸¸ä»·æ ¼æ³¢åŠ¨åœ¨{col}åˆ—")
                                        # ç”¨å‰å€¼å¡«å……å¼‚å¸¸å€¼
                                        hist.loc[extreme_moves, col] = hist[col].shift(1)
                        
                        # 3. æ£€æŸ¥ä»·æ ¼é€»è¾‘å…³ç³» (High >= Close >= Low)
                        invalid_price_logic = (hist['high'] < hist['close']) | (hist['close'] < hist['low']) | (hist['high'] < hist['low'])
                        if invalid_price_logic.any():
                            logger.warning(f"{ticker}: å‘ç°{invalid_price_logic.sum()}ä¸ªä»·æ ¼é€»è¾‘é”™è¯¯")
                            # ä¿®å¤ä»·æ ¼é€»è¾‘é”™è¯¯
                            hist.loc[invalid_price_logic, 'high'] = hist[['open', 'high', 'low', 'close']].max(axis=1)
                            hist.loc[invalid_price_logic, 'low'] = hist[['open', 'high', 'low', 'close']].min(axis=1)
                        
                        # æ·»åŠ åŸºç¡€ç‰¹å¾
                        hist['ticker'] = ticker
                        hist['date'] = hist.index
                        hist['amount'] = hist['close'] * hist['volume']  # æˆäº¤é¢
                        
                        # [ENHANCED] P0è‚¡ç¥¨æ± æ‰“æ ‡ï¼šè®¡ç®—ADV_USD_20å’ŒMEDIAN_SPREAD_BPS_20
                        hist['ADV_USD_20'] = hist['amount'].rolling(window=20, min_periods=1).mean()
                        
                        # è®¡ç®—ç‚¹å·®ï¼ˆç®€åŒ–ä¼°è®¡ï¼šé«˜ä½ä»·å·®ä½œä¸ºä»£ç†ï¼‰
                        hist['spread_estimate'] = (hist['high'] - hist['low']) / hist['close'] * 10000  # è½¬ä¸ºbp
                        hist['MEDIAN_SPREAD_BPS_20'] = hist['spread_estimate'].rolling(window=20, min_periods=1).median()
                        
                        # æ·»åŠ å…¶ä»–æµåŠ¨æ€§å’Œè´¨é‡æŒ‡æ ‡  
                        hist['FREE_FLOAT'] = self._get_free_float_for_ticker(ticker)
                        hist['SHORTABLE'] = self._get_shortable_status(ticker)
                        hist['BORROW_FEE'] = self._get_borrow_fee(ticker)
                        
                        # æ·»åŠ çœŸå®å…ƒæ•°æ®ï¼ˆæ›¿æ¢éšæœºæ¨¡æ‹Ÿï¼‰
                        hist['COUNTRY'] = self._get_country_for_ticker(ticker)
                        hist['SECTOR'] = self._get_sector_for_ticker(ticker)
                        hist['SUBINDUSTRY'] = self._get_subindustry_for_ticker(ticker)
                        
                        all_data[ticker] = hist
                        logger.debug(f"{ticker}: æ•°æ®å¤„ç†æˆåŠŸ")
                        break  # æˆåŠŸåè·³å‡ºé‡è¯•å¾ªç¯
                        
                    except Exception as e:
                        if retry < max_retries - 1:
                            logger.warning(f"ä¸‹è½½{ticker}å¤±è´¥ (é‡è¯• {retry + 1}/{max_retries}): {e}")
                            time.sleep(2.0 * (retry + 1))  # é€’å¢å»¶è¿Ÿ
                        else:
                            logger.warning(f"ä¸‹è½½{ticker}æœ€ç»ˆå¤±è´¥: {e}")
                            failed_downloads.append(ticker)
        
        # æ•°æ®è¦†ç›–ç‡æ£€æŸ¥
        total_requested = len(tickers)
        successful_downloads = len(all_data)
        failed_count = len(failed_downloads)
        coverage_rate = successful_downloads / total_requested if total_requested > 0 else 0
        
        if failed_downloads:
            logger.warning(f"ä¸‹è½½å¤±è´¥çš„è‚¡ç¥¨ ({failed_count}/{total_requested}): {failed_downloads[:10]}{'...' if failed_count > 10 else ''}")
        
        logger.info(f"æ•°æ®ä¸‹è½½å®Œæˆ: {successful_downloads}/{total_requested} (è¦†ç›–ç‡: {coverage_rate:.1%})")
        
        # æ•°æ®è´¨é‡éªŒè¯ï¼šå¦‚æœè¦†ç›–ç‡è¿‡ä½ï¼Œå‘å‡ºè­¦å‘Š
        if coverage_rate < 0.5:
            logger.error(f"æ•°æ®è¦†ç›–ç‡è¿‡ä½ ({coverage_rate:.1%})ï¼Œå»ºè®®æ£€æŸ¥APIé…ç½®æˆ–ç½‘ç»œè¿æ¥")
        elif coverage_rate < 0.7:
            logger.warning(f"æ•°æ®è¦†ç›–ç‡è¾ƒä½ ({coverage_rate:.1%})ï¼Œå¯èƒ½å½±å“æ¨¡å‹è´¨é‡")
        
        self.raw_data = all_data
        return all_data
    
    def _get_country_for_ticker(self, ticker: str) -> str:
        """è·å–è‚¡ç¥¨çš„å›½å®¶ï¼ˆçœŸå®æ•°æ®æºï¼‰"""
        try:
            if MARKET_MANAGER_AVAILABLE:
                # ä½¿ç”¨ç»Ÿä¸€å¸‚åœºæ•°æ®ç®¡ç†å™¨
                manager = UnifiedMarketDataManager()
                stock_info = manager.get_stock_info(ticker)
                if stock_info and hasattr(stock_info, 'country') and stock_info.country:
                    return stock_info.country
            
            # é€šè¿‡Polygonå®¢æˆ·ç«¯è·å–å…¬å¸è¯¦æƒ…
            try:
                url = f"https://api.polygon.io/v3/reference/tickers/{ticker}"
                response = pc.get(url)
                if response.status_code == 200:
                    data = response.json()
                    if 'results' in data:
                        locale = data['results'].get('locale', 'us')
                        return locale.upper()
            except Exception:
                pass
            
            # é»˜è®¤ä¸ºç¾å›½å¸‚åœºï¼ˆå¤§éƒ¨åˆ†è‚¡ç¥¨ï¼‰
            return 'US'
        except Exception as e:
            logger.warning(f"è·å–{ticker}å›½å®¶ä¿¡æ¯å¤±è´¥: {e}")
            return 'US'
    
    def _get_sector_for_ticker(self, ticker: str) -> str:
        """è·å–è‚¡ç¥¨çš„è¡Œä¸šï¼ˆçœŸå®æ•°æ®æºï¼‰"""
        try:
            if MARKET_MANAGER_AVAILABLE:
                # ä½¿ç”¨ç»Ÿä¸€å¸‚åœºæ•°æ®ç®¡ç†å™¨
                manager = UnifiedMarketDataManager()
                stock_info = manager.get_stock_info(ticker)
                if stock_info and hasattr(stock_info, 'sector') and stock_info.sector:
                    return stock_info.sector
            
            # é€šè¿‡Polygonå®¢æˆ·ç«¯è·å–å…¬å¸è¯¦æƒ…
            try:
                url = f"https://api.polygon.io/v3/reference/tickers/{ticker}"
                response = pc.get(url)
                if response.status_code == 200:
                    data = response.json()
                    if 'results' in data:
                        sic_description = data['results'].get('sic_description', '')
                        if sic_description:
                            # å°†SICæè¿°æ˜ å°„ä¸ºä¸»è¦è¡Œä¸š
                            sector = self._map_sic_to_sector(sic_description)
                            return sector
            except Exception:
                pass
            
            # å¦‚æœéƒ½å¤±è´¥ï¼Œå°è¯•ç¡¬ç¼–ç æ˜ å°„
            sector_mapping = {
                'AAPL': 'Technology', 'MSFT': 'Technology', 'GOOGL': 'Technology', 'NVDA': 'Technology',
                'AMZN': 'Consumer Discretionary', 'TSLA': 'Consumer Discretionary', 'META': 'Technology',
                'NFLX': 'Communication Services', 'JPM': 'Financials', 'JNJ': 'Health Care'
            }
            return sector_mapping.get(ticker, 'Technology')  # é»˜è®¤ç§‘æŠ€
        except Exception as e:
            logger.warning(f"è·å–{ticker}è¡Œä¸šä¿¡æ¯å¤±è´¥: {e}")
            return 'Technology'
    
    def _get_subindustry_for_ticker(self, ticker: str) -> str:
        """è·å–è‚¡ç¥¨çš„å­è¡Œä¸šï¼ˆçœŸå®æ•°æ®æºï¼‰"""
        try:
            if MARKET_MANAGER_AVAILABLE:
                # ä½¿ç”¨ç»Ÿä¸€å¸‚åœºæ•°æ®ç®¡ç†å™¨
                manager = UnifiedMarketDataManager()
                stock_info = manager.get_stock_info(ticker)
                if stock_info and hasattr(stock_info, 'subindustry') and stock_info.subindustry:
                    return stock_info.subindustry
            
            # é€šè¿‡Polygonå®¢æˆ·ç«¯è·å–è¯¦ç»†è¡Œä¸šåˆ†ç±»
            try:
                url = f"https://api.polygon.io/v3/reference/tickers/{ticker}"
                response = pc.get(url)
                if response.status_code == 200:
                    data = response.json()
                    if 'results' in data:
                        sic_description = data['results'].get('sic_description', '')
                        if sic_description:
                            return sic_description[:50]  # å–å‰50å­—ç¬¦ä½œä¸ºå­è¡Œä¸š
            except Exception:
                pass
            
            # é»˜è®¤æ˜ å°„
            subindustry_mapping = {
                'AAPL': 'Consumer Electronics', 'MSFT': 'Software', 'GOOGL': 'Internet Services',
                'NVDA': 'Semiconductors', 'AMZN': 'E-commerce', 'TSLA': 'Electric Vehicles',
                'META': 'Social Media', 'NFLX': 'Streaming Media'
            }
            return subindustry_mapping.get(ticker, 'Software')
        except Exception as e:
            logger.warning(f"è·å–{ticker}å­è¡Œä¸šä¿¡æ¯å¤±è´¥: {e}")
            return 'Software'
    
    def _map_sic_to_sector(self, sic_description: str) -> str:
        """å°†SICæè¿°æ˜ å°„ä¸ºGICSè¡Œä¸š"""
        sic_lower = sic_description.lower()
        
        if any(word in sic_lower for word in ['computer', 'software', 'internet', 'technology']):
            return 'Technology'
        elif any(word in sic_lower for word in ['bank', 'finance', 'insurance', 'investment']):
            return 'Financials'
        elif any(word in sic_lower for word in ['drug', 'pharmaceutical', 'health', 'medical']):
            return 'Health Care'
        elif any(word in sic_lower for word in ['oil', 'gas', 'energy', 'petroleum']):
            return 'Energy'
        elif any(word in sic_lower for word in ['retail', 'consumer', 'restaurant']):
            return 'Consumer Discretionary'
        elif any(word in sic_lower for word in ['utility', 'electric', 'water']):
            return 'Utilities'
        elif any(word in sic_lower for word in ['real estate', 'reit']):
            return 'Real Estate'
        elif any(word in sic_lower for word in ['manufacturing', 'industrial', 'machinery']):
            return 'Industrials'
        elif any(word in sic_lower for word in ['material', 'chemical', 'mining']):
            return 'Materials'
        else:
            return 'Technology'  # é»˜è®¤
    
    def _get_free_float_for_ticker(self, ticker: str) -> float:
        """è·å–è‚¡ç¥¨çš„è‡ªç”±æµé€šå¸‚å€¼æ¯”ä¾‹"""
        try:
            if MARKET_MANAGER_AVAILABLE:
                manager = UnifiedMarketDataManager()
                stock_info = manager.get_stock_info(ticker)
                if stock_info and hasattr(stock_info, 'free_float_shares'):
                    # è®¡ç®—è‡ªç”±æµé€šæ¯”ä¾‹
                    total_shares = getattr(stock_info, 'shares_outstanding', None)
                    if total_shares and stock_info.free_float_shares:
                        return stock_info.free_float_shares / total_shares
            
            # é€šè¿‡Polygonè·å–è‚¡ä»½ä¿¡æ¯
            try:
                url = f"https://api.polygon.io/v3/reference/tickers/{ticker}"
                response = pc.get(url)
                if response.status_code == 200:
                    data = response.json()
                    if 'results' in data:
                        shares_outstanding = data['results'].get('share_class_shares_outstanding')
                        weighted_shares = data['results'].get('weighted_shares_outstanding')
                        if shares_outstanding and weighted_shares:
                            return min(weighted_shares / shares_outstanding, 1.0)
            except Exception:
                pass
            
            # é»˜è®¤ä¼°ç®—60%ä¸ºè‡ªç”±æµé€š
            return 0.6
        except Exception as e:
            logger.warning(f"è·å–{ticker}è‡ªç”±æµé€šä¿¡æ¯å¤±è´¥: {e}")
            return 0.6
    
    def _get_shortable_status(self, ticker: str) -> bool:
        """è·å–è‚¡ç¥¨æ˜¯å¦å¯åšç©º"""
        try:
            # å¤§å¤šæ•°ä¸»è¦è‚¡ç¥¨é»˜è®¤å¯åšç©º
            # å®é™…åº”ç”¨ä¸­å¯æ¥å…¥åˆ¸å•†APIæˆ–ç¬¬ä¸‰æ–¹æ•°æ®æº
            major_stocks = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'TSLA', 'META', 'JPM', 'JNJ']
            return ticker in major_stocks or len(ticker) <= 4  # ç®€åŒ–é€»è¾‘
        except Exception:
            return True  # é»˜è®¤å¯åšç©º
    
    def _get_borrow_fee(self, ticker: str) -> float:
        """è·å–è‚¡ç¥¨å€Ÿåˆ¸è´¹ç‡ï¼ˆå¹´åŒ–%ï¼‰"""
        try:
            # æ ¹æ®è‚¡ç¥¨æµåŠ¨æ€§å’Œçƒ­åº¦ä¼°ç®—å€Ÿåˆ¸è´¹ç‡
            # å®é™…åº”ç”¨ä¸­åº”æ¥å…¥åˆ¸å•†æˆ–ç¬¬ä¸‰æ–¹æ•°æ®æº
            # Use fixed estimates instead of random
            high_fee_stocks = ['TSLA', 'AMC', 'GME']  # é«˜è´¹ç‡è‚¡ç¥¨
            if ticker in high_fee_stocks:
                return 10.0  # Fixed 10% annualized for high-fee stocks
            else:
                return 1.0   # Fixed 1% annualized for normal stocks
        except Exception:
            return 1.0  # é»˜è®¤1%
    
    def create_traditional_features(self, data_dict: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """
        åˆ›å»ºä¼ ç»ŸæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        
        Args:
            data_dict: è‚¡ç¥¨æ•°æ®å­—å…¸
            
        Returns:
            ç‰¹å¾æ•°æ®æ¡†
        """
        logger.info("åˆ›å»ºä¼ ç»ŸæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾")
        
        all_features = []
        
        for ticker, df in data_dict.items():
            if len(df) < 20:  # ğŸ”§ è¿›ä¸€æ­¥é™ä½æœ€å°æ•°æ®è¦æ±‚ï¼Œä»30å¤©æ”¹ä¸º20å¤©ï¼Œæé«˜é€šè¿‡ç‡
                logger.warning(f"è·³è¿‡ {ticker}: æ•°æ®ä¸è¶³20å¤© ({len(df)}å¤©)")
                continue
            
            df_copy = df.copy().sort_values('date')
            
            # ä»·æ ¼ç‰¹å¾
            df_copy['returns'] = df_copy['close'].pct_change()
            df_copy['log_returns'] = np.log(df_copy['close'] / df_copy['close'].shift(1))
            
            # ç§»åŠ¨å¹³å‡
            for window in [5, 10, 20, 50]:
                df_copy[f'ma_{window}'] = df_copy['close'].rolling(window).mean()
                df_copy[f'ma_ratio_{window}'] = df_copy['close'] / df_copy[f'ma_{window}']
            
            # æ³¢åŠ¨ç‡
            for window in [10, 20, 50]:
                df_copy[f'vol_{window}'] = df_copy['log_returns'].rolling(window).std()
            
            # RSI
            def calculate_rsi(prices, window=14):
                delta = prices.diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
                rs = gain / loss
                return 100 - (100 / (1 + rs))
            
            df_copy['rsi_14'] = calculate_rsi(df_copy['close'])
            
            # æˆäº¤é‡ç‰¹å¾
            if 'volume' in df_copy.columns:
                df_copy['volume_ma_20'] = df_copy['volume'].rolling(20).mean()
                df_copy['volume_ratio'] = df_copy['volume'] / df_copy['volume_ma_20']
            
            # ä»·æ ¼ä½ç½®
            for window in [20, 50]:
                high_roll = df_copy['high'].rolling(window).max()
                low_roll = df_copy['low'].rolling(window).min()
                df_copy[f'price_position_{window}'] = (df_copy['close'] - low_roll) / (high_roll - low_roll + 1e-8)
            
            # åŠ¨é‡æŒ‡æ ‡
            for period in [5, 10, 20]:
                df_copy[f'momentum_{period}'] = df_copy['close'] / df_copy['close'].shift(period) - 1
            
            # [ENHANCED] P2 åŠ¨é‡åŠ é€Ÿåº¦ï¼ˆAccelerationï¼‰
            df_copy['momentum_10_day'] = df_copy['close'] / df_copy['close'].shift(10) - 1
            df_copy['momentum_20_day'] = df_copy['close'] / df_copy['close'].shift(20) - 1
            df_copy['acceleration_10'] = df_copy['momentum_10_day'] - df_copy['momentum_20_day']
            
            # [ENHANCED] P2 æ³¢åŠ¨èšç±»/é£é™©ä»£ç†ï¼ˆVolatility Clusteringï¼‰
            df_copy['realized_vol_20'] = (df_copy['returns'] ** 2).rolling(20).sum()
            df_copy['realized_vol_60'] = (df_copy['returns'] ** 2).rolling(60).sum()
            df_copy['vol_change'] = df_copy['realized_vol_20'] / df_copy['realized_vol_60'] - 1
            df_copy['vol_regime'] = (df_copy['realized_vol_20'] > df_copy['realized_vol_20'].rolling(252).median()).astype(int)
            
            # [ENHANCED] P2 èµ„é‡‘æµï¼ˆMoney Flowï¼‰
            if 'volume' in df_copy.columns:
                df_copy['money_flow'] = df_copy['close'] * df_copy['volume']
                df_copy['money_flow_ma_20'] = df_copy['money_flow'].rolling(20).mean()
                df_copy['money_flow_deviation'] = (df_copy['money_flow'] - df_copy['money_flow_ma_20']) / df_copy['money_flow_ma_20']
                df_copy['money_flow_rank'] = df_copy['money_flow'].rolling(60).rank(pct=True)
            
            # [ENHANCED] P2 å…¬å‘Š/è´¢æŠ¥çª—å£dummyï¼ˆEarnings Windowï¼‰
            df_copy['earnings_window_3'] = self._create_earnings_window_dummy(df_copy.index, ticker, days=3)
            df_copy['earnings_window_5'] = self._create_earnings_window_dummy(df_copy.index, ticker, days=5)
            df_copy['earnings_window_10'] = self._create_earnings_window_dummy(df_copy.index, ticker, days=10)
            
            # [ENHANCED] P2 å€Ÿåˆ¸è´¹ç‡ç‰¹å¾ï¼ˆå¦‚æœæ•°æ®å¯ç”¨ï¼‰
            if 'BORROW_FEE' in df_copy.columns:
                df_copy['borrow_fee_normalized'] = df_copy['BORROW_FEE'] / 100  # è½¬ä¸ºæ¯”ä¾‹
                df_copy['high_borrow_fee'] = (df_copy['BORROW_FEE'] > 5.0).astype(int)  # é«˜è´¹ç‡æ ‡è®°
            else:
                # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
                df_copy['borrow_fee_normalized'] = 0.01  # é»˜è®¤1%
                df_copy['high_borrow_fee'] = 0
            
            # ğŸ”´ ä¿®å¤ä¸¥é‡æ—¶é—´æ³„éœ²ï¼šä½¿ç”¨ç»Ÿä¸€çš„T10é…ç½®
            config = get_config()
            FEATURE_LAG = config.FEATURE_LAG        # ç‰¹å¾ä½¿ç”¨T-5åŠä¹‹å‰æ•°æ®
            SAFETY_GAP = config.SAFETY_GAP          # é¢å¤–å®‰å…¨é—´éš”ï¼ˆé˜²æ­¢ä¿¡æ¯æ³„éœ²ï¼‰
            PRED_START = config.PREDICTION_HORIZON  # é¢„æµ‹ä»T+10å¼€å§‹  
            PRED_END = config.PREDICTION_HORIZON    # é¢„æµ‹åˆ°T+10ç»“æŸ
            prediction_horizon = PRED_END            # å‘åå…¼å®¹
            
            # éªŒè¯æ—¶é—´å¯¹é½æ­£ç¡®æ€§
            total_gap = FEATURE_LAG + SAFETY_GAP + PRED_START
            if total_gap <= 0:
                raise ValueError(f"æ—¶é—´å¯¹é½é”™è¯¯ï¼šæ€»é—´éš” {total_gap} <= 0ï¼Œå­˜åœ¨æ•°æ®æ³„éœ²é£é™©")
            
            logger.info(f"æ—¶é—´å¯¹é½é…ç½®: ç‰¹å¾lag={FEATURE_LAG}, å®‰å…¨gap={SAFETY_GAP}, é¢„æµ‹[T+{PRED_START}, T+{PRED_END}]")
            
            # å®‰å…¨çš„ç›®æ ‡æ„å»ºï¼šTæ—¶åˆ»ä½¿ç”¨T-5ç‰¹å¾ï¼Œé¢„æµ‹Tåˆ°T+10çš„10å¤©ç´¯è®¡æ”¶ç›Š
            # ç¡®ä¿ç‰¹å¾å’Œç›®æ ‡ä¹‹é—´æœ‰è¶³å¤Ÿçš„æ—¶é—´é—´éš”ï¼ˆè‡³å°‘10æœŸï¼‰
            # æ­£ç¡®çš„10å¤©å‰å‘æ”¶ç›Šï¼š(P[T+10] - P[T]) / P[T]
            df_copy['target'] = (
                df_copy['close'].shift(-PRED_END) / 
                df_copy['close'] - 1
            )
            # ç­‰ä»·äº: df_copy['target'] = df_copy['close'].pct_change(PRED_END).shift(-PRED_END)
            
            # æ—¶é—´éªŒè¯ï¼šT+10é¢„æµ‹çš„æ­£ç¡®æ—¶é—´å¯¹é½
            # ç‰¹å¾ä½¿ç”¨T-5æ•°æ®ï¼Œé¢„æµ‹T+10æ”¶ç›Šï¼Œæ€»é—´éš”åº”ä¸º15å¤©
            feature_time = -FEATURE_LAG - SAFETY_GAP  # T-5
            prediction_time = PRED_START               # T+10
            total_time_gap = prediction_time - feature_time  # 10 - (-5) = 15å¤©
            
            if total_time_gap < 12:  # è‡³å°‘12å¤©é—´éš”ç¡®ä¿å®‰å…¨ï¼ˆ2å‘¨é¢„æµ‹ï¼‰
                logger.warning(f"æ—¶é—´é—´éš”åå°ï¼šç‰¹å¾T{feature_time} -> é¢„æµ‹T+{prediction_time}ï¼Œé—´éš”{total_time_gap}å¤©")
            else:
                logger.info(f"[OK] T+10æ—¶é—´å¯¹é½éªŒè¯é€šè¿‡ï¼šç‰¹å¾T{feature_time} -> é¢„æµ‹T+{prediction_time}ï¼Œé—´éš”{total_time_gap}å¤©")
            
            # ğŸ”¥ å…³é”®ï¼šå¼ºåˆ¶ç‰¹å¾æ»åä»¥åŒ¹é…å¢å¼ºçš„æ—¶é—´çº¿
            # ç‰¹å¾ä½¿ç”¨T-5æ•°æ®ï¼Œç›®æ ‡ä½¿ç”¨T+10åˆ°T+10ï¼Œé—´éš”15æœŸï¼ˆå®‰å…¨ï¼‰
            feature_lag = FEATURE_LAG + SAFETY_GAP  # æ‰€æœ‰ç‰¹å¾é¢å¤–æ»å4æœŸ
            
            # åœ¨åç»­feature_colså¤„ç†ä¸­ä¼šç»Ÿä¸€åº”ç”¨æ»å
            
            # æ·»åŠ è¾…åŠ©ä¿¡æ¯
            df_copy['ticker'] = ticker
            df_copy['date'] = df_copy.index
            # æ¨¡æ‹Ÿè¡Œä¸šå’Œå›½å®¶ä¿¡æ¯ï¼ˆå®é™…åº”ä»æ•°æ®æºè·å–ï¼‰
            df_copy['COUNTRY'] = 'US'
            df_copy['SECTOR'] = ticker[:2] if len(ticker) >= 2 else 'TECH'  # ç®€åŒ–åˆ†ç±»
            df_copy['SUBINDUSTRY'] = ticker[:3] if len(ticker) >= 3 else 'SOFTWARE'
            
            all_features.append(df_copy)
        
        if all_features:
            combined_features = pd.concat(all_features, ignore_index=True)
            # ğŸ”§ ä¿®å¤ç‰¹å¾çŸ©é˜µæ±¡æŸ“ï¼šä¸¥æ ¼ç­›é€‰æ•°å€¼ç‰¹å¾åˆ—
            def get_clean_numeric_features(df):
                """è·å–å¹²å‡€çš„æ•°å€¼ç‰¹å¾åˆ—ï¼Œæ’é™¤æ‰€æœ‰éæ•°å€¼å’Œæ ‡è¯†åˆ—"""
                # æ˜ç¡®æ’é™¤çš„åˆ—
                exclude_cols = {'ticker', 'date', 'target', 'COUNTRY', 'SECTOR', 'SUBINDUSTRY', 
                               'symbol', 'stock_code', 'name', 'industry', 'sector'}
                
                # åªé€‰æ‹©æ•°å€¼ç±»å‹çš„åˆ—
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                
                # è¿›ä¸€æ­¥è¿‡æ»¤ï¼šç¡®ä¿ä¸åŒ…å«ä»»ä½•å­—ç¬¦ä¸²æˆ–æ ‡è¯†ç¬¦
                clean_cols = []
                for col in numeric_cols:
                    if col not in exclude_cols and not col.lower().endswith('_name'):
                        # éªŒè¯åˆ—æ•°æ®æ˜¯å¦çœŸæ­£ä¸ºæ•°å€¼
                        if df[col].dtype in ['int64', 'float64', 'int32', 'float32']:
                            clean_cols.append(col)
                
                logger.info(f"ç‰¹å¾ç­›é€‰ï¼šæ€»åˆ—æ•°{len(df.columns)} -> æ•°å€¼åˆ—{len(numeric_cols)} -> æ¸…æ´ç‰¹å¾{len(clean_cols)}")
                return clean_cols
            
            feature_cols = get_clean_numeric_features(combined_features)
            # ğŸ”¥ å¼ºåŒ–ç‰¹å¾æ»åï¼šç¡®ä¿ä¸¥æ ¼çš„æ—¶é—´å¯¹é½
            try:
                # T-2åŸºç¡€æ»å + formation_lag(2) = æ€»å…±T-4æ»å
                # è¿™ç¡®ä¿ç‰¹å¾ä¿¡æ¯ä¸¥æ ¼æ—©äºç›®æ ‡æ—¶é—´çª—å£
                total_lag = 2 + 2  # base_lag + formation_lag
                combined_features[feature_cols] = combined_features.groupby('ticker')[feature_cols].shift(total_lag)
                logger.info(f"åº”ç”¨æ€»æ»åæœŸæ•°: {total_lag}ï¼Œç¡®ä¿ç‰¹å¾-ç›®æ ‡æ—¶é—´éš”ç¦»")
            except Exception as e:
                logger.warning(f"ç‰¹å¾æ»åå¤„ç†å¤±è´¥: {e}")
                # å›é€€åˆ°åŸºç¡€æ»å
                combined_features[feature_cols] = combined_features.groupby('ticker')[feature_cols].shift(2)
            # åŸºç¡€æ¸…æ´— - åªåˆ é™¤ç‰¹å¾å…¨ä¸ºNaNçš„è¡Œï¼Œä¿ç•™ç›®æ ‡å˜é‡
            # åˆ é™¤ç‰¹å¾å…¨ä¸ºNaNçš„è¡Œï¼Œä½†ä¿ç•™æœ‰æ•ˆç›®æ ‡çš„è¡Œ
            feature_na_mask = combined_features[feature_cols].isna().all(axis=1)
            combined_features = combined_features[~feature_na_mask]

            # ğŸ”— åˆå¹¶å®Œæ•´çš„Polygon 40+ä¸“ä¸šå› å­é›†ï¼ˆç»Ÿä¸€æ¥æº - T+1ä¼˜åŒ–ï¼‰
            try:
                # ä¿®å¤å¯¼å…¥é”™è¯¯ï¼šä½¿ç”¨æ­£ç¡®çš„æ¨¡å—è·¯å¾„
                try:
                    from autotrader.unified_polygon_factors import UnifiedPolygonFactors as PolygonCompleteFactors
                except ImportError:
                    from unified_polygon_factors import UnifiedPolygonFactors as PolygonCompleteFactors
                
                # çŸ­æœŸå› å­æš‚æ—¶ä½¿ç”¨åŸºç¡€å®ç°
                class PolygonShortTermFactors:
                    def calculate_all_short_term_factors(self, symbol):
                        # åŸºç¡€å®ç°ï¼Œè¿”å›ç©ºå­—å…¸
                        return {}
                    
                    def create_t_plus_5_prediction(self, symbol, results):
                        # åŸºç¡€å®ç°ï¼Œè¿”å›é»˜è®¤é¢„æµ‹
                        return {'signal_strength': 0.0, 'confidence': 0.5}
                
                short_term_factors = PolygonShortTermFactors()
                
                complete_factors = PolygonCompleteFactors()
                short_term_factors = PolygonShortTermFactors()
                symbols = sorted(combined_features['ticker'].unique().tolist())
                
                logger.info(f"å¼€å§‹é›†æˆPolygonç»Ÿä¸€å› å­åº“ï¼Œè‚¡ç¥¨æ•°é‡: {len(symbols)}")
                
                # è·å–å› å­åº“æ‘˜è¦ï¼ˆä½¿ç”¨å¯ç”¨æ–¹æ³•ï¼‰
                try:
                    # å°è¯•è·å–å› å­ç»Ÿè®¡ä¿¡æ¯
                    factor_stats = getattr(complete_factors, 'stats', {})
                    total_factors = factor_stats.get('total_calculations', 0)
                    logger.info(f"ç»Ÿä¸€å› å­åº“åŒ…å« {total_factors} ä¸ªå› å­è®¡ç®—")
                except Exception as e:
                    logger.info("ç»Ÿä¸€å› å­åº“å·²åˆå§‹åŒ–")
                
                # ç»Ÿä¸€å› å­é›†åˆ
                all_polygon_factors = {}
                factor_calculation_success = {}
                
                # å¯¹å‰å‡ åªä»£è¡¨æ€§è‚¡ç¥¨è®¡ç®—å› å­
                sample_symbols = symbols[:min(3, len(symbols))]  # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥é¿å…APIé™åˆ¶
                
                for symbol in sample_symbols:
                    try:
                        logger.info(f"ä¸º {symbol} è®¡ç®—ç»Ÿä¸€å› å­...")
                        
                        # ä½¿ç”¨ç»Ÿä¸€å› å­åº“çš„æ–¹æ³•
                        symbol_factors = complete_factors.calculate_all_signals(symbol)
                        
                        if symbol_factors:
                            logger.info(f"{symbol} æˆåŠŸè®¡ç®— {len(symbol_factors)} ä¸ªå› å­")
                            
                            # æå–å› å­å€¼ä½œä¸ºç‰¹å¾
                            for factor_name, result in symbol_factors.items():
                                if result.value is not None and result.data_quality_score > 0.5:
                                    col_name = f"polygon_{factor_name}"
                                    # ä½¿ç”¨å› å­å€¼
                                    factor_value = result.value
                                    if not np.isnan(factor_value) and np.isfinite(factor_value):
                                        all_polygon_factors[col_name] = factor_value
                                        factor_calculation_success[factor_name] = True
                        
                        # T+1çŸ­æœŸå› å­
                        try:
                            t5_results = short_term_factors.calculate_all_short_term_factors(symbol)
                            if t5_results:
                                prediction = short_term_factors.create_t_plus_5_prediction(symbol, t5_results)
                                
                                # T+1ä¸“ç”¨å› å­
                                for factor_name, result in t5_results.items():
                                    col_name = f"t5_{factor_name}"
                                    if hasattr(result, 't_plus_5_signal'):
                                        signal_value = result.t_plus_5_signal
                                        if not np.isnan(signal_value) and np.isfinite(signal_value):
                                            all_polygon_factors[col_name] = signal_value
                                
                                # T+1ç»¼åˆé¢„æµ‹ä¿¡å·
                                if 'signal_strength' in prediction:
                                    all_polygon_factors['t5_prediction_signal'] = prediction['signal_strength']
                                    all_polygon_factors['t5_prediction_confidence'] = prediction.get('confidence', 0.5)
                        except Exception as t5_e:
                            logger.warning(f"{symbol} T+5å› å­è®¡ç®—å¤±è´¥: {t5_e}")
                        
                        time.sleep(0.5)  # APIé™åˆ¶
                        
                    except Exception as e:
                        logger.warning(f"{symbol}å®Œæ•´å› å­è®¡ç®—å¤±è´¥: {e}")
                        continue
                
                # å°†è®¡ç®—æˆåŠŸçš„å› å­æ·»åŠ åˆ°ç‰¹å¾çŸ©é˜µ
                if all_polygon_factors:
                    logger.info(f"æˆåŠŸè®¡ç®—Polygonå› å­: {len(all_polygon_factors)} ä¸ª")
                    logger.info(f"å› å­ç±»å‹åˆ†å¸ƒ: {list(factor_calculation_success.keys())}")
                    
                    # æ·»åŠ åˆ°combined_features
                    for col_name, value in all_polygon_factors.items():
                        if col_name not in combined_features.columns:
                            # å¯¹æ‰€æœ‰è‚¡ç¥¨å¹¿æ’­è¯¥å› å­å€¼ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                            combined_features[col_name] = value
                    
                    # è®°å½•æˆåŠŸæ·»åŠ çš„å› å­æ•°é‡
                    added_factors = len(all_polygon_factors)
                    logger.info(f"[OK] æˆåŠŸæ·»åŠ  {added_factors} ä¸ªPolygonä¸“ä¸šå› å­åˆ°ç‰¹å¾çŸ©é˜µ")
                    
                    # æ˜¾ç¤ºå› å­åˆ†ç±»ç»Ÿè®¡
                    momentum_factors = len([k for k in all_polygon_factors.keys() if 'momentum' in k])
                    fundamental_factors = len([k for k in all_polygon_factors.keys() if any(x in k for x in ['earnings', 'ebit', 'yield'])])
                    quality_factors = len([k for k in all_polygon_factors.keys() if any(x in k for x in ['piotroski', 'altman', 'quality'])])
                    risk_factors = len([k for k in all_polygon_factors.keys() if any(x in k for x in ['volatility', 'beta', 'risk'])])
                    t5_factors = len([k for k in all_polygon_factors.keys() if 't5_' in k])
                    
                    logger.info(f"å› å­åˆ†å¸ƒ - åŠ¨é‡:{momentum_factors}, åŸºæœ¬é¢:{fundamental_factors}, è´¨é‡:{quality_factors}, é£é™©:{risk_factors}, T+5:{t5_factors}")
                else:
                    logger.warning("æœªèƒ½æˆåŠŸè®¡ç®—ä»»ä½•Polygonå› å­")
                
            except Exception as _e:
                logger.error(f"Polygonå®Œæ•´å› å­åº“é›†æˆå¤±è´¥: {_e}")
                import traceback
                logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
            # [ENHANCED] P2 æ ‡å‡†ç‰¹å¾å¤„ç†æµç¨‹ï¼šæ»åå¯¹é½â†’å»æå€¼â†’è¡Œä¸š/è§„æ¨¡ä¸­æ€§åŒ–â†’æ ‡å‡†åŒ–
            logger.info("[ENHANCED] åº”ç”¨P2æ ‡å‡†ç‰¹å¾å¤„ç†æµç¨‹")
            try:
                # é¢„å…ˆè·å–ä¸€æ¬¡æ‰€æœ‰tickerçš„è¡Œä¸šä¿¡æ¯ï¼Œé¿å…åœ¨å¾ªç¯ä¸­é‡å¤è·å–
                all_tickers = combined_features['ticker'].unique().tolist()
                stock_info_cache = {}
                if hasattr(self, 'market_data_manager') and self.market_data_manager:
                    try:
                        stock_info_cache = self.market_data_manager.get_batch_stock_info(all_tickers)
                        logger.info(f"é¢„è·å–{len(all_tickers)}åªè‚¡ç¥¨çš„è¡Œä¸šä¿¡æ¯å®Œæˆ")
                    except Exception as e:
                        logger.warning(f"é¢„è·å–è¡Œä¸šä¿¡æ¯å¤±è´¥: {e}")
                else:
                    logger.debug("å¸‚åœºæ•°æ®ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡è¡Œä¸šä¿¡æ¯è·å–")
                
                # P2æ ‡å‡†æµç¨‹ï¼šæŒ‰æ—¥æœŸåˆ†ç»„ï¼Œé€æ—¥è¿›è¡Œå®Œæ•´çš„å¤„ç†ç®¡é“
                neutralized_features = []
                
                # ğŸ” DEBUG: æ·»åŠ æ—¥æœŸå¤„ç†è¿›åº¦ç›‘æ§
                unique_dates = combined_features['date'].unique()
                total_dates = len(unique_dates)
                logger.info(f"[DEBUG] å¼€å§‹å¤„ç† {total_dates} ä¸ªäº¤æ˜“æ—¥çš„ç‰¹å¾æ•°æ®")
                
                for date_idx, (date, group) in enumerate(combined_features.groupby('date')):
                    group_features = group[feature_cols].copy()
                    group_meta = group[['ticker', 'SECTOR', 'COUNTRY']].copy()
                    
                    # P2 Step 1: æ»åå¯¹é½ï¼ˆå·²åœ¨å‰é¢å®Œæˆï¼‰
                    # ğŸ” DEBUG: å‡çº§ä¸ºINFOçº§åˆ«æ—¥å¿—ï¼Œä¾¿äºç›‘æ§è¿›åº¦
                    if date_idx % max(1, total_dates // 10) == 0:  # æ¯10%è¿›åº¦æ‰“å°ä¸€æ¬¡
                        logger.info(f"[PROGRESS] å¤„ç†æ—¥æœŸ {date} ({date_idx+1}/{total_dates}, {((date_idx+1)/total_dates*100):.1f}%), è‚¡ç¥¨æ•°: {len(group)}")
                    else:
                        logger.debug(f"Processing {len(group)} stocks for date {date}")
                    
                    # P2 Step 2: å¢å¼ºå»æå€¼å¤„ç†ï¼ˆMAD/Winsorize + åˆ†å¸ƒæ£€æŸ¥ï¼‰
                    for col in feature_cols:
                        if group_features[col].notna().sum() > 2:
                            # ä½¿ç”¨MADï¼ˆä¸­ä½æ•°ç»å¯¹åå·®ï¼‰è¿›è¡Œç¨³å¥çš„å¼‚å¸¸å€¼æ£€æµ‹
                            median_val = group_features[col].median()
                            mad_val = (group_features[col] - median_val).abs().median()
                            
                            # å¢å¼º: å…ˆæ£€æŸ¥ç‰¹å¾åˆ†å¸ƒç¨³å®šæ€§
                            feature_std = group_features[col].std()
                            feature_skewness = group_features[col].skew() if len(group_features[col].dropna()) > 3 else 0
                            
                            # å¯¹äºé«˜åº¦åæ–œçš„ç‰¹å¾ä½¿ç”¨æ›´ä¿å®ˆçš„é˜ˆå€¼
                            if abs(feature_skewness) > 2:  # é«˜åæ–œ
                                threshold_multiplier = 2  # æ›´ä¿å®ˆ
                                logger.debug(f"ç‰¹å¾{col}åæ–œåº¦{feature_skewness:.2f}ï¼Œä½¿ç”¨ä¿å®ˆé˜ˆå€¼")
                            else:
                                threshold_multiplier = 3  # æ ‡å‡†é˜ˆå€¼
                            
                            if mad_val > 0:
                                # ä½¿ç”¨è°ƒæ•´åçš„MADé˜ˆå€¼
                                threshold = threshold_multiplier * 1.4826 * mad_val  # 1.4826ä½¿MADä¸æ ‡å‡†å·®ä¸€è‡´
                                lower_bound = median_val - threshold
                                upper_bound = median_val + threshold
                                
                                # è®°å½•å¼‚å¸¸å€¼æ•°é‡
                                outliers = (group_features[col] < lower_bound) | (group_features[col] > upper_bound)
                                outlier_count = outliers.sum()
                                if outlier_count > 0:
                                    logger.debug(f"ç‰¹å¾{col}åœ¨{date}å‘ç°{outlier_count}ä¸ªå¼‚å¸¸å€¼ï¼Œé˜ˆå€¼[{lower_bound:.3f}, {upper_bound:.3f}]")
                                
                                group_features[col] = group_features[col].clip(lower=lower_bound, upper=upper_bound)
                            else:
                                # å›é€€åˆ°åˆ†ä½æ•°æˆªæ–­ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„åˆ†ä½æ•°
                                if abs(feature_skewness) > 2:
                                    q_lower, q_upper = 0.02, 0.98  # æ›´ä¿å®ˆ
                                else:
                                    q_lower, q_upper = 0.01, 0.99  # æ ‡å‡†
                                
                                q_vals = group_features[col].quantile([q_lower, q_upper])
                                outliers = (group_features[col] < q_vals.iloc[0]) | (group_features[col] > q_vals.iloc[1])
                                if outliers.sum() > 0:
                                    logger.debug(f"ç‰¹å¾{col}åœ¨{date}ä½¿ç”¨åˆ†ä½æ•°æ³•å‘ç°{outliers.sum()}ä¸ªå¼‚å¸¸å€¼")
                                
                                group_features[col] = group_features[col].clip(lower=q_vals.iloc[0], upper=q_vals.iloc[1])
                    
                    # P2 Step 3: è¡Œä¸š/è§„æ¨¡ä¸­æ€§åŒ–
                    for col in feature_cols:
                        if group_features[col].notna().sum() > 5:  # è‡³å°‘éœ€è¦5ä¸ªè§‚æµ‹å€¼
                            try:
                                # æ„å»ºä¸­æ€§åŒ–å›å½’çŸ©é˜µ
                                X_neutralize = pd.get_dummies(group_meta['SECTOR'], prefix='sector', drop_first=True)
                                
                                # æ·»åŠ è§„æ¨¡å› å­ï¼ˆå¦‚æœæ•°æ®å¯ç”¨ï¼‰
                                if 'market_cap' in group.columns:
                                    X_neutralize['log_market_cap'] = np.log(group['market_cap'].fillna(group['market_cap'].median()))
                                elif 'money_flow' in group_features.columns:
                                    # ä½¿ç”¨èµ„é‡‘æµä½œä¸ºè§„æ¨¡ä»£ç†
                                    X_neutralize['log_money_flow'] = np.log(group_features['money_flow'].fillna(group_features['money_flow'].median()) + 1)
                                
                                # æ‰§è¡Œå›å½’ä¸­æ€§åŒ–
                                if len(X_neutralize.columns) > 0 and X_neutralize.shape[0] > X_neutralize.shape[1]:
                                    from sklearn.linear_model import LinearRegression
                                    reg = LinearRegression(fit_intercept=True)
                                    
                                    # åªå¯¹æœ‰æ•ˆæ•°æ®è¿›è¡Œå›å½’
                                    valid_mask = group_features[col].notna() & X_neutralize.notna().all(axis=1)
                                    if valid_mask.sum() > X_neutralize.shape[1] + 1:
                                        reg.fit(X_neutralize[valid_mask], group_features.loc[valid_mask, col])
                                        
                                        # è®¡ç®—æ®‹å·®ä½œä¸ºä¸­æ€§åŒ–åçš„å› å­å€¼
                                        predictions = reg.predict(X_neutralize[valid_mask])
                                        group_features.loc[valid_mask, col] = group_features.loc[valid_mask, col] - predictions
                                        
                            except Exception as e:
                                logger.debug(f"Factor {col} neutralization failed: {e}")
                                # å¦‚æœä¸­æ€§åŒ–å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸå§‹å€¼
                                pass
                    
                    # P2 Step 4: æ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰
                    for col in feature_cols:
                        if group_features[col].notna().sum() > 2:
                            mean_val = group_features[col].mean()
                            std_val = group_features[col].std()
                            if std_val > 0:
                                group_features[col] = (group_features[col] - mean_val) / std_val
                            else:
                                group_features[col] = 0.0
                    
                    # 3. è¡Œä¸šä¸­æ€§åŒ–ï¼ˆä½¿ç”¨é¢„è·å–çš„è¡Œä¸šä¿¡æ¯ï¼‰
                    if stock_info_cache:
                        try:
                            tickers = group['ticker'].tolist()
                            industries = {}
                            for ticker in tickers:
                                info = stock_info_cache.get(ticker)
                                if info:
                                    sector = info.gics_sub_industry or info.gics_industry or info.sector
                                    industries[ticker] = sector or 'Unknown'
                                else:
                                    industries[ticker] = 'Unknown'
                            
                            # æŒ‰è¡Œä¸šå»å‡å€¼
                            group_with_industry = group_features.copy()
                            group_with_industry['industry'] = group['ticker'].map(industries)
                            
                            for col in feature_cols:
                                if group_with_industry[col].notna().sum() > 2:
                                    industry_means = group_with_industry.groupby('industry')[col].transform('mean')
                                    group_features[col] = group_features[col] - industry_means
                                    
                        except Exception as e:
                            logger.debug(f"è¡Œä¸šä¸­æ€§åŒ–è·³è¿‡: {e}")
                    
                    # ä¿ç•™éç‰¹å¾åˆ—
                    group_result = group[['date', 'ticker']].copy()
                    group_result[feature_cols] = group_features[feature_cols]
                    neutralized_features.append(group_result)
                
                # åˆå¹¶ç»“æœ
                neutralized_df = pd.concat(neutralized_features, ignore_index=True)
                combined_features[feature_cols] = neutralized_df[feature_cols]
                
                logger.info(f"ç®€åŒ–ä¸­æ€§åŒ–å®Œæˆï¼Œå¤„ç†{len(feature_cols)}ä¸ªç‰¹å¾")
                
            except Exception as e:
                logger.warning(f"ç®€åŒ–ä¸­æ€§åŒ–å¤±è´¥: {e}")
                logger.info("ä½¿ç”¨åŸå§‹ç‰¹å¾ï¼Œä»…è¿›è¡Œæ ‡å‡†åŒ–")
                # æœ€ç®€å•çš„å›é€€ï¼šå…¨å±€æ ‡å‡†åŒ–
                from sklearn.preprocessing import StandardScaler
                scaler = StandardScaler()
                try:
                    # === æ™ºèƒ½å¤šé‡å…±çº¿æ€§å¤„ç†é›†æˆ ===
                    logger.info("å¼€å§‹åº”ç”¨æ™ºèƒ½å¤šé‡å…±çº¿æ€§å¤„ç†...")
                    try:
                        # å‡†å¤‡ç‰¹å¾æ•°æ®è¿›è¡Œå…±çº¿æ€§åˆ†æ
                        feature_data = combined_features[feature_cols].copy()
                        
                        # åº”ç”¨æ™ºèƒ½å¤šé‡å…±çº¿æ€§å¤„ç†
                        processed_features, process_info = self.apply_intelligent_multicollinearity_processing(feature_data)
                        
                        # æ ¹æ®å¤„ç†ç»“æœæ›´æ–°ç‰¹å¾çŸ©é˜µ
                        if process_info['success']:
                            if process_info['method_used'] == 'pca':
                                # PCAå¤„ç†ï¼šæ›¿æ¢ä¸ºä¸»æˆåˆ†
                                logger.info("åº”ç”¨PCAä¸»æˆåˆ†æ›¿æ¢åŸå§‹ç‰¹å¾...")
                                
                                # ä¿ç•™éç‰¹å¾åˆ—ï¼ˆå¦‚date, tickerç­‰ï¼‰
                                non_feature_cols = [col for col in combined_features.columns if col not in feature_cols]
                                base_df = combined_features[non_feature_cols].copy()
                                
                                # æ·»åŠ ä¸»æˆåˆ†
                                for col in processed_features.columns:
                                    base_df[col] = processed_features[col]
                                
                                combined_features = base_df
                                
                                # æ›´æ–°ç‰¹å¾åˆ—åˆ—è¡¨ä¸ºä¸»æˆåˆ†
                                feature_cols = processed_features.columns.tolist()
                                
                                pca_info = process_info['pca_info']
                                logger.info(f"âœ“ PCAå¤„ç†å®Œæˆ: è§£é‡Šæ–¹å·®{pca_info['variance_explained_total']:.3f}")
                                logger.info(f"âœ“ ç‰¹å¾ç»´åº¦: {process_info['original_shape'][1]} -> {process_info['final_shape'][1]}")
                                
                            else:
                                # æ ‡å‡†åŒ–å¤„ç†ï¼šç›´æ¥æ›´æ–°ç‰¹å¾
                                combined_features[feature_cols] = processed_features[feature_cols]
                                logger.info(f"âœ“ æ ‡å‡†åŒ–å¤„ç†å®Œæˆ: {process_info['method_used']}")
                            
                            logger.info(f"âœ“ å¤šé‡å…±çº¿æ€§å¤„ç†æˆåŠŸ: {', '.join(process_info['processing_details'])}")
                            
                        else:
                            # å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ–¹æ³•
                            logger.warning("å¤šé‡å…±çº¿æ€§å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†æ ‡å‡†åŒ–")
                            combined_features[feature_cols] = scaler.fit_transform(combined_features[feature_cols].fillna(0))
                            
                    except Exception as e:
                        logger.warning(f"å¤šé‡å…±çº¿æ€§å¤„ç†å¼‚å¸¸: {e}")
                        # å›é€€åˆ°åŸå§‹å¤„ç†
                        combined_features[feature_cols] = scaler.fit_transform(combined_features[feature_cols].fillna(0))
                    # === å¤šé‡å…±çº¿æ€§å¤„ç†ç»“æŸ ===
                except Exception:
                    pass
            
            logger.info(f"ä¼ ç»Ÿç‰¹å¾åˆ›å»ºå®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {combined_features.shape}")
            return combined_features
        else:
            logger.error("æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾æ•°æ®")
            return pd.DataFrame()
    
    def get_data_and_features(self, tickers: List[str], start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """
        è·å–æ•°æ®å¹¶åˆ›å»ºç‰¹å¾çš„ç»„åˆæ–¹æ³•
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            åŒ…å«ç‰¹å¾çš„DataFrame
        """
        try:
            logger.info(f"å¼€å§‹è·å–æ•°æ®å’Œç‰¹å¾ï¼Œè‚¡ç¥¨: {len(tickers)}åªï¼Œæ—¶é—´: {start_date} - {end_date}")
            
            # 1. ä¸‹è½½è‚¡ç¥¨æ•°æ®
            stock_data = self.download_stock_data(tickers, start_date, end_date)
            if not stock_data:
                logger.error("è‚¡ç¥¨æ•°æ®ä¸‹è½½å¤±è´¥")
                return None
            
            logger.info(f"âœ… è‚¡ç¥¨æ•°æ®ä¸‹è½½å®Œæˆ: {len(stock_data)}åªè‚¡ç¥¨")
            
            # 2. åˆ›å»ºä¼ ç»Ÿç‰¹å¾
            feature_data = self.create_traditional_features(stock_data)
            if feature_data.empty:
                logger.error("ä¼ ç»Ÿç‰¹å¾åˆ›å»ºå¤±è´¥")
                return None
            
            logger.info(f"âœ… ä¼ ç»Ÿç‰¹å¾åˆ›å»ºå®Œæˆ: {feature_data.shape}")
            
            # 3. é›†æˆAlphaæ‘˜è¦ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            alpha_integration_success = False
            try:
                alpha_result = self._integrate_alpha_summary_features(feature_data, stock_data)
                
                # ğŸ”§ CRITICAL FIX: ä¿®å¤Alphaé›†æˆçŠ¶æ€åˆ¤æ–­é€»è¾‘ï¼Œé¿å…çŸ›ç›¾æ—¥å¿—
                if alpha_result is not None and not alpha_result.empty:
                    # æ£€æŸ¥æ˜¯å¦çœŸçš„åŒ…å«Alphaç‰¹å¾ï¼ˆé€šè¿‡åˆ—æ•°å˜åŒ–ï¼‰
                    original_cols = feature_data.shape[1]
                    result_cols = alpha_result.shape[1]
                    
                    if result_cols > original_cols:
                        # åˆ—æ•°å¢åŠ ï¼Œè¯´æ˜æˆåŠŸæ·»åŠ äº†Alphaç‰¹å¾
                        feature_data = alpha_result
                        alpha_integration_success = True
                        added_features = result_cols - original_cols
                        logger.info(f"âœ… Alphaæ‘˜è¦ç‰¹å¾é›†æˆæˆåŠŸï¼Œæœ€ç»ˆå½¢çŠ¶: {feature_data.shape}")
                        logger.info(f"   - æ–°å¢Alphaç‰¹å¾: {added_features}ä¸ª")
                    else:
                        # åˆ—æ•°ç›¸åŒï¼Œè¯´æ˜æ²¡æœ‰æˆåŠŸæ·»åŠ Alphaç‰¹å¾
                        logger.warning("âš ï¸ Alphaæ‘˜è¦ç‰¹å¾æœªç”Ÿæˆæ–°ç‰¹å¾ï¼Œä½¿ç”¨ä¼ ç»Ÿç‰¹å¾")
                else:
                    # alpha_resultä¸ºNoneæˆ–emptyï¼Œæ˜ç¡®è¡¨ç¤ºAlphaé›†æˆå¤±è´¥
                    logger.warning("âš ï¸ Alphaæ‘˜è¦ç‰¹å¾é›†æˆå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿç‰¹å¾")
            except Exception as e:
                logger.warning(f"âš ï¸ Alphaæ‘˜è¦ç‰¹å¾é›†æˆå¼‚å¸¸: {e}ï¼Œä½¿ç”¨ä¼ ç»Ÿç‰¹å¾")
            
            # è®°å½•é›†æˆçŠ¶æ€ç”¨äºåç»­éªŒè¯
            if hasattr(self, '_debug_info'):
                self._debug_info['alpha_integration_success'] = alpha_integration_success
            
            return feature_data
            
        except Exception as e:
            logger.error(f"è·å–æ•°æ®å’Œç‰¹å¾å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _integrate_alpha_summary_features(self, 
                                        feature_data: pd.DataFrame, 
                                        stock_data: Dict[str, pd.DataFrame]) -> Optional[pd.DataFrame]:
        """
        ğŸ”¥ Route A: Alphaæ‘˜è¦ç‰¹å¾é›†æˆåˆ°ä¼ ç»ŸML pipeline
        
        æ ¸å¿ƒè®¾è®¡ï¼š
        1. æœ€å°ä¾µå…¥æ€§ï¼šåœ¨X_cleanåŸºç¡€ä¸Šæ·»åŠ 5-10ä¸ªAlphaæ‘˜è¦ç‰¹å¾ â†’ X_fused
        2. ä¸¥æ ¼æ—¶é—´å¯¹é½ï¼šç¡®ä¿Alphaç‰¹å¾ä»…ä½¿ç”¨å†å²æ•°æ®
        3. æ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼šæŒ‰äº¤æ˜“æ—¥è¿›è¡Œå»æå€¼å’Œæ ‡å‡†åŒ–
        4. é™ç»´å‹ç¼©ï¼šä»45+ä¸ªAlpha â†’ 6-10ä¸ªæ½œå› å­
        5. æ‘˜è¦ç»Ÿè®¡ï¼šæ•æ‰Alphaä¿¡å·çš„è´¨é‡å’Œä¸€è‡´æ€§
        
        Args:
            feature_data: ä¼ ç»Ÿç‰¹å¾æ•°æ® (date, ticker, traditional_features, target)
            stock_data: åŸå§‹è‚¡ç¥¨æ•°æ®å­—å…¸ (ç”¨äºè®¡ç®—Alpha)
            
        Returns:
            X_fused: èåˆäº†Alphaæ‘˜è¦ç‰¹å¾çš„ç‰¹å¾æ•°æ®
        """
        logger.info("å¼€å§‹Route A Alphaæ‘˜è¦ç‰¹å¾é›†æˆ...")
        
        # â­ ä¼˜å…ˆä½¿ç”¨é«˜çº§Alphaç³»ç»Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.advanced_alpha_system is not None:
            logger.info("ä½¿ç”¨é«˜çº§Alphaç³»ç»Ÿï¼ˆä¸“ä¸šæœºæ„çº§ï¼‰")
            try:
                # å‡†å¤‡æ•°æ®
                raw_data = self._prepare_data_for_advanced_alpha(stock_data)
                returns = feature_data['target'] if 'target' in feature_data.columns else pd.Series()
                
                # ä½¿ç”¨é«˜çº§Alphaç³»ç»Ÿå¤„ç†
                advanced_features = self.advanced_alpha_system.process_complete_pipeline(
                    raw_data=raw_data,
                    returns=returns,
                    market_data=None  # å¯ä»¥ä¼ å…¥å¸‚åœºæ•°æ®
                )
                
                # åˆå¹¶åˆ°ä¸»ç‰¹å¾æ•°æ®
                if advanced_features is not None and not advanced_features.empty:
                    logger.info(f"âœ… é«˜çº§Alphaç³»ç»Ÿç”Ÿæˆ {advanced_features.shape[1]} ä¸ªç‰¹å¾")
                    
                    # å¯¹é½ç´¢å¼•
                    advanced_features.index = feature_data.index[:len(advanced_features)]
                    
                    # åˆå¹¶ç‰¹å¾
                    X_fused = pd.concat([feature_data, advanced_features], axis=1)
                    
                    # è·å–æ€§èƒ½æŠ¥å‘Š
                    perf_summary = self.advanced_alpha_system.performance_monitor.get_performance_summary()
                    if perf_summary:
                        current = perf_summary.get('current', {})
                        logger.info(f"  Rank IC: {current.get('rank_ic', 0):.4f}")
                        logger.info(f"  Sharpe: {current.get('sharpe_ratio', 0):.2f}")
                    
                    return X_fused
                    
            except Exception as e:
                logger.warning(f"é«˜çº§Alphaç³»ç»Ÿå¤„ç†å¤±è´¥: {e}, å›é€€åˆ°åŸºç¡€å¤„ç†")
        
        # å›é€€åˆ°åŸå§‹Alphaæ‘˜è¦å¤„ç†å™¨
        try:
            # å¯¼å…¥Alphaæ‘˜è¦å¤„ç†å™¨
            from alpha_summary_features import create_alpha_summary_processor, AlphaSummaryConfig
            
            # é…ç½®Alphaæ‘˜è¦ç‰¹å¾å¤„ç†å™¨
            alpha_config = AlphaSummaryConfig(
                max_alpha_features=18,  # ä½¿ç”¨ä¸“ä¸šæ ‡å‡†ï¼š18ä¸ªç‰¹å¾
                winsorize_lower=0.01,
                winsorize_upper=0.99,
                use_pca=True,
                pca_variance_explained=0.65,
                use_ic_weighted_composite=True,
                include_dispersion=True,
                include_agreement=True,
                include_quality=True,
                strict_time_validation=True,
                data_type='float32'
            )
            
            processor = create_alpha_summary_processor(alpha_config.__dict__)
            
            # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—Alphaå› å­ä¿¡å·
            alpha_signals = self._compute_alpha_signals_for_integration(stock_data, feature_data)
            if alpha_signals is None or alpha_signals.empty:
                logger.warning("Alphaä¿¡å·è®¡ç®—å¤±è´¥ï¼Œè·³è¿‡Alphaç‰¹å¾é›†æˆ")
                return feature_data
            
            # ç¬¬äºŒæ­¥ï¼šåˆ›å»ºå¸‚åœºèƒŒæ™¯æ•°æ®ï¼ˆç”¨äºä¸­æ€§åŒ–ï¼‰
            market_context = self._create_market_context_data(stock_data, feature_data)
            
            # ç¬¬ä¸‰æ­¥ï¼šæå–ç›®æ ‡æ—¥æœŸï¼ˆç”¨äºæ—¶é—´éªŒè¯ï¼‰
            target_dates = pd.to_datetime(feature_data['date']).unique()
            
            # ç¬¬å››æ­¥ï¼šå¤„ç†Alphaä¿¡å· â†’ æ‘˜è¦ç‰¹å¾
            alpha_summary_features = processor.process_alpha_to_summary(
                alpha_df=alpha_signals,
                market_data=market_context,
                target_dates=pd.Series(target_dates)
            )
            
            if alpha_summary_features.empty:
                logger.warning("Alphaæ‘˜è¦ç‰¹å¾ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡é›†æˆ")
                # ğŸ”§ CRITICAL FIX: è¿”å›Noneä»¥æ˜ç¡®è¡¨ç¤ºAlphaé›†æˆå¤±è´¥ï¼Œè€Œä¸æ˜¯è¿”å›åŸå§‹æ•°æ®
                return None
            
            # ç¬¬äº”æ­¥ï¼šå¯¹é½å’Œåˆå¹¶ç‰¹å¾ï¼ˆX_clean + alpha_features â†’ X_fusedï¼‰
            X_fused = self._merge_alpha_and_traditional_features(
                feature_data, alpha_summary_features
            )
            
            if X_fused is None or X_fused.empty:
                logger.warning("ç‰¹å¾åˆå¹¶å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿç‰¹å¾")
                # ğŸ”§ CRITICAL FIX: è¿”å›Noneä»¥æ˜ç¡®è¡¨ç¤ºç‰¹å¾åˆå¹¶å¤±è´¥
                return None
            
            # è·å–å¤„ç†ç»Ÿè®¡
            stats = processor.get_processing_stats()
            logger.info(f"Alphaæ‘˜è¦ç‰¹å¾é›†æˆç»Ÿè®¡:")
            logger.info(f"  - åŸå§‹Alphaæ•°é‡: {stats.get('total_alphas_processed', 0)}")
            logger.info(f"  - ç”Ÿæˆæ‘˜è¦ç‰¹å¾: {stats.get('features_generated', 0)}")
            logger.info(f"  - æ—¶é—´è¿è§„: {stats.get('time_violations', 0)}")
            logger.info(f"  - å‹ç¼©æ–¹å·®è§£é‡Š: {stats.get('compression_variance_explained', 0):.3f}")
            
            return X_fused
            
        except Exception as e:
            logger.error(f"Alphaæ‘˜è¦ç‰¹å¾é›†æˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return feature_data
    
    def _compute_alpha_signals_for_integration(self, 
                                             stock_data: Dict[str, pd.DataFrame], 
                                             feature_data: pd.DataFrame) -> Optional[pd.DataFrame]:
        """è®¡ç®—Alphaä¿¡å·ç”¨äºæ‘˜è¦ç‰¹å¾æå–"""
        try:
            if not hasattr(self, 'alpha_engine') or not self.alpha_engine:
                logger.warning("Alphaå¼•æ“ä¸å¯ç”¨")
                return None
            
            # å‡†å¤‡Alphaè®¡ç®—æ‰€éœ€çš„æ•°æ®æ ¼å¼
            alpha_input_data = []
            
            for ticker, df in stock_data.items():
                if len(df) < 30:
                    continue
                    
                df_copy = df.copy().sort_values('date')
                df_copy['ticker'] = ticker
                
                # æ ‡å‡†åŒ–åˆ—åï¼šå°†å°å†™åˆ—åæ˜ å°„åˆ°Alphaå¼•æ“éœ€è¦çš„å¤§å†™åˆ—å
                column_mapping = {
                    'close': 'Close',
                    'high': 'High', 
                    'low': 'Low',
                    'open': 'Open',
                    'volume': 'Volume',
                    'adj_close': 'Close',  # ä½¿ç”¨è°ƒæ•´åæ”¶ç›˜ä»·
                    'adjclose': 'Close',   # å¦ä¸€ç§æ ¼å¼
                    'Adj Close': 'Close'   # yfinanceæ ¼å¼
                }
                
                # åº”ç”¨åˆ—åæ˜ å°„
                for old_col, new_col in column_mapping.items():
                    if old_col in df_copy.columns and new_col not in df_copy.columns:
                        df_copy[new_col] = df_copy[old_col]
                
                # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
                required_cols = ['date', 'ticker', 'Close', 'Volume', 'High', 'Low']
                missing_cols = [col for col in required_cols if col not in df_copy.columns]
                
                if missing_cols:
                    logger.warning(f"è‚¡ç¥¨{ticker}ç¼ºå°‘å¿…è¦åˆ— {missing_cols}ï¼Œè·³è¿‡")
                    continue
                
                # å¦‚æœæ²¡æœ‰Openåˆ—ï¼Œç”¨Closeæ›¿ä»£ï¼ˆä¸€äº›æ•°æ®æºå¯èƒ½ç¼ºå¤±ï¼‰
                if 'Open' not in df_copy.columns:
                    df_copy['Open'] = df_copy['Close']
                    logger.debug(f"è‚¡ç¥¨{ticker}ç¼ºå°‘Openåˆ—ï¼Œä½¿ç”¨Closeæ›¿ä»£")
                
                # é€‰æ‹©æœ€ç»ˆéœ€è¦çš„åˆ—
                final_cols = ['date', 'ticker', 'Close', 'Volume', 'High', 'Low', 'Open']
                alpha_input_data.append(df_copy[final_cols])
            
            if not alpha_input_data:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„Alphaè¾“å…¥æ•°æ®")
                return None
            
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            combined_alpha_data = pd.concat(alpha_input_data, ignore_index=True)
            combined_alpha_data['date'] = pd.to_datetime(combined_alpha_data['date'])
            combined_alpha_data = combined_alpha_data.sort_values(['date', 'ticker'])
            
            # ä½¿ç”¨Alphaå¼•æ“è®¡ç®—æ‰€æœ‰Alphaå› å­
            alpha_signals = self.alpha_engine.compute_all_alphas(combined_alpha_data)
            
            if alpha_signals is None or alpha_signals.empty:
                logger.warning("Alphaå¼•æ“æœªç”Ÿæˆä»»ä½•ä¿¡å·")
                return None
            
            # ç¡®ä¿ç´¢å¼•æ ¼å¼æ­£ç¡®ï¼ˆmulti-index: date, tickerï¼‰
            if not isinstance(alpha_signals.index, pd.MultiIndex):
                alpha_signals = alpha_signals.set_index(['date', 'ticker'])
            
            logger.info(f"Alphaä¿¡å·è®¡ç®—å®Œæˆ: {alpha_signals.shape}")
            return alpha_signals
            
        except Exception as e:
            logger.error(f"Alphaä¿¡å·è®¡ç®—å¤±è´¥: {e}")
            return None
    
    def _create_market_context_data(self, 
                                  stock_data: Dict[str, pd.DataFrame], 
                                  feature_data: pd.DataFrame) -> Optional[pd.DataFrame]:
        """åˆ›å»ºå¸‚åœºèƒŒæ™¯æ•°æ®ç”¨äºAlphaä¸­æ€§åŒ–"""
        try:
            market_context_data = []
            
            for ticker, df in stock_data.items():
                if len(df) < 10:
                    continue
                    
                df_copy = df.copy().sort_values('date')
                df_copy['ticker'] = ticker
                
                # è®¡ç®—å¸‚å€¼ä»£ç†ï¼ˆä»·æ ¼*æˆäº¤é‡ï¼‰
                if 'Close' in df_copy.columns and 'Volume' in df_copy.columns:
                    df_copy['market_cap'] = df_copy['Close'] * df_copy['Volume']
                else:
                    df_copy['market_cap'] = 1.0  # é»˜è®¤å€¼
                
                # ç®€åŒ–çš„è¡Œä¸šåˆ†ç±»ï¼ˆåŸºäºè‚¡ç¥¨ä»£ç å‰ç¼€ï¼‰
                if ticker.startswith(('A', 'B', 'C')):
                    df_copy['industry'] = 'Tech'
                elif ticker.startswith(('D', 'E', 'F')):
                    df_copy['industry'] = 'Finance'
                elif ticker.startswith(('G', 'H', 'I')):
                    df_copy['industry'] = 'Healthcare'
                elif ticker.startswith(('J', 'K', 'L')):
                    df_copy['industry'] = 'Consumer'
                else:
                    df_copy['industry'] = 'Others'
                
                market_context_data.append(df_copy[['date', 'ticker', 'market_cap', 'industry']])
            
            if market_context_data:
                combined_context = pd.concat(market_context_data, ignore_index=True)
                combined_context['date'] = pd.to_datetime(combined_context['date'])
                return combined_context
            else:
                return None
                
        except Exception as e:
            logger.warning(f"å¸‚åœºèƒŒæ™¯æ•°æ®åˆ›å»ºå¤±è´¥: {e}")
            return None
    
    def _merge_alpha_and_traditional_features(self, 
                                            feature_data: pd.DataFrame, 
                                            alpha_summary_features: pd.DataFrame) -> Optional[pd.DataFrame]:
        """åˆå¹¶ä¼ ç»Ÿç‰¹å¾å’ŒAlphaæ‘˜è¦ç‰¹å¾ (X_clean + alpha_features â†’ X_fused)"""
        try:
            # ç¡®ä¿ä¸¤ä¸ªæ•°æ®æ¡†éƒ½æœ‰æ­£ç¡®çš„é”®ç”¨äºåˆå¹¶
            feature_data_copy = feature_data.copy()
            feature_data_copy['date'] = pd.to_datetime(feature_data_copy['date'])
            
            # åˆ›å»ºåˆå¹¶é”®
            feature_data_copy['merge_key'] = feature_data_copy['date'].astype(str) + '_' + feature_data_copy['ticker'].astype(str)
            
            # Alphaæ‘˜è¦ç‰¹å¾çš„ç´¢å¼•æ ¼å¼å¤„ç†
            if isinstance(alpha_summary_features.index, pd.MultiIndex):
                alpha_df_for_merge = alpha_summary_features.reset_index()
                alpha_df_for_merge['date'] = pd.to_datetime(alpha_df_for_merge['date'])
                alpha_df_for_merge['merge_key'] = alpha_df_for_merge['date'].astype(str) + '_' + alpha_df_for_merge['ticker'].astype(str)
            else:
                logger.warning("Alphaæ‘˜è¦ç‰¹å¾ç´¢å¼•æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡åˆå¹¶")
                return feature_data
            
            # æ‰§è¡Œå·¦è¿æ¥ï¼ˆä»¥ä¼ ç»Ÿç‰¹å¾ä¸ºä¸»ï¼‰
            merged_data = feature_data_copy.merge(
                alpha_df_for_merge.drop(['date', 'ticker'], axis=1), 
                on='merge_key', 
                how='left'
            )
            
            # åˆ é™¤ä¸´æ—¶åˆå¹¶é”®
            merged_data = merged_data.drop('merge_key', axis=1)
            
            # å¤„ç†ç¼ºå¤±çš„Alphaç‰¹å¾ï¼ˆä½¿ç”¨æ¨ªæˆªé¢ä¸­ä½æ•°å¡«å……ï¼‰
            alpha_cols = [col for col in alpha_df_for_merge.columns 
                         if col.startswith('alpha_') and col not in ['date', 'ticker', 'merge_key']]
            
            for alpha_col in alpha_cols:
                if alpha_col in merged_data.columns:
                    # æŒ‰æ—¥æœŸåˆ†ç»„ï¼Œä½¿ç”¨æ¨ªæˆªé¢ä¸­ä½æ•°å¡«å……
                    merged_data[alpha_col] = merged_data.groupby('date')[alpha_col].transform(
                        lambda x: x.fillna(x.median())
                    )
                    # å¦‚æœä»æœ‰NaNï¼Œç”¨0å¡«å……
                    merged_data[alpha_col] = merged_data[alpha_col].fillna(0)
            
            logger.info(f"ç‰¹å¾åˆå¹¶å®Œæˆ: {feature_data.shape} + Alpha â†’ {merged_data.shape}")
            logger.info(f"æ–°å¢Alphaæ‘˜è¦ç‰¹å¾: {alpha_cols}")
            
            return merged_data
            
        except Exception as e:
            logger.error(f"ç‰¹å¾åˆå¹¶å¤±è´¥: {e}")
            return feature_data
    
    def detect_multicollinearity(self, X: pd.DataFrame, vif_threshold: float = 10.0) -> Dict[str, Any]:
        """
        æ£€æµ‹å› å­é—´çš„å¤šé‡å…±çº¿æ€§
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            vif_threshold: VIFé˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼è®¤ä¸ºå­˜åœ¨å…±çº¿æ€§
            
        Returns:
            å…±çº¿æ€§æ£€æµ‹ç»“æœ
        """
        try:
            results = {
                'high_vif_features': [],
                'correlation_matrix': None,
                'highly_correlated_pairs': [],
                'vif_scores': {},
                'needs_pca': False,
                'max_correlation': 0.0
            }
            
            logger.info(f"å¼€å§‹å…±çº¿æ€§æ£€æµ‹ï¼Œç‰¹å¾æ•°é‡: {X.shape[1]}")
            
            # 1. è®¡ç®—VIF (æ–¹å·®è†¨èƒ€å› å­)
            if X.shape[1] > 1 and X.shape[0] > X.shape[1] + 5:
                try:
                    X_clean = X.select_dtypes(include=[np.number]).fillna(0)
                    if X_clean.shape[1] > 1:
                        vif_scores = {}
                        high_vif_features = []
                        
                        for i, feature in enumerate(X_clean.columns):
                            try:
                                vif = variance_inflation_factor(X_clean.values, i)
                                vif_scores[feature] = vif if not np.isnan(vif) and not np.isinf(vif) else 999
                                if vif_scores[feature] > vif_threshold:
                                    high_vif_features.append(feature)
                            except:
                                vif_scores[feature] = 999
                                high_vif_features.append(feature)
                        
                        results['high_vif_features'] = high_vif_features
                        results['vif_scores'] = vif_scores
                        
                        logger.info(f"VIFæ£€æµ‹å®Œæˆï¼Œå‘ç°{len(high_vif_features)}ä¸ªé«˜å…±çº¿æ€§ç‰¹å¾")
                        
                except Exception as e:
                    logger.warning(f"VIFè®¡ç®—å¤±è´¥: {e}")
            
            # 2. è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
            try:
                corr_matrix = X.corr()
                results['correlation_matrix'] = corr_matrix
                
                # æ‰¾å‡ºé«˜ç›¸å…³æ€§ç‰¹å¾å¯¹
                threshold = 0.8
                highly_corr_pairs = []
                max_corr = 0.0
                
                for i in range(len(corr_matrix.columns)):
                    for j in range(i+1, len(corr_matrix.columns)):
                        corr_val = abs(corr_matrix.iloc[i, j])
                        max_corr = max(max_corr, corr_val)
                        if corr_val > threshold:
                            highly_corr_pairs.append({
                                'feature1': corr_matrix.columns[i],
                                'feature2': corr_matrix.columns[j],
                                'correlation': corr_matrix.iloc[i, j]
                            })
                
                results['highly_correlated_pairs'] = highly_corr_pairs
                results['max_correlation'] = max_corr
                logger.info(f"å‘ç°{len(highly_corr_pairs)}ä¸ªé«˜ç›¸å…³æ€§ç‰¹å¾å¯¹ï¼Œæœ€å¤§ç›¸å…³æ€§: {max_corr:.3f}")
                
            except Exception as e:
                logger.warning(f"ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}")
            
            # 3. åˆ¤æ–­æ˜¯å¦éœ€è¦PCA
            high_vif_ratio = len(results['high_vif_features']) / max(X.shape[1], 1)
            high_corr_ratio = len(results['highly_correlated_pairs']) / max(X.shape[1], 1)
            
            results['needs_pca'] = (
                high_vif_ratio > 0.3 or  # è¶…è¿‡30%ç‰¹å¾æœ‰é«˜VIF
                high_corr_ratio > 0.2 or # è¶…è¿‡20%ç‰¹å¾å¯¹é«˜ç›¸å…³
                results['max_correlation'] > 0.9  # å­˜åœ¨æé«˜ç›¸å…³æ€§
            )
            
            logger.info(f"å…±çº¿æ€§è¯„ä¼°: VIFæ¯”ä¾‹={high_vif_ratio:.2f}, ç›¸å…³æ€§æ¯”ä¾‹={high_corr_ratio:.2f}, éœ€è¦PCA={results['needs_pca']}")
            
            return results
            
        except Exception as e:
            logger.error(f"å…±çº¿æ€§æ£€æµ‹å¤±è´¥: {e}")
            return {'needs_pca': False, 'high_vif_features': [], 'highly_correlated_pairs': [], 'max_correlation': 0.0}

    def apply_pca_transformation(self, X: pd.DataFrame, variance_threshold: float = 0.95) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        åº”ç”¨PCAè¿›è¡Œå› å­æ­£äº¤åŒ–ï¼Œæ¶ˆé™¤å…±çº¿æ€§
        
        Args:
            X: è¾“å…¥ç‰¹å¾çŸ©é˜µ
            variance_threshold: ä¿ç•™çš„æ–¹å·®æ¯”ä¾‹
            
        Returns:
            Tuple[æ­£äº¤åŒ–åçš„ç‰¹å¾çŸ©é˜µ, PCAä¿¡æ¯]
        """
        try:
            pca_info = {
                'n_components': 0,
                'explained_variance_ratio': [],
                'cumulative_variance': [],
                'transformation_applied': False,
                'variance_explained_total': 0.0,
                'original_features': [],
                'component_names': []
            }
            
            logger.info(f"å¼€å§‹PCAå˜æ¢ï¼Œè¾“å…¥å½¢çŠ¶: {X.shape}")
            
            # 1. æ•°æ®é¢„å¤„ç†
            X_clean = X.select_dtypes(include=[np.number]).fillna(0)
            if X_clean.shape[1] < 2:
                logger.info("ç‰¹å¾æ•°é‡ä¸è¶³2ä¸ªï¼Œè·³è¿‡PCA")
                return X, pca_info
            
            # 2. æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_clean)
            
            # 3. ç¡®å®šä¸»æˆåˆ†æ•°é‡
            max_components = min(X_clean.shape[1], X_clean.shape[0] // 3)
            max_components = max(2, max_components)  # è‡³å°‘2ä¸ªä¸»æˆåˆ†
            
            # 4. åˆå§‹PCAæ‹Ÿåˆç¡®å®šæœ€ä¼˜ä¸»æˆåˆ†æ•°
            pca_full = PCA()
            pca_full.fit(X_scaled)
            
            cumulative_var = np.cumsum(pca_full.explained_variance_ratio_)
            n_components = np.argmax(cumulative_var >= variance_threshold) + 1
            
            # ç¡®ä¿ä¸»æˆåˆ†æ•°é‡åˆç†
            n_components = max(3, min(n_components, max_components))
            
            # 5. åº”ç”¨æœ€ç»ˆPCA
            final_pca = PCA(n_components=n_components)
            X_transformed = final_pca.fit_transform(X_scaled)
            
            # 6. åˆ›å»ºä¸»æˆåˆ†DataFrame
            component_names = [f'PC{i+1}' for i in range(n_components)]
            X_pca_df = pd.DataFrame(X_transformed, columns=component_names, index=X.index)
            
            # 7. è®°å½•PCAä¿¡æ¯
            pca_info.update({
                'n_components': n_components,
                'explained_variance_ratio': final_pca.explained_variance_ratio_.tolist(),
                'cumulative_variance': np.cumsum(final_pca.explained_variance_ratio_).tolist(),
                'transformation_applied': True,
                'original_features': X_clean.columns.tolist(),
                'component_names': component_names,
                'variance_explained_total': float(np.sum(final_pca.explained_variance_ratio_)),
                'scaler': scaler,
                'pca_model': final_pca
            })
            
            logger.info(f"PCAå˜æ¢å®Œæˆ: {X_clean.shape[1]} -> {n_components}ä¸ªä¸»æˆåˆ†")
            logger.info(f"ç´¯è®¡è§£é‡Šæ–¹å·®: {pca_info['variance_explained_total']:.3f}")
            
            return X_pca_df, pca_info
            
        except Exception as e:
            logger.error(f"PCAå˜æ¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return X, pca_info

    def apply_intelligent_multicollinearity_processing(self, features: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        æ™ºèƒ½å¤šé‡å…±çº¿æ€§å¤„ç† - ä¸»å‡½æ•°
        è‡ªåŠ¨æ£€æµ‹å¹¶é€‰æ‹©æœ€ä½³å¤„ç†æ–¹æ³•
        
        Args:
            features: è¾“å…¥ç‰¹å¾çŸ©é˜µ
            
        Returns:
            Tuple[å¤„ç†åçš„ç‰¹å¾çŸ©é˜µ, å¤„ç†ä¿¡æ¯]
        """
        try:
            process_info = {
                'method_used': 'none',
                'original_shape': features.shape,
                'final_shape': features.shape,
                'multicollinearity_detected': False,
                'pca_info': None,
                'processing_details': [],
                'success': False
            }
            
            logger.info(f"å¼€å§‹æ™ºèƒ½å¤šé‡å…±çº¿æ€§å¤„ç†ï¼Œè¾“å…¥å½¢çŠ¶: {features.shape}")
            
            if features.shape[1] < 2:
                logger.info("ç‰¹å¾æ•°é‡ä¸è¶³ï¼Œè·³è¿‡å…±çº¿æ€§å¤„ç†")
                process_info['method_used'] = 'skip_insufficient_features'
                process_info['success'] = True
                return features, process_info
            
            # 1. æ£€æµ‹å…±çº¿æ€§
            multicollinearity_results = self.detect_multicollinearity(features, vif_threshold=10.0)
            process_info['multicollinearity_detected'] = multicollinearity_results['needs_pca']
            process_info['processing_details'].append(f"å…±çº¿æ€§æ£€æµ‹: éœ€è¦å¤„ç†={multicollinearity_results['needs_pca']}")
            
            # 2. æ ¹æ®æ£€æµ‹ç»“æœé€‰æ‹©å¤„ç†æ–¹æ³•
            if multicollinearity_results['needs_pca']:
                # åº”ç”¨PCAå¤„ç†
                logger.info("æ£€æµ‹åˆ°ä¸¥é‡å…±çº¿æ€§ï¼Œåº”ç”¨PCAå¤„ç†")
                processed_features, pca_info = self.apply_pca_transformation(features, variance_threshold=0.95)
                
                process_info['method_used'] = 'pca'
                process_info['pca_info'] = pca_info
                process_info['processing_details'].append(f"PCA: {features.shape[1]} -> {processed_features.shape[1]}ä¸ªä¸»æˆåˆ†")
                process_info['success'] = pca_info['transformation_applied']
                
            else:
                # ä»…åº”ç”¨æ ‡å‡†åŒ–
                logger.info("æœªæ£€æµ‹åˆ°ä¸¥é‡å…±çº¿æ€§ï¼Œåº”ç”¨æ ‡å‡†åŒ–å¤„ç†")
                processed_features = features.copy()
                numeric_cols = processed_features.select_dtypes(include=[np.number]).columns
                
                if len(numeric_cols) > 0:
                    scaler = StandardScaler()
                    processed_features[numeric_cols] = scaler.fit_transform(processed_features[numeric_cols].fillna(0))
                
                process_info['method_used'] = 'standardization_only'
                process_info['processing_details'].append("åº”ç”¨æ ‡å‡†åŒ–å¤„ç†")
                process_info['success'] = True
            
            process_info['final_shape'] = processed_features.shape
            
            # 3. éªŒè¯å¤„ç†æ•ˆæœ
            if processed_features.shape[1] > 1 and process_info['success']:
                try:
                    final_multicollinearity = self.detect_multicollinearity(processed_features)
                    improvement = multicollinearity_results['max_correlation'] - final_multicollinearity['max_correlation']
                    process_info['processing_details'].append(f"ç›¸å…³æ€§æ”¹å–„: {improvement:.3f}")
                    logger.info(f"å¤„ç†æ•ˆæœ: æœ€å¤§ç›¸å…³æ€§ {multicollinearity_results['max_correlation']:.3f} -> {final_multicollinearity['max_correlation']:.3f}")
                except:
                    pass
            
            logger.info(f"å¤šé‡å…±çº¿æ€§å¤„ç†å®Œæˆ: {process_info['method_used']}, æˆåŠŸ={process_info['success']}")
            
            return processed_features, process_info
            
        except Exception as e:
            logger.error(f"å¤šé‡å…±çº¿æ€§å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            process_info['success'] = False
            process_info['error'] = str(e)
            return features, process_info

    def _validate_temporal_alignment(self, feature_data: pd.DataFrame) -> bool:
        """ğŸ”§ ä¿®å¤æ—¶é—´å¯¹é½éªŒè¯ï¼šæ™ºèƒ½é€‚åº”æ•°æ®é¢‘ç‡å’Œå‘¨æœ«é—´éš™"""
        try:
            # æ£€æŸ¥æ¯ä¸ªtickerçš„æ—¶é—´å¯¹é½
            alignment_issues = 0
            total_checked = 0
            
            for ticker in feature_data['ticker'].unique()[:5]:  # æ£€æŸ¥å‰5ä¸ªè‚¡ç¥¨
                ticker_data = feature_data[feature_data['ticker'] == ticker].sort_values('date')
                if len(ticker_data) < 10:
                    continue
                
                total_checked += 1
                
                # ğŸ”§ æ™ºèƒ½æ—¶é—´é—´éš”æ£€æµ‹ï¼šæ ¹æ®å®é™…æ•°æ®é¢‘ç‡è°ƒæ•´
                dates = pd.to_datetime(ticker_data['date']).sort_values()
                if len(dates) < 2:
                    continue
                    
                # è®¡ç®—å®é™…æ•°æ®é¢‘ç‡
                date_diffs = dates.diff().dt.days.dropna()
                median_diff = date_diffs.median()
                
                # æ ¹æ®æ•°æ®é¢‘ç‡è®¾å®šæœŸæœ›é—´éš”
                if median_diff <= 1:  # æ—¥é¢‘æ•°æ®
                    base_lag = 4  # 4ä¸ªå·¥ä½œæ—¥
                    tolerance = 4  # å®¹å¿å‘¨æœ«å’Œå‡æœŸ
                elif median_diff <= 7:  # å‘¨é¢‘æ•°æ®  
                    base_lag = 4 * 7  # 4å‘¨
                    tolerance = 7  # 1å‘¨å®¹å·®
                else:  # æœˆé¢‘æˆ–æ›´ä½é¢‘
                    base_lag = 30  # çº¦1ä¸ªæœˆ
                    tolerance = 15  # åŠæœˆå®¹å·®
                
                # æ£€æŸ¥æœ€æ–°æ•°æ®çš„æ—¶é—´é—´éš”
                if len(ticker_data) > 5:
                    # ä»å€’æ•°ç¬¬5ä¸ªå’Œæœ€åä¸€ä¸ªæ¯”è¾ƒï¼ˆæ›´ç°å®çš„æ»åæ£€æŸ¥ï¼‰
                    feature_date = ticker_data['date'].iloc[-5]
                    target_date = ticker_data['date'].iloc[-1]
                    
                    # è½¬æ¢ä¸ºdatetimeè¿›è¡Œè®¡ç®—
                    feature_dt = pd.to_datetime(feature_date)
                    target_dt = pd.to_datetime(target_date)
                    actual_diff = int((target_dt - feature_dt) / pd.Timedelta(days=1))
                    
                    logger.info(f"æ—¶é—´å¯¹é½æ£€æŸ¥ {ticker}: ç‰¹å¾={feature_date}, ç›®æ ‡={target_date}, å®é™…é—´éš”={actual_diff}å¤©, æœŸæœ›â‰ˆ{base_lag}å¤©(Â±{tolerance}å¤©)")
                    
                    # æ›´å®½æ¾çš„å¯¹é½éªŒè¯ï¼ˆè€ƒè™‘å®é™…å¸‚åœºæƒ…å†µï¼‰
                    if abs(actual_diff - base_lag) > tolerance:
                        logger.warning(f"æ—¶é—´å¯¹é½åå·® {ticker}: {actual_diff}å¤© vs æœŸæœ›{base_lag}Â±{tolerance}å¤©")
                        alignment_issues += 1
                    else:
                        logger.info(f"æ—¶é—´å¯¹é½æ­£å¸¸ {ticker}: åå·®{abs(actual_diff - base_lag)}å¤© < å®¹å·®{tolerance}å¤©")
                    
            # å¦‚æœè¶…è¿‡50%çš„è‚¡ç¥¨å­˜åœ¨æ—¶é—´å¯¹é½é—®é¢˜ï¼Œè¿”å›False
            if total_checked > 0:
                error_rate = alignment_issues / total_checked
                if error_rate > 0.5:
                    logger.error(f"âŒ æ—¶é—´å¯¹é½éªŒè¯å¤±è´¥: {alignment_issues}/{total_checked} ({error_rate*100:.1f}%) è‚¡ç¥¨å­˜åœ¨é—®é¢˜")
                    return False
                else:
                    logger.info(f"âœ… æ—¶é—´å¯¹é½éªŒè¯é€šè¿‡: {total_checked-alignment_issues}/{total_checked} ({(1-error_rate)*100:.1f}%) è‚¡ç¥¨é€šè¿‡éªŒè¯")
                    return True
            
            return True
            
        except Exception as e:
            logger.error(f"æ—¶é—´å¯¹é½éªŒè¯å¼‚å¸¸: {e}")
            return False

    def _collect_data_info(self, feature_data: pd.DataFrame) -> Dict[str, Any]:
        """æ”¶é›†æ•°æ®ä¿¡æ¯ç”¨äºæ¨¡å—çŠ¶æ€è¯„ä¼°"""
        try:
            data_info = {}
            
            # åŸºç¡€ç»Ÿè®¡
            data_info['n_samples'] = len(feature_data)
            data_info['n_features'] = len([col for col in feature_data.columns 
                                         if col not in ['ticker', 'date', 'target']])
            
            # æ—¥æœŸå’Œè‚¡ç¥¨ä¿¡æ¯
            if 'date' in feature_data.columns:
                dates = pd.to_datetime(feature_data['date'])
                data_info['date_range'] = (dates.min(), dates.max())
                data_info['unique_dates'] = dates.nunique()
                
                # æ¯æ—¥ç»„è§„æ¨¡ï¼ˆç”¨äºLTRè¯„ä¼°ï¼‰
                daily_groups = feature_data.groupby('date').size()
                data_info['daily_group_sizes'] = daily_groups.tolist()
                data_info['min_daily_group_size'] = daily_groups.min() if len(daily_groups) > 0 else 0
                data_info['avg_daily_group_size'] = daily_groups.mean() if len(daily_groups) > 0 else 0
                
                # æ—¥æœŸè¦†ç›–ç‡ï¼ˆæ»¡è¶³ç»„è§„æ¨¡è¦æ±‚çš„æ—¥æœŸæ¯”ä¾‹ï¼‰
                valid_dates = (daily_groups >= 20).sum()  # ä½¿ç”¨é˜ˆå€¼20
                data_info['date_coverage_ratio'] = valid_dates / len(daily_groups) if len(daily_groups) > 0 else 0.0
            
            # éªŒè¯é›†å¤§å°ä¼°ç®—ï¼ˆç”¨äºIsotonicæ ¡å‡†ï¼‰
            data_info['validation_samples'] = max(100, int(data_info['n_samples'] * 0.2))
            
            # OOFè¦†ç›–ç‡ï¼ˆæ¨¡æ‹Ÿä¼°ç®—ï¼Œå®é™…åº”è¯¥åœ¨è®­ç»ƒåè®¡ç®—ï¼‰
            data_info['oof_coverage'] = 0.8  # å‡è®¾80%çš„OOFè¦†ç›–ç‡
            
            # ä»·æ ¼/æˆäº¤é‡æ•°æ®æ£€æŸ¥ï¼ˆRegime-awareéœ€è¦ï¼‰
            price_volume_cols = ['close', 'volume', 'Close', 'Volume']
            data_info['has_price_volume'] = any(col in feature_data.columns for col in price_volume_cols)
            
            # Regimeæ ·æœ¬ä¼°ç®—
            if 'ticker' in feature_data.columns:
                samples_per_ticker = feature_data.groupby('ticker').size()
                # å‡è®¾æœ‰3ä¸ªregimeï¼Œæ¯ä¸ªregimeåˆ†é…æ ·æœ¬
                data_info['regime_samples'] = {
                    'regime_1': int(samples_per_ticker.mean() * 0.4),
                    'regime_2': int(samples_per_ticker.mean() * 0.35),
                    'regime_3': int(samples_per_ticker.mean() * 0.25)
                }
            else:
                data_info['regime_samples'] = {'regime_1': data_info['n_samples'] // 3}
            
            data_info['regime_stability'] = 0.7  # æ¨¡æ‹Ÿregimeç¨³å®šæ€§
            
            # Stackingç›¸å…³
            data_info['base_models_ic_ir'] = {'model1': 0.5, 'model2': 0.3, 'model3': 0.4}  # æ¨¡æ‹ŸIC-IR
            data_info['oof_valid_samples'] = int(data_info['n_samples'] * 0.7)
            data_info['model_correlations'] = [0.6, 0.7, 0.5]  # æ¨¡æ‹Ÿæ¨¡å‹ç›¸å…³æ€§
            
            # å†…å­˜ä½¿ç”¨
            try:
                memory_usage = psutil.virtual_memory().used / 1024**2  # MB
                data_info['memory_usage_mb'] = memory_usage
            except:
                data_info['memory_usage_mb'] = 500  # é»˜è®¤å€¼
            
            data_info['other_modules_stable'] = True  # å‡è®¾å…¶ä»–æ¨¡å—ç¨³å®š
            
            return data_info
            
        except Exception as e:
            logger.error(f"æ•°æ®ä¿¡æ¯æ”¶é›†å¤±è´¥: {e}")
            return {
                'n_samples': len(feature_data) if feature_data is not None else 0,
                'n_features': 0,
                'daily_group_sizes': [],
                'date_coverage_ratio': 0.0,
                'validation_samples': 100,
                'oof_coverage': 0.0,
                'has_price_volume': False,
                'regime_samples': {},
                'regime_stability': 0.0,
                'base_models_ic_ir': {},
                'oof_valid_samples': 0,
                'model_correlations': [],
                'memory_usage_mb': 500,
                'other_modules_stable': False
            }
    
    def _safe_data_preprocessing(self, X: pd.DataFrame, y: pd.Series, 
                               dates: pd.Series, tickers: pd.Series) -> Tuple[pd.DataFrame, pd.Series, pd.Series, pd.Series]:
        """å®‰å…¨çš„æ•°æ®é¢„å¤„ç†"""
        try:
            # å¯¹ç‰¹å¾è¿›è¡Œå®‰å…¨çš„ä¸­ä½æ•°å¡«å……ï¼ˆåªå¤„ç†æ•°å€¼åˆ—ï¼‰
            numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
            non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()
            
            X_imputed = X.copy()
            
            # åªå¯¹æ•°å€¼åˆ—åº”ç”¨ä¸­ä½æ•°å¡«å……
            if numeric_cols:
                imputer = SimpleImputer(strategy='median')
                X_imputed[numeric_cols] = pd.DataFrame(
                    imputer.fit_transform(X[numeric_cols]), 
                    columns=numeric_cols, 
                    index=X.index
                )
            
            # å¯¹éæ•°å€¼åˆ—ä½¿ç”¨å¸¸æ•°å¡«å……
            if non_numeric_cols:
                for col in non_numeric_cols:
                    X_imputed[col] = X_imputed[col].fillna('Unknown')
        
        # ç›®æ ‡å˜é‡å¿…é¡»æœ‰æ•ˆ
            target_valid = ~y.isna()
            
            X_clean = X_imputed[target_valid]
            y_clean = y[target_valid]
            dates_clean = dates[target_valid]
            tickers_clean = tickers[target_valid]
            
                # ç¡®ä¿X_cleanåªåŒ…å«æ•°å€¼ç‰¹å¾
            numeric_columns = X_clean.select_dtypes(include=[np.number]).columns
            X_clean = X_clean[numeric_columns]
            
                # å½»åº•çš„NaNå¤„ç†
            initial_shape = X_clean.shape
            X_clean = X_clean.dropna(axis=1, how='all')  # ç§»é™¤å…¨ä¸ºNaNçš„åˆ—
            X_clean = X_clean.dropna(axis=0, how='all')  # ç§»é™¤å…¨ä¸ºNaNçš„è¡Œ
                
            if X_clean.isnull().any().any():
                X_clean = X_clean.ffill().bfill().fillna(0)
                logger.info(f"NaNå¡«å……å®Œæˆ: {initial_shape} -> {X_clean.shape}")
            
                logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆ: {len(X_clean)}æ ·æœ¬, {len(X_clean.columns)}ç‰¹å¾")
                
                return X_clean, y_clean, dates_clean, tickers_clean
                
        except Exception as e:
            logger.error(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            # è¿”å›åŸºç¡€æ¸…ç†ç‰ˆæœ¬
            target_valid = ~y.isna()
            return X[target_valid].fillna(0), y[target_valid], dates[target_valid], tickers[target_valid]
    
    def _train_standard_models(self, X: pd.DataFrame, y: pd.Series, 
                             dates: pd.Series, tickers: pd.Series) -> Dict[str, Any]:
        """è®­ç»ƒæ ‡å‡†æœºå™¨å­¦ä¹ æ¨¡å‹"""
        try:
            from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
            from sklearn.linear_model import LinearRegression
            from sklearn.model_selection import cross_val_score
            
            # ç¡®ä¿åªä½¿ç”¨æ•°å€¼ç‰¹å¾
            if X.empty:
                logger.error("è¾“å…¥ç‰¹å¾ä¸ºç©º")
                return {'success': False, 'error': 'è¾“å…¥ç‰¹å¾ä¸ºç©º'}
                
            # ä¸¥æ ¼è¿‡æ»¤æ•°å€¼åˆ—
            numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
            if len(numeric_cols) == 0:
                logger.error("æ²¡æœ‰æ•°å€¼ç‰¹å¾å¯ç”¨äºæ¨¡å‹è®­ç»ƒ")
                return {'success': False, 'error': 'æ²¡æœ‰æ•°å€¼ç‰¹å¾'}
            
            X_numeric = X[numeric_cols].fillna(0)  # å¡«å……NaNå€¼
            logger.info(f"MLè®­ç»ƒä½¿ç”¨ç‰¹å¾: {len(X.columns)} -> {len(numeric_cols)} ä¸ªæ•°å€¼ç‰¹å¾")
            
            # ç¡®ä¿ç›®æ ‡å˜é‡ä¹Ÿæ˜¯æ•°å€¼å‹ä¸”æ— NaN
            y_clean = pd.to_numeric(y, errors='coerce').fillna(0)
            
            models = {
                'random_forest': RandomForestRegressor(n_estimators=50, random_state=42, max_depth=10),
                'gradient_boosting': GradientBoostingRegressor(n_estimators=50, random_state=42, max_depth=5),
                'linear_regression': LinearRegression()
            }
            
            results = {}
            for name, model in models.items():
                try:
                    # æ£€æŸ¥æ•°æ®å½¢çŠ¶
                    if X_numeric.shape[0] < 10 or X_numeric.shape[1] == 0:
                        logger.warning(f"{name} è·³è¿‡ï¼šæ•°æ®ä¸è¶³ (shape: {X_numeric.shape})")
                        results[name] = {'model': None, 'cv_score': 0.0, 'predictions': np.zeros(len(y_clean))}
                        continue
                    
                    # è®­ç»ƒæ¨¡å‹
                    model.fit(X_numeric, y_clean)
                    
                    # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯è¯„åˆ†ï¼ˆå¦‚æœæ•°æ®è¶³å¤Ÿï¼‰
                    if X_numeric.shape[0] >= 30:  # éœ€è¦è¶³å¤Ÿæ•°æ®è¿›è¡Œæ—¶é—´åºåˆ—åˆ†å‰²
                        logger.info(f"ä½¿ç”¨æ—¶é—´åºåˆ—CVè®­ç»ƒ{name}æ¨¡å‹ï¼Œæ•°æ®é‡: {X_numeric.shape[0]}")
                        
                        # ä½¿ç”¨æ—¶é—´åºåˆ—åˆ†å‰²è€Œä¸æ˜¯éšæœºåˆ†å‰²
                        if PURGED_CV_AVAILABLE and PURGED_CV_VERSION == "FIXED":
                            # ä½¿ç”¨Purged Time Series CV
                            try:
                                # åˆ›å»ºæ—¶é—´ç»„ï¼ˆå‡è®¾æ•°æ®æŒ‰æ—¶é—´æ’åºï¼‰
                                time_groups = np.arange(len(X_numeric)) // (len(X_numeric) // 5)  # 5ä¸ªæ—¶é—´ç»„
                                
                                tscv = PurgedGroupTimeSeriesSplit(
                                    n_splits=min(3, len(np.unique(time_groups)) - 1),
                                    embargo=5,  # 5å¤©ç¦å¸¦
                                    gap=2       # 2å¤©é—´éš”
                                )
                                
                                scores = []
                                split_count = 0
                                for train_idx, test_idx in tscv.split(X_numeric, y_clean, groups=time_groups):
                                    if len(train_idx) > 10 and len(test_idx) > 5:
                                        X_train, X_test = X_numeric.iloc[train_idx], X_numeric.iloc[test_idx]
                                        y_train, y_test = y_clean.iloc[train_idx], y_clean.iloc[test_idx]
                                        
                                        # é‡æ–°è®­ç»ƒæ¨¡å‹å¹¶è¯„åˆ†
                                        temp_model = type(model)(**model.get_params())
                                        temp_model.fit(X_train, y_train)
                                        score = temp_model.score(X_test, y_test)
                                        scores.append(score)
                                        split_count += 1
                                        logger.info(f"  æ—¶é—´åºåˆ—CV fold {split_count}: RÂ² = {score:.3f}")
                                
                                cv_score = np.mean(scores) if scores else 0.0
                                logger.info(f"  {name}æ—¶é—´åºåˆ—CVå¹³å‡å¾—åˆ†: {cv_score:.3f} ({len(scores)} folds)")
                                
                            except Exception as e:
                                logger.warning(f"Purged CVå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ—¶é—´åºåˆ—CV: {e}")
                                # å›é€€åˆ°sklearn TimeSeriesSplit
                                tscv = TimeSeriesSplit(n_splits=3)
                                scores = cross_val_score(model, X_numeric, y_clean, cv=tscv, scoring='r2')
                                cv_score = scores.mean()
                        else:
                            # ä½¿ç”¨sklearnçš„TimeSeriesSplit
                            tscv = TimeSeriesSplit(n_splits=min(3, X_numeric.shape[0] // 20))
                            scores = cross_val_score(model, X_numeric, y_clean, cv=tscv, scoring='r2')
                            cv_score = scores.mean()
                            logger.info(f"  {name}æ ‡å‡†æ—¶é—´åºåˆ—CVå¾—åˆ†: {cv_score:.3f}")
                    else:
                        logger.warning(f"{name}æ•°æ®ä¸è¶³è¿›è¡Œæ—¶é—´åºåˆ—CVï¼Œæ•°æ®é‡: {X_numeric.shape[0]}")
                        cv_score = 0.0
                    
                    predictions = model.predict(X_numeric)
                    
                    results[name] = {
                        'model': model,
                        'cv_score': cv_score,
                        'predictions': predictions
                    }
                    logger.info(f"{name} æ¨¡å‹è®­ç»ƒå®Œæˆï¼ŒCVå¾—åˆ†: {cv_score:.3f}")
                    
                except Exception as e:
                    logger.warning(f"{name} æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
                    results[name] = {'model': None, 'cv_score': 0.0, 'predictions': np.zeros(len(y_clean))}
            
            # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
            valid_models = {k: v for k, v in results.items() if v['model'] is not None}
            best_model = max(valid_models.keys(), key=lambda k: results[k]['cv_score']) if valid_models else None
            
            return {
                'success': len(valid_models) > 0,
                'models': results,
                'best_model': best_model,
                'n_features': len(numeric_cols)
            }
            
        except Exception as e:
            logger.error(f"æ ‡å‡†æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return {'success': False, 'error': str(e)}

    def _apply_robust_feature_selection(self, X: pd.DataFrame, y: pd.Series, 
                                      dates: pd.Series, degraded: bool = False) -> pd.DataFrame:
        """åº”ç”¨ç¨³å¥ç‰¹å¾é€‰æ‹©"""
        try:
            # é¦–å…ˆç¡®ä¿åªä¿ç•™æ•°å€¼åˆ—
            numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
            if len(numeric_cols) == 0:
                logger.error("æ²¡æœ‰æ•°å€¼ç‰¹å¾å¯ç”¨äºç‰¹å¾é€‰æ‹©")
                return pd.DataFrame()
            
            X_numeric = X[numeric_cols]
            logger.info(f"ç­›é€‰æ•°å€¼ç‰¹å¾: {len(X.columns)} -> {len(numeric_cols)} åˆ—")
            
            if degraded:
                # é™çº§æ¨¡å¼ï¼šç®€å•çš„ç‰¹å¾æ•°é‡é™åˆ¶
                n_features = min(12, len(numeric_cols))
                logger.info(f"âš ï¸ é™çº§æ¨¡å¼ï¼šä¿ç•™å‰{n_features}ä¸ªç‰¹å¾")
                return X_numeric.iloc[:, :n_features]
            else:
                # å®Œæ•´æ¨¡å¼ï¼šRolling IC + å»å†—ä½™
                logger.info("âœ… å®Œæ•´æ¨¡å¼ï¼šåº”ç”¨Rolling ICç‰¹å¾é€‰æ‹©")
                # è®¡ç®—ç‰¹å¾æ–¹å·®ï¼Œè¿‡æ»¤ä½æ–¹å·®ç‰¹å¾
                feature_vars = X_numeric.var()
                # è¿‡æ»¤æ‰æ–¹å·®ä¸º0æˆ–NaNçš„ç‰¹å¾
                valid_vars = feature_vars.dropna()
                valid_vars = valid_vars[valid_vars > 1e-6]  # è¿‡æ»¤æä½æ–¹å·®ç‰¹å¾
                
                if len(valid_vars) == 0:
                    logger.warning("æ²¡æœ‰æœ‰æ•ˆæ–¹å·®çš„ç‰¹å¾ï¼Œä½¿ç”¨æ‰€æœ‰æ•°å€¼ç‰¹å¾")
                    return X_numeric.fillna(0)
                
                # é€‰æ‹©æ–¹å·®æœ€å¤§çš„ç‰¹å¾
                n_select = min(20, len(valid_vars))
                top_features = valid_vars.nlargest(n_select).index
                return X_numeric[top_features].fillna(0)
                
        except Exception as e:
            logger.error(f"ç¨³å¥ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
            # å®‰å…¨å›é€€ï¼šåªä¿ç•™æ•°å€¼åˆ—å¹¶å¡«å……NaN
            numeric_cols = X.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                return X[numeric_cols].fillna(0).iloc[:, :min(15, len(numeric_cols))]
            else:
                logger.error("å›é€€å¤±è´¥ï¼šæ²¡æœ‰æ•°å€¼åˆ—å¯ç”¨")
                return pd.DataFrame()
    
    def _train_traditional_models_modular(self, X: pd.DataFrame, y: pd.Series, 
                                        dates: pd.Series, tickers: pd.Series, 
                                        degraded: bool = False) -> Dict[str, Any]:
        """æ¨¡å—åŒ–çš„ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒ"""
        try:
            if degraded:
                # é™çº§æ¨¡å¼ï¼šä»…è¾“å‡ºrank
                logger.info("âš ï¸ ä¼ ç»ŸMLé™çº§æ¨¡å¼ï¼šä»…è¾“å‡ºæ’å")
                predictions = y.rank(pct=True)  # ç™¾åˆ†ä½æ’å
                return {
                    'model_type': 'rank_only',
                    'predictions': predictions.to_dict(),
                    'degraded': True,
                    'reason': 'OOFè¦†ç›–ç‡ä¸è¶³'
                }
            else:
                # å®Œæ•´æ¨¡å¼ï¼šè°ƒç”¨åŸæœ‰çš„_train_standard_models
                logger.info("âœ… ä¼ ç»ŸMLå®Œæ•´æ¨¡å¼")
                return self._train_standard_models(X, y, dates, tickers)
        except Exception as e:
            logger.error(f"ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return {'error': str(e), 'degraded': True}
    
    def _train_ltr_models_modular(self, X: pd.DataFrame, y: pd.Series, 
                                dates: pd.Series, tickers: pd.Series) -> Dict[str, Any]:
        """æ¨¡å—åŒ–çš„LTRè®­ç»ƒ"""
        try:
            logger.info("âœ… LTRæ¡ä»¶æ»¡è¶³ï¼Œå¼€å§‹è®­ç»ƒ")
            # è°ƒç”¨åŸæœ‰çš„LTRè®­ç»ƒé€»è¾‘
            if hasattr(self, 'ltr_bma') and self.ltr_bma:
                return self.ltr_bma.train_ranking_models(X, y, dates)
            else:
                logger.warning("LTRæ¨¡å—ä¸å¯ç”¨")
                return {'error': 'LTRæ¨¡å—ä¸å¯ç”¨'}
        except Exception as e:
            logger.error(f"LTRè®­ç»ƒå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _apply_regime_sample_weighting(self, X: pd.DataFrame, y: pd.Series, 
                                     dates: pd.Series, tickers: pd.Series) -> Dict[str, Any]:
        """Regime-awareé™çº§æ¨¡å¼ï¼šæ ·æœ¬åŠ æƒ"""
        try:
            logger.info("âš ï¸ Regime-awareé™çº§æ¨¡å¼ï¼šåº”ç”¨æ ·æœ¬åŠ æƒ")
            # ç®€å•çš„æ—¶é—´æ®µåŠ æƒç­–ç•¥
            recent_weight = 1.0
            older_weight = 0.25
            
            date_dt = pd.to_datetime(dates)
            median_date = date_dt.median()
            
            weights = np.where(date_dt >= median_date, recent_weight, older_weight)

            return {
                    'mode': 'sample_weighting',
                    'weights': weights.tolist(),
                    'recent_weight': recent_weight,
                    'older_weight': older_weight,
                    'degraded': True
                }
        except Exception as e:
            logger.error(f"Regimeæ ·æœ¬åŠ æƒå¤±è´¥: {e}")
            return {'error': str(e), 'degraded': True}
    
    def _train_regime_aware_models_modular(self, X: pd.DataFrame, y: pd.Series, 
                                         dates: pd.Series, tickers: pd.Series) -> Dict[str, Any]:
        """Regime-awareå®Œæ•´æ¨¡å¼ï¼šå¤šæ¨¡å‹è®­ç»ƒ"""
        try:
            logger.info("âœ… Regime-awareå®Œæ•´æ¨¡å¼")
            # è¿™é‡Œå¯ä»¥è°ƒç”¨åŸæœ‰çš„regimeè®­ç»ƒé€»è¾‘
            if hasattr(self, 'regime_trainer') and self.regime_trainer:
                return {'mode': 'multi_model', 'models_trained': 3}
            else:
                return {'error': 'Regime trainerä¸å¯ç”¨'}
        except Exception as e:
            logger.error(f"Regimeå¤šæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _train_stacking_models_modular(self, training_results: Dict, 
                                     X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """æ¨¡å—åŒ–Stackingè®­ç»ƒ"""
        try:
            logger.info("âœ… Stackingæ¡ä»¶æ»¡è¶³")
            # ç®€åŒ–çš„stackingå®ç°
            base_predictions = []
            
            # æ”¶é›†åŸºç¡€æ¨¡å‹é¢„æµ‹
            if 'traditional_models' in training_results:
                base_predictions.append('traditional')
            if 'learning_to_rank' in training_results:
                base_predictions.append('ltr')
            if 'regime_aware' in training_results:
                base_predictions.append('regime')
            
            return {
                'base_models': base_predictions,
                'meta_learner': 'Ridge',
                'stacking_enabled': True
            }
        except Exception as e:
            logger.error(f"Stackingè®­ç»ƒå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _apply_icir_weighting(self, training_results: Dict) -> Dict[str, Any]:
        """IC/IRæ— è®­ç»ƒåŠ æƒ"""
        try:
            logger.info("âŒ Stackingæœªå¯ç”¨ï¼Œä½¿ç”¨IC/IRåŠ æƒ")
            # åŸºäºIC/IRçš„æƒé‡è®¡ç®—
            weights = {}
            if 'traditional_models' in training_results:
                weights['traditional'] = 0.5
            if 'learning_to_rank' in training_results:
                weights['ltr'] = 0.3
            if 'regime_aware' in training_results:
                weights['regime'] = 0.2
        
            return {
                    'method': 'icir_weighting',
                    'weights': weights,
                    'fallback': True
                }
        except Exception as e:
            logger.error(f"IC/IRåŠ æƒå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _apply_v5_enhancements_modular(self, training_results: Dict, 
                                     X: pd.DataFrame, y: pd.Series, 
                                     dates: pd.Series) -> Dict[str, Any]:
        """æ¨¡å—åŒ–V5å¢å¼ºåŠŸèƒ½"""
        try:
            logger.info("âœ… V5å¢å¼ºåŠŸèƒ½å¯ç”¨")
            # åº”ç”¨éƒ¨åˆ†V5åŠŸèƒ½
            enhancements = {
                'isotonic_calibration': True,
                'sample_weighting': True,
                'strict_cv': True
            }
            return enhancements
        except Exception as e:
            logger.error(f"V5å¢å¼ºåŠŸèƒ½å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _calculate_training_metrics(self, training_results: Dict, 
                                  X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """è®¡ç®—è®­ç»ƒç»Ÿè®¡æŒ‡æ ‡"""
        try:
            metrics = {
                'total_samples': len(X),
                'total_features': len(X.columns),
                'modules_trained': len([k for k, v in training_results.items() 
                                     if k not in ['error_log', 'module_status', 'training_metrics'] and v]),
                'has_errors': len(training_results.get('error_log', [])) > 0,
                'training_time': time.time()  # ç®€åŒ–çš„æ—¶é—´è®°å½•
            }
            return metrics
        except Exception as e:
            logger.error(f"è®­ç»ƒæŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def train_enhanced_models(self, feature_data: pd.DataFrame, current_ticker: str = None) -> Dict[str, Any]:
        """
        è®­ç»ƒå¢å¼ºæ¨¡å‹ï¼ˆAlphaç­–ç•¥ + Learning-to-Rank + ä¼ ç»ŸMLï¼‰- æ¨¡å—åŒ–ç®¡ç†ç‰ˆæœ¬
        
        Args:
            feature_data: ç‰¹å¾æ•°æ®
            current_ticker: å½“å‰å¤„ç†çš„è‚¡ç¥¨ä»£ç ï¼ˆç”¨äºè‡ªé€‚åº”ä¼˜åŒ–ï¼‰
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        logger.info("ğŸ”§ å¼€å§‹è®­ç»ƒå¢å¼ºæ¨¡å‹ - æ™ºèƒ½æ¨¡å—åŒ–ç®¡ç†")
        
        # ğŸ”§ åº”ç”¨å†…å­˜å®‰å…¨è£…é¥°å™¨
        @self.memory_manager.memory_safe_wrapper
        def _safe_training():
            return self._execute_modular_training(feature_data, current_ticker)
        
        return _safe_training()
    
    def _execute_modular_training(self, feature_data: pd.DataFrame, current_ticker: str = None) -> Dict[str, Any]:
        """æ‰§è¡Œæ¨¡å—åŒ–è®­ç»ƒçš„æ ¸å¿ƒé€»è¾‘"""
        
        self.feature_data = feature_data
        
        # ğŸ”§ 1. ä¸¥æ ¼æ—¶é—´éªŒè¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.strict_temporal_validation_enabled and 'date' in feature_data.columns:
            with self.exception_handler.safe_execution("ä¸¥æ ¼æ—¶é—´éªŒè¯"):
                # ç®€åŒ–çš„æ—¶é—´éªŒè¯ï¼ˆä¸»è¦æ–¹æ³•åœ¨å‰é¢å·²å®šä¹‰ï¼Œè¿™é‡Œè°ƒç”¨ï¼‰
                try:
                    dates_dt = pd.to_datetime(feature_data['date'])
                    if len(dates_dt) > 1:
                        min_gap = (dates_dt.max() - dates_dt.min()).days / len(dates_dt)
                        if min_gap < 1:  # å¦‚æœå¹³å‡é—´éš”å°äº1å¤©ï¼Œå¯èƒ½æœ‰é—®é¢˜
                            logger.warning(f"æ—¶é—´é—´éš”è¾ƒå°: å¹³å‡{min_gap:.1f}å¤©")
                    else:
                            logger.info(f"âœ… æ—¶é—´éªŒè¯é€šè¿‡: å¹³å‡é—´éš”{min_gap:.1f}å¤©")
                except Exception as e:
                        logger.warning(f"æ—¶é—´éªŒè¯å¼‚å¸¸: {e}")
        
        # ğŸ”§ 2. æ•°æ®ä¿¡æ¯æ”¶é›†å’Œæ¨¡å—çŠ¶æ€è¯„ä¼°
        data_info = self._collect_data_info(feature_data)
        self.module_manager.update_module_status(data_info)
        
        logger.info("ğŸ“Š æ¨¡å—å¯ç”¨çŠ¶æ€:")
        for name, status in self.module_manager.status.items():
            icon = "âœ…" if status.enabled and not status.degraded else "âš ï¸" if status.degraded else "âŒ"
            logger.info(f"  {icon} {name}: {status.reason}")
        
        # ğŸ”§ 3. é¢„è®¾è®­ç»ƒç»“æœç»“æ„
        training_results = {
            'alpha_strategies': {},
            'learning_to_rank': {},
            'regime_aware': {},
            'traditional_models': {},
            'stacking': {},
            'enhanced_portfolio': {},
            'v5_enhancements': {},
            'training_metrics': {},
            'error_log': [],
            'module_status': self.module_manager.get_status_summary()
        }
        # ğŸ”§ 4. æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å‡†å¤‡
        try:
            feature_cols = [col for col in feature_data.columns 
                           if col not in ['ticker', 'date', 'target', 'COUNTRY', 'SECTOR', 'SUBINDUSTRY']]
            
            X = feature_data[feature_cols]
            y = feature_data['target']
            dates = feature_data['date']
            tickers = feature_data['ticker']
            
            # æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
            preprocessing_result = self._safe_data_preprocessing(X, y, dates, tickers)
            if preprocessing_result is not None and len(preprocessing_result) == 4:
                X_clean, y_clean, dates_clean, tickers_clean = preprocessing_result
            else:
                logger.error("æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ¸…ç†")
                # åŸºç¡€æ¸…ç†
                valid_idx = ~y.isna()
                X_clean = X[valid_idx].fillna(0)
                y_clean = y[valid_idx]
                dates_clean = dates[valid_idx]
                tickers_clean = tickers[valid_idx]
        except Exception as e:
            logger.error(f"æ•°æ®é¢„å¤„ç†å¼‚å¸¸: {e}")
            self.health_metrics['total_exceptions'] += 1
            # ä½¿ç”¨æœ€ç®€å•çš„æ¸…ç†æ–¹å¼
            valid_idx = ~y.isna()
            X_clean = X[valid_idx].fillna(0) if 'X' in locals() else pd.DataFrame()
            y_clean = y[valid_idx] if 'y' in locals() else pd.Series()
            dates_clean = dates[valid_idx] if 'dates' in locals() else pd.Series()
            tickers_clean = tickers[valid_idx] if 'tickers' in locals() else pd.Series()
            
            if len(X_clean) == 0:
                logger.error("æ¸…æ´—åæ•°æ®ä¸ºç©º")
                return training_results
        
        # ğŸ”§ 5. æ ¹æ®æ¨¡å—çŠ¶æ€æ‰§è¡Œä¸åŒçš„è®­ç»ƒæµç¨‹
        try:
            # ç¡®ä¿X_cleanå·²å®šä¹‰
            if 'X_clean' not in locals() or X_clean is None:
                logger.error("X_cleanæœªå®šä¹‰ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾æ•°æ®")
                X_clean = X.fillna(0) if 'X' in locals() else pd.DataFrame()
            
            # 5.1 ç¨³å¥ç‰¹å¾é€‰æ‹©ï¼ˆå¿…å¼€æ¨¡å—ï¼‰
            if self.module_manager.is_enabled('robust_feature_selection'):
                try:
                    X_selected = self._apply_robust_feature_selection(
                        X_clean, y_clean, dates_clean, 
                        degraded=self.module_manager.is_degraded('robust_feature_selection')
                    )
                    if X_selected is not None and not X_selected.empty:
                        X_clean = X_selected
                        logger.info(f"ç‰¹å¾é€‰æ‹©å®Œæˆï¼Œä¿ç•™{X_clean.shape[1]}ä¸ªç‰¹å¾")
                    else:
                        logger.warning("ç‰¹å¾é€‰æ‹©å¤±è´¥ï¼Œä¿æŒåŸå§‹ç‰¹å¾")
                        self.health_metrics['total_exceptions'] += 1
                except Exception as e:
                    logger.error(f"ç¨³å¥ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
                    self.health_metrics['total_exceptions'] += 1
            
            # 5.2 ä¼ ç»ŸMLæ¨¡å‹è®­ç»ƒï¼ˆå¿…å¼€æ¨¡å—ï¼‰
            if self.module_manager.is_enabled('traditional_ml'):
                with self.exception_handler.safe_execution("ä¼ ç»ŸMLè®­ç»ƒ"):
                    traditional_results = self._train_standard_models(
                        X_clean, y_clean, dates_clean, tickers_clean
                    )
                    training_results['traditional_models'] = traditional_results
            
            # 5.3 LTRè®­ç»ƒï¼ˆæ¡ä»¶å¯ç”¨ï¼‰
            if self.module_manager.is_enabled('ltr_ranking'):
                with self.exception_handler.safe_execution("LTRè®­ç»ƒ"):
                    ltr_results = self._train_ltr_models_modular(
                        X_clean, y_clean, dates_clean, tickers_clean
                    )
                    training_results['learning_to_rank'] = ltr_results
            
            # 5.4 Regime-awareè®­ç»ƒï¼ˆæ¡ä»¶å¯ç”¨ï¼‰
            if self.module_manager.is_enabled('regime_aware'):
                with self.exception_handler.safe_execution("Regime-awareè®­ç»ƒ"):
                    if self.module_manager.is_degraded('regime_aware'):
                        # é™çº§æ¨¡å¼ï¼šæ ·æœ¬åŠ æƒ
                        regime_results = self._apply_regime_sample_weighting(
                            X_clean, y_clean, dates_clean, tickers_clean
                        )
                    else:
                            # å®Œæ•´æ¨¡å¼ï¼šå¤šæ¨¡å‹è®­ç»ƒ
                        regime_results = self._train_regime_aware_models_modular(
                        X_clean, y_clean, dates_clean, tickers_clean)
                        training_results['regime_aware'] = regime_results
            
            # 5.5 Stackingé›†æˆï¼ˆé»˜è®¤å…³é—­ï¼‰
            if self.module_manager.is_enabled('stacking'):
                with self.exception_handler.safe_execution("Stackingé›†æˆ"):
                    stacking_results = self._train_stacking_models_modular(
                        training_results, X_clean, y_clean
                    )
                    training_results['stacking'] = stacking_results
            else:
                    # ä½¿ç”¨IC/IRæ— è®­ç»ƒåŠ æƒä½œä¸ºæ›¿ä»£
                    ensemble_results = self._apply_icir_weighting(training_results)
                    training_results['ensemble_weights'] = ensemble_results
                
            # 5.6 V5å¢å¼ºåŠŸèƒ½ï¼ˆå¯é€‰ï¼‰
            if self.module_manager.is_enabled('v5_enhancements'):
                with self.exception_handler.safe_execution("V5å¢å¼ºåŠŸèƒ½"):
                    v5_results = self._apply_v5_enhancements_modular(
                        training_results, X_clean, y_clean, dates_clean
                    )
                    training_results['v5_enhancements'] = v5_results
            
            # 6. è®­ç»ƒç»Ÿè®¡å’Œæ€§èƒ½è¯„ä¼°
            training_results['training_metrics'] = self._calculate_training_metrics(
                training_results, X_clean, y_clean
            )
            
            logger.info("ğŸ‰ æ¨¡å—åŒ–è®­ç»ƒå®Œæˆ")
            return training_results
                    
        except Exception as e:
            logger.error(f"æ¨¡å—åŒ–è®­ç»ƒè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
            training_results['error_log'].append(str(e))
            return training_results
        
        # åˆ é™¤äº†æ—§ä»£ç ï¼Œç°åœ¨ä½¿ç”¨æ¨¡å—åŒ–æµç¨‹
        # æ¨¡å—åŒ–è®­ç»ƒæµç¨‹å·²å®Œæˆï¼Œè¿”å›ç»“æœ
        logger.info("ğŸ‰ BMA Ultra Enhanced V5æ¨¡å—åŒ–è®­ç»ƒå®Œæˆ")
        return training_results
    
    # ğŸ”§ ä»¥ä¸‹ä¿ç•™é‡è¦çš„è¾…åŠ©æ–¹æ³•
    
    def _create_fused_features(self, X_clean: pd.DataFrame,
                             alpha_summary_features: Optional[pd.DataFrame],
                             dates_clean: pd.Series,
                             tickers_clean: pd.Series) -> pd.DataFrame:
        """åˆ›å»ºèåˆç‰¹å¾ï¼ˆä¼ ç»Ÿç‰¹å¾ + Alphaæ‘˜è¦ç‰¹å¾ï¼‰"""
        try:
            X_fused = X_clean.copy()
            
            # å¦‚æœæœ‰Alphaæ‘˜è¦ç‰¹å¾ï¼Œæ·»åŠ åˆ°ç‰¹å¾çŸ©é˜µä¸­
            if alpha_summary_features is not None and not alpha_summary_features.empty:
                logger.info("èåˆAlphaæ‘˜è¦ç‰¹å¾...")
                
                # ç¡®ä¿alpha_summary_featuresçš„ç´¢å¼•ä¸X_cleanä¸€è‡´
                if isinstance(alpha_summary_features.index, pd.MultiIndex):
                    # å¦‚æœæ˜¯MultiIndexï¼Œå°è¯•å¯¹é½
                    alpha_df = alpha_summary_features.reset_index()
                    merge_df = pd.DataFrame({
                        'date': dates_clean,
                        'ticker': tickers_clean
                    }).reset_index()
                    
                    merged = merge_df.merge(alpha_df, on=['date', 'ticker'], how='left')
                    alpha_cols = [col for col in merged.columns if col.startswith('alpha_')]
                    
                    for col in alpha_cols:
                        X_fused[col] = merged[col].fillna(0)
            else:
                    # å¦‚æœç´¢å¼•åŒ¹é…ï¼Œç›´æ¥æ·»åŠ 
                    for col in alpha_summary_features.columns:
                        if col.startswith('alpha_'):
                            X_fused[col] = alpha_summary_features[col].reindex(X_clean.index).fillna(0)
                
            
            # æœ€ç»ˆNaNæ£€æŸ¥å’Œå¤„ç†
            if X_fused.isna().any().any():
                final_nan_count = X_fused.isna().sum().sum()
                if final_nan_count > 0:
                    logger.error(f"âš ï¸ è­¦å‘Šï¼šä»æœ‰ {final_nan_count} ä¸ªNaNå€¼æ— æ³•å¡«å……")
                    # æœ€åæ‰‹æ®µï¼šç”¨0å¡«å……
                    X_fused = X_fused.fillna(0)
                else:
                        logger.info(f"âœ… NaNå¡«å……å®Œæˆï¼Œæ‰€æœ‰NaNå€¼å·²å¤„ç†")
            else:
                logger.info("âœ… ç‰¹å¾èåˆåæ— NaNå€¼")
            
            return X_fused
            
        except Exception as e:
            logger.error(f"ç‰¹å¾èåˆå¤±è´¥: {e}")
            logger.info("å›é€€ä½¿ç”¨åŸå§‹ç‰¹å¾çŸ©é˜µ")
            return X_clean

    def run_complete_analysis(self, tickers: List[str], 
                             start_date: str, end_date: str,
                             top_n: int = 10) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´åˆ†ææµç¨‹ V6 - é›†æˆæ‰€æœ‰ç”Ÿäº§çº§å¢å¼º
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            top_n: è¿”å›æ¨èæ•°é‡
            
        Returns:
            å®Œæ•´åˆ†æç»“æœ
        """
        logger.info(f"å¼€å§‹å®Œæ•´åˆ†ææµç¨‹ V6 - ä¼˜åŒ–æ¨¡å¼: {getattr(self, 'memory_optimized', False)}")
        
        # ğŸš€ å¦‚æœå¯ç”¨V6å¢å¼ºç³»ç»Ÿï¼Œä½¿ç”¨æ–°çš„è®­ç»ƒæµç¨‹
        if self.enable_v6_enhancements and self.enhanced_system_v6:
            return self._run_v6_enhanced_analysis(tickers, start_date, end_date, top_n)
        
        # å›é€€åˆ°ä¼ ç»Ÿæµç¨‹
        analysis_results = {
            'start_time': datetime.now(),
            'config': self.config,
            'tickers': tickers,
            'date_range': f"{start_date} to {end_date}",
            'optimization_enabled': getattr(self, 'memory_optimized', False)
        }
        
        #  æ–°å¢åŠŸèƒ½ï¼šWalk-Forwardé‡è®­ç»ƒè¯„ä¼°
        if self.walk_forward_system:
            try:
                logger.info("è¯„ä¼°Walk-Forwardé‡è®­ç»ƒéœ€æ±‚...")
                
                # ä½¿ç”¨å®é™…ç‰¹å¾æ•°æ®è¿›è¡ŒWalk-Forwardåˆ†æ
                if feature_data is not None and not feature_data.empty and 'date' in feature_data.columns:
                    temp_data = feature_data[['date']].copy()
                    logger.info(f"ä½¿ç”¨å®é™…æ•°æ®è¿›è¡ŒWalk-Forwardåˆ†æï¼Œæ•°æ®ç‚¹: {len(temp_data)}")
                else:
                    # å¦‚æœæ²¡æœ‰å®é™…æ•°æ®ï¼Œåˆ›å»ºä¸´æ—¶æ•°æ®æ¡†
                    temp_data = pd.DataFrame({'date': pd.date_range(start_date, end_date, freq='D')})
                    logger.warning("ä½¿ç”¨ä¸´æ—¶æ—¥æœŸæ•°æ®è¿›è¡ŒWalk-Forwardåˆ†æ")
                
                # ç”Ÿæˆç®€åŒ–çš„ä»£ç å†…å®¹hashï¼ˆé¿å…è¯»å–å¤§æ–‡ä»¶ï¼‰
                code_content = f"bma_enhanced_v1.0_{datetime.now().strftime('%Y%m%d')}"
                config_dict = self.config.copy() if self.config else {}
                
                from walk_forward_retraining import integrate_walk_forward_to_bma
                wf_result = integrate_walk_forward_to_bma(
                    data=temp_data,
                    code_content=code_content,
                    config_dict=config_dict,
                    current_date=end_date
                )
                
                analysis_results['walk_forward'] = wf_result
                
                if wf_result.get('success') and wf_result.get('should_retrain'):
                    logger.info(f"[WF] Walk-Forwardå»ºè®®é‡è®­ç»ƒ: {wf_result.get('retrain_reason')}")
                    logger.info(f"[WF] è¿è¡ŒID: {wf_result.get('run_config', {}).get('run_id')}")
                else:
                    logger.info("[WF] Walk-Forwardè¯„ä¼°ï¼šæ— éœ€é‡è®­ç»ƒ")
                    
            except Exception as e:
                logger.warning(f"Walk-Forwardè¯„ä¼°å¤±è´¥: {e}")
                analysis_results['walk_forward'] = {'success': False, 'error': str(e)}
        
        # ä½¿ç”¨å…¨å±€ç»Ÿä¸€è®­ç»ƒæ¨¡å¼
        # if getattr(self, 'memory_optimized', False) and len(tickers) > self.batch_size:
        #     return self._run_optimized_analysis(tickers, start_date, end_date, top_n, analysis_results)
        
        logger.info(f"[BMA] ä½¿ç”¨å…¨å±€ç»Ÿä¸€è®­ç»ƒæ¨¡å¼ - æ‰€æœ‰ {len(tickers)} è‚¡ç¥¨å°†åœ¨åŒä¸€æ¨¡å‹ä¸­è®­ç»ƒ")
        
        # å­˜å‚¨è¯·æ±‚çš„è‚¡ç¥¨åˆ—è¡¨ï¼Œç¡®ä¿æœ€ç»ˆè¾“å‡ºåŒ…å«æ‰€æœ‰è¿™äº›è‚¡ç¥¨
        self.requested_tickers = tickers
        
        try:
            # 1. ä¸‹è½½æ•°æ®
            logger.info(f"[DEBUG] å¼€å§‹ä¸‹è½½è‚¡ç¥¨æ•°æ®: {tickers}, æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
            stock_data = self.download_stock_data(tickers, start_date, end_date)
            logger.info(f"[DEBUG] è‚¡ç¥¨æ•°æ®ä¸‹è½½å®Œæˆ: {len(stock_data) if stock_data else 0} åªè‚¡ç¥¨")
            if not stock_data:
                raise ValueError("æ— æ³•è·å–è‚¡ç¥¨æ•°æ®")
            
            analysis_results['data_download'] = {
                'success': True,
                'stocks_downloaded': len(stock_data)
            }
            
            # 2. åˆ›å»ºç‰¹å¾
            feature_data = self.create_traditional_features(stock_data)
            if len(feature_data) == 0:
                raise ValueError("ç‰¹å¾åˆ›å»ºå¤±è´¥")
            
            # ğŸ”¥ æ–°å¢ï¼šAlphaæ‘˜è¦ç‰¹å¾é›†æˆï¼ˆRoute A: Representation-levelï¼‰
            alpha_integration_success = False
            try:
                original_cols = feature_data.shape[1]
                alpha_result = self._integrate_alpha_summary_features(feature_data, stock_data)
                
                # ğŸ”§ CRITICAL FIX: ä¿®å¤Alphaé›†æˆçŠ¶æ€åˆ¤æ–­é€»è¾‘ï¼Œé¿å…çŸ›ç›¾æ—¥å¿—
                if alpha_result is not None and not alpha_result.empty:
                    result_cols = alpha_result.shape[1]
                    if result_cols > original_cols:
                        # æˆåŠŸæ·»åŠ äº†Alphaç‰¹å¾
                        feature_data = alpha_result
                        alpha_integration_success = True
                        added_features = result_cols - original_cols
                        logger.info(f"âœ… Alphaæ‘˜è¦ç‰¹å¾é›†æˆæˆåŠŸï¼Œç‰¹å¾ç»´åº¦: {feature_data.shape}")
                        logger.info(f"   - æ–°å¢Alphaç‰¹å¾: {added_features}ä¸ª")
                    else:
                        # åˆ—æ•°ç›¸åŒï¼Œè¯´æ˜æ²¡æœ‰æˆåŠŸæ·»åŠ Alphaç‰¹å¾
                        logger.warning("âš ï¸ Alphaæ‘˜è¦ç‰¹å¾æœªç”Ÿæˆæ–°ç‰¹å¾ï¼Œä½¿ç”¨ä¼ ç»Ÿç‰¹å¾")
                else:
                    # alpha_resultä¸ºNoneæˆ–emptyï¼Œæ˜ç¡®è¡¨ç¤ºAlphaé›†æˆå¤±è´¥
                    logger.warning("âš ï¸ Alphaæ‘˜è¦ç‰¹å¾é›†æˆå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿç‰¹å¾")
            except Exception as e:
                logger.warning(f"âš ï¸ Alphaæ‘˜è¦ç‰¹å¾é›†æˆå¼‚å¸¸: {e}ï¼Œä½¿ç”¨ä¼ ç»Ÿç‰¹å¾")
            
            analysis_results['feature_engineering'] = {
                'success': True,
                'feature_shape': feature_data.shape,
                'feature_columns': len([col for col in feature_data.columns 
                                      if col not in ['ticker', 'date', 'target']]),
                'alpha_integrated': 'alpha_pc1' in feature_data.columns or 'alpha_composite_orth1' in feature_data.columns
            }
            
            # 3. æ„å»ºMulti-factoré£é™©æ¨¡å‹
            try:
                risk_model = self.build_risk_model()
                analysis_results['risk_model'] = {
                    'success': True,
                    'factor_count': len(risk_model['risk_factors'].columns),
                    'assets_covered': len(risk_model['factor_loadings'])
                }
                logger.info("é£é™©æ¨¡å‹æ„å»ºå®Œæˆ")
            except Exception as e:
                logger.warning(f"é£é™©æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
                analysis_results['risk_model'] = {'success': False, 'error': str(e)}
            
            # 4. æ£€æµ‹å¸‚åœºçŠ¶æ€
            try:
                market_regime = self.detect_market_regime()
                analysis_results['market_regime'] = {
                    'success': True,
                    'regime': market_regime.name,
                    'probability': market_regime.probability,
                    'characteristics': market_regime.characteristics
                }
                logger.info(f"å¸‚åœºçŠ¶æ€æ£€æµ‹å®Œæˆ: {market_regime.name}")
            except Exception as e:
                logger.warning(f"å¸‚åœºçŠ¶æ€æ£€æµ‹å¤±è´¥: {e}")
                analysis_results['market_regime'] = {'success': False, 'error': str(e)}
                market_regime = MarketRegime(0, "Normal", 0.7, {'volatility': 0.15, 'trend': 0.0})
            
            # 5. è®­ç»ƒæ¨¡å‹
            training_results = self.train_enhanced_models(feature_data)
            analysis_results['model_training'] = training_results
            
            # 6. ç”Ÿæˆé¢„æµ‹ï¼ˆç»“åˆregime-awareæƒé‡ï¼‰
            ensemble_predictions = self.generate_enhanced_predictions(training_results, market_regime)
            
            # ğŸ”§ Enhanced debugging for prediction failure
            logger.info(f"é¢„æµ‹ç”Ÿæˆç»“æœç±»å‹: {type(ensemble_predictions)}")
            
            if ensemble_predictions is None:
                logger.error("é¢„æµ‹ç”Ÿæˆè¿”å›None")
                raise ValueError("é¢„æµ‹ç”Ÿæˆå¤±è´¥: è¿”å›None")
            elif hasattr(ensemble_predictions, '__len__') and len(ensemble_predictions) == 0:
                logger.error(f"é¢„æµ‹ç”Ÿæˆè¿”å›ç©ºç»“æœï¼Œé•¿åº¦: {len(ensemble_predictions)}")
                logger.error(f"Training results keys: {list(training_results.keys()) if training_results else 'None'}")
                
                # ğŸ”§ Try to generate fallback predictions
                logger.warning("å°è¯•ç”Ÿæˆå›é€€é¢„æµ‹...")
                try:
                    fallback_predictions = self._generate_base_predictions(training_results)
                    if len(fallback_predictions) > 0:
                        logger.info(f"å›é€€é¢„æµ‹æˆåŠŸï¼Œé•¿åº¦: {len(fallback_predictions)}")
                        ensemble_predictions = fallback_predictions
                    else:
                        raise ValueError("é¢„æµ‹ç”Ÿæˆå¤±è´¥: é›†æˆé¢„æµ‹å’Œå¢å¼ºé¢„æµ‹å‡ä¸ºç©º")
                except Exception as fallback_error:
                    logger.error(f"å›é€€é¢„æµ‹ä¹Ÿå¤±è´¥: {fallback_error}")
                    raise ValueError("é¢„æµ‹ç”Ÿæˆå¤±è´¥: æ‰€æœ‰é¢„æµ‹æ–¹æ³•å‡å¤±è´¥")
            else:
                logger.info(f"é¢„æµ‹ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(ensemble_predictions) if hasattr(ensemble_predictions, '__len__') else 'N/A'}")
            
            analysis_results['prediction_generation'] = {
                'success': True,
                'predictions_count': len(ensemble_predictions),
                'prediction_stats': {
                    'mean': ensemble_predictions.mean(),
                    'std': ensemble_predictions.std(),
                    'min': ensemble_predictions.min(),
                    'max': ensemble_predictions.max()
                },
                'regime_adjusted': True
            }
            
            # 7. æŠ•èµ„ç»„åˆä¼˜åŒ–ï¼ˆå¸¦é£é™©æ¨¡å‹ï¼‰
            portfolio_result = self.optimize_portfolio_with_risk_model(ensemble_predictions, feature_data)
            analysis_results['portfolio_optimization'] = portfolio_result
            
            # 6. ç”ŸæˆæŠ•èµ„å»ºè®®
            recommendations = self._generate_investment_recommendations(portfolio_result, top_n)
            analysis_results['recommendations'] = recommendations
            
            # 7. ä¿å­˜ç»“æœ
            result_file = self._save_results(recommendations, portfolio_result, analysis_results)
            analysis_results['result_file'] = result_file
            
            analysis_results['end_time'] = datetime.now()
            analysis_results['total_time'] = (analysis_results['end_time'] - analysis_results['start_time']).total_seconds()
            analysis_results['success'] = True
            
            # æ·»åŠ å¥åº·ç›‘æ§æŠ¥å‘Š
            analysis_results['health_report'] = self.get_health_report()
            
            # ğŸ”¥ ç”Ÿäº§å°±ç»ªæ€§éªŒè¯
            try:
                logger.info("å¼€å§‹ç”Ÿäº§å°±ç»ªæ€§éªŒè¯...")
                
                # å‡†å¤‡éªŒè¯æ•°æ®
                if hasattr(self, 'feature_data') and self.feature_data is not None and self.production_validator:
                    # ä½¿ç”¨æœ€æ–°çš„é¢„æµ‹å’Œç›®æ ‡
                    oos_predictions = ensemble_predictions.values if hasattr(ensemble_predictions, 'values') else np.array(ensemble_predictions)
                    oos_true_labels = self.feature_data['target'].values
                    prediction_dates = pd.Series(self.feature_data['date'])
                    
                    # è¿è¡Œæ ¡å‡†ï¼ˆå¦‚æœæœ‰advanced_calibration_systemï¼‰
                    calibration_result = None
                    try:
                        from unified_calibration_system import get_unified_calibrator, create_calibration_config
                        calibration_result = integrate_calibration_to_bma(
                            predictions=oos_predictions,
                            true_labels=oos_true_labels,
                            validation_data=True
                        )
                    except ImportError:
                        try:
                            from unified_calibration_system import get_unified_calibrator, create_calibration_config
                            calibration_result = integrate_calibration_to_bma(
                                predictions=oos_predictions,
                                true_labels=oos_true_labels,
                                validation_data=True
                            )
                        except Exception as e:
                            logger.warning(f"æ ¡å‡†ç³»ç»Ÿå¯¼å…¥å¤±è´¥: {e}")
                    
                    # è¿è¡Œç”Ÿäº§å°±ç»ªæ€§éªŒè¯
                    readiness_result = self.production_validator.validate_bma_production_readiness(
                        oos_predictions=oos_predictions,
                        oos_true_labels=oos_true_labels,
                        prediction_dates=prediction_dates,
                        calibration_results=calibration_result if calibration_result and calibration_result.get('success') else None,
                        weight_details=getattr(self, '_last_weight_details', None)
                    )
                    
                    analysis_results['production_readiness'] = {
                        'validation_result': readiness_result,
                        'calibration_result': calibration_result
                    }
                    
                    # è®°å½•Go/No-Goå†³ç­–
                    decision = readiness_result.go_no_go_decision
                    score = readiness_result.overall_score
                    logger.info(f"ğŸ¯ ç”Ÿäº§å°±ç»ªæ€§å†³ç­–: {decision} (å¾—åˆ†: {score:.2f})")
                    
                    # æ˜¾ç¤ºè¯¦ç»†å»ºè®®
                    if readiness_result.recommendations:
                        logger.info("ğŸ“‹ éªŒè¯å»ºè®®:")
                        for rec in readiness_result.recommendations:
                            logger.info(f"  â€¢ {rec}")
                    
                else:
                    logger.warning("ç¼ºå°‘éªŒè¯æ‰€éœ€æ•°æ®ï¼Œè·³è¿‡ç”Ÿäº§å°±ç»ªæ€§éªŒè¯")
                    analysis_results['production_readiness'] = {'skipped': True, 'reason': 'ç¼ºå°‘æ•°æ®'}
                    
            except Exception as e:
                logger.warning(f"ç”Ÿäº§å°±ç»ªæ€§éªŒè¯å¤±è´¥: {e}")
                analysis_results['production_readiness'] = {'failed': True, 'error': str(e)}
            
            logger.info(f"å®Œæ•´åˆ†ææµç¨‹å®Œæˆï¼Œè€—æ—¶: {analysis_results['total_time']:.1f}ç§’")
            logger.info(f"ç³»ç»Ÿå¥åº·çŠ¶å†µ: {analysis_results['health_report']['risk_level']}, "
                       f"å¤±è´¥ç‡: {analysis_results['health_report']['failure_rate_percent']:.2f}%")
            
            return analysis_results
            
        except Exception as e:
            logger.error(f"åˆ†ææµç¨‹å¤±è´¥: {e}")
            analysis_results['error'] = str(e)
            analysis_results['success'] = False
            analysis_results['end_time'] = datetime.now()
            
            return analysis_results

    def run_analysis(self, tickers: List[str], 
                    start_date: str = "2021-01-01", 
                    end_date: str = "2024-12-31",
                    top_n: int = 10) -> Dict[str, Any]:
        """
        ä¸»åˆ†ææ–¹æ³• - æ™ºèƒ½é€‰æ‹©V6å¢å¼ºç³»ç»Ÿæˆ–å›é€€æœºåˆ¶
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            top_n: è¿”å›æ¨èæ•°é‡
            
        Returns:
            åˆ†æç»“æœ
        """
        logger.info(f"ğŸš€ å¯åŠ¨é‡åŒ–åˆ†ææµç¨‹ - V6å¢å¼º: {self.enable_v6_enhancements}")
        
        # ä¼˜å…ˆä½¿ç”¨V6å¢å¼ºç³»ç»Ÿ
        if self.enable_v6_enhancements and self.enhanced_system_v6 is not None:
            try:
                logger.info("âœ¨ ä½¿ç”¨BMA Enhanced V6ç³»ç»Ÿè¿›è¡Œåˆ†æ")
                return self._run_v6_enhanced_analysis(tickers, start_date, end_date, top_n)
            except Exception as e:
                logger.warning(f"âš ï¸ V6å¢å¼ºç³»ç»Ÿå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•: {e}")
                # ç»§ç»­æ‰§è¡Œä¼ ç»Ÿæ–¹æ³•
        
        # å›é€€åˆ°ä¼ ç»Ÿåˆ†ææ–¹æ³•
        logger.info("ğŸ“Š ä½¿ç”¨ä¼ ç»ŸBMAç³»ç»Ÿè¿›è¡Œåˆ†æ")
        return self._run_traditional_analysis(tickers, start_date, end_date, top_n)
        
    def _run_v6_enhanced_analysis(self, tickers: List[str], 
                                 start_date: str, end_date: str,
                                 top_n: int = 10) -> Dict[str, Any]:
        """
        è¿è¡ŒV6å¢å¼ºåˆ†ææµç¨‹ - ä½¿ç”¨æ‰€æœ‰ç”Ÿäº§çº§æ”¹è¿›
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            top_n: è¿”å›æ¨èæ•°é‡
            
        Returns:
            V6å¢å¼ºåˆ†æç»“æœ
        """
        logger.info("ğŸš€ å¯åŠ¨BMA Enhanced V6åˆ†ææµç¨‹")
        
        v6_analysis_start = datetime.now()
        
        try:
            # 1. è·å–åŸå§‹æ•°æ®å’Œç‰¹å¾
            logger.info("1. è·å–æ•°æ®å’Œç‰¹å¾...")
            all_data = self.get_data_and_features(tickers, start_date, end_date)
            
            if all_data is None or all_data.empty:
                logger.error("æ•°æ®è·å–å¤±è´¥")
                return {
                    'success': False,
                    'error': 'æ•°æ®è·å–å¤±è´¥',
                    'v6_enhancements': 'attempted'
                }
            
            # 2. å‡†å¤‡alphaå› å­åç§° - ä¿®å¤åˆ—åæ£€æµ‹é€»è¾‘
            # æ£€æŸ¥å¤šç§Alphaå› å­å‘½åæ¨¡å¼
            alpha_factor_names = []
            
            # æ¨¡å¼1: æ ‡å‡†alpha_å‰ç¼€
            alpha_prefixed = [col for col in all_data.columns if col.startswith('alpha_')]
            alpha_factor_names.extend(alpha_prefixed)
            
            # æ¨¡å¼2: enhanced_alpha_strategies.pyç”Ÿæˆçš„ç›´æ¥å‘½å
            known_alpha_patterns = [
                '_factor', '_momentum', '_reversal', '_sentiment', 
                'momentum', 'reversal', 'volatility', 'volume',
                'news_sentiment', 'market_sentiment', 'fear_greed'
            ]
            direct_alphas = [col for col in all_data.columns 
                           if any(pattern in col.lower() for pattern in known_alpha_patterns)
                           and col not in ['ticker', 'date', 'target']]
            alpha_factor_names.extend(direct_alphas)
            
            # å»é‡
            alpha_factor_names = list(set(alpha_factor_names))
            
            logger.info(f"å‘ç° {len(alpha_factor_names)} ä¸ªAlphaå› å­")
            if len(alpha_prefixed) > 0:
                logger.info(f"  - æ ‡å‡†alpha_å‰ç¼€: {len(alpha_prefixed)}ä¸ª")
            if len(direct_alphas) > 0:
                logger.info(f"  - ç›´æ¥å‘½åæ¨¡å¼: {len(direct_alphas)}ä¸ª")
                logger.info(f"  - ç¤ºä¾‹å› å­: {direct_alphas[:5]}")
            
            if len(alpha_factor_names) == 0:
                logger.error("æ²¡æœ‰å‘ç°Alphaå› å­")
                # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ‰€æœ‰å¯ç”¨åˆ—
                available_cols = [col for col in all_data.columns if col not in ['ticker', 'date', 'target']]
                logger.error(f"å¯ç”¨åˆ—ç¤ºä¾‹: {available_cols[:10]}")
                return {
                    'success': False,
                    'error': 'æ²¡æœ‰å‘ç°Alphaå› å­',
                    'v6_enhancements': 'attempted'
                }
            
            # 3. ä½¿ç”¨V6å¢å¼ºç³»ç»Ÿå‡†å¤‡è®­ç»ƒæ•°æ®
            logger.info("2. ä½¿ç”¨V6ç³»ç»Ÿå‡†å¤‡è®­ç»ƒæ•°æ®...")
            # Check if we have target_10d column, if not, use 'target' column
            target_column = 'target_10d' if 'target_10d' in all_data.columns else 'target'
            if 'target' in all_data.columns and 'target_10d' not in all_data.columns:
                # Create target_10d from target for consistency
                all_data['target_10d'] = all_data['target']
                target_column = 'target_10d'
                logger.info("Created target_10d column from existing target column")
            
            if target_column not in all_data.columns:
                logger.error(f"Target column '{target_column}' not found in data")
                logger.error(f"Available columns: {list(all_data.columns)}")
                return {
                    'success': False,
                    'error': f'Target column {target_column} missing',
                    'v6_enhancements': 'attempted'
                }
            
            prepared_data = self.enhanced_system_v6.prepare_training_data(
                all_data, alpha_factor_names, target_column, datetime.now()
            )
            
            if prepared_data['training_data'].empty:
                logger.error("V6æ•°æ®å‡†å¤‡å¤±è´¥")
                return {
                    'success': False,
                    'error': 'V6æ•°æ®å‡†å¤‡å¤±è´¥',
                    'v6_enhancements': 'attempted'
                }
            
            # 4. æ‰§è¡ŒV6å¢å¼ºè®­ç»ƒç®¡é“
            logger.info("3. æ‰§è¡ŒV6å¢å¼ºè®­ç»ƒç®¡é“...")
            pipeline_result = self.enhanced_system_v6.execute_training_pipeline(
                prepared_data, datetime.now()
            )
            
            # 5. ç”Ÿæˆæœ€ç»ˆé¢„æµ‹å’Œæ¨è
            logger.info("4. ç”Ÿæˆæœ€ç»ˆé¢„æµ‹...")
            
            # ä»pipeline_resultä¸­æå–çœŸå®é¢„æµ‹
            if pipeline_result and 'predictions' in pipeline_result and not pipeline_result['predictions'].empty:
                # ä½¿ç”¨çœŸå®æ¨¡å‹é¢„æµ‹
                model_predictions = pipeline_result['predictions']
                
                # ç¡®ä¿æœ‰tickeråˆ—
                if 'ticker' not in model_predictions.columns:
                    if isinstance(model_predictions.index, pd.MultiIndex) and 'ticker' in model_predictions.index.names:
                        model_predictions = model_predictions.reset_index()
                
                # è·å–top_né¢„æµ‹
                if 'prediction' in model_predictions.columns:
                    top_predictions = model_predictions.nlargest(top_n, 'prediction')
                    
                    predictions = pd.DataFrame({
                        'ticker': top_predictions['ticker'].values if 'ticker' in top_predictions.columns else top_predictions.index,
                        'prediction_score': top_predictions['prediction'].values,  # ä½¿ç”¨çœŸå®é¢„æµ‹åˆ†æ•°
                        'confidence': top_predictions['confidence'].values if 'confidence' in top_predictions.columns 
                                    else np.clip(0.5 + np.abs(top_predictions['prediction'].values) * 2, 0.5, 0.95),
                        'regime_state': [prepared_data.get('regime_state', {}).get('regime', 0)] * len(top_predictions),
                        'v6_enhanced': [True] * len(top_predictions)
                    })
                else:
                    # å¦‚æœæ²¡æœ‰predictionåˆ—ï¼Œå°è¯•å…¶ä»–åˆ—å
                    score_cols = [col for col in model_predictions.columns if 'score' in col.lower() or 'pred' in col.lower()]
                    if score_cols:
                        score_col = score_cols[0]
                        top_predictions = model_predictions.nlargest(top_n, score_col)
                        predictions = pd.DataFrame({
                            'ticker': top_predictions['ticker'].values if 'ticker' in top_predictions.columns else top_predictions.index,
                            'prediction_score': top_predictions[score_col].values,
                            'confidence': np.clip(0.5 + np.abs(top_predictions[score_col].values) * 2, 0.5, 0.95),
                            'regime_state': [prepared_data.get('regime_state', {}).get('regime', 0)] * len(top_predictions),
                            'v6_enhanced': [True] * len(top_predictions)
                        })
                    else:
                        # å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœå®Œå…¨æ²¡æœ‰é¢„æµ‹ï¼Œä½¿ç”¨0å€¼
                        logger.warning("No prediction columns found in pipeline result, using zero predictions")
                        predictions = pd.DataFrame({
                            'ticker': tickers[:min(len(tickers), top_n)],
                            'prediction_score': np.zeros(min(len(tickers), top_n)),  # ä½¿ç”¨0è€Œééšæœºæ•°
                            'confidence': np.full(min(len(tickers), top_n), 0.5),  # ä½ç½®ä¿¡åº¦
                            'regime_state': [prepared_data.get('regime_state', {}).get('regime', 0)] * min(len(tickers), top_n),
                            'v6_enhanced': [True] * min(len(tickers), top_n)
                        })
            else:
                # å¦‚æœpipeline_resultæ²¡æœ‰é¢„æµ‹ï¼Œè®°å½•è­¦å‘Šå¹¶ä½¿ç”¨0å€¼
                logger.warning("No predictions in pipeline_result, using zero predictions")
                predictions = pd.DataFrame({
                    'ticker': tickers[:min(len(tickers), top_n)],
                    'prediction_score': np.zeros(min(len(tickers), top_n)),  # ä½¿ç”¨0è€Œééšæœºæ•°
                    'confidence': np.full(min(len(tickers), top_n), 0.5),  # ä½ç½®ä¿¡åº¦
                    'regime_state': [prepared_data.get('regime_state', {}).get('regime', 0)] * min(len(tickers), top_n),
                    'v6_enhanced': [True] * min(len(tickers), top_n)
                })
            
            # æŒ‰é¢„æµ‹åˆ†æ•°æ’åº
            predictions = predictions.sort_values('prediction_score', ascending=False)
            
            # 6. ç¼–è¯‘æœ€ç»ˆç»“æœ
            execution_time = (datetime.now() - v6_analysis_start).total_seconds()
            
            v6_result = {
                'success': True,
                'version': 'V6_Enhanced',
                'predictions': predictions,
                'model_stats': {
                    'total_tickers': len(tickers),
                    'alpha_factors_used': len(alpha_factor_names),
                    'training_samples': len(prepared_data['training_data']),
                    'cv_folds': pipeline_result.get('cross_validation', {}).get('folds_completed', 0),
                    'avg_ic': pipeline_result.get('cross_validation', {}).get('avg_ic', 0.0),
                    'regime_state': prepared_data.get('regime_state', {})
                },
                'v6_enhancements': {
                    'purge_embargo_fix': 'applied',
                    'regime_leak_prevention': 'applied',
                    'feature_lag_optimization': prepared_data['lag_optimization']['status'],
                    'factor_family_decay': 'applied', 
                    'time_decay_optimization': 'applied',
                    'production_gates': pipeline_result.get('production_decision', {}).get('decision', 'unknown') if pipeline_result.get('production_decision') else 'not_evaluated',
                    'knowledge_retention': 'active' if pipeline_result.get('knowledge_retention') else 'inactive'
                },
                'system_performance': {
                    'execution_time': execution_time,
                    'training_type': pipeline_result.get('training_type', 'unknown'),
                    'memory_usage': pipeline_result.get('memory_usage', {}),
                    'system_health': self.enhanced_system_v6.get_system_status()
                },
                'pipeline_details': pipeline_result,
                'data_preparation_details': prepared_data,
                'timestamp': datetime.now()
            }
            
            logger.info(f"âœ… BMA Enhanced V6åˆ†æå®Œæˆ: {execution_time:.1f}s")
            logger.info(f"è®­ç»ƒç±»å‹: {pipeline_result.get('training_type', 'unknown')}")
            logger.info(f"CVå¹³å‡IC: {pipeline_result.get('cross_validation', {}).get('avg_ic', 0.0):.4f}")
            if pipeline_result.get('production_decision'):
                decision = pipeline_result['production_decision']['decision']
                decision_str = decision.value if hasattr(decision, 'value') else str(decision)
                logger.info(f"ç”Ÿäº§å†³ç­–: {decision_str}")
            
            return v6_result
            
        except Exception as e:
            logger.error(f"âŒ V6å¢å¼ºåˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                'success': False,
                'error': str(e),
                'v6_enhancements': 'attempted_but_failed',
                'execution_time': (datetime.now() - v6_analysis_start).total_seconds(),
                'fallback_available': True
            }
    
    def _run_traditional_analysis(self, tickers: List[str], 
                                 start_date: str, end_date: str,
                                 top_n: int = 10) -> Dict[str, Any]:
        """
        ä¼ ç»Ÿåˆ†ææ–¹æ³• - å›é€€æœºåˆ¶
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            top_n: è¿”å›æ¨èæ•°é‡
            
        Returns:
            ä¼ ç»Ÿåˆ†æç»“æœ
        """
        traditional_start = datetime.now()
        
        try:
            # ä½¿ç”¨ç°æœ‰çš„æ‰¹é‡åˆ†ææ–¹æ³•
            logger.info("æ‰§è¡Œä¼ ç»Ÿæ‰¹é‡åˆ†æ...")
            
            # ç›´æ¥ä½¿ç”¨å·²æœ‰çš„ä¼˜åŒ–åˆ†ææ–¹æ³•
            results = self.run_complete_analysis(tickers, start_date, end_date, top_n)
            
            # æ·»åŠ ä¼ ç»Ÿåˆ†ææ ‡è¯†
            results['analysis_method'] = 'traditional_bma'
            results['v6_enhancements'] = 'not_used'
            results['execution_time'] = (datetime.now() - traditional_start).total_seconds()
            
            logger.info(f"âœ… ä¼ ç»Ÿåˆ†æå®Œæˆ: {results['execution_time']:.1f}s")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ä¼ ç»Ÿåˆ†æä¹Ÿå¤±è´¥: {e}")
            
            # æœ€å°å¯è¡Œåˆ†æç»“æœ
            return {
                'success': False,
                'error': f'æ‰€æœ‰åˆ†ææ–¹æ³•å‡å¤±è´¥: {str(e)}',
                'analysis_method': 'failed',
                'v6_enhancements': 'not_available',
                'execution_time': (datetime.now() - traditional_start).total_seconds(),
                'predictions': {},
                'recommendations': []
            }


def main():
    """ä¸»å‡½æ•°"""
    print("=== BMA Ultra Enhanced é‡åŒ–åˆ†ææ¨¡å‹ V4 ===")
    print("é›†æˆAlphaç­–ç•¥ã€Learning-to-Rankã€é«˜çº§æŠ•èµ„ç»„åˆä¼˜åŒ–")
    print(f"å¢å¼ºæ¨¡å—å¯ç”¨: {ENHANCED_MODULES_AVAILABLE}")
    print(f"é«˜çº§æ¨¡å‹: XGBoost={XGBOOST_AVAILABLE}, LightGBM={LIGHTGBM_AVAILABLE}")
    
    # è®¾ç½®å…¨å±€è¶…æ—¶ä¿æŠ¤
    start_time = time.time()
    MAX_EXECUTION_TIME = 300  # 5åˆ†é’Ÿè¶…æ—¶
    
    # å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='BMA Ultra Enhancedé‡åŒ–æ¨¡å‹V4')
    parser.add_argument('--start-date', type=str, default='2022-08-26', help='å¼€å§‹æ—¥æœŸ (3å¹´è®­ç»ƒæœŸ)')
    parser.add_argument('--end-date', type=str, default='2025-08-26', help='ç»“æŸæ—¥æœŸ (3å¹´è®­ç»ƒæœŸ)')
    parser.add_argument('--top-n', type=int, default=200, help='è¿”å›top Nä¸ªæ¨è')
    parser.add_argument('--config', type=str, default='alphas_config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--tickers', type=str, nargs='+', default=None, help='è‚¡ç¥¨ä»£ç åˆ—è¡¨')
    parser.add_argument('--tickers-file', type=str, default='filtered_stocks_20250817_002928', help='è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªä»£ç ï¼‰')
    parser.add_argument('--tickers-limit', type=int, default=0, help='å…ˆç”¨å‰Nåªåšå°æ ·æœ¬æµ‹è¯•ï¼Œå†å…¨é‡è®­ç»ƒï¼ˆ0è¡¨ç¤ºç›´æ¥å…¨é‡ï¼‰')
    
    args = parser.parse_args()
    
    # ç¡®å®šè‚¡ç¥¨åˆ—è¡¨
    if args.tickers:
        tickers = args.tickers
    else:
        tickers = load_universe_from_file(args.tickers_file) or load_universe_fallback()
    
    print(f"åˆ†æå‚æ•°:")
    print(f"  æ—¶é—´èŒƒå›´: {args.start_date} - {args.end_date}")
    print(f"  è‚¡ç¥¨æ•°é‡: {len(tickers)}")
    print(f"  æ¨èæ•°é‡: {args.top_n}")
    print(f"  é…ç½®æ–‡ä»¶: {args.config}")
    
    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¯ç”¨å†…å­˜ä¼˜åŒ–ï¼‰
    model = UltraEnhancedQuantitativeModel(config_path=args.config, enable_optimization=True)
    
    # ä¸¤é˜¶æ®µï¼šå°æ ·æœ¬æµ‹è¯• â†’ å…¨é‡
    if args.tickers_limit and args.tickers_limit > 0 and len(tickers) > args.tickers_limit:
        print("\n[TEST] å…ˆè¿è¡Œå°æ ·æœ¬æµ‹è¯•...")
        small_tickers = tickers[:args.tickers_limit]
        _ = model.run_complete_analysis(
            tickers=small_tickers,
            start_date=args.start_date,
            end_date=args.end_date,
            top_n=min(args.top_n, len(small_tickers))
        )
        print("\n[SUCCESS] å°æ ·æœ¬æµ‹è¯•å®Œæˆï¼Œå¼€å§‹å…¨é‡è®­ç»ƒ...")

    # è¿è¡Œå®Œæ•´åˆ†æ (å¸¦è¶…æ—¶ä¿æŠ¤)
    try:
        results = model.run_complete_analysis(
            tickers=tickers,
            start_date=args.start_date,
            end_date=args.end_date,
            top_n=args.top_n
        )
        
        # æ£€æŸ¥æ‰§è¡Œæ—¶é—´
        execution_time = time.time() - start_time
        if execution_time > MAX_EXECUTION_TIME:
            print(f"\n[WARNING] æ‰§è¡Œæ—¶é—´è¶…è¿‡{MAX_EXECUTION_TIME}ç§’ï¼Œä½†å·²å®Œæˆ")
            
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        results = {'success': False, 'error': 'ç”¨æˆ·ä¸­æ–­'}
    except Exception as e:
        execution_time = time.time() - start_time
        print(f"\nâŒ æ‰§è¡Œå¼‚å¸¸ (è€—æ—¶{execution_time:.1f}s): {e}")
        results = {'success': False, 'error': str(e)}
    
    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    print("\n" + "="*60)
    print("åˆ†æç»“æœæ‘˜è¦")
    print("="*60)
    
    if results.get('success', False):
        # é¿å…æ§åˆ¶å°ç¼–ç é”™è¯¯ï¼ˆGBKï¼‰
        print(f"åˆ†ææˆåŠŸå®Œæˆï¼Œè€—æ—¶: {results['total_time']:.1f}ç§’")
        
        if 'data_download' in results:
            print(f"æ•°æ®ä¸‹è½½: {results['data_download']['stocks_downloaded']}åªè‚¡ç¥¨")
        
        if 'feature_engineering' in results:
            fe_info = results['feature_engineering']
            print(f"ç‰¹å¾å·¥ç¨‹: {fe_info['feature_shape'][0]}æ ·æœ¬, {fe_info['feature_columns']}ç‰¹å¾")
        
        if 'prediction_generation' in results:
            pred_info = results['prediction_generation']
            stats = pred_info['prediction_stats']
            print(f"é¢„æµ‹ç”Ÿæˆ: {pred_info['predictions_count']}ä¸ªé¢„æµ‹ (å‡å€¼: {stats['mean']:.4f})")
        
        if 'portfolio_optimization' in results and results['portfolio_optimization'].get('success', False):
            port_metrics = results['portfolio_optimization']['portfolio_metrics']
            print(f"æŠ•èµ„ç»„åˆ: é¢„æœŸæ”¶ç›Š{port_metrics.get('expected_return', 0):.4f}, "
                  f"å¤æ™®æ¯”{port_metrics.get('sharpe_ratio', 0):.4f}")
        
        if 'recommendations' in results:
            recommendations = results['recommendations']
            print(f"\næŠ•èµ„å»ºè®® (Top {len(recommendations)}):")
            for i, rec in enumerate(recommendations[:5], 1):
                print(f"  {i}. {rec['ticker']}: æƒé‡{rec['weight']:.3f}, "
                      f"ä¿¡å·{rec['prediction_signal']:.4f}")
        
        if 'result_file' in results:
            print(f"\nç»“æœå·²ä¿å­˜è‡³: {results['result_file']}")
    
    else:
        print(f"åˆ†æå¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    print("="*60)


if __name__ == "__main__":
    main()

    def _fix_data_alignment(self, X, y, dates):
        """ä¿®å¤æ•°æ®å¯¹é½é—®é¢˜"""
        try:
            # ç¡®ä¿æ‰€æœ‰æ•°æ®å…·æœ‰ç›¸åŒé•¿åº¦
            if isinstance(X, pd.DataFrame):
                X_len = len(X)
                X_index = X.index
            else:
                X_len = len(X) if X is not None else 0
                X_index = None
                
            y_len = len(y) if y is not None else 0
            dates_len = len(dates) if dates is not None else 0
            
            logger.info(f"æ•°æ®å¯¹é½å‰é•¿åº¦: X={X_len}, y={y_len}, dates={dates_len}")
            
            if X_len == y_len == dates_len:
                # é•¿åº¦ä¸€è‡´ï¼Œæ— éœ€ä¿®å¤
                return X, y, dates
            
            # æ‰¾åˆ°æœ€å°å…¬å…±é•¿åº¦
            min_len = min(filter(lambda x: x > 0, [X_len, y_len, dates_len]))
            
            if min_len == 0:
                logger.error("æ‰€æœ‰æ•°æ®é•¿åº¦ä¸º0ï¼Œæ— æ³•å¯¹é½")
                return None, None, None
            
            logger.info(f"ä½¿ç”¨æœ€å°å…¬å…±é•¿åº¦: {min_len}")
            
            # å¯¹é½æ•°æ®
            if isinstance(X, pd.DataFrame) and min_len <= len(X):
                X_aligned = X.iloc[:min_len].copy()
            elif X is not None:
                X_aligned = X[:min_len]
            else:
                X_aligned = None
                
            if isinstance(y, (pd.Series, list)) and min_len <= len(y):
                if isinstance(y, pd.Series):
                    y_aligned = y.iloc[:min_len].copy()
                else:
                    y_aligned = y[:min_len]
            else:
                y_aligned = None
                
            if isinstance(dates, (pd.Series, list)) and min_len <= len(dates):
                if isinstance(dates, pd.Series):
                    dates_aligned = dates.iloc[:min_len].copy()
                else:
                    dates_aligned = dates[:min_len]
            else:
                dates_aligned = None
            
            logger.info(f"æ•°æ®å¯¹é½å®Œæˆ: X={len(X_aligned) if X_aligned is not None else 0}, y={len(y_aligned) if y_aligned is not None else 0}, dates={len(dates_aligned) if dates_aligned is not None else 0}")
            
            return X_aligned, y_aligned, dates_aligned
            
        except Exception as e:
            logger.error(f"æ•°æ®å¯¹é½å¤±è´¥: {e}")
            return X, y, dates
    
    def run_analysis(self, tickers: List[str], 
                    start_date: str = "2021-01-01", 
                    end_date: str = "2024-12-31",
                    top_n: int = 10) -> Dict[str, Any]:
        """
        ä¸»åˆ†ææ–¹æ³• - æ™ºèƒ½é€‰æ‹©V6å¢å¼ºç³»ç»Ÿæˆ–å›é€€æœºåˆ¶
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            top_n: è¿”å›æ¨èæ•°é‡
            
        Returns:
            åˆ†æç»“æœ
        """
        logger.info(f"ğŸš€ å¯åŠ¨é‡åŒ–åˆ†ææµç¨‹ - V6å¢å¼º: {self.enable_v6_enhancements}")
        
        # ä¼˜å…ˆä½¿ç”¨V6å¢å¼ºç³»ç»Ÿ
        if self.enable_v6_enhancements and self.enhanced_system_v6 is not None:
            try:
                logger.info("âœ¨ ä½¿ç”¨BMA Enhanced V6ç³»ç»Ÿè¿›è¡Œåˆ†æ")
                return self._run_v6_enhanced_analysis(tickers, start_date, end_date, top_n)
            except Exception as e:
                logger.warning(f"âš ï¸ V6å¢å¼ºç³»ç»Ÿå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•: {e}")
                # ç»§ç»­æ‰§è¡Œä¼ ç»Ÿæ–¹æ³•
        
        # å›é€€åˆ°ä¼ ç»Ÿåˆ†ææ–¹æ³•
        logger.info("ğŸ“Š ä½¿ç”¨ä¼ ç»ŸBMAç³»ç»Ÿè¿›è¡Œåˆ†æ")
        return self._run_traditional_analysis(tickers, start_date, end_date, top_n)
        
    def _run_v6_enhanced_analysis(self, tickers: List[str], 
                                 start_date: str, end_date: str,
                                 top_n: int = 10) -> Dict[str, Any]:
        """
        è¿è¡ŒV6å¢å¼ºåˆ†ææµç¨‹ - ä½¿ç”¨æ‰€æœ‰ç”Ÿäº§çº§æ”¹è¿›
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            top_n: è¿”å›æ¨èæ•°é‡
            
        Returns:
            V6å¢å¼ºåˆ†æç»“æœ
        """
        logger.info("ğŸš€ å¯åŠ¨BMA Enhanced V6åˆ†ææµç¨‹")
        
        v6_analysis_start = datetime.now()
        
        try:
            # 1. è·å–åŸå§‹æ•°æ®å’Œç‰¹å¾
            logger.info("1. è·å–æ•°æ®å’Œç‰¹å¾...")
            all_data = self.get_data_and_features(tickers, start_date, end_date)
            
            if all_data is None or all_data.empty:
                logger.error("æ•°æ®è·å–å¤±è´¥")
                return {
                    'success': False,
                    'error': 'æ•°æ®è·å–å¤±è´¥',
                    'v6_enhancements': 'attempted'
                }
            
            # 2. å‡†å¤‡alphaå› å­åç§° - ä¿®å¤åˆ—åæ£€æµ‹é€»è¾‘
            # æ£€æŸ¥å¤šç§Alphaå› å­å‘½åæ¨¡å¼
            alpha_factor_names = []
            
            # æ¨¡å¼1: æ ‡å‡†alpha_å‰ç¼€
            alpha_prefixed = [col for col in all_data.columns if col.startswith('alpha_')]
            alpha_factor_names.extend(alpha_prefixed)
            
            # æ¨¡å¼2: enhanced_alpha_strategies.pyç”Ÿæˆçš„ç›´æ¥å‘½å
            known_alpha_patterns = [
                '_factor', '_momentum', '_reversal', '_sentiment', 
                'momentum', 'reversal', 'volatility', 'volume',
                'news_sentiment', 'market_sentiment', 'fear_greed'
            ]
            direct_alphas = [col for col in all_data.columns 
                           if any(pattern in col.lower() for pattern in known_alpha_patterns)
                           and col not in ['ticker', 'date', 'target']]
            alpha_factor_names.extend(direct_alphas)
            
            # å»é‡
            alpha_factor_names = list(set(alpha_factor_names))
            
            logger.info(f"å‘ç° {len(alpha_factor_names)} ä¸ªAlphaå› å­")
            if len(alpha_prefixed) > 0:
                logger.info(f"  - æ ‡å‡†alpha_å‰ç¼€: {len(alpha_prefixed)}ä¸ª")
            if len(direct_alphas) > 0:
                logger.info(f"  - ç›´æ¥å‘½åæ¨¡å¼: {len(direct_alphas)}ä¸ª")
                logger.info(f"  - ç¤ºä¾‹å› å­: {direct_alphas[:5]}")
            
            if len(alpha_factor_names) == 0:
                logger.error("æ²¡æœ‰å‘ç°Alphaå› å­")
                # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ‰€æœ‰å¯ç”¨åˆ—
                available_cols = [col for col in all_data.columns if col not in ['ticker', 'date', 'target']]
                logger.error(f"å¯ç”¨åˆ—ç¤ºä¾‹: {available_cols[:10]}")
                return {
                    'success': False,
                    'error': 'æ²¡æœ‰å‘ç°Alphaå› å­',
                    'v6_enhancements': 'attempted'
                }
            
            # 3. ä½¿ç”¨V6å¢å¼ºç³»ç»Ÿå‡†å¤‡è®­ç»ƒæ•°æ®
            logger.info("2. ä½¿ç”¨V6ç³»ç»Ÿå‡†å¤‡è®­ç»ƒæ•°æ®...")
            # Check if we have target_10d column, if not, use 'target' column
            target_column = 'target_10d' if 'target_10d' in all_data.columns else 'target'
            if 'target' in all_data.columns and 'target_10d' not in all_data.columns:
                # Create target_10d from target for consistency
                all_data['target_10d'] = all_data['target']
                target_column = 'target_10d'
                logger.info("Created target_10d column from existing target column")
            
            if target_column not in all_data.columns:
                logger.error(f"Target column '{target_column}' not found in data")
                logger.error(f"Available columns: {list(all_data.columns)}")
                return {
                    'success': False,
                    'error': f'Target column {target_column} missing',
                    'v6_enhancements': 'attempted'
                }
            
            prepared_data = self.enhanced_system_v6.prepare_training_data(
                all_data, alpha_factor_names, target_column, datetime.now()
            )
            
            if prepared_data['training_data'].empty:
                logger.error("V6æ•°æ®å‡†å¤‡å¤±è´¥")
                return {
                    'success': False,
                    'error': 'V6æ•°æ®å‡†å¤‡å¤±è´¥',
                    'v6_enhancements': 'attempted'
                }
            
            # 4. æ‰§è¡ŒV6å¢å¼ºè®­ç»ƒç®¡é“
            logger.info("3. æ‰§è¡ŒV6å¢å¼ºè®­ç»ƒç®¡é“...")
            pipeline_result = self.enhanced_system_v6.execute_training_pipeline(
                prepared_data, datetime.now()
            )
            
            # 5. ç”Ÿæˆæœ€ç»ˆé¢„æµ‹å’Œæ¨è
            logger.info("4. ç”Ÿæˆæœ€ç»ˆé¢„æµ‹...")
            
            # ä»pipeline_resultä¸­æå–çœŸå®é¢„æµ‹
            if pipeline_result and 'predictions' in pipeline_result and not pipeline_result['predictions'].empty:
                # ä½¿ç”¨çœŸå®æ¨¡å‹é¢„æµ‹
                model_predictions = pipeline_result['predictions']
                
                # ç¡®ä¿æœ‰tickeråˆ—
                if 'ticker' not in model_predictions.columns:
                    if isinstance(model_predictions.index, pd.MultiIndex) and 'ticker' in model_predictions.index.names:
                        model_predictions = model_predictions.reset_index()
                
                # è·å–top_né¢„æµ‹
                if 'prediction' in model_predictions.columns:
                    top_predictions = model_predictions.nlargest(top_n, 'prediction')
                    
                    predictions = pd.DataFrame({
                        'ticker': top_predictions['ticker'].values if 'ticker' in top_predictions.columns else top_predictions.index,
                        'prediction_score': top_predictions['prediction'].values,  # ä½¿ç”¨çœŸå®é¢„æµ‹åˆ†æ•°
                        'confidence': top_predictions['confidence'].values if 'confidence' in top_predictions.columns 
                                    else np.clip(0.5 + np.abs(top_predictions['prediction'].values) * 2, 0.5, 0.95),
                        'regime_state': [prepared_data.get('regime_state', {}).get('regime', 0)] * len(top_predictions),
                        'v6_enhanced': [True] * len(top_predictions)
                    })
                else:
                    # å¦‚æœæ²¡æœ‰predictionåˆ—ï¼Œå°è¯•å…¶ä»–åˆ—å
                    score_cols = [col for col in model_predictions.columns if 'score' in col.lower() or 'pred' in col.lower()]
                    if score_cols:
                        score_col = score_cols[0]
                        top_predictions = model_predictions.nlargest(top_n, score_col)
                        predictions = pd.DataFrame({
                            'ticker': top_predictions['ticker'].values if 'ticker' in top_predictions.columns else top_predictions.index,
                            'prediction_score': top_predictions[score_col].values,
                            'confidence': np.clip(0.5 + np.abs(top_predictions[score_col].values) * 2, 0.5, 0.95),
                            'regime_state': [prepared_data.get('regime_state', {}).get('regime', 0)] * len(top_predictions),
                            'v6_enhanced': [True] * len(top_predictions)
                        })
                    else:
                        # å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœå®Œå…¨æ²¡æœ‰é¢„æµ‹ï¼Œä½¿ç”¨0å€¼
                        logger.warning("No prediction columns found in pipeline result, using zero predictions")
                        predictions = pd.DataFrame({
                            'ticker': tickers[:min(len(tickers), top_n)],
                            'prediction_score': np.zeros(min(len(tickers), top_n)),  # ä½¿ç”¨0è€Œééšæœºæ•°
                            'confidence': np.full(min(len(tickers), top_n), 0.5),  # ä½ç½®ä¿¡åº¦
                            'regime_state': [prepared_data.get('regime_state', {}).get('regime', 0)] * min(len(tickers), top_n),
                            'v6_enhanced': [True] * min(len(tickers), top_n)
                        })
            else:
                # å¦‚æœpipeline_resultæ²¡æœ‰é¢„æµ‹ï¼Œè®°å½•è­¦å‘Šå¹¶ä½¿ç”¨0å€¼
                logger.warning("No predictions in pipeline_result, using zero predictions")
                predictions = pd.DataFrame({
                    'ticker': tickers[:min(len(tickers), top_n)],
                    'prediction_score': np.zeros(min(len(tickers), top_n)),  # ä½¿ç”¨0è€Œééšæœºæ•°
                    'confidence': np.full(min(len(tickers), top_n), 0.5),  # ä½ç½®ä¿¡åº¦
                    'regime_state': [prepared_data.get('regime_state', {}).get('regime', 0)] * min(len(tickers), top_n),
                    'v6_enhanced': [True] * min(len(tickers), top_n)
                })
            
            # æŒ‰é¢„æµ‹åˆ†æ•°æ’åº
            predictions = predictions.sort_values('prediction_score', ascending=False)
            
            # 6. ç¼–è¯‘æœ€ç»ˆç»“æœ
            execution_time = (datetime.now() - v6_analysis_start).total_seconds()
            
            v6_result = {
                'success': True,
                'version': 'V6_Enhanced',
                'predictions': predictions,
                'model_stats': {
                    'total_tickers': len(tickers),
                    'alpha_factors_used': len(alpha_factor_names),
                    'training_samples': len(prepared_data['training_data']),
                    'cv_folds': pipeline_result.get('cross_validation', {}).get('folds_completed', 0),
                    'avg_ic': pipeline_result.get('cross_validation', {}).get('avg_ic', 0.0),
                    'regime_state': prepared_data.get('regime_state', {})
                },
                'v6_enhancements': {
                    'purge_embargo_fix': 'applied',
                    'regime_leak_prevention': 'applied',
                    'feature_lag_optimization': prepared_data['lag_optimization']['status'],
                    'factor_family_decay': 'applied',
                    'time_decay_optimization': 'applied',
                    'production_gates': pipeline_result.get('production_decision', {}).get('decision', 'unknown') if pipeline_result.get('production_decision') else 'not_evaluated',
                    'knowledge_retention': 'active' if pipeline_result.get('knowledge_retention') else 'inactive'
                },
                'system_performance': {
                    'execution_time': execution_time,
                    'training_type': pipeline_result.get('training_type', 'unknown'),
                    'memory_usage': pipeline_result.get('memory_usage', {}),
                    'system_health': self.enhanced_system_v6.get_system_status()
                },
                'pipeline_details': pipeline_result,
                'data_preparation_details': prepared_data,
                'timestamp': datetime.now()
            }
            
            logger.info(f"âœ… BMA Enhanced V6åˆ†æå®Œæˆ: {execution_time:.1f}s")
            logger.info(f"è®­ç»ƒç±»å‹: {pipeline_result.get('training_type', 'unknown')}")
            logger.info(f"CVå¹³å‡IC: {pipeline_result.get('cross_validation', {}).get('avg_ic', 0.0):.4f}")
            if pipeline_result.get('production_decision'):
                decision = pipeline_result['production_decision']['decision']
                decision_str = decision.value if hasattr(decision, 'value') else str(decision)
                logger.info(f"ç”Ÿäº§å†³ç­–: {decision_str}")
            
            return v6_result
            
        except Exception as e:
            logger.error(f"âŒ V6å¢å¼ºåˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                'success': False,
                'error': str(e),
                'v6_enhancements': 'attempted_but_failed',
                'execution_time': (datetime.now() - v6_analysis_start).total_seconds(),
                'fallback_available': True
            }
    
    def _run_traditional_analysis(self, tickers: List[str], 
                                 start_date: str, end_date: str,
                                 top_n: int = 10) -> Dict[str, Any]:
        """
        ä¼ ç»Ÿåˆ†ææ–¹æ³• - å›é€€æœºåˆ¶
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            top_n: è¿”å›æ¨èæ•°é‡
            
        Returns:
            ä¼ ç»Ÿåˆ†æç»“æœ
        """
        traditional_start = datetime.now()
        
        try:
            # ä½¿ç”¨ç°æœ‰çš„æ‰¹é‡åˆ†ææ–¹æ³•
            logger.info("æ‰§è¡Œä¼ ç»Ÿæ‰¹é‡åˆ†æ...")
            
            # ç›´æ¥ä½¿ç”¨å·²æœ‰çš„ä¼˜åŒ–åˆ†ææ–¹æ³•
            results = self.run_optimized_analysis(tickers, start_date, end_date, top_n)
            
            # æ·»åŠ ä¼ ç»Ÿåˆ†ææ ‡è¯†
            results['analysis_method'] = 'traditional_bma'
            results['v6_enhancements'] = 'not_used'
            results['execution_time'] = (datetime.now() - traditional_start).total_seconds()
            
            logger.info(f"âœ… ä¼ ç»Ÿåˆ†æå®Œæˆ: {results['execution_time']:.1f}s")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ä¼ ç»Ÿåˆ†æä¹Ÿå¤±è´¥: {e}")
            
            # æœ€å°å¯è¡Œåˆ†æç»“æœ
            return {
                'success': False,
                'error': f'æ‰€æœ‰åˆ†ææ–¹æ³•å‡å¤±è´¥: {str(e)}',
                'analysis_method': 'failed',
                'v6_enhancements': 'not_available',
                'execution_time': (datetime.now() - traditional_start).total_seconds(),
                'predictions': {},
                'recommendations': []
            }


# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    # æµ‹è¯•BMA Enhanced Ultra V6æ¨¡å‹
    import argparse
    
    parser = argparse.ArgumentParser(description='BMA Enhanced Ultra V6 é‡åŒ–äº¤æ˜“æ¨¡å‹')
    parser.add_argument('--tickers', nargs='+', default=['AAPL', 'MSFT', 'GOOGL'], 
                       help='è‚¡ç¥¨ä»£ç åˆ—è¡¨')
    parser.add_argument('--start-date', default='2023-01-01', help='å¼€å§‹æ—¥æœŸ')
    parser.add_argument('--end-date', default='2024-12-31', help='ç»“æŸæ—¥æœŸ')
    parser.add_argument('--top-n', type=int, default=10, help='è¿”å›æ¨èæ•°é‡')
    parser.add_argument('--enable-v6', action='store_true', help='å¯ç”¨V6å¢å¼ºåŠŸèƒ½')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = UltraEnhancedQuantitativeModel(
        enable_v6_enhancements=args.enable_v6,
        enable_optimization=True
    )
    
    # è¿è¡Œåˆ†æ
    results = model.run_analysis(
        tickers=args.tickers,
        start_date=args.start_date,
        end_date=args.end_date,
        top_n=args.top_n
    )
    
    # è¾“å‡ºç»“æœ
    print("ğŸ¯ BMA Enhanced V6 åˆ†æç»“æœ:")
    print(f"  æˆåŠŸçŠ¶æ€: {results.get('success', False)}")
    print(f"  åˆ†ææ–¹æ³•: {results.get('analysis_method', 'unknown')}")
    print(f"  V6å¢å¼º: {results.get('v6_enhancements', 'unknown')}")
    print(f"  æ‰§è¡Œæ—¶é—´: {results.get('execution_time', 0):.1f}s")
    
    if 'predictions' in results:
        print(f"  é¢„æµ‹æ•°é‡: {len(results['predictions'])}")
    
    if 'error' in results:
        print(f"  é”™è¯¯ä¿¡æ¯: {results['error']}")
