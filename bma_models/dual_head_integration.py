#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Dual Head Integration Module - åŒå¤´èåˆé›†æˆæ¨¡å—
==============================================

æ— ç¼é›†æˆåŒå¤´æ™šèåˆåˆ°ç°æœ‰BMAç³»ç»Ÿï¼Œæœ€å°åŒ–ä»£ç æ”¹åŠ¨ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æå–Ridgeå’ŒLambdaçš„OOFé¢„æµ‹
2. è®­ç»ƒåŒå¤´èåˆæ¨¡å‹
3. åœ¨é¢„æµ‹æ—¶åº”ç”¨èåˆ
4. ä¸ç°æœ‰ExcelExporterå…¼å®¹

ä½œè€…: BMA Enhanced System
æ—¥æœŸ: 2025-01-XX
ç‰ˆæœ¬: 1.0.0
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, Optional
import logging

try:
    from bma_models.dual_head_late_fusion import DualHeadLateFusion
    FUSION_AVAILABLE = True
except ImportError:
    FUSION_AVAILABLE = False

logger = logging.getLogger(__name__)


class DualHeadFusionManager:
    """
    åŒå¤´èåˆç®¡ç†å™¨ - é›†æˆåˆ°BMAç³»ç»Ÿçš„æ¡¥æ¥ç±»

    èŒè´£ï¼š
    1. ä»è®­ç»ƒç»“æœä¸­æå–OOFé¢„æµ‹
    2. è®­ç»ƒåŒå¤´èåˆæ¨¡å‹
    3. ç®¡ç†èåˆæ¨¡å‹çš„ä¿å­˜å’ŒåŠ è½½
    4. åœ¨é¢„æµ‹æ—¶åº”ç”¨èåˆ
    """

    def __init__(self,
                 enable_fusion: bool = True,
                 alpha: float = 0.7,
                 beta: float = 0.3,
                 auto_tune: bool = True):
        """
        åˆå§‹åŒ–åŒå¤´èåˆç®¡ç†å™¨

        Args:
            enable_fusion: æ˜¯å¦å¯ç”¨åŒå¤´èåˆ
            alpha: å›å½’å¤´æƒé‡ï¼ˆé»˜è®¤0.7ï¼‰
            beta: æ’åºå¤´æƒé‡ï¼ˆé»˜è®¤0.3ï¼‰
            auto_tune: æ˜¯å¦è‡ªåŠ¨è°ƒå‚
        """
        if not FUSION_AVAILABLE:
            logger.warning("âš ï¸ DualHeadLateFusionæ¨¡å—ä¸å¯ç”¨ï¼Œå°†ç¦ç”¨åŒå¤´èåˆ")
            self.enable_fusion = False
        else:
            self.enable_fusion = enable_fusion

        self.alpha = alpha
        self.beta = beta
        self.auto_tune = auto_tune

        # èåˆæ¨¡å‹
        self.fusion_model: Optional[DualHeadLateFusion] = None

        # è®­ç»ƒçŠ¶æ€
        self.fitted_ = False

        if self.enable_fusion:
            logger.info("âœ… åŒå¤´èåˆç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            logger.info(f"   åˆå§‹æƒé‡: Î±={alpha:.2f}, Î²={beta:.2f}")
            logger.info(f"   è‡ªåŠ¨è°ƒå‚: {auto_tune}")
        else:
            logger.info("â­ï¸ åŒå¤´èåˆå·²ç¦ç”¨")

    def extract_oof_predictions(self, training_results: Dict[str, Any], y_true: Optional[pd.Series] = None, dates: Optional[pd.Series] = None) -> Optional[Dict[str, pd.Series]]:
        """
        ä»è®­ç»ƒç»“æœä¸­æå–OOFé¢„æµ‹

        Args:
            training_results: è®­ç»ƒç»“æœå­—å…¸ï¼ˆæ¥è‡ª_unified_parallel_trainingï¼‰

        Returns:
            åŒ…å«ridge_oofå’Œlambda_oofçš„å­—å…¸ï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        if not self.enable_fusion:
            return None

        logger.info("ğŸ“Š å¼€å§‹æå–OOFé¢„æµ‹...")

        try:
            import pandas as pd
            from typing import Dict as _Dict
            ridge_oof = None

            # 1) ä¼˜å…ˆä»æ ‡å‡†ä½ç½®è·å–ç¬¬ä¸€å±‚OOF
            oof_preds: _Dict[str, pd.Series] = {}
            if 'oof_predictions' in training_results and isinstance(training_results['oof_predictions'], dict):
                oof_preds = training_results['oof_predictions']
            elif 'traditional_models' in training_results and isinstance(training_results['traditional_models'], dict):
                oof_preds = training_results['traditional_models'].get('oof_predictions', {}) or {}

            # 2) è‹¥å¯ç”¨ï¼ŒåŸºäºç¬¬ä¸€å±‚OOFå’Œæ—¶é—´CVç”Ÿæˆ Ridge çš„ OOFï¼ˆæ— æ³„æ¼ï¼‰
            if ridge_oof is None and oof_preds and y_true is not None and dates is not None:
                try:
                    # ä»…ä½¿ç”¨ä¸‰å›å½’å¤´
                    base_keys = ['elastic_net', 'xgboost', 'catboost']
                    missing = [k for k in base_keys if k not in oof_preds]
                    if not missing:
                        # ç»„è£…ç‰¹å¾çŸ©é˜µï¼ˆä¿æŒç´¢å¼•ï¼‰
                        X_df = pd.DataFrame({
                            'pred_elastic': pd.Series(oof_preds['elastic_net']).reindex(y_true.index),
                            'pred_xgb': pd.Series(oof_preds['xgboost']).reindex(y_true.index),
                            'pred_catboost': pd.Series(oof_preds['catboost']).reindex(y_true.index),
                        })
                        y_vec = pd.Series(y_true).reindex(y_true.index)

                        # æ—¶é—´å®‰å…¨CV
                        try:
                            from bma_models.unified_purged_cv_factory import create_unified_cv
                            cv = create_unified_cv(n_splits=6, gap=5, embargo=5)
                            # å°†æ—¥æœŸæ˜ å°„ä¸ºç»„ç´¢å¼•
                            date_index = y_true.index.get_level_values('date') if isinstance(y_true.index, pd.MultiIndex) else pd.to_datetime(dates)
                            date_codes = pd.Series(date_index).astype('category').cat.codes.values
                            splits = list(cv.split(X_df.values, y_vec.values, groups=date_codes))
                        except Exception:
                            # å›é€€ï¼šç®€å•KFoldï¼ˆéç†æƒ³ï¼Œä½†ä¿è¯ä¸å´©ï¼‰
                            from sklearn.model_selection import KFold
                            kf = KFold(n_splits=6, shuffle=False)
                            splits = list(kf.split(X_df.values, y_vec.values))

                        # é€æŠ˜è®­ç»ƒRidgeå¹¶äº§å‡ºOOF
                        import numpy as np
                        from sklearn.linear_model import Ridge
                        ridge_oof_arr = np.full(len(X_df), np.nan)
                        for tr_idx, va_idx in splits:
                            X_tr, X_va = X_df.values[tr_idx], X_df.values[va_idx]
                            y_tr = y_vec.values[tr_idx]
                            model = Ridge(alpha=1.0, fit_intercept=False)
                            model.fit(X_tr, y_tr)
                            ridge_oof_arr[va_idx] = model.predict(X_va)
                        ridge_oof = pd.Series(ridge_oof_arr, index=y_true.index, name='ridge_oof')
                        logger.info(f"   âœ“ Ridge OOF(derived): {ridge_oof.notna().sum()} samples")
                    else:
                        logger.warning(f"   âš ï¸ ç¼ºå°‘ç¬¬ä¸€å±‚OOF: {missing}ï¼Œæ— æ³•æ´¾ç”ŸRidge OOF")
                except Exception as e:
                    logger.warning(f"   âš ï¸ åŸºäºç¬¬ä¸€å±‚OOFç”ŸæˆRidge OOFå¤±è´¥: {e}")

            # æå–Lambda OOFï¼ˆä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€OOFå­—å…¸ï¼‰
            lambda_oof = None
            if oof_preds and 'lambdarank' in oof_preds:
                lambda_oof = pd.Series(oof_preds['lambdarank'], name='lambda_oof')
                logger.info(f"   âœ“ Lambda OOF: {len(lambda_oof)} samples")
            else:
                # æ¬¡é€‰ï¼šä»modelsç»“æ„æˆ–æ¨¡å‹å†…å­˜å–
                if 'models' in training_results and 'lambdarank' in training_results['models']:
                    lambda_info = training_results['models']['lambdarank']
                    if 'oof_predictions' in lambda_info:
                        lambda_oof = pd.Series(lambda_info['oof_predictions'], name='lambda_oof')
                        logger.info(f"   âœ“ Lambda OOF: {len(lambda_oof)} samples")
                    elif 'model' in lambda_info:
                        model = lambda_info['model']
                        if hasattr(model, '_oof_predictions') and model._oof_predictions is not None:
                            lambda_oof = pd.Series(model._oof_predictions, name='lambda_oof')
                            logger.info(f"   âœ“ Lambda OOF (from model): {len(lambda_oof)} samples")

            # éªŒè¯
            if ridge_oof is None or lambda_oof is None:
                logger.error("âŒ æ— æ³•æå–å®Œæ•´çš„OOFé¢„æµ‹")
                logger.error(f"   Ridge OOF: {'âœ“' if ridge_oof is not None else 'âœ—'}")
                logger.error(f"   Lambda OOF: {'âœ“' if lambda_oof is not None else 'âœ—'}")
                return None

            # ç¡®ä¿ç´¢å¼•å¯¹é½
            if not isinstance(ridge_oof, pd.Series):
                ridge_oof = pd.Series(ridge_oof, name='ridge_oof')
            if not isinstance(lambda_oof, pd.Series):
                lambda_oof = pd.Series(lambda_oof, name='lambda_oof')

            logger.info("âœ… OOFé¢„æµ‹æå–æˆåŠŸ")
            return {
                'ridge_oof': ridge_oof,
                'lambda_oof': lambda_oof
            }

        except Exception as e:
            logger.error(f"âŒ æå–OOFé¢„æµ‹æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None

    def fit_fusion_model(self,
                         ridge_oof: pd.Series,
                         lambda_oof: pd.Series,
                         y_true: pd.Series,
                         dates: pd.Series) -> bool:
        """
        è®­ç»ƒåŒå¤´èåˆæ¨¡å‹

        Args:
            ridge_oof: Ridgeå›å½’å¤´OOFé¢„æµ‹
            lambda_oof: Lambdaæ’åºå¤´OOFé¢„æµ‹
            y_true: çœŸå®æ ‡ç­¾
            dates: æ—¥æœŸåºåˆ—

        Returns:
            è®­ç»ƒæ˜¯å¦æˆåŠŸ
        """
        if not self.enable_fusion:
            return False

        logger.info("="*80)
        logger.info("ğŸ”¥ å¼€å§‹è®­ç»ƒåŒå¤´èåˆæ¨¡å‹")
        logger.info("="*80)

        try:
            # åˆå§‹åŒ–èåˆæ¨¡å‹
            self.fusion_model = DualHeadLateFusion(
                alpha=self.alpha,
                beta=self.beta,
                auto_tune_weights=self.auto_tune,
                use_isotonic_calibration=False,  # ğŸ”§ å…³é—­Isotonicï¼šæ”¹å–„å°(1%)ä¸”åç»­z-scoreä¼šè¦†ç›–
                tanh_clip_c=2.5
            )

            # è®­ç»ƒ
            self.fusion_model.fit(
                ridge_oof_preds=ridge_oof,
                lambda_oof_preds=lambda_oof,
                y_true=y_true,
                dates=dates
            )

            self.fitted_ = True

            # è¾“å‡ºèåˆæ¨¡å‹ä¿¡æ¯
            fusion_info = self.fusion_model.get_model_info()
            logger.info("="*80)
            logger.info("âœ… åŒå¤´èåˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
            logger.info(f"   æœ€ä¼˜æƒé‡: Î±={fusion_info['best_alpha']:.3f} (å›å½’), Î²={fusion_info['best_beta']:.3f} (æ’åº)")
            logger.info(f"   Isotonicæ ¡å‡†: {'å¯ç”¨' if fusion_info['isotonic_calibration'] else 'ç¦ç”¨'}")
            logger.info(f"   åˆ†ä½æ•°æ˜ å°„: {'å¯ç”¨' if fusion_info['rank_quantile_mapping'] else 'ç¦ç”¨'}")
            logger.info("="*80)

            return True

        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒåŒå¤´èåˆæ¨¡å‹æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            self.fitted_ = False
            return False

    def apply_fusion(self,
                     ridge_preds: pd.Series,
                     lambda_preds: pd.Series,
                     dates: pd.Series) -> Optional[pd.Series]:
        """
        åº”ç”¨èåˆï¼šåœ¨é¢„æµ‹æ—¶ä½¿ç”¨

        Args:
            ridge_preds: Ridgeé¢„æµ‹
            lambda_preds: Lambdaé¢„æµ‹
            dates: æ—¥æœŸåºåˆ—

        Returns:
            èåˆåçš„æœ€ç»ˆåˆ†æ•°ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        if not self.enable_fusion or not self.fitted_:
            logger.warning("âš ï¸ èåˆæ¨¡å‹æœªè®­ç»ƒæˆ–å·²ç¦ç”¨ï¼Œè¿”å›Ridgeé¢„æµ‹")
            return ridge_preds

        try:
            fused_scores = self.fusion_model.predict(
                ridge_preds=ridge_preds,
                lambda_preds=lambda_preds,
                dates=dates
            )

            logger.info(f"âœ… åŒå¤´èåˆé¢„æµ‹å®Œæˆ: {fused_scores.notna().sum()}/{len(fused_scores)} æœ‰æ•ˆæ ·æœ¬")
            return fused_scores

        except Exception as e:
            logger.error(f"âŒ åº”ç”¨èåˆæ—¶å‡ºé”™: {e}")
            logger.warning("   é™çº§åˆ°Ridgeé¢„æµ‹")
            return ridge_preds

    def integrate_with_training(self, training_results: Dict[str, Any],
                                y_true: pd.Series,
                                dates: pd.Series) -> Dict[str, Any]:
        """
        ä¸ç°æœ‰è®­ç»ƒæµç¨‹é›†æˆçš„ä¸»è¦æ¥å£

        åœ¨è®­ç»ƒå®Œæˆåè°ƒç”¨ï¼Œè‡ªåŠ¨æå–OOFå¹¶è®­ç»ƒèåˆæ¨¡å‹

        Args:
            training_results: è®­ç»ƒç»“æœå­—å…¸
            y_true: çœŸå®æ ‡ç­¾
            dates: æ—¥æœŸåºåˆ—

        Returns:
            æ›´æ–°åçš„è®­ç»ƒç»“æœï¼ˆæ·»åŠ fusion_modelå­—æ®µï¼‰
        """
        if not self.enable_fusion:
            logger.info("â­ï¸ åŒå¤´èåˆå·²ç¦ç”¨ï¼Œè·³è¿‡é›†æˆ")
            return training_results

        logger.info("="*80)
        logger.info("ğŸ”— é›†æˆåŒå¤´èåˆåˆ°è®­ç»ƒæµç¨‹")
        logger.info("="*80)

        # Step 1: æå–OOFé¢„æµ‹
        oof_dict = self.extract_oof_predictions(training_results, y_true=y_true, dates=dates)
        if oof_dict is None:
            logger.error("âŒ OOFæå–å¤±è´¥ï¼Œæ— æ³•è®­ç»ƒèåˆæ¨¡å‹")
            return training_results

        ridge_oof = oof_dict['ridge_oof']
        lambda_oof = oof_dict['lambda_oof']

        # Step 2: è®­ç»ƒèåˆæ¨¡å‹
        success = self.fit_fusion_model(
            ridge_oof=ridge_oof,
            lambda_oof=lambda_oof,
            y_true=y_true,
            dates=dates
        )

        if success:
            # å°†èåˆæ¨¡å‹æ·»åŠ åˆ°è®­ç»ƒç»“æœä¸­
            training_results['fusion_model'] = {
                'model': self.fusion_model,
                'manager': self,
                'fitted': True,
                'info': self.fusion_model.get_model_info()
            }
            logger.info("âœ… åŒå¤´èåˆå·²æˆåŠŸé›†æˆåˆ°è®­ç»ƒç»“æœ")
        else:
            logger.error("âŒ åŒå¤´èåˆé›†æˆå¤±è´¥")

        return training_results

    def get_info(self) -> Dict[str, Any]:
        """è·å–ç®¡ç†å™¨ä¿¡æ¯"""
        return {
            'enable_fusion': self.enable_fusion,
            'fitted': self.fitted_,
            'alpha': self.alpha,
            'beta': self.beta,
            'auto_tune': self.auto_tune,
            'fusion_model_info': self.fusion_model.get_model_info() if self.fusion_model else None
        }


def enable_dual_head_fusion_for_model(model_instance,
                                      enable: bool = True,
                                      alpha: float = 0.7,
                                      beta: float = 0.3,
                                      auto_tune: bool = True):
    """
    ä¸ºç°æœ‰æ¨¡å‹å®ä¾‹å¯ç”¨åŒå¤´èåˆ

    è¿™æ˜¯ä¸€ä¸ªä¾¿æ·å‡½æ•°ï¼Œç”¨äºå¿«é€Ÿä¸ºBMAæ¨¡å‹æ·»åŠ åŒå¤´èåˆåŠŸèƒ½

    Args:
        model_instance: BMAæ¨¡å‹å®ä¾‹ï¼ˆé‡åŒ–æ¨¡å‹_bma_ultra_enhancedçš„å®ä¾‹ï¼‰
        enable: æ˜¯å¦å¯ç”¨èåˆ
        alpha: å›å½’å¤´æƒé‡
        beta: æ’åºå¤´æƒé‡
        auto_tune: æ˜¯å¦è‡ªåŠ¨è°ƒå‚

    Example:
        >>> model = QuantitativeModel(...)
        >>> enable_dual_head_fusion_for_model(model, enable=True)
        >>> # ä¹‹åè®­ç»ƒä¼šè‡ªåŠ¨ä½¿ç”¨åŒå¤´èåˆ
    """
    fusion_manager = DualHeadFusionManager(
        enable_fusion=enable,
        alpha=alpha,
        beta=beta,
        auto_tune=auto_tune
    )

    # å°†ç®¡ç†å™¨é™„åŠ åˆ°æ¨¡å‹å®ä¾‹
    model_instance.dual_head_fusion_manager = fusion_manager

    logger.info("âœ… åŒå¤´èåˆå·²å¯ç”¨")
    logger.info("   åœ¨run_complete_analysisæˆ–train_modelså®Œæˆåï¼Œèåˆæ¨¡å‹å°†è‡ªåŠ¨è®­ç»ƒ")

    return model_instance
