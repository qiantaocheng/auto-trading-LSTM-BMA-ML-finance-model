#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Alpha Factor Quality Monitoring System
å®æ—¶ç›‘æ§æ¯ä¸ªalphaå› å­çš„è®¡ç®—è´¨é‡å’Œæ•°æ®å®Œæ•´æ€§
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Any, Tuple
from datetime import datetime
import json
import os

logger = logging.getLogger(__name__)

class AlphaFactorQualityMonitor:
    """
    Alphaå› å­è´¨é‡ç›‘æ§ç³»ç»Ÿ
    
    åŠŸèƒ½:
    1. å®æ—¶ç›‘æ§æ¯ä¸ªå› å­è®¡ç®—
    2. æ•°æ®è´¨é‡æ£€æŸ¥
    3. å¼‚å¸¸å€¼æ£€æµ‹
    4. è¦†ç›–ç‡ç»Ÿè®¡
    5. æ€§èƒ½è¿½è¸ª
    """
    
    def __init__(self, save_reports: bool = True, report_dir: str = "cache/factor_quality"):
        self.save_reports = save_reports
        self.report_dir = report_dir
        self.factor_stats = {}
        self.quality_issues = []
        self.computation_times = {}
        
        if save_reports:
            os.makedirs(report_dir, exist_ok=True)
    
    def monitor_factor_computation(self, factor_name: str, factor_data: pd.Series, 
                                  computation_time: float = None) -> Dict[str, Any]:
        """
        ç›‘æ§å•ä¸ªå› å­çš„è®¡ç®—è´¨é‡
        
        Args:
            factor_name: å› å­åç§°
            factor_data: å› å­æ•°æ®
            computation_time: è®¡ç®—è€—æ—¶
            
        Returns:
            è´¨é‡æŠ¥å‘Šå­—å…¸
        """
        logger.info(f"ğŸ” [MONITOR] Analyzing {factor_name}...")
        
        quality_report = {
            'factor_name': factor_name,
            'timestamp': datetime.now().isoformat(),
            'data_points': len(factor_data),
            'computation_time': computation_time
        }
        
        # 1. åŸºç¡€ç»Ÿè®¡
        quality_report['statistics'] = {
            'mean': float(factor_data.mean()),
            'std': float(factor_data.std()),
            'min': float(factor_data.min()),
            'max': float(factor_data.max()),
            'median': float(factor_data.median()),
            'skew': float(factor_data.skew()),
            'kurtosis': float(factor_data.kurtosis())
        }
        
        # 2. æ•°æ®è´¨é‡æŒ‡æ ‡
        total_count = len(factor_data)
        quality_report['data_quality'] = {
            'non_zero_count': int((factor_data != 0).sum()),
            'non_zero_ratio': float((factor_data != 0).sum() / total_count) if total_count > 0 else 0.0,
            'nan_count': int(factor_data.isna().sum()),
            'nan_ratio': float(factor_data.isna().sum() / total_count) if total_count > 0 else 0.0,
            'inf_count': int(np.isinf(factor_data).sum()),
            'unique_values': int(factor_data.nunique()),
            'unique_ratio': float(factor_data.nunique() / total_count) if total_count > 0 else 0.0
        }
        
        # 3. è¦†ç›–ç‡è¯„ä¼°
        coverage = quality_report['data_quality']['non_zero_ratio'] * 100
        quality_report['coverage'] = {
            'percentage': coverage,
            'status': self._get_coverage_status(coverage)
        }
        
        # 4. å¼‚å¸¸å€¼æ£€æµ‹
        outliers = self._detect_outliers(factor_data)
        quality_report['outliers'] = {
            'count': len(outliers),
            'percentage': float(len(outliers) / total_count * 100) if total_count > 0 else 0.0,
            'values': outliers[:10].tolist() if len(outliers) > 0 else []
        }
        
        # 5. åˆ†å¸ƒå¥åº·åº¦
        quality_report['distribution_health'] = self._assess_distribution_health(factor_data)
        
        # 6. æ—¶é—´åºåˆ—ç‰¹æ€§ï¼ˆå¦‚æœæ˜¯æ—¶é—´åºåˆ—æ•°æ®ï¼‰
        if isinstance(factor_data.index, pd.MultiIndex):
            quality_report['temporal_properties'] = self._analyze_temporal_properties(factor_data)
        
        # 7. è´¨é‡è¯„åˆ†
        quality_score = self._calculate_quality_score(quality_report)
        quality_report['quality_score'] = quality_score
        
        # è®°å½•é—®é¢˜
        if quality_score['overall'] < 70:
            self.quality_issues.append({
                'factor': factor_name,
                'score': quality_score['overall'],
                'issues': quality_score['issues']
            })
        
        # ä¿å­˜ç»Ÿè®¡
        self.factor_stats[factor_name] = quality_report
        
        # è®°å½•è®¡ç®—æ—¶é—´
        if computation_time:
            self.computation_times[factor_name] = computation_time
        
        # å®æ—¶æ—¥å¿—è¾“å‡º
        self._log_factor_quality(factor_name, quality_report)
        
        return quality_report
    
    def _get_coverage_status(self, coverage: float) -> str:
        """è¯„ä¼°è¦†ç›–ç‡çŠ¶æ€"""
        if coverage >= 90:
            return "EXCELLENT"
        elif coverage >= 70:
            return "GOOD"
        elif coverage >= 50:
            return "FAIR"
        elif coverage >= 30:
            return "POOR"
        else:
            return "CRITICAL"
    
    def _detect_outliers(self, data: pd.Series, n_std: float = 3) -> pd.Series:
        """æ£€æµ‹å¼‚å¸¸å€¼ï¼ˆè¶…è¿‡nä¸ªæ ‡å‡†å·®ï¼‰"""
        mean = data.mean()
        std = data.std()
        if std == 0:
            return pd.Series([])
        outliers = data[np.abs(data - mean) > n_std * std]
        return outliers
    
    def _assess_distribution_health(self, data: pd.Series) -> Dict[str, Any]:
        """è¯„ä¼°æ•°æ®åˆ†å¸ƒå¥åº·åº¦"""
        health = {
            'is_constant': bool(data.std() == 0),
            'is_binary': bool(data.nunique() == 2),
            'is_highly_skewed': bool(abs(data.skew()) > 2),
            'has_fat_tails': bool(data.kurtosis() > 3),
            'variance_ratio': float(data.var() / (data.mean()**2 + 1e-10))
        }
        
        # å¥åº·è¯„åˆ†
        health_score = 100
        if health['is_constant']:
            health_score -= 50
        if health['is_highly_skewed']:
            health_score -= 20
        if health['has_fat_tails']:
            health_score -= 10
        
        health['score'] = health_score
        return health
    
    def _analyze_temporal_properties(self, data: pd.Series) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´åºåˆ—ç‰¹æ€§"""
        temporal = {}
        
        try:
            # æŒ‰æ—¥æœŸåˆ†ç»„åˆ†æ
            dates = data.index.get_level_values(0)
            unique_dates = dates.unique()
            
            temporal['date_coverage'] = {
                'unique_dates': len(unique_dates),
                'first_date': str(unique_dates.min()),
                'last_date': str(unique_dates.max())
            }
            
            # æ¯æ—¥æ•°æ®ç‚¹æ•°é‡
            daily_counts = data.groupby(level=0).count()
            temporal['daily_distribution'] = {
                'mean_points_per_day': float(daily_counts.mean()),
                'std_points_per_day': float(daily_counts.std()),
                'min_points_per_day': int(daily_counts.min()),
                'max_points_per_day': int(daily_counts.max())
            }
            
        except Exception as e:
            logger.warning(f"Temporal analysis failed: {e}")
            temporal = {'error': str(e)}
        
        return temporal
    
    def _calculate_quality_score(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°"""
        score_components = {
            'coverage': min(report['coverage']['percentage'], 100) * 0.3,
            'uniqueness': min(report['data_quality']['unique_ratio'] * 100, 100) * 0.2,
            'distribution': report['distribution_health']['score'] * 0.25,
            'outliers': max(100 - report['outliers']['percentage'] * 10, 0) * 0.15,
            'completeness': (1 - report['data_quality']['nan_ratio']) * 100 * 0.1
        }
        
        overall_score = sum(score_components.values())
        
        # è¯†åˆ«é—®é¢˜
        issues = []
        if report['coverage']['percentage'] < 50:
            issues.append(f"Low coverage: {report['coverage']['percentage']:.1f}%")
        if report['data_quality']['unique_ratio'] < 0.1:
            issues.append(f"Low uniqueness: {report['data_quality']['unique_ratio']:.2%}")
        if report['distribution_health']['is_constant']:
            issues.append("Constant values detected")
        if report['outliers']['percentage'] > 5:
            issues.append(f"High outlier ratio: {report['outliers']['percentage']:.1f}%")
        
        return {
            'overall': overall_score,
            'components': score_components,
            'issues': issues
        }
    
    def monitor_sentiment_factor(self, sentiment_data: pd.DataFrame,
                               news_count_by_ticker: Dict[str, int] = None,
                               processing_time: float = None) -> Dict[str, Any]:
        """
        ä¸“é—¨ç›‘æ§æƒ…æ„Ÿå› å­çš„æ•°æ®è´¨é‡

        Args:
            sentiment_data: æƒ…æ„Ÿå› å­æ•°æ® (MultiIndex: date, ticker)
            news_count_by_ticker: æ¯åªè‚¡ç¥¨çš„æ–°é—»æ•°é‡ç»Ÿè®¡
            processing_time: å¤„ç†è€—æ—¶

        Returns:
            æƒ…æ„Ÿå› å­ä¸“ç”¨è´¨é‡æŠ¥å‘Š
        """
        logger.info("ğŸ“° [SENTIMENT MONITOR] Analyzing sentiment factor quality...")

        # åŸºç¡€ç›‘æ§
        if 'sentiment_score' in sentiment_data.columns:
            base_report = self.monitor_factor_computation(
                'sentiment_score',
                sentiment_data['sentiment_score'],
                processing_time
            )
        else:
            logger.warning("sentiment_score column not found in data")
            return {}

        # æƒ…æ„Ÿç‰¹æœ‰çš„è´¨é‡æ£€æŸ¥
        sentiment_quality = {
            'timestamp': datetime.now().isoformat(),
            'base_quality': base_report,
            'sentiment_specific': {}
        }

        # 1. æ–°é—»è¦†ç›–ç‡æ£€æŸ¥
        if news_count_by_ticker:
            sentiment_quality['sentiment_specific']['news_coverage'] = self._analyze_news_coverage(news_count_by_ticker)

        # 2. æƒ…æ„Ÿåˆ†å¸ƒæ£€æŸ¥
        sentiment_quality['sentiment_specific']['sentiment_distribution'] = self._analyze_sentiment_distribution(sentiment_data['sentiment_score'])

        # 3. æ¨ªæˆªé¢æ ‡å‡†åŒ–éªŒè¯
        sentiment_quality['sentiment_specific']['cross_sectional_standardization'] = self._verify_cross_sectional_standardization(sentiment_data)

        # 4. æ—¶é—´åºåˆ—ä¸€è‡´æ€§æ£€æŸ¥
        sentiment_quality['sentiment_specific']['temporal_consistency'] = self._check_temporal_consistency(sentiment_data)

        # 5. è®¡ç®—æƒ…æ„Ÿå› å­ä¸“ç”¨è¯„åˆ†
        sentiment_score = self._calculate_sentiment_quality_score(sentiment_quality)
        sentiment_quality['sentiment_quality_score'] = sentiment_score

        # 6. ç”ŸæˆæŠ¥å‘Šå’Œå»ºè®®
        sentiment_quality['recommendations'] = self._generate_sentiment_recommendations(sentiment_quality)

        # è®°å½•åˆ°ç³»ç»Ÿ
        self.factor_stats['sentiment_score_detailed'] = sentiment_quality

        # è¾“å‡ºä¸“ç”¨æ—¥å¿—
        self._log_sentiment_quality(sentiment_quality)

        return sentiment_quality

    def _analyze_news_coverage(self, news_count: Dict[str, int]) -> Dict[str, Any]:
        """åˆ†ææ–°é—»è¦†ç›–æƒ…å†µ"""
        if not news_count:
            return {'status': 'NO_DATA', 'total_tickers': 0, 'tickers_with_news': 0}

        total_tickers = len(news_count)
        tickers_with_news = sum(1 for count in news_count.values() if count > 0)
        news_counts = list(news_count.values())

        return {
            'total_tickers': total_tickers,
            'tickers_with_news': tickers_with_news,
            'coverage_ratio': tickers_with_news / total_tickers if total_tickers > 0 else 0.0,
            'avg_news_per_ticker': np.mean(news_counts),
            'median_news_per_ticker': np.median(news_counts),
            'max_news_per_ticker': max(news_counts) if news_counts else 0,
            'min_news_per_ticker': min(news_counts) if news_counts else 0,
            'status': 'GOOD' if tickers_with_news / total_tickers > 0.7 else 'POOR'
        }

    def _analyze_sentiment_distribution(self, sentiment_scores: pd.Series) -> Dict[str, Any]:
        """åˆ†ææƒ…æ„Ÿåˆ†æ•°åˆ†å¸ƒç‰¹æ€§"""
        clean_scores = sentiment_scores.dropna()

        if len(clean_scores) == 0:
            return {'status': 'NO_DATA'}

        # åˆ†å¸ƒç‰¹æ€§
        distribution = {
            'range': {
                'min': float(clean_scores.min()),
                'max': float(clean_scores.max()),
                'span': float(clean_scores.max() - clean_scores.min())
            },
            'central_tendency': {
                'mean': float(clean_scores.mean()),
                'median': float(clean_scores.median()),
                'mode_exists': len(clean_scores.mode()) > 0
            },
            'spread': {
                'std': float(clean_scores.std()),
                'var': float(clean_scores.var()),
                'iqr': float(clean_scores.quantile(0.75) - clean_scores.quantile(0.25))
            },
            'shape': {
                'skewness': float(clean_scores.skew()),
                'kurtosis': float(clean_scores.kurtosis()),
                'is_normal_like': bool(abs(clean_scores.skew()) < 1 and abs(clean_scores.kurtosis()) < 3)
            }
        }

        # æƒ…æ„Ÿç‰¹æœ‰æ£€æŸ¥
        distribution['sentiment_characteristics'] = {
            'has_positive_sentiment': bool((clean_scores > 0).any()),
            'has_negative_sentiment': bool((clean_scores < 0).any()),
            'positive_ratio': float((clean_scores > 0).mean()),
            'negative_ratio': float((clean_scores < 0).mean()),
            'neutral_ratio': float((clean_scores == 0).mean()),
            'extreme_positive_ratio': float((clean_scores > 2).mean()),
            'extreme_negative_ratio': float((clean_scores < -2).mean())
        }

        return distribution

    def _verify_cross_sectional_standardization(self, sentiment_data: pd.DataFrame) -> Dict[str, Any]:
        """éªŒè¯æ¨ªæˆªé¢æ ‡å‡†åŒ–æ˜¯å¦æ­£ç¡®"""
        if 'sentiment_score' not in sentiment_data.columns:
            return {'status': 'NO_SENTIMENT_COLUMN'}

        standardization_check = {
            'daily_means': [],
            'daily_stds': [],
            'dates_checked': 0,
            'standardization_quality': 'UNKNOWN'
        }

        try:
            # æ£€æŸ¥æ¯æ—¥çš„å‡å€¼å’Œæ ‡å‡†å·®
            for date in sentiment_data.index.get_level_values('date').unique()[:10]:  # æ£€æŸ¥å‰10å¤©
                daily_scores = sentiment_data.loc[date, 'sentiment_score'].dropna()
                if len(daily_scores) > 1:
                    daily_mean = daily_scores.mean()
                    daily_std = daily_scores.std()
                    standardization_check['daily_means'].append(float(daily_mean))
                    standardization_check['daily_stds'].append(float(daily_std))
                    standardization_check['dates_checked'] += 1

            if standardization_check['daily_means']:
                avg_mean = np.mean(standardization_check['daily_means'])
                avg_std = np.mean(standardization_check['daily_stds'])

                # è¯„ä¼°æ ‡å‡†åŒ–è´¨é‡
                if abs(avg_mean) < 0.1 and abs(avg_std - 1.0) < 0.2:
                    standardization_check['standardization_quality'] = 'EXCELLENT'
                elif abs(avg_mean) < 0.5 and abs(avg_std - 1.0) < 0.5:
                    standardization_check['standardization_quality'] = 'GOOD'
                else:
                    standardization_check['standardization_quality'] = 'POOR'

                standardization_check['average_daily_mean'] = float(avg_mean)
                standardization_check['average_daily_std'] = float(avg_std)

        except Exception as e:
            logger.warning(f"Cross-sectional standardization check failed: {e}")
            standardization_check['error'] = str(e)

        return standardization_check

    def _check_temporal_consistency(self, sentiment_data: pd.DataFrame) -> Dict[str, Any]:
        """æ£€æŸ¥æ—¶é—´åºåˆ—ä¸€è‡´æ€§"""
        if 'sentiment_score' not in sentiment_data.columns:
            return {'status': 'NO_SENTIMENT_COLUMN'}

        consistency = {
            'date_gaps': [],
            'ticker_coverage_consistency': {},
            'temporal_stability': 'UNKNOWN'
        }

        try:
            # æ£€æŸ¥æ—¥æœŸè¿ç»­æ€§
            dates = sorted(sentiment_data.index.get_level_values('date').unique())
            if len(dates) > 1:
                for i in range(1, len(dates)):
                    gap_days = (dates[i] - dates[i-1]).days
                    if gap_days > 1:  # å‡è®¾å·¥ä½œæ—¥
                        consistency['date_gaps'].append({
                            'from': str(dates[i-1]),
                            'to': str(dates[i]),
                            'gap_days': gap_days
                        })

            # æ£€æŸ¥æ¯æ—¥è‚¡ç¥¨è¦†ç›–ä¸€è‡´æ€§
            daily_ticker_counts = sentiment_data.groupby(level='date').size()
            consistency['ticker_coverage_consistency'] = {
                'mean_tickers_per_day': float(daily_ticker_counts.mean()),
                'std_tickers_per_day': float(daily_ticker_counts.std()),
                'min_tickers_per_day': int(daily_ticker_counts.min()),
                'max_tickers_per_day': int(daily_ticker_counts.max()),
                'coverage_stability': 'STABLE' if daily_ticker_counts.std() / daily_ticker_counts.mean() < 0.1 else 'UNSTABLE'
            }

        except Exception as e:
            logger.warning(f"Temporal consistency check failed: {e}")
            consistency['error'] = str(e)

        return consistency

    def _calculate_sentiment_quality_score(self, sentiment_quality: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—æƒ…æ„Ÿå› å­ä¸“ç”¨è´¨é‡è¯„åˆ†"""
        base_score = sentiment_quality['base_quality']['quality_score']['overall']

        # æƒ…æ„Ÿç‰¹æœ‰è¯„åˆ†ç»„ä»¶
        sentiment_components = {
            'base_quality': base_score * 0.4,
            'news_coverage': 0,
            'sentiment_distribution': 0,
            'standardization': 0,
            'temporal_consistency': 0
        }

        # æ–°é—»è¦†ç›–è¯„åˆ†
        news_cov = sentiment_quality['sentiment_specific'].get('news_coverage', {})
        if news_cov.get('status') == 'GOOD':
            sentiment_components['news_coverage'] = 25
        elif news_cov.get('coverage_ratio', 0) > 0.5:
            sentiment_components['news_coverage'] = 15
        else:
            sentiment_components['news_coverage'] = 5

        # æ ‡å‡†åŒ–è´¨é‡è¯„åˆ†
        std_check = sentiment_quality['sentiment_specific'].get('cross_sectional_standardization', {})
        if std_check.get('standardization_quality') == 'EXCELLENT':
            sentiment_components['standardization'] = 20
        elif std_check.get('standardization_quality') == 'GOOD':
            sentiment_components['standardization'] = 15
        else:
            sentiment_components['standardization'] = 5

        # æ—¶é—´ä¸€è‡´æ€§è¯„åˆ†
        temp_check = sentiment_quality['sentiment_specific'].get('temporal_consistency', {})
        if temp_check.get('ticker_coverage_consistency', {}).get('coverage_stability') == 'STABLE':
            sentiment_components['temporal_consistency'] = 15
        else:
            sentiment_components['temporal_consistency'] = 8

        overall_sentiment_score = sum(sentiment_components.values())

        return {
            'overall': overall_sentiment_score,
            'components': sentiment_components,
            'grade': self._get_sentiment_grade(overall_sentiment_score)
        }

    def _get_sentiment_grade(self, score: float) -> str:
        """è·å–æƒ…æ„Ÿå› å­è´¨é‡ç­‰çº§"""
        if score >= 90:
            return 'A+'
        elif score >= 80:
            return 'A'
        elif score >= 70:
            return 'B+'
        elif score >= 60:
            return 'B'
        elif score >= 50:
            return 'C'
        else:
            return 'D'

    def _generate_sentiment_recommendations(self, sentiment_quality: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæƒ…æ„Ÿå› å­æ”¹è¿›å»ºè®®"""
        recommendations = []

        # åŸºäºå„é¡¹æ£€æŸ¥ç»“æœç”Ÿæˆå»ºè®®
        base_score = sentiment_quality['base_quality']['quality_score']['overall']
        if base_score < 70:
            recommendations.append("Consider improving base data quality (coverage, uniqueness)")

        news_cov = sentiment_quality['sentiment_specific'].get('news_coverage', {})
        if news_cov.get('coverage_ratio', 0) < 0.5:
            recommendations.append("Increase news coverage - consider expanding news sources or date range")

        std_check = sentiment_quality['sentiment_specific'].get('cross_sectional_standardization', {})
        if std_check.get('standardization_quality') != 'EXCELLENT':
            recommendations.append("Review cross-sectional standardization - daily means should be ~0, std ~1")

        sent_dist = sentiment_quality['sentiment_specific'].get('sentiment_distribution', {})
        if sent_dist.get('sentiment_characteristics', {}).get('neutral_ratio', 0) > 0.8:
            recommendations.append("High neutral sentiment ratio - verify news sentiment analysis is working")

        temp_check = sentiment_quality['sentiment_specific'].get('temporal_consistency', {})
        if temp_check.get('ticker_coverage_consistency', {}).get('coverage_stability') != 'STABLE':
            recommendations.append("Inconsistent daily ticker coverage - check data pipeline stability")

        if not recommendations:
            recommendations.append("Sentiment factor quality is good - maintain current pipeline")

        return recommendations

    def _log_sentiment_quality(self, sentiment_quality: Dict[str, Any]):
        """è¾“å‡ºæƒ…æ„Ÿå› å­è´¨é‡ä¸“ç”¨æ—¥å¿—"""
        score = sentiment_quality['sentiment_quality_score']['overall']
        grade = sentiment_quality['sentiment_quality_score']['grade']

        # è·å–å…³é”®æŒ‡æ ‡
        base_score = sentiment_quality['base_quality']['quality_score']['overall']
        news_coverage = sentiment_quality['sentiment_specific'].get('news_coverage', {}).get('coverage_ratio', 0)
        std_quality = sentiment_quality['sentiment_specific'].get('cross_sectional_standardization', {}).get('standardization_quality', 'UNKNOWN')

        logger.info("=" * 80)
        logger.info(f"ğŸ“° SENTIMENT FACTOR QUALITY REPORT")
        logger.info("=" * 80)
        logger.info(f"Overall Sentiment Score: {score:.1f}/100 (Grade: {grade})")
        logger.info(f"Base Factor Quality: {base_score:.1f}/100")
        logger.info(f"News Coverage: {news_coverage:.1%}")
        logger.info(f"Standardization: {std_quality}")

        # æ˜¾ç¤ºå»ºè®®
        recommendations = sentiment_quality.get('recommendations', [])
        if recommendations:
            logger.info("\nğŸ“‹ Recommendations:")
            for i, rec in enumerate(recommendations, 1):
                logger.info(f"  {i}. {rec}")

        logger.info("=" * 80)

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        if self.save_reports:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = os.path.join(self.report_dir, f"sentiment_quality_report_{timestamp}.json")
            try:
                with open(report_file, 'w', encoding='utf-8') as f:
                    json.dump(sentiment_quality, f, ensure_ascii=False, indent=2)
                logger.info(f"ğŸ“„ Detailed report saved: {report_file}")
            except Exception as e:
                logger.warning(f"Failed to save sentiment quality report: {e}")

    def _log_factor_quality(self, factor_name: str, report: Dict[str, Any]):
        """è¾“å‡ºå› å­è´¨é‡æ—¥å¿—"""
        score = report['quality_score']['overall']
        coverage = report['coverage']['percentage']

        # é€‰æ‹©åˆé€‚çš„emoji
        if score >= 90:
            emoji = "EXCELLENT"
            level = "EXCELLENT"
        elif score >= 70:
            emoji = "GOOD"
            level = "GOOD"
        elif score >= 50:
            emoji = "WARNING"
            level = "WARNING"
        else:
            emoji = "âŒ"
            level = "CRITICAL"
        
        logger.info(f"   {emoji} {factor_name}: Score={score:.1f}, Coverage={coverage:.1f}%, Status={level}")
        
        # è¾“å‡ºå…·ä½“é—®é¢˜
        if report['quality_score']['issues']:
            for issue in report['quality_score']['issues']:
                logger.warning(f"      âš ï¸ {issue}")
    
    def get_summary_report(self) -> Dict[str, Any]:
        """è·å–æ±‡æ€»æŠ¥å‘Š"""
        if not self.factor_stats:
            return {'error': 'No factors monitored yet'}
        
        summary = {
            'timestamp': datetime.now().isoformat(),
            'total_factors': len(self.factor_stats),
            'total_computation_time': sum(self.computation_times.values()),
            'average_quality_score': np.mean([s['quality_score']['overall'] 
                                             for s in self.factor_stats.values()]),
            'factors_with_issues': len(self.quality_issues),
            'quality_distribution': self._get_quality_distribution(),
            'top_issues': self.quality_issues[:5],
            'performance_metrics': self._get_performance_metrics()
        }
        
        return summary
    
    def _get_quality_distribution(self) -> Dict[str, int]:
        """è·å–è´¨é‡åˆ†å¸ƒ"""
        distribution = {
            'excellent': 0,
            'good': 0,
            'fair': 0,
            'poor': 0,
            'critical': 0
        }
        
        for stats in self.factor_stats.values():
            score = stats['quality_score']['overall']
            if score >= 90:
                distribution['excellent'] += 1
            elif score >= 70:
                distribution['good'] += 1
            elif score >= 50:
                distribution['fair'] += 1
            elif score >= 30:
                distribution['poor'] += 1
            else:
                distribution['critical'] += 1
        
        return distribution
    
    def _get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        if not self.computation_times:
            return {}
        
        times = list(self.computation_times.values())
        return {
            'total_time': sum(times),
            'average_time': np.mean(times),
            'fastest_factor': min(self.computation_times.items(), key=lambda x: x[1]),
            'slowest_factor': max(self.computation_times.items(), key=lambda x: x[1])
        }
    
    def save_report(self, filename: str = None):
        """ä¿å­˜è´¨é‡æŠ¥å‘Š"""
        if not self.save_reports:
            return
        
        if filename is None:
            filename = f"factor_quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        filepath = os.path.join(self.report_dir, filename)
        
        report = {
            'summary': self.get_summary_report(),
            'factor_details': self.factor_stats,
            'quality_issues': self.quality_issues,
            'computation_times': self.computation_times
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, default=str)
        
        logger.info(f"ğŸ“Š Quality report saved to {filepath}")
    
    def print_summary(self):
        """æ‰“å°è´¨é‡æ‘˜è¦"""
        summary = self.get_summary_report()
        
        print("\n" + "="*60)
        print("ALPHA FACTOR QUALITY SUMMARY")
        print("="*60)
        print(f"Total Factors: {summary['total_factors']}")
        print(f"Average Quality Score: {summary['average_quality_score']:.1f}")
        print(f"Factors with Issues: {summary['factors_with_issues']}")
        print(f"Total Computation Time: {summary['total_computation_time']:.3f}s")
        
        print("\nQuality Distribution:")
        for level, count in summary['quality_distribution'].items():
            print(f"  {level.upper()}: {count}")
        
        if summary['top_issues']:
            print("\nTop Issues:")
            for issue in summary['top_issues']:
                print(f"  - {issue['factor']}: {issue['issues']}")
        
        print("="*60)


# å…¨å±€ç›‘æ§å®ä¾‹
factor_monitor = AlphaFactorQualityMonitor()

def monitor_factor(factor_name: str, factor_data: pd.Series, computation_time: float = None) -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•°ï¼šç›‘æ§å•ä¸ªå› å­"""
    return factor_monitor.monitor_factor_computation(factor_name, factor_data, computation_time)