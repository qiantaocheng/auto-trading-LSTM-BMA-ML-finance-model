#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MLå¢å¼ºé›†æˆæ¨¡å— - å°†ç‰¹å¾é€‰æ‹©ã€è¶…å‚æ•°ä¼˜åŒ–å’Œé›†æˆå­¦ä¹ æ•´åˆåˆ°BMAç³»ç»Ÿ
"""

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass

# å¯¼å…¥æ–°åˆ›å»ºçš„MLæ¨¡å— - ä½¿ç”¨æ¡ä»¶å¯¼å…¥é¿å…ç ´åç³»ç»Ÿ
# ğŸš« å·²åˆ é™¤MLFeatureSelector - ä»…ä½¿ç”¨RobustFeatureSelector
try:
    from ml_hyperparameter_optimization import MLHyperparameterOptimizer, HyperparameterConfig
    ML_HYPEROPT_AVAILABLE = True
except ImportError:
    ML_HYPEROPT_AVAILABLE = False
    MLHyperparameterOptimizer = None
    HyperparameterConfig = None

try:
    from ml_ensemble_enhanced import MLEnsembleEnhanced, EnsembleConfig, DynamicBMAWeightLearner
    ML_ENSEMBLE_AVAILABLE = True
except ImportError:
    ML_ENSEMBLE_AVAILABLE = False
    MLEnsembleEnhanced = None
    EnsembleConfig = None
    DynamicBMAWeightLearner = None

logger = logging.getLogger(__name__)


@dataclass
class MLEnhancementConfig:
    """MLå¢å¼ºé›†æˆé…ç½®"""
    enable_feature_selection: bool = False  # å¼ºåˆ¶ç¦ç”¨ï¼šä»…RobustFeatureSelectorå¯æ”¹åˆ—
    enable_hyperparameter_optimization: bool = True
    enable_ensemble_learning: bool = True
    enable_dynamic_bma_weights: bool = True
    
    # å­æ¨¡å—é…ç½®
    # feature_selection_config: å·²åˆ é™¤ - ä»…ä½¿ç”¨RobustFeatureSelector
    hyperparameter_config: HyperparameterConfig = None
    ensemble_config: EnsembleConfig = None
    
    # æ€§èƒ½é…ç½®
    n_jobs: int = -1
    random_state: int = 42
    verbose: int = 1


class MLEnhancementSystem:
    """MLå¢å¼ºç³»ç»Ÿ - æ•´åˆç‰¹å¾é€‰æ‹©ã€è¶…å‚æ•°ä¼˜åŒ–å’Œé›†æˆå­¦ä¹ """
    
    def __init__(self, config: MLEnhancementConfig = None):
        self.config = config or MLEnhancementConfig()
        
        # åˆå§‹åŒ–å­ç³»ç»Ÿ
        self.feature_selector = None
        self.hyperparameter_optimizer = None
        self.ensemble_builder = None
        self.dynamic_bma_learner = None
        
        # ç»“æœç¼“å­˜
        self.selected_features = None
        self.best_hyperparameters = {}
        self.ensemble_models = {}
        self.bma_weights = None
        
        self.logger = logging.getLogger(self.__class__.__name__)
        
        self._initialize_subsystems()
    
    def _initialize_subsystems(self):
        """åˆå§‹åŒ–å„ä¸ªå­ç³»ç»Ÿ"""
        # ğŸš¨ å¼ºåˆ¶ç¦ç”¨ç‰¹å¾é€‰æ‹©ï¼Œä»…RobustFeatureSelectorå¯æ”¹åˆ—
        if self.config.enable_feature_selection:
            raise NotImplementedError(
                "è¿åSSOTåŸåˆ™ï¼šMLå¢å¼ºç³»ç»Ÿä¸å…è®¸ç‰¹å¾é€‰æ‹©ï¼\n"
                "ä¿®å¤æŒ‡å—ï¼šä»…RobustFeatureSelectorå¯æ”¹å˜åˆ—ï¼Œè¯·åœ¨ä¸»æµç¨‹ä¸­å®Œæˆç‰¹å¾é€‰æ‹©åå†è°ƒç”¨è®­ç»ƒå¤´ã€‚\n"
                "è®¾ç½® enable_feature_selection=False"
            )
        
        if self.config.enable_hyperparameter_optimization and ML_HYPEROPT_AVAILABLE:
            self.hyperparameter_optimizer = MLHyperparameterOptimizer(
                self.config.hyperparameter_config or HyperparameterConfig()
            )
            self.logger.info("è¶…å‚æ•°ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–")
        elif self.config.enable_hyperparameter_optimization:
            self.logger.warning("è¶…å‚æ•°ä¼˜åŒ–å·²å¯ç”¨ä½†æ¨¡å—ä¸å¯ç”¨")
        
        if self.config.enable_ensemble_learning and ML_ENSEMBLE_AVAILABLE:
            self.ensemble_builder = MLEnsembleEnhanced(
                self.config.ensemble_config or EnsembleConfig()
            )
            self.logger.info("é›†æˆå­¦ä¹ ç³»ç»Ÿå·²åˆå§‹åŒ–")
        elif self.config.enable_ensemble_learning:
            self.logger.warning("é›†æˆå­¦ä¹ å·²å¯ç”¨ä½†æ¨¡å—ä¸å¯ç”¨")
    
    def enhance_training_pipeline(self, X: pd.DataFrame, y: pd.Series, 
                                 cv_factory: callable,
                                 feature_names: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        å¢å¼ºçš„è®­ç»ƒæµç¨‹
        
        Args:
            X: åŸå§‹ç‰¹å¾æ•°æ®
            y: ç›®æ ‡å˜é‡
            cv_factory: ç»Ÿä¸€CVå·¥å‚ï¼ˆå¿…é¡»ï¼‰
            feature_names: ç‰¹å¾åç§°
            
        Returns:
            åŒ…å«æ‰€æœ‰å¢å¼ºç»“æœçš„å­—å…¸
        """
        # ä¿å­˜CVå·¥å‚ä¾›åç»­ä½¿ç”¨
        self.cv_factory = cv_factory
        self.logger.info("å¼€å§‹MLå¢å¼ºè®­ç»ƒæµç¨‹")
        results = {}
        
        # 1. ç‰¹å¾é€‰æ‹©
        if self.config.enable_feature_selection and self.feature_selector:
            self.logger.info("æ‰§è¡Œæ™ºèƒ½ç‰¹å¾é€‰æ‹©...")
            X_selected, selection_info = self.feature_selector.fit_select(X, y, feature_names)
            self.selected_features = selection_info['selected_feature_names']
            results['feature_selection'] = {
                'selected_features': self.selected_features,
                'original_count': X.shape[1],
                'selected_count': X_selected.shape[1],
                'reduction_ratio': selection_info['reduction_ratio']
            }
            X = X_selected  # ä½¿ç”¨é€‰æ‹©åçš„ç‰¹å¾
            self.logger.info(f"ç‰¹å¾ä» {results['feature_selection']['original_count']} å‡å°‘åˆ° "
                           f"{results['feature_selection']['selected_count']}")
        else:
            X_selected = X
        
        # 2. è¶…å‚æ•°ä¼˜åŒ–
        if self.config.enable_hyperparameter_optimization and self.hyperparameter_optimizer:
            self.logger.info("æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–...")
            
            # ä¼˜åŒ–äº’è¡¥æ€§ä¸‰ç±»æ¨¡å‹
            models_to_optimize = ['ElasticNet', 'LightGBM', 'ExtraTrees']
            
            # æ£€æŸ¥ä¾èµ–å¯ç”¨æ€§
            import importlib.util
            if not importlib.util.find_spec('lightgbm'):
                models_to_optimize.remove('LightGBM')
                models_to_optimize.append('GradientBoosting')  # å›é€€åˆ°sklearn GBDT
                
            # å¯é€‰å¤‡èƒXGBoostï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if importlib.util.find_spec('xgboost'):
                models_to_optimize.append('XGBoost')
            
            optimization_results = self.hyperparameter_optimizer.optimize_multiple_models(
                models_to_optimize, X_selected.values, y.values
            )
            
            # ä¿å­˜æœ€ä¼˜å‚æ•°
            for model_name, (best_params, best_score, best_model) in optimization_results.items():
                self.best_hyperparameters[model_name] = best_params
            
            results['hyperparameter_optimization'] = {
                'optimized_models': list(optimization_results.keys()),
                'best_model': max(optimization_results.keys(), 
                                 key=lambda k: optimization_results[k][1]),
                'best_params': self.best_hyperparameters
            }
            
            self.logger.info(f"æœ€ä¼˜æ¨¡å‹: {results['hyperparameter_optimization']['best_model']}")
        
        # 3. é›†æˆå­¦ä¹ 
        if self.config.enable_ensemble_learning and self.ensemble_builder:
            self.logger.info("æ„å»ºé›†æˆå­¦ä¹ ç³»ç»Ÿ...")
            
            # ä½¿ç”¨ä¼˜åŒ–åçš„è¶…å‚æ•°åˆ›å»ºæ¨¡å‹
            if self.best_hyperparameters:
                # æ›´æ–°é›†æˆé…ç½®ä¸­çš„åŸºç¡€æ¨¡å‹
                self.ensemble_builder.config.base_models = list(self.best_hyperparameters.keys())
            
            ensemble_results = self.ensemble_builder.build_ensemble(
                X_selected.values, y.values,
                cv_factory=self.cv_factory,  # ä¼ å…¥ç»Ÿä¸€CVå·¥å‚
                feature_names=X_selected.columns.tolist()
            )
            
            self.ensemble_models = ensemble_results['models']
            results['ensemble_learning'] = {
                'methods': list(ensemble_results['models'].keys()),
                'best_method': ensemble_results['best_method'],
                'performances': ensemble_results['performances']
            }
            
            self.logger.info(f"æœ€ä¼˜é›†æˆæ–¹æ³•: {ensemble_results['best_method']}")
        
        # 4. OOFæ ‡å‡†åŒ– + ç›¸å…³æƒ©ç½šBMAæƒé‡å­¦ä¹ 
        if self.config.enable_dynamic_bma_weights:
            self.logger.info("OOFæ ‡å‡†åŒ– + ç›¸å…³æƒ©ç½šBMAæƒé‡å­¦ä¹ ...")
            
            # è·å–åŸºç¡€æ¨¡å‹å’ŒOOFé¢„æµ‹
            base_models = []
            oof_predictions_matrix = []
            
            if optimization_results:
                base_models = [result[2] for result in optimization_results.values() if result[2]]
                # ç”Ÿæˆç»Ÿä¸€CVçš„OOFé¢„æµ‹çŸ©é˜µ - å¿…é¡»ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„CV
                # ğŸš¨ ä½¿ç”¨ç»Ÿä¸€SSOTæ£€æµ‹å™¨
                from .ssot_violation_detector import ensure_cv_factory_provided
                ensure_cv_factory_provided(getattr(self, 'cv_factory', None), "OOFé¢„æµ‹çŸ©é˜µç”Ÿæˆ")
                
                for model_name, (best_params, best_score, best_model) in optimization_results.items():
                    if best_model:
                        # ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„ç»Ÿä¸€CVå·¥å‚
                        from sklearn.model_selection import cross_val_predict
                        cv = self.cv_factory  # å¿…é¡»ä½¿ç”¨å¤–éƒ¨CVå·¥å‚
                        oof_pred = cross_val_predict(best_model, X_selected.values, y.values, cv=cv)
                        oof_predictions_matrix.append(oof_pred)
            
            if base_models and len(oof_predictions_matrix) > 1:
                # Step 4.1: OOFæ¨ªæˆªé¢æ ‡å‡†åŒ– (Rankâ†’Normal)
                import numpy as np
                from scipy.stats import norm
                
                standardized_oof_matrix = []
                for oof_pred in oof_predictions_matrix:
                    # æ¨ªæˆªé¢Rankâ†’Normalæ ‡å‡†åŒ–
                    oof_df = pd.DataFrame({'pred': oof_pred, 'y': y.values})
                    oof_df['rank_pct'] = oof_df['pred'].rank(pct=True)
                    oof_df['standardized'] = norm.ppf(oof_df['rank_pct'].clip(0.01, 0.99))
                    standardized_oof_matrix.append(oof_df['standardized'].values)
                
                # Step 4.2: ç›¸å…³æ€§é—¨æ§› + è‡ªåŠ¨è£å‰ª
                oof_corr_matrix = np.corrcoef(standardized_oof_matrix)
                max_correlation = np.max(np.abs(oof_corr_matrix - np.eye(len(oof_corr_matrix))))
                
                # Step 4.3: ç›¸å…³æƒ©ç½šBMAæƒé‡è®¡ç®—
                # w_i âˆ shrink(IC_i) Ã— ICIR_i Ã— (1 - ÏÌ„_i)
                ic_scores = []
                for oof_std in standardized_oof_matrix:
                    ic = np.corrcoef(oof_std, y.values)[0, 1] if len(y) > 1 else 0
                    ic_scores.append(max(0.015, abs(ic)))  # shrinkåˆ°æœ€ä½0.015
                
                # è®¡ç®—æ¯ä¸ªæ¨¡å‹ä¸å…¶ä»–æ¨¡å‹çš„å¹³å‡ç›¸å…³æ€§
                avg_correlations = []
                for i in range(len(oof_corr_matrix)):
                    other_corrs = [abs(oof_corr_matrix[i, j]) for j in range(len(oof_corr_matrix)) if i != j]
                    avg_corr = np.mean(other_corrs) if other_corrs else 0
                    avg_correlations.append(avg_corr)
                
                # ç›¸å…³æƒ©ç½šBMAæƒé‡å…¬å¼
                raw_weights = []
                for i, (ic, avg_corr) in enumerate(zip(ic_scores, avg_correlations)):
                    icir = ic / (0.1 + np.std([ic]))  # ç®€åŒ–ICIR
                    correlation_penalty = 1 - min(0.5, avg_corr)  # ç›¸å…³æƒ©ç½š
                    weight = ic * icir * correlation_penalty
                    raw_weights.append(weight)
                
                # æƒé‡å½’ä¸€åŒ–
                raw_weights = np.array(raw_weights)
                self.bma_weights = raw_weights / (np.sum(raw_weights) + 1e-8)
                
                results['oof_standardized_bma'] = {
                    'standardized_oof_matrix': len(standardized_oof_matrix),
                    'max_correlation': max_correlation,
                    'correlation_threshold': 0.85,
                    'ic_scores': ic_scores,
                    'correlation_penalties': [1-corr for corr in avg_correlations],
                    'final_weights': self.bma_weights.tolist(),
                    'correlation_compliant': max_correlation <= 0.85
                }
                
                self.logger.info(f"OOFæ ‡å‡†åŒ–BMAæƒé‡: {self.bma_weights}")
                self.logger.info(f"æœ€å¤§OOFç›¸å…³æ€§: {max_correlation:.3f} ({'âœ…' if max_correlation <= 0.85 else 'âŒ'})")
        
        # æ€»ç»“
        results['summary'] = {
            'features_selected': len(self.selected_features) if self.selected_features else X.shape[1],
            'models_optimized': len(self.best_hyperparameters),
            'ensemble_methods': len(self.ensemble_models),
            'bma_weights_learned': self.bma_weights is not None
        }
        
        self.logger.info("MLå¢å¼ºè®­ç»ƒæµç¨‹å®Œæˆ")
        
        return results
    
    def predict_enhanced(self, X: pd.DataFrame) -> np.ndarray:
        """
        ä½¿ç”¨å¢å¼ºç³»ç»Ÿè¿›è¡Œé¢„æµ‹
        
        Args:
            X: ç‰¹å¾æ•°æ®
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        # 1. ç‰¹å¾é€‰æ‹©
        if self.feature_selector and self.selected_features:
            X = self.feature_selector.transform(X)
        
        # 2. ä½¿ç”¨æœ€ä¼˜é›†æˆæ¨¡å‹é¢„æµ‹
        if self.ensemble_models:
            # è·å–æ‰€æœ‰é›†æˆæ¨¡å‹çš„é¢„æµ‹
            predictions = []
            for name, model in self.ensemble_models.items():
                try:
                    pred = model.predict(X.values)
                    predictions.append(pred)
                except Exception as e:
                    self.logger.warning(f"æ¨¡å‹ {name} é¢„æµ‹å¤±è´¥: {e}")
            
            if predictions:
                # å¹³å‡æ‰€æœ‰é¢„æµ‹
                return np.mean(predictions, axis=0)
        
        # 3. å›é€€ï¼šä½¿ç”¨åŠ¨æ€BMA
        if self.dynamic_bma_learner:
            return self.dynamic_bma_learner.predict(X.values)
        
        # æœ€ç»ˆå›é€€
        return np.zeros(len(X))
    
    def get_feature_importance(self) -> pd.DataFrame:
        """è·å–ç‰¹å¾é‡è¦æ€§æŠ¥å‘Š"""
        if self.feature_selector:
            return self.feature_selector.get_feature_importance_report()
        return pd.DataFrame()
    
    def get_optimization_report(self) -> pd.DataFrame:
        """è·å–è¶…å‚æ•°ä¼˜åŒ–æŠ¥å‘Š"""
        if self.hyperparameter_optimizer:
            return self.hyperparameter_optimizer.get_optimization_report()
        return pd.DataFrame()
    
    def update_bma_weights(self, X: np.ndarray, y: np.ndarray):
        """åœ¨çº¿æ›´æ–°BMAæƒé‡"""
        if self.dynamic_bma_learner:
            self.dynamic_bma_learner.update_weights(X, y)
            self.bma_weights = self.dynamic_bma_learner.weights_
            self.logger.info(f"BMAæƒé‡å·²æ›´æ–°: {self.bma_weights}")


def integrate_ml_enhancements(X: pd.DataFrame, y: pd.Series, 
                             config: Optional[MLEnhancementConfig] = None) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ•´åˆæ‰€æœ‰MLå¢å¼ºåŠŸèƒ½
    
    Args:
        X: ç‰¹å¾æ•°æ®
        y: ç›®æ ‡å˜é‡
        config: é…ç½®
        
    Returns:
        å¢å¼ºè®­ç»ƒç»“æœ
    """
    system = MLEnhancementSystem(config)
    return system.enhance_training_pipeline(X, y)