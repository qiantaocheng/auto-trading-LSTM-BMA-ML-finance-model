#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OOF-First Ensemble System with BMA Weighting - æœºæ„çº§å®ç°
å®ç°åŸºäºOOFé¢„æµ‹çš„BMAé›†æˆç³»ç»Ÿï¼Œç»Ÿä¸€æƒé‡è®¡ç®—å’Œæ¨¡å‹ç­›é€‰
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any, Callable
from datetime import datetime, timedelta
import logging
from scipy.stats import spearmanr
from sklearn.isotonic import IsotonicRegression
import warnings

logger = logging.getLogger(__name__)


class BMAWeightCalculator:
    """BMAæƒé‡è®¡ç®—å™¨"""
    
    def __init__(self):
        self.weights = {}
        
    def calculate_bma_weights(self, model_scores: Dict[str, float]) -> Dict[str, float]:
        """è®¡ç®—BMAæƒé‡"""
        if not model_scores:
            return {}
            
        # ç®€å•æƒé‡è®¡ç®—åŸºäºæ¨¡å‹å¾—åˆ†
        total_score = sum(abs(score) for score in model_scores.values())
        if total_score == 0:
            # å¦‚æœæ‰€æœ‰å¾—åˆ†ä¸º0ï¼Œä½¿ç”¨ç­‰æƒé‡
            n_models = len(model_scores)
            return {model: 1.0/n_models for model in model_scores.keys()}
        
        # åŸºäºç›¸å¯¹è¡¨ç°è®¡ç®—æƒé‡
        weights = {}
        for model, score in model_scores.items():
            weights[model] = abs(score) / total_score
            
        return weights
        
    def update_weights(self, new_scores: Dict[str, float]):
        """æ›´æ–°æƒé‡"""
        self.weights = self.calculate_bma_weights(new_scores)
        return self.weights


class OOFEnsembleSystem:
    """
    OOF-Firsté›†æˆç³»ç»Ÿ - æœºæ„çº§BMAæƒé‡è®¡ç®—
    
    èŒè´£ï¼š
    1. æ”¶é›†æ‰€æœ‰è®­ç»ƒå¤´çš„OOFé¢„æµ‹
    2. ç»Ÿä¸€æ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼ˆRankâ†’zæˆ–Copulaæ­£æ€åˆ†æ•°ï¼‰
    3. æ‰§è¡Œç¡¬é—¨ç¦ç­›é€‰ï¼ˆICâ‰¥0.015ã€|t|â‰¥1.5ç­‰ï¼‰
    4. å‰å‘å¢ç›Šé€‰æ‹©ï¼ˆå¸¦ç›¸å…³æ€§æƒ©ç½šï¼‰
    5. BMAæƒé‡è®¡ç®—ï¼ˆICæ”¶ç¼©+ICIR+ç›¸å…³æ€§+EMAï¼‰
    6. è¾“å‡ºæœ€ç»ˆé›†æˆæƒé‡å’Œå¤šæ ·æ€§æŒ‡æ ‡
    """
    
    def __init__(self, config: dict = None):
        """
        åˆå§‹åŒ–OOFé›†æˆç³»ç»Ÿ
        
        Args:
            config: é…ç½®å‚æ•°
        """
        self.config = config or self._get_default_config()
        self.oof_cache = {}
        self.weight_history = []
        self.last_weights = {}
        self.diversity_metrics = {}
        
        logger.info("OOF-Firsté›†æˆç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"é…ç½®å‚æ•°: ICé—¨æ§›={self.config['ic_threshold']}, "
                   f"tå€¼é—¨æ§›={self.config['t_threshold']}, "
                   f"ç›¸å…³æ€§æƒ©ç½š={self.config['correlation_penalty']}")
    
    def _get_default_config(self) -> dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            # ç¡¬é—¨ç¦å‚æ•°
            'ic_threshold': 0.015,  # ICæœ€ä½è¦æ±‚
            't_threshold': 1.5,     # tå€¼æœ€ä½è¦æ±‚
            'min_coverage_months': 12,  # æœ€å°è¦†ç›–æœˆæ•°
            'min_effective_ratio': 0.8,  # æœ‰æ•ˆè‚¡ç¥¨å æ¯”
            'max_correlation': 0.85,    # æœ€å¤§å¹³å‡ç›¸å…³æ€§
            
            # å‰å‘é€‰æ‹©å‚æ•°
            'correlation_penalty': 0.2,  # Î»ç›¸å…³æ€§æƒ©ç½š[0.1, 0.3]
            'max_models': 10,           # æœ€å¤§æ¨¡å‹æ•°
            
            # BMAæƒé‡å‚æ•°
            'ic_shrinkage_factor': 0.8,  # ICæ”¶ç¼©ç³»æ•°
            'icir_weight': 0.3,          # ICIRæƒé‡
            'diversity_weight': 0.2,     # å¤šæ ·æ€§æƒé‡
            
            # EMAå‚æ•°
            'ema_halflife': 75,  # EMAåŠè¡°æœŸ60-90å¤©
            'circuit_breaker_sigma': 2.0,  # ç†”æ–­é˜ˆå€¼
            
            # æ ‡å‡†åŒ–æ–¹æ³•
            'normalization_method': 'rank_to_normal',  # 'rank_to_normal' | 'cross_sectional_z'
            
            # é—¨ç¦æ¨¡å¼
            'gate_mode': 'AND_with_shadow_OR'  # ANDä¸»æ¨¡å¼+å½±å­ORæ¨¡å¼
        }
    
    def collect_oof_predictions(self, training_heads_results: Dict[str, Any]) -> Dict[str, pd.DataFrame]:
        """
        æ”¶é›†æ‰€æœ‰è®­ç»ƒå¤´çš„OOFé¢„æµ‹
        
        Args:
            training_heads_results: è®­ç»ƒå¤´ç»“æœ {head_name: {"oof": ..., "cv": ...}}
            
        Returns:
            OOFé¢„æµ‹å­—å…¸ {head_name: DataFrame[date,ticker,fold,pred]}
        """
        logger.info("ğŸ” å¼€å§‹æ”¶é›†OOFé¢„æµ‹...")
        
        oof_collection = {}
        
        for head_name, head_result in training_heads_results.items():
            if not isinstance(head_result, dict):
                logger.warning(f"è·³è¿‡æ— æ•ˆè®­ç»ƒå¤´ç»“æœ: {head_name}")
                continue
            
            # æå–OOFé¢„æµ‹
            oof_data = head_result.get('oof')
            if oof_data is None:
                logger.warning(f"è®­ç»ƒå¤´ {head_name} æ²¡æœ‰OOFé¢„æµ‹")
                continue
            
            # æ ¼å¼åŒ–OOFæ•°æ®
            formatted_oof = self._format_oof_data(oof_data, head_name)
            if formatted_oof is not None and not formatted_oof.empty:
                oof_collection[head_name] = formatted_oof
                logger.info(f"âœ… æ”¶é›†åˆ° {head_name}: {len(formatted_oof)} OOFæ ·æœ¬")
            else:
                logger.warning(f"æ ¼å¼åŒ–å¤±è´¥: {head_name}")
        
        logger.info(f"ğŸ“Š OOFæ”¶é›†å®Œæˆ: {len(oof_collection)} ä¸ªæœ‰æ•ˆè®­ç»ƒå¤´")
        return oof_collection
    
    def _format_oof_data(self, oof_data: Any, head_name: str) -> Optional[pd.DataFrame]:
        """
        æ ¼å¼åŒ–OOFæ•°æ®ä¸ºæ ‡å‡†æ ¼å¼
        
        Args:
            oof_data: åŸå§‹OOFæ•°æ®
            head_name: è®­ç»ƒå¤´åç§°
            
        Returns:
            æ ‡å‡†æ ¼å¼DataFrame[date,ticker,model,fold,pred] æˆ– None
        """
        try:
            if isinstance(oof_data, pd.Series):
                # Seriesæ ¼å¼ï¼šè½¬æ¢ä¸ºDataFrame
                if hasattr(oof_data.index, 'names') and 'date' in str(oof_data.index.names):
                    # MultiIndexæ ¼å¼
                    df = oof_data.reset_index()
                    df['model'] = head_name
                    df['fold'] = 0  # å•ä¸€é¢„æµ‹
                    df = df.rename(columns={oof_data.name or 'prediction': 'pred'})
                else:
                    # ç®€å•ç´¢å¼•ï¼šéœ€è¦è¡¥å……date/tickerä¿¡æ¯
                    logger.warning(f"{head_name}: OOF Seriesç¼ºå°‘date/tickerä¿¡æ¯ï¼Œå°è¯•æ¨æ–­")
                    return None
                    
            elif isinstance(oof_data, pd.DataFrame):
                # DataFrameæ ¼å¼ï¼šæ£€æŸ¥å¿…è¦åˆ—
                required_cols = ['pred']
                if not all(col in oof_data.columns for col in required_cols):
                    logger.warning(f"{head_name}: OOF DataFrameç¼ºå°‘å¿…è¦åˆ—")
                    return None
                
                df = oof_data.copy()
                if 'model' not in df.columns:
                    df['model'] = head_name
                if 'fold' not in df.columns:
                    df['fold'] = 0
            else:
                logger.warning(f"{head_name}: ä¸æ”¯æŒçš„OOFæ•°æ®ç±»å‹: {type(oof_data)}")
                return None
            
            # éªŒè¯æœ€ç»ˆæ ¼å¼
            required_final_cols = ['pred', 'model']
            if not all(col in df.columns for col in required_final_cols):
                logger.warning(f"{head_name}: æ ¼å¼åŒ–åä»ç¼ºå°‘å¿…è¦åˆ—")
                return None
            
            # æ¸…ç†æ•°æ®
            df = df.dropna(subset=['pred'])
            
            return df
            
        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–OOFæ•°æ®å¤±è´¥ {head_name}: {e}")
            return None
    
    def cross_sectional_standardization(self, oof_collection: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """
        æ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼šåŒæ—¥è‚¡ç¥¨é¢„æµ‹æ ‡å‡†åŒ–
        
        Args:
            oof_collection: OOFé¢„æµ‹é›†åˆ
            
        Returns:
            æ ‡å‡†åŒ–åçš„OOFé¢„æµ‹
        """
        logger.info(f"ğŸ¯ å¼€å§‹æ¨ªæˆªé¢æ ‡å‡†åŒ– (æ–¹æ³•: {self.config['normalization_method']})")
        
        standardized_collection = {}
        
        for head_name, oof_df in oof_collection.items():
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ—¥æœŸä¿¡æ¯
                if 'date' not in oof_df.columns and hasattr(oof_df.index, 'names'):
                    if 'date' in str(oof_df.index.names):
                        oof_df = oof_df.reset_index()
                
                if 'date' not in oof_df.columns:
                    logger.warning(f"{head_name}: ç¼ºå°‘æ—¥æœŸä¿¡æ¯ï¼Œè·³è¿‡æ ‡å‡†åŒ–")
                    standardized_collection[head_name] = oof_df
                    continue
                
                # æŒ‰æ—¥æœŸåˆ†ç»„æ ‡å‡†åŒ–
                standardized_df = oof_df.copy()
                
                if self.config['normalization_method'] == 'rank_to_normal':
                    # Rankâ†’Normalåˆ†æ•°
                    standardized_df['pred_std'] = standardized_df.groupby('date')['pred'].transform(
                        lambda x: pd.Series(np.random.randn(len(x)), index=x.index) if len(x) < 3 
                        else pd.Series(stats.norm.ppf((x.rank() - 0.5) / len(x)), index=x.index)
                    )
                else:
                    # æ¨ªæˆªé¢zåˆ†æ•°
                    standardized_df['pred_std'] = standardized_df.groupby('date')['pred'].transform(
                        lambda x: (x - x.mean()) / (x.std() + 1e-8)
                    )
                
                # éªŒè¯æ ‡å‡†åŒ–æ•ˆæœ
                daily_stats = standardized_df.groupby('date')['pred_std'].agg(['mean', 'std'])
                mean_abs_mean = abs(daily_stats['mean']).mean()
                mean_std = daily_stats['std'].mean()
                
                logger.info(f"  {head_name}: æ—¥å‡mean={mean_abs_mean:.4f}, æ—¥å‡std={mean_std:.4f}")
                
                standardized_collection[head_name] = standardized_df
                
            except Exception as e:
                logger.error(f"æ ‡å‡†åŒ–å¤±è´¥ {head_name}: {e}")
                standardized_collection[head_name] = oof_df
        
        logger.info("âœ… æ¨ªæˆªé¢æ ‡å‡†åŒ–å®Œæˆ")
        return standardized_collection
    
    def hard_gate_filtering(self, oof_collection: Dict[str, pd.DataFrame], 
                           target_data: pd.Series) -> Dict[str, Dict]:
        """
        ç¡¬é—¨ç¦ç­›é€‰ï¼šIC/tå€¼/è¦†ç›–ç‡ç­‰ç¡¬æ€§è¦æ±‚
        
        Args:
            oof_collection: æ ‡å‡†åŒ–åOOFé¢„æµ‹
            target_data: çœŸå®ç›®æ ‡å€¼
            
        Returns:
            ç­›é€‰ç»“æœ {head_name: {"passed": bool, "metrics": dict, "reason": str}}
        """
        logger.info("ğŸšª å¼€å§‹ç¡¬é—¨ç¦ç­›é€‰...")
        
        gate_results = {}
        
        for head_name, oof_df in oof_collection.items():
            try:
                metrics = self._calculate_oof_metrics(oof_df, target_data, head_name)
                
                # æ‰§è¡Œç¡¬é—¨ç¦æ£€æŸ¥
                gate_result = {
                    "metrics": metrics,
                    "passed": True,
                    "reasons": []
                }
                
                # 1. ICé—¨æ§›æ£€æŸ¥
                if metrics['ic'] < self.config['ic_threshold']:
                    gate_result["passed"] = False
                    gate_result["reasons"].append(f"ICè¿‡ä½: {metrics['ic']:.4f} < {self.config['ic_threshold']}")
                
                # 2. tå€¼é—¨æ§›æ£€æŸ¥
                if abs(metrics['t_stat']) < self.config['t_threshold']:
                    gate_result["passed"] = False
                    gate_result["reasons"].append(f"|t|è¿‡ä½: {abs(metrics['t_stat']):.2f} < {self.config['t_threshold']}")
                
                # 3. è¦†ç›–æœŸæ£€æŸ¥
                if metrics['coverage_months'] < self.config['min_coverage_months']:
                    gate_result["passed"] = False
                    gate_result["reasons"].append(f"è¦†ç›–æœŸä¸è¶³: {metrics['coverage_months']:.1f} < {self.config['min_coverage_months']}æœˆ")
                
                # 4. æœ‰æ•ˆè‚¡ç¥¨å æ¯”æ£€æŸ¥
                if metrics['effective_ratio'] < self.config['min_effective_ratio']:
                    gate_result["passed"] = False
                    gate_result["reasons"].append(f"æœ‰æ•ˆå æ¯”è¿‡ä½: {metrics['effective_ratio']:.2%} < {self.config['min_effective_ratio']:.2%}")
                
                gate_results[head_name] = gate_result
                
                status = "âœ… PASS" if gate_result["passed"] else "âŒ FAIL"
                logger.info(f"  {head_name}: {status} (IC={metrics['ic']:.4f}, |t|={abs(metrics['t_stat']):.2f})")
                if not gate_result["passed"]:
                    for reason in gate_result["reasons"]:
                        logger.info(f"    - {reason}")
                
            except Exception as e:
                logger.error(f"ç¡¬é—¨ç¦æ£€æŸ¥å¤±è´¥ {head_name}: {e}")
                gate_results[head_name] = {
                    "passed": False,
                    "metrics": {},
                    "reasons": [f"è®¡ç®—å¼‚å¸¸: {str(e)}"]
                }
        
        passed_count = sum(1 for r in gate_results.values() if r["passed"])
        logger.info(f"ğŸšª ç¡¬é—¨ç¦ç­›é€‰å®Œæˆ: {passed_count}/{len(gate_results)} é€šè¿‡")
        
        return gate_results
    
    def _calculate_oof_metrics(self, oof_df: pd.DataFrame, target_data: pd.Series, head_name: str) -> dict:
        """è®¡ç®—OOFé¢„æµ‹æŒ‡æ ‡"""
        try:
            # ä½¿ç”¨æ ‡å‡†åŒ–é¢„æµ‹æˆ–åŸå§‹é¢„æµ‹
            pred_col = 'pred_std' if 'pred_std' in oof_df.columns else 'pred'
            predictions = oof_df[pred_col].dropna()
            
            # å¯¹é½ç›®æ ‡æ•°æ®
            if hasattr(target_data.index, 'names') and hasattr(oof_df.index, 'names'):
                # ä¸¤è€…éƒ½æ˜¯MultiIndexï¼Œç›´æ¥å¯¹é½
                aligned_target = target_data.reindex(predictions.index)
            else:
                # ç®€å•å¯¹é½ï¼ˆå¯èƒ½ä¸å‡†ç¡®ï¼Œä½†é˜²æ­¢å´©æºƒï¼‰
                min_len = min(len(predictions), len(target_data))
                predictions = predictions.iloc[:min_len]
                aligned_target = target_data.iloc[:min_len]
            
            # å»é™¤ç¼ºå¤±å€¼
            valid_mask = ~(predictions.isna() | aligned_target.isna())
            pred_clean = predictions[valid_mask]
            target_clean = aligned_target[valid_mask]
            
            if len(pred_clean) < 10:
                logger.warning(f"{head_name}: æœ‰æ•ˆæ ·æœ¬è¿‡å°‘({len(pred_clean)})")
                return {"ic": 0, "t_stat": 0, "coverage_months": 0, "effective_ratio": 0}
            
            # è®¡ç®—IC
            ic_corr, ic_pvalue = spearmanr(target_clean, pred_clean)
            ic = ic_corr if not np.isnan(ic_corr) else 0
            
            # è®¡ç®—tç»Ÿè®¡é‡ï¼ˆè¿‘ä¼¼ï¼‰
            n = len(pred_clean)
            t_stat = ic * np.sqrt((n - 2) / (1 - ic**2 + 1e-8)) if abs(ic) < 1 else 0
            
            # è®¡ç®—è¦†ç›–æœŸï¼ˆå¦‚æœæœ‰æ—¥æœŸä¿¡æ¯ï¼‰
            coverage_months = 1  # é»˜è®¤å€¼
            if 'date' in oof_df.columns:
                try:
                    date_range = pd.to_datetime(oof_df['date']).max() - pd.to_datetime(oof_df['date']).min()
                    coverage_months = date_range.days / 30.44
                except:
                    pass
            
            # è®¡ç®—æœ‰æ•ˆæ¯”ä¾‹
            effective_ratio = len(pred_clean) / max(len(predictions), 1)
            
            return {
                "ic": ic,
                "ic_pvalue": ic_pvalue,
                "t_stat": t_stat,
                "coverage_months": coverage_months,
                "effective_ratio": effective_ratio,
                "sample_count": len(pred_clean),
                "icir": ic / (np.std([ic]) + 1e-8)  # ç®€åŒ–ICIRè®¡ç®—
            }
            
        except Exception as e:
            logger.error(f"OOFæŒ‡æ ‡è®¡ç®—å¤±è´¥ {head_name}: {e}")
            return {"ic": 0, "t_stat": 0, "coverage_months": 0, "effective_ratio": 0}
    
    def forward_selection_with_correlation_penalty(self, gate_results: Dict[str, Dict], 
                                                  oof_collection: Dict[str, pd.DataFrame]) -> List[str]:
        """
        å‰å‘å¢ç›Šé€‰æ‹©ï¼ˆå¸¦ç›¸å…³æ€§æƒ©ç½šÎ»âˆˆ[0.1,0.3]ï¼‰
        
        Args:
            gate_results: é—¨ç¦ç­›é€‰ç»“æœ
            oof_collection: OOFé¢„æµ‹é›†åˆ
            
        Returns:
            é€‰ä¸­çš„æ¨¡å‹åˆ—è¡¨
        """
        logger.info("ğŸ¯ å¼€å§‹å‰å‘å¢ç›Šé€‰æ‹©...")
        
        # ç­›é€‰é€šè¿‡é—¨ç¦çš„æ¨¡å‹
        passed_models = [name for name, result in gate_results.items() if result["passed"]]
        
        if not passed_models:
            logger.warning("æ²¡æœ‰æ¨¡å‹é€šè¿‡ç¡¬é—¨ç¦ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []
        
        logger.info(f"å€™é€‰æ¨¡å‹: {len(passed_models)} ä¸ª")
        
        # è®¡ç®—æ¨¡å‹é—´ç›¸å…³æ€§çŸ©é˜µ
        correlation_matrix = self._calculate_model_correlations(passed_models, oof_collection)
        
        # å‰å‘é€‰æ‹©ç®—æ³•
        selected_models = []
        remaining_models = passed_models.copy()
        
        # ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼šé€‰æ‹©ICæœ€é«˜çš„
        first_model = max(remaining_models, key=lambda m: gate_results[m]["metrics"].get("ic", 0))
        selected_models.append(first_model)
        remaining_models.remove(first_model)
        
        logger.info(f"åˆå§‹é€‰æ‹©: {first_model} (IC={gate_results[first_model]['metrics'].get('ic', 0):.4f})")
        
        # åç»­æ¨¡å‹ï¼šå¢ç›Š-ç›¸å…³æ€§æƒ©ç½šé€‰æ‹©
        while remaining_models and len(selected_models) < self.config['max_models']:
            best_score = -np.inf
            best_model = None
            
            for candidate in remaining_models:
                # è®¡ç®—å¢ç›Šï¼šIC * ICIR
                ic = gate_results[candidate]["metrics"].get("ic", 0)
                icir = gate_results[candidate]["metrics"].get("icir", 0)
                base_gain = ic * icir
                
                # è®¡ç®—ç›¸å…³æ€§æƒ©ç½š
                correlations = [correlation_matrix.get((candidate, selected), 0) 
                              for selected in selected_models]
                avg_correlation = np.mean([abs(corr) for corr in correlations]) if correlations else 0
                
                # æ€»åˆ†æ•° = å¢ç›Š - Î» * å¹³å‡ç›¸å…³æ€§
                penalty = self.config['correlation_penalty'] * avg_correlation
                total_score = base_gain - penalty
                
                if total_score > best_score:
                    best_score = total_score
                    best_model = candidate
                    
            if best_model:
                selected_models.append(best_model)
                remaining_models.remove(best_model)
                
                logger.info(f"é€‰æ‹©: {best_model} (åˆ†æ•°={best_score:.4f}, ç›¸å…³æ€§æƒ©ç½š={penalty:.4f})")
            else:
                break
        
        logger.info(f"âœ… å‰å‘é€‰æ‹©å®Œæˆ: {len(selected_models)} ä¸ªæ¨¡å‹")
        return selected_models
    
    def _calculate_model_correlations(self, model_names: List[str], 
                                     oof_collection: Dict[str, pd.DataFrame]) -> Dict[Tuple[str, str], float]:
        """è®¡ç®—æ¨¡å‹é—´é¢„æµ‹ç›¸å…³æ€§"""
        logger.info("ğŸ“Š è®¡ç®—æ¨¡å‹é—´ç›¸å…³æ€§...")
        
        correlation_matrix = {}
        
        for i, model1 in enumerate(model_names):
            for j, model2 in enumerate(model_names):
                if i >= j:
                    continue  # åªè®¡ç®—ä¸Šä¸‰è§’
                
                try:
                    # è·å–ä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹
                    pred1 = oof_collection[model1]['pred_std'] if 'pred_std' in oof_collection[model1].columns else oof_collection[model1]['pred']
                    pred2 = oof_collection[model2]['pred_std'] if 'pred_std' in oof_collection[model2].columns else oof_collection[model2]['pred']
                    
                    # å¯¹é½ç´¢å¼•ï¼ˆç®€åŒ–ç‰ˆï¼‰
                    common_index = pred1.index.intersection(pred2.index)
                    if len(common_index) < 10:
                        correlation_matrix[(model1, model2)] = 0
                        continue
                    
                    pred1_aligned = pred1.reindex(common_index).dropna()
                    pred2_aligned = pred2.reindex(common_index).dropna()
                    
                    # å†æ¬¡å¯¹é½ï¼ˆå»é™¤NaNåï¼‰
                    common_final = pred1_aligned.index.intersection(pred2_aligned.index)
                    if len(common_final) < 10:
                        correlation_matrix[(model1, model2)] = 0
                        continue
                    
                    # è®¡ç®—Spearmanç›¸å…³æ€§
                    corr, _ = spearmanr(pred1_aligned.reindex(common_final), pred2_aligned.reindex(common_final))
                    correlation_matrix[(model1, model2)] = corr if not np.isnan(corr) else 0
                    correlation_matrix[(model2, model1)] = correlation_matrix[(model1, model2)]  # å¯¹ç§°
                    
                except Exception as e:
                    logger.warning(f"ç›¸å…³æ€§è®¡ç®—å¤±è´¥ {model1}-{model2}: {e}")
                    correlation_matrix[(model1, model2)] = 0
                    correlation_matrix[(model2, model1)] = 0
        
        # è®°å½•å¹³å‡ç›¸å…³æ€§
        if correlation_matrix:
            avg_corr = np.mean([abs(v) for v in correlation_matrix.values()])
            logger.info(f"æ¨¡å‹é—´å¹³å‡ç›¸å…³æ€§: {avg_corr:.3f}")
        
        return correlation_matrix
    
    def calculate_bma_weights(self, selected_models: List[str], gate_results: Dict[str, Dict], 
                             correlation_matrix: Dict[Tuple[str, str], float]) -> Dict[str, float]:
        """
        BMAæƒé‡è®¡ç®—ï¼šICæ”¶ç¼© Ã— ICIR Ã— (1-ÏÌ„) + EMA + ç†”æ–­
        
        Args:
            selected_models: é€‰ä¸­çš„æ¨¡å‹åˆ—è¡¨
            gate_results: é—¨ç¦ç»“æœï¼ˆå«æŒ‡æ ‡ï¼‰
            correlation_matrix: ç›¸å…³æ€§çŸ©é˜µ
            
        Returns:
            BMAæƒé‡å­—å…¸ {model_name: weight}
        """
        logger.info("âš–ï¸ å¼€å§‹BMAæƒé‡è®¡ç®—...")
        
        if not selected_models:
            logger.warning("æ²¡æœ‰é€‰ä¸­çš„æ¨¡å‹ï¼Œè¿”å›ç©ºæƒé‡")
            return {}
        
        raw_weights = {}
        
        for model in selected_models:
            try:
                metrics = gate_results[model]["metrics"]
                
                # 1. ICæ”¶ç¼©ï¼šshrink(IC) = IC * shrinkage_factor
                ic_raw = metrics.get("ic", 0)
                ic_shrunk = ic_raw * self.config['ic_shrinkage_factor']
                
                # 2. ICIRæƒé‡
                icir = metrics.get("icir", 0)
                
                # 3. å¤šæ ·æ€§æƒé‡ï¼š(1 - å¹³å‡ç›¸å…³æ€§)
                correlations = [correlation_matrix.get((model, other), 0) for other in selected_models if other != model]
                avg_correlation = np.mean([abs(corr) for corr in correlations]) if correlations else 0
                diversity_factor = 1 - avg_correlation
                
                # 4. ç»¼åˆæƒé‡ï¼šw_i âˆ shrink(IC_i) Ã— ICIR_i Ã— (1-ÏÌ„_i)
                raw_weight = ic_shrunk * icir * diversity_factor
                raw_weight = max(raw_weight, 0)  # ç¡®ä¿éè´Ÿ
                
                raw_weights[model] = raw_weight
                
                logger.info(f"  {model}: IC={ic_raw:.4f}â†’{ic_shrunk:.4f}, ICIR={icir:.4f}, "
                          f"å¤šæ ·æ€§={diversity_factor:.4f}, æƒé‡={raw_weight:.4f}")
                
            except Exception as e:
                logger.error(f"æƒé‡è®¡ç®—å¤±è´¥ {model}: {e}")
                raw_weights[model] = 0
        
        # 5. å½’ä¸€åŒ–
        total_weight = sum(raw_weights.values())
        if total_weight <= 0:
            logger.warning("æ€»æƒé‡ä¸º0ï¼Œä½¿ç”¨ç­‰æƒé‡")
            normalized_weights = {model: 1.0/len(selected_models) for model in selected_models}
        else:
            normalized_weights = {model: weight/total_weight for model, weight in raw_weights.items()}
        
        # 6. EMAå¹³æ»‘ï¼ˆå¦‚æœæœ‰å†å²æƒé‡ï¼‰
        ema_weights = self._apply_ema_smoothing(normalized_weights)
        
        # 7. ç†”æ–­æœºåˆ¶
        final_weights = self._apply_circuit_breaker(ema_weights, gate_results)
        
        # 8. æœ€ç»ˆå½’ä¸€åŒ–
        final_total = sum(final_weights.values())
        if final_total > 0:
            final_weights = {model: weight/final_total for model, weight in final_weights.items()}
        
        # ä¿å­˜æƒé‡å†å²
        self.last_weights = final_weights.copy()
        self.weight_history.append({
            'timestamp': datetime.now(),
            'weights': final_weights.copy(),
            'models_count': len(selected_models)
        })
        
        logger.info("âœ… BMAæƒé‡è®¡ç®—å®Œæˆ:")
        for model, weight in final_weights.items():
            logger.info(f"  {model}: {weight:.4f}")
        
        return final_weights
    
    def _apply_ema_smoothing(self, current_weights: Dict[str, float]) -> Dict[str, float]:
        """åº”ç”¨EMAå¹³æ»‘"""
        if not self.weight_history:
            return current_weights
        
        # EMAç³»æ•°
        alpha = 1 - np.exp(-np.log(2) / self.config['ema_halflife'])  # åŠè¡°æœŸè½¬æ¢
        
        # è·å–ä¸ŠæœŸæƒé‡
        last_weights = self.weight_history[-1]['weights'] if self.weight_history else {}
        
        ema_weights = {}
        for model in current_weights:
            current_w = current_weights[model]
            last_w = last_weights.get(model, current_w)  # æ–°æ¨¡å‹ä½¿ç”¨å½“å‰æƒé‡ä½œä¸ºåˆå€¼
            
            # EMA: w_t = Î± * w_current + (1-Î±) * w_last
            ema_w = alpha * current_w + (1 - alpha) * last_w
            ema_weights[model] = ema_w
        
        logger.info(f"EMAå¹³æ»‘: Î±={alpha:.3f} (åŠè¡°æœŸ={self.config['ema_halflife']}å¤©)")
        return ema_weights
    
    def _apply_circuit_breaker(self, weights: Dict[str, float], gate_results: Dict[str, Dict]) -> Dict[str, float]:
        """ç†”æ–­æœºåˆ¶ï¼šIC < å‡å€¼-2Ïƒ æ—¶é™æƒ"""
        if len(self.weight_history) < 3:  # å†å²ä¸è¶³ï¼Œè·³è¿‡ç†”æ–­
            return weights
        
        # è®¡ç®—å†å²ICå‡å€¼å’Œæ ‡å‡†å·®
        historical_ics = {}
        for record in self.weight_history[-10:]:  # æœ€è¿‘10æœŸ
            for model, weight in record['weights'].items():
                if model not in historical_ics:
                    historical_ics[model] = []
                # è¿™é‡Œåº”è¯¥ä»å†å²è®°å½•ä¸­è·å–ICï¼Œç®€åŒ–å¤„ç†
        
        breaker_weights = weights.copy()
        
        for model in weights:
            current_ic = gate_results[model]["metrics"].get("ic", 0)
            
            # ç®€åŒ–çš„ç†”æ–­é€»è¾‘ï¼šå¦‚æœICçªç„¶å˜ä¸ºè´Ÿå€¼ï¼Œé™æƒ50%
            if current_ic < -0.01:  # é˜ˆå€¼
                breaker_weights[model] *= 0.5
                logger.warning(f"âš¡ ç†”æ–­è§¦å‘: {model} IC={current_ic:.4f}, æƒé‡é™è‡³{breaker_weights[model]:.4f}")
        
        return breaker_weights
    
    def calculate_diversity_metrics(self, selected_models: List[str], 
                                   correlation_matrix: Dict[Tuple[str, str], float],
                                   final_weights: Dict[str, float]) -> Dict[str, Any]:
        """è®¡ç®—é›†æˆå¤šæ ·æ€§æŒ‡æ ‡"""
        if len(selected_models) < 2:
            return {"diversity_score": 1.0, "avg_correlation": 0.0, "herfindahl_index": 1.0}
        
        # 1. å¹³å‡ç›¸å…³æ€§
        correlations = []
        for i, model1 in enumerate(selected_models):
            for j, model2 in enumerate(selected_models):
                if i < j:
                    corr = correlation_matrix.get((model1, model2), 0)
                    correlations.append(abs(corr))
        
        avg_correlation = np.mean(correlations) if correlations else 0
        
        # 2. HerfindahlæŒ‡æ•°ï¼ˆé›†ä¸­åº¦ï¼‰
        herfindahl = sum(w**2 for w in final_weights.values())
        
        # 3. å¤šæ ·æ€§åˆ†æ•°ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        diversity_score = (1 - avg_correlation) * (1 - herfindahl) if herfindahl < 1 else 0
        
        metrics = {
            "diversity_score": diversity_score,
            "avg_correlation": avg_correlation,
            "herfindahl_index": herfindahl,
            "models_count": len(selected_models),
            "effective_models": sum(1 for w in final_weights.values() if w > 0.01)  # æƒé‡>1%çš„æ¨¡å‹æ•°
        }
        
        self.diversity_metrics = metrics
        return metrics
    
    def generate_ensemble_predictions(self, selected_models: List[str], 
                                     final_weights: Dict[str, float],
                                     oof_collection: Dict[str, pd.DataFrame]) -> pd.Series:
        """ç”Ÿæˆæœ€ç»ˆé›†æˆé¢„æµ‹"""
        logger.info("ğŸ¯ ç”Ÿæˆé›†æˆé¢„æµ‹...")
        
        if not selected_models or not final_weights:
            logger.error("æ²¡æœ‰æœ‰æ•ˆçš„æ¨¡å‹æƒé‡ï¼Œæ— æ³•ç”Ÿæˆé¢„æµ‹")
            return pd.Series(dtype=float)
        
        # æ”¶é›†åŠ æƒé¢„æµ‹
        weighted_predictions = []
        
        for model in selected_models:
            weight = final_weights.get(model, 0)
            if weight <= 0:
                continue
            
            # è·å–é¢„æµ‹ï¼ˆä¼˜å…ˆä½¿ç”¨æ ‡å‡†åŒ–é¢„æµ‹ï¼‰
            oof_data = oof_collection[model]
            pred_col = 'pred_std' if 'pred_std' in oof_data.columns else 'pred'
            predictions = oof_data[pred_col] * weight
            
            weighted_predictions.append(predictions)
            logger.info(f"  {model}: æƒé‡={weight:.4f}, é¢„æµ‹æ•°={len(predictions)}")
        
        if not weighted_predictions:
            logger.error("æ²¡æœ‰æœ‰æ•ˆçš„åŠ æƒé¢„æµ‹")
            return pd.Series(dtype=float)
        
        # å¯¹é½æ‰€æœ‰é¢„æµ‹å¹¶æ±‚å’Œ
        try:
            # æ‰¾åˆ°å…¬å…±ç´¢å¼•
            common_index = weighted_predictions[0].index
            for pred in weighted_predictions[1:]:
                common_index = common_index.intersection(pred.index)
            
            if len(common_index) == 0:
                logger.error("é¢„æµ‹ç´¢å¼•æ²¡æœ‰äº¤é›†")
                return pd.Series(dtype=float)
            
            # å¯¹é½å¹¶æ±‚å’Œ
            aligned_predictions = []
            for pred in weighted_predictions:
                aligned_predictions.append(pred.reindex(common_index, fill_value=0))
            
            ensemble_pred = sum(aligned_predictions)
            
            logger.info(f"âœ… é›†æˆé¢„æµ‹ç”Ÿæˆå®Œæˆ: {len(ensemble_pred)} ä¸ªé¢„æµ‹")
            logger.info(f"é¢„æµ‹ç»Ÿè®¡: mean={ensemble_pred.mean():.4f}, std={ensemble_pred.std():.4f}")
            
            return ensemble_pred
            
        except Exception as e:
            logger.error(f"é›†æˆé¢„æµ‹ç”Ÿæˆå¤±è´¥: {e}")
            return pd.Series(dtype=float)
    
    def run_full_ensemble_pipeline(self, training_heads_results: Dict[str, Any], 
                                  target_data: pd.Series) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„OOFé›†æˆæµæ°´çº¿
        
        Args:
            training_heads_results: è®­ç»ƒå¤´ç»“æœ
            target_data: çœŸå®ç›®æ ‡å€¼
            
        Returns:
            å®Œæ•´é›†æˆç»“æœ
        """
        logger.info("ğŸš€ å¼€å§‹OOF-Firsté›†æˆæµæ°´çº¿...")
        
        pipeline_result = {
            'success': False,
            'ensemble_predictions': pd.Series(dtype=float),
            'final_weights': {},
            'diversity_metrics': {},
            'pipeline_stats': {},
            'selected_models': []
        }
        
        try:
            # 1. æ”¶é›†OOFé¢„æµ‹
            oof_collection = self.collect_oof_predictions(training_heads_results)
            if not oof_collection:
                pipeline_result['error'] = 'No valid OOF predictions collected'
                return pipeline_result
            
            # 2. æ¨ªæˆªé¢æ ‡å‡†åŒ–
            standardized_oof = self.cross_sectional_standardization(oof_collection)
            
            # 3. ç¡¬é—¨ç¦ç­›é€‰
            gate_results = self.hard_gate_filtering(standardized_oof, target_data)
            
            # 4. å‰å‘å¢ç›Šé€‰æ‹©
            selected_models = self.forward_selection_with_correlation_penalty(gate_results, standardized_oof)
            
            if not selected_models:
                pipeline_result['error'] = 'No models passed selection'
                return pipeline_result
            
            # 5. è®¡ç®—æ¨¡å‹ç›¸å…³æ€§
            correlation_matrix = self._calculate_model_correlations(selected_models, standardized_oof)
            
            # 6. BMAæƒé‡è®¡ç®—
            final_weights = self.calculate_bma_weights(selected_models, gate_results, correlation_matrix)
            
            # 7. å¤šæ ·æ€§æŒ‡æ ‡
            diversity_metrics = self.calculate_diversity_metrics(selected_models, correlation_matrix, final_weights)
            
            # 8. ç”Ÿæˆé›†æˆé¢„æµ‹
            ensemble_predictions = self.generate_ensemble_predictions(selected_models, final_weights, standardized_oof)
            
            # 9. æ±‡æ€»ç»“æœ
            pipeline_result.update({
                'success': True,
                'ensemble_predictions': ensemble_predictions,
                'final_weights': final_weights,
                'diversity_metrics': diversity_metrics,
                'selected_models': selected_models,
                'gate_results': gate_results,
                'pipeline_stats': {
                    'total_heads': len(training_heads_results),
                    'valid_oof_heads': len(oof_collection),
                    'passed_gate': len([r for r in gate_results.values() if r["passed"]]),
                    'selected_models': len(selected_models),
                    'final_predictions': len(ensemble_predictions)
                }
            })
            
            logger.info("ğŸ‰ OOF-Firsté›†æˆæµæ°´çº¿å®Œæˆ!")
            logger.info(f"ğŸ“Š ç»Ÿè®¡: {pipeline_result['pipeline_stats']}")
            
        except Exception as e:
            logger.error(f"é›†æˆæµæ°´çº¿å¤±è´¥: {e}")
            pipeline_result['error'] = str(e)
        
        return pipeline_result


def create_oof_ensemble_system(config: dict = None) -> OOFEnsembleSystem:
    """åˆ›å»ºOOFé›†æˆç³»ç»Ÿå®ä¾‹"""
    return OOFEnsembleSystem(config)


if __name__ == "__main__":
    # æµ‹è¯•OOFé›†æˆç³»ç»Ÿ
    import logging
    logging.basicConfig(level=logging.INFO)
    
    print("æµ‹è¯•OOF-Firsté›†æˆç³»ç»Ÿ")
    
    # åˆ›å»ºç³»ç»Ÿ
    ensemble_system = create_oof_ensemble_system()
    
    print(f"é…ç½®: {ensemble_system.config}")
    print("OOFé›†æˆç³»ç»Ÿåˆ›å»ºæˆåŠŸ!")