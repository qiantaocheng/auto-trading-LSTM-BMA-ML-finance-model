#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ridge Regression Stacker - æ›¿æ¢LTR Isotonic Stacker
ç®€æ´é«˜æ•ˆçš„äºŒå±‚çº¿æ€§å›å½’æ¨¡å‹ï¼Œç›´æ¥ä¼˜åŒ–è¿ç»­æ”¶ç›Šç‡
âœ… å¢å¼ºCVéªŒè¯ï¼šä½¿ç”¨æ—¶é—´åºåˆ—éªŒè¯é˜²æ­¢è¿‡æ‹Ÿåˆ
"""

import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import logging
from sklearn.linear_model import Ridge, RidgeCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from scipy.stats import spearmanr
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit

logger = logging.getLogger(__name__)

class RidgeStacker:
    """
    Ridgeå›å½’äºŒå±‚Stacker - ç®€æ´æ›¿ä»£LTR

    æ ¸å¿ƒä¼˜åŠ¿ï¼š
    - ç›´æ¥ä¼˜åŒ–è¿ç»­æ”¶ç›Šç‡ï¼Œæ— ä¿¡æ¯æŸå¤±
    - çº¿æ€§æ¨¡å‹ï¼Œè§£é‡Šæ€§å¼º
    - è®­ç»ƒå¿«é€Ÿï¼Œç¨³å®šæ€§å¥½
    - è‡ªåŠ¨ç‰¹å¾æ ‡å‡†åŒ–
    - âœ… æ—¶é—´åºåˆ—CVï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæå‡æ³›åŒ–æ€§èƒ½
    """

    def __init__(self,
                 base_cols: Tuple[str, ...] = ('pred_catboost', 'pred_elastic', 'pred_xgb'),
                 alpha: float = 1.0,
                 fit_intercept: bool = False,
                 solver: str = "auto",
                 tol: float = 1e-6,
                 auto_tune_alpha: bool = False,
                 alpha_grid: Tuple[float, ...] = (0.5, 1.0, 2.0, 3.0, 5.0, 8.0),
                 use_cv: bool = True,
                 cv_splits: int = 5,
                 cv_test_size: float = 0.2,
                 use_lambda_percentile: bool = True,  # æ–°å¢ï¼šä½¿ç”¨Lambda percentileç‰¹å¾
                 random_state: int = 42,
                 **kwargs):
        """
        åˆå§‹åŒ–Ridge Stacker

        Args:
            base_cols: ç¬¬ä¸€å±‚æ¨¡å‹é¢„æµ‹åˆ—å
            alpha: Ridgeæ­£åˆ™åŒ–å¼ºåº¦ (é»˜è®¤1.0ï¼Œç®€æ´ç‰ˆ)
            fit_intercept: æ˜¯å¦æ‹Ÿåˆæˆªè· (é»˜è®¤Falseï¼Œå› ä¸ºå·²åšz-score)
            solver: æ±‚è§£å™¨ (é»˜è®¤auto)
            tol: æ”¶æ•›å®¹å·® (é»˜è®¤1e-6)
            auto_tune_alpha: æ˜¯å¦è‡ªåŠ¨è°ƒå‚ (é»˜è®¤Falseï¼Œä¿æŒç®€æ´)
            alpha_grid: è°ƒå‚ç½‘æ ¼ (é»˜è®¤[0.5,1,2,3,5,8])
            use_cv: æ˜¯å¦ä½¿ç”¨äº¤å‰éªŒè¯ (é»˜è®¤True)
            cv_splits: CVæŠ˜æ•° (é»˜è®¤3)
            cv_test_size: æ¯æŠ˜éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤0.2)
            use_lambda_percentile: æ˜¯å¦ä½¿ç”¨Lambda percentileç‰¹å¾ (é»˜è®¤True)
            random_state: éšæœºç§å­
        """
        self.base_cols = base_cols
        self.alpha = alpha
        self.fit_intercept = fit_intercept
        self.solver = solver
        self.tol = tol
        self.auto_tune_alpha = auto_tune_alpha
        self.alpha_grid = alpha_grid
        self.use_cv = use_cv
        self.cv_splits = cv_splits
        self.cv_test_size = cv_test_size
        self.use_lambda_percentile = use_lambda_percentile
        self.random_state = random_state
        self.actual_feature_cols_ = None  # ğŸ”§ è®­ç»ƒæ—¶å®é™…ä½¿ç”¨çš„ç‰¹å¾åˆ—ï¼ˆCritical Fixï¼‰

        # æ¨¡å‹ç»„ä»¶
        self.ridge_model = None
        self.scaler = None
        self.feature_importance_ = None
        self.fitted_ = False

        # è°ƒå‚ç›¸å…³
        self.best_alpha_ = alpha
        self.alpha_scores_ = {}

        # è®­ç»ƒç»Ÿè®¡
        self.train_score_ = None
        self.feature_names_ = None

        logger.info(f"âœ… Ridge Stacker åˆå§‹åŒ–å®Œæˆ (Percentileå¢å¼ºç‰ˆ)")
        logger.info(f"   åŸºç¡€ç‰¹å¾: {self.base_cols}")
        logger.info(f"   Lambda Percentile: {'å¯ç”¨' if self.use_lambda_percentile else 'ç¦ç”¨'}")
        logger.info(f"   æ­£åˆ™åŒ–å¼ºåº¦Î±: {self.alpha}")
        logger.info(f"   æ‹Ÿåˆæˆªè·: {self.fit_intercept} (å·²åšz-score)")
        logger.info(f"   æ±‚è§£å™¨: {self.solver}, å®¹å·®: {self.tol}")
        logger.info(f"   è‡ªåŠ¨è°ƒå‚: {self.auto_tune_alpha}")
        logger.info(f"   ä½¿ç”¨CV: {self.use_cv}, æŠ˜æ•°: {self.cv_splits}")
        logger.info(f"   ç‰¹å¾æ ‡å‡†åŒ–: æ¨ªæˆªé¢z-score")

    def _validate_input(self, df: pd.DataFrame) -> pd.DataFrame:
        """éªŒè¯è¾“å…¥æ•°æ®æ ¼å¼å¹¶ç¡®ä¿åˆ—é¡ºåºä¸€è‡´"""
        if not isinstance(df.index, pd.MultiIndex):
            raise ValueError("è¾“å…¥æ•°æ®å¿…é¡»å…·æœ‰MultiIndex(date, ticker)")

        if df.index.names != ['date', 'ticker']:
            raise ValueError(f"Index nameså¿…é¡»æ˜¯['date', 'ticker'], å®é™…: {df.index.names}")

        missing_cols = [col for col in self.base_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿åˆ—é¡ºåºä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
        return df[list(self.base_cols) + [col for col in df.columns if col not in self.base_cols]]

    def _prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾"""
        # æå–åŸºç¡€ç‰¹å¾
        feature_cols = list(self.base_cols)

        # å¦‚æœå¯ç”¨lambda percentileä¸”æ•°æ®ä¸­æœ‰è¯¥åˆ—ï¼ŒåŠ å…¥ç‰¹å¾
        if self.use_lambda_percentile and 'lambda_percentile' in df.columns:
            feature_cols.append('lambda_percentile')
            logger.debug("âœ“ åŠ å…¥Lambda Percentileç‰¹å¾")

        # ğŸ”§ Critical Fix: ä¿å­˜å®é™…ä½¿ç”¨çš„ç‰¹å¾åˆ—ï¼ˆä»…åœ¨è®­ç»ƒæ—¶ï¼Œå³é¦–æ¬¡è°ƒç”¨ï¼‰
        if self.actual_feature_cols_ is None:
            self.actual_feature_cols_ = feature_cols
            logger.info(f"ğŸ”§ ä¿å­˜å®é™…ç‰¹å¾åˆ—: {self.actual_feature_cols_}")

        X = df[feature_cols].values

        # æå–æ ‡ç­¾ï¼ˆå‡è®¾æ ‡ç­¾åˆ—ä»¥ret_fwdå¼€å¤´ï¼‰
        label_cols = [col for col in df.columns if col.startswith('ret_fwd')]
        if not label_cols:
            raise ValueError("æœªæ‰¾åˆ°æ ‡ç­¾åˆ— (ret_fwd_*)")

        label_col = label_cols[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ ‡ç­¾åˆ—
        y = df[label_col].values

        # ç§»é™¤NaNæ ·æœ¬
        valid_mask = ~(np.isnan(X).any(axis=1) | np.isnan(y))
        X_clean = X[valid_mask]
        y_clean = y[valid_mask]

        logger.info(f"   åŸå§‹æ ·æœ¬: {len(X)}")
        logger.info(f"   æœ‰æ•ˆæ ·æœ¬: {len(X_clean)}")
        logger.info(f"   æ•°æ®è¦†ç›–ç‡: {len(X_clean)/len(X)*100:.1f}%")

        return X_clean, y_clean

    def _winsorize_labels(self, y: np.ndarray, lower_pct: float = 1.0, upper_pct: float = 99.0) -> np.ndarray:
        """Winsorizeæ ‡ç­¾ï¼Œå¤„ç†æç«¯å€¼"""
        lower_bound = np.percentile(y, lower_pct)
        upper_bound = np.percentile(y, upper_pct)
        y_winsorized = np.clip(y, lower_bound, upper_bound)

        n_clipped = np.sum((y != y_winsorized))
        if n_clipped > 0:
            logger.info(f"   Winsorize: {n_clipped}/{len(y)} ({n_clipped/len(y)*100:.1f}%) æ ·æœ¬è¢«è£å‰ª")

        return y_winsorized

    def _calculate_rank_ic(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """è®¡ç®—RankIC"""
        try:
            return spearmanr(y_true, y_pred)[0]
        except:
            return 0.0

    def _time_series_cv_split(self, n_samples: int):
        """åˆ›å»ºæ—¶é—´åºåˆ—CVåˆ†å‰²"""
        splits = []
        test_size = int(n_samples * self.cv_test_size)
        train_min_size = max(100, int(n_samples * 0.3))  # è‡³å°‘30%è®­ç»ƒæ•°æ®

        for i in range(self.cv_splits):
            # é€’å¢è®­ç»ƒé›†å¤§å°
            train_end = train_min_size + i * ((n_samples - test_size - train_min_size) // self.cv_splits)
            test_start = train_end
            test_end = min(test_start + test_size, n_samples)

            if test_end > n_samples or test_start >= test_end:
                break

            train_idx = np.arange(0, train_end)
            test_idx = np.arange(test_start, test_end)

            splits.append((train_idx, test_idx))

        return splits

    def _auto_tune_alpha(self, X: np.ndarray, y: np.ndarray, df: pd.DataFrame) -> float:
        """
        è‡ªåŠ¨è°ƒå‚é€‰æ‹©æœ€ä¼˜alpha - å¢å¼ºCVç‰ˆæœ¬
        """
        if not self.auto_tune_alpha:
            return self.alpha

        logger.info(f"ğŸ¯ å¼€å§‹è‡ªåŠ¨è°ƒå‚ï¼Œç½‘æ ¼: {self.alpha_grid}")

        if self.use_cv:
            logger.info(f"   ä½¿ç”¨æ—¶é—´åºåˆ—CVï¼ŒæŠ˜æ•°: {self.cv_splits}")
            return self._auto_tune_alpha_with_cv(X, y)
        else:
            logger.info(f"   ä½¿ç”¨å…¨é‡è®­ç»ƒï¼ˆæ— CVï¼‰")
            return self._auto_tune_alpha_no_cv(X, y)

    def _auto_tune_alpha_with_cv(self, X: np.ndarray, y: np.ndarray) -> float:
        """ä½¿ç”¨CVè¿›è¡Œè°ƒå‚"""
        best_alpha = self.alpha
        best_score = -999

        # åˆ›å»ºæ—¶é—´åºåˆ—åˆ†å‰²
        cv_splits = self._time_series_cv_split(len(X))

        for alpha in self.alpha_grid:
            cv_scores = []

            for fold, (train_idx, test_idx) in enumerate(cv_splits):
                X_train, X_test = X[train_idx], X[test_idx]
                y_train, y_test = y[train_idx], y[test_idx]

                # è®­ç»ƒæ¨¡å‹
                model = Ridge(
                    alpha=alpha,
                    fit_intercept=self.fit_intercept,
                    solver=self.solver,
                    tol=self.tol,
                    random_state=self.random_state
                )
                model.fit(X_train, y_train)

                # éªŒè¯é›†é¢„æµ‹
                y_pred = model.predict(X_test)
                rank_ic = self._calculate_rank_ic(y_test, y_pred)
                cv_scores.append(rank_ic)

            avg_score = np.mean(cv_scores)
            std_score = np.std(cv_scores)
            self.alpha_scores_[alpha] = avg_score

            logger.info(f"   Î±={alpha}: CV RankIC={avg_score:.4f} (Â±{std_score:.4f})")

            # é€‰æ‹©æœ€ä¼˜alpha
            if avg_score > best_score:
                best_score = avg_score
                best_alpha = alpha

        logger.info(f"âœ… æœ€ä¼˜Î±: {best_alpha}, CV RankIC: {best_score:.4f}")
        return best_alpha

    def _auto_tune_alpha_no_cv(self, X: np.ndarray, y: np.ndarray) -> float:
        """ä¸ä½¿ç”¨CVçš„è°ƒå‚ï¼ˆåŸç‰ˆï¼‰"""
        best_alpha = self.alpha
        best_score = -999

        for alpha in self.alpha_grid:
            try:
                # å…¨é‡è®­ç»ƒæ¨¡å‹
                model = Ridge(
                    alpha=alpha,
                    fit_intercept=self.fit_intercept,
                    solver=self.solver,
                    tol=self.tol,
                    random_state=self.random_state
                )
                model.fit(X, y)

                # å…¨é‡æ•°æ®é¢„æµ‹å¹¶è®¡ç®—RankIC
                y_pred = model.predict(X)
                rank_ic = self._calculate_rank_ic(y, y_pred)

                # ä½¿ç”¨RankICä½œä¸ºä¸»è¦è¯„åˆ†
                score = rank_ic
                self.alpha_scores_[alpha] = rank_ic

                logger.info(f"   Î±={alpha}: RankIC={rank_ic:.4f}")

                # å¦‚æœRankICæ›´å¥½ï¼Œåˆ™æ›´æ–°
                tolerance = 0.001
                if (score > best_score + tolerance) or \
                   (abs(score - best_score) <= tolerance and alpha > best_alpha):
                    best_score = score
                    best_alpha = alpha

            except Exception as e:
                logger.debug(f"è°ƒå‚å¼‚å¸¸ alpha={alpha}: {e}")
                self.alpha_scores_[alpha] = 0.0

        self.best_alpha_ = best_alpha
        logger.info(f"âœ… æœ€ä¼˜Î±: {best_alpha} (RankIC: {self.alpha_scores_[best_alpha]:.4f}, æ— CV)")

        return best_alpha

    def fit(self, df: pd.DataFrame, **kwargs) -> 'RidgeStacker':
        """
        è®­ç»ƒRidge Stackerï¼ˆå¢å¼ºCVç‰ˆï¼‰

        Args:
            df: åŒ…å«ç¬¬ä¸€å±‚é¢„æµ‹å’Œæ ‡ç­¾çš„DataFrame
            **kwargs: å…¼å®¹å‚æ•°ï¼ˆmax_train_to_todayç­‰ï¼‰
        """
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒRidge Stacker")
        logger.info(f"   æœŸæœ›ç‰¹å¾é¡ºåº: {list(self.base_cols)}")
        logger.info(f"   CVæ¨¡å¼: {'å¯ç”¨' if self.use_cv else 'ç¦ç”¨'}")

        # éªŒè¯æ•°æ®
        df_clean = self._validate_input(df)

        # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        X, y = self._prepare_features(df_clean)

        # å°æ ·æœ¬è‡ªé€‚åº”ï¼šå…è®¸åœ¨æå°æ ·æœ¬ä¸‹è®­ç»ƒï¼Œä½†å‘å‡ºè­¦å‘Š
        if len(X) < 10:
            raise ValueError(f"è®­ç»ƒæ ·æœ¬è¿‡å°‘: {len(X)} < 10")
        if len(X) < 50:
            logger.warning(f"å°æ ·æœ¬è®­ç»ƒRidge: {len(X)} < 50ï¼Œå¯ç”¨ä¿å®ˆå‚æ•°å’Œæ­£åˆ™åŒ–")

        # æ ‡ç­¾Winsorization (1%, 99%)
        y_winsorized = self._winsorize_labels(y, 1.0, 99.0)

        # ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆæ¨ªæˆªé¢z-scoreï¼‰
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)

        logger.info(f"   æ ‡å‡†åŒ–å®Œæˆ: ç‰¹å¾å‡å€¼={X_scaled.mean(axis=0)[:3]}, ç‰¹å¾æ ‡å‡†å·®={X_scaled.std(axis=0)[:3]}")

        # ç›´æ¥ä½¿ç”¨æŒ‡å®šalphaï¼ˆæ— è°ƒå‚ï¼‰
        if self.auto_tune_alpha:
            optimal_alpha = self._auto_tune_alpha(X_scaled, y_winsorized, df_clean)
        else:
            optimal_alpha = self.alpha
            logger.info(f"ğŸ¯ ä½¿ç”¨æŒ‡å®šÎ±: {optimal_alpha} (æ— è°ƒå‚)")

        # ä½¿ç”¨CVéªŒè¯æœ€ç»ˆæ¨¡å‹æ€§èƒ½
        val_score = None
        val_rank_ic = None

        # ç»Ÿä¸€5æŠ˜CVï¼šåœ¨æ ·æœ¬é‡å……è¶³æ—¶å§‹ç»ˆä½¿ç”¨ï¼ˆé˜ˆå€¼é™ä½ï¼Œé¿å…å°æ ·æœ¬è·³è¿‡ï¼‰
        if self.use_cv and len(X_scaled) >= max(self.cv_splits * 10, 50):
            # ä½¿ç”¨æœ€åä¸€æŠ˜ä½œä¸ºéªŒè¯é›†
            val_size = int(len(X_scaled) * self.cv_test_size)
            train_size = len(X_scaled) - val_size

            X_train = X_scaled[:train_size]
            y_train = y_winsorized[:train_size]
            X_val = X_scaled[train_size:]
            y_val = y_winsorized[train_size:]

            # è®­ç»ƒæ¨¡å‹
            self.ridge_model = Ridge(
                alpha=optimal_alpha,
                fit_intercept=self.fit_intercept,
                solver=self.solver,
                tol=self.tol,
                random_state=self.random_state
            )
            self.ridge_model.fit(X_train, y_train)

            # éªŒè¯é›†è¯„ä¼°
            y_val_pred = self.ridge_model.predict(X_val)
            val_score = self.ridge_model.score(X_val, y_val)
            val_rank_ic = self._calculate_rank_ic(y_val, y_val_pred)

            logger.info(f"   éªŒè¯é›†RÂ²: {val_score:.4f}")
            logger.info(f"   éªŒè¯é›†RankIC: {val_rank_ic:.4f}")

            # é‡æ–°ä½¿ç”¨å…¨é‡æ•°æ®è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            self.ridge_model.fit(X_scaled, y_winsorized)
        else:
            # ç›´æ¥å…¨é‡è®­ç»ƒ
            self.ridge_model = Ridge(
                alpha=optimal_alpha,
                fit_intercept=self.fit_intercept,
                solver=self.solver,
                tol=self.tol,
                random_state=self.random_state
            )
            self.ridge_model.fit(X_scaled, y_winsorized)

        # è®¡ç®—è®­ç»ƒæ€§èƒ½
        y_pred = self.ridge_model.predict(X_scaled)
        self.train_score_ = self.ridge_model.score(X_scaled, y_winsorized)
        train_rmse = np.sqrt(mean_squared_error(y_winsorized, y_pred))
        train_rank_ic = self._calculate_rank_ic(y_winsorized, y_pred)

        # è®¡ç®—åŸå§‹æ ‡ç­¾çš„RankIC
        original_rank_ic = self._calculate_rank_ic(y, y_pred)

        # ä¿å­˜éªŒè¯åˆ†æ•°
        self.val_score_ = val_score
        self.val_rank_ic_ = val_rank_ic

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§ï¼ˆå›å½’ç³»æ•°ï¼‰ - ä½¿ç”¨è®­ç»ƒæ—¶å®é™…ç‰¹å¾åˆ—ï¼Œç¡®ä¿é•¿åº¦ä¸€è‡´
        used_feature_names = list(self.actual_feature_cols_) if self.actual_feature_cols_ is not None else list(self.base_cols)
        coef_array = np.ravel(self.ridge_model.coef_)

        if len(coef_array) != len(used_feature_names):
            logger.warning(
                f"Ridgeç³»æ•°é•¿åº¦({len(coef_array)})ä¸ç‰¹å¾æ•°({len(used_feature_names)})ä¸ä¸€è‡´ï¼Œå°è¯•è‡ªåŠ¨å¯¹é½"
            )
            # å®‰å…¨å…œåº•ï¼šæˆªæ–­æˆ–å¡«å……åˆ°åŒ¹é…é•¿åº¦ï¼ˆæç«¯æƒ…å†µä¸‹é¿å…æŠ¥é”™ï¼Œä»ä¿ç•™æ’åºå¯è¯»æ€§ï¼‰
            if len(coef_array) > len(used_feature_names):
                coef_array = coef_array[:len(used_feature_names)]
            else:
                pad = np.zeros(len(used_feature_names) - len(coef_array))
                coef_array = np.concatenate([coef_array, pad])

        self.feature_names_ = used_feature_names
        self.feature_importance_ = pd.DataFrame({
            'feature': used_feature_names,
            'coefficient': coef_array,
            'abs_coefficient': np.abs(coef_array)
        }).sort_values('abs_coefficient', ascending=False)

        self.fitted_ = True

        logger.info("âœ… Ridge Stacker è®­ç»ƒå®Œæˆ")
        logger.info(f"   ä½¿ç”¨Î±: {optimal_alpha}")
        logger.info(f"   è®­ç»ƒRÂ²: {self.train_score_:.4f}")
        logger.info(f"   è®­ç»ƒRMSE: {train_rmse:.6f}")
        logger.info(f"   RankIC(winsorized): {train_rank_ic:.4f}")
        logger.info(f"   RankIC(åŸå§‹): {original_rank_ic:.4f}")
        if self.val_score_ is not None:
            logger.info(f"   CVéªŒè¯RÂ²: {self.val_score_:.4f}")
            logger.info(f"   CVéªŒè¯RankIC: {self.val_rank_ic_:.4f}")
        logger.info("   ç‰¹å¾é‡è¦æ€§ (ç³»æ•°):")
        for _, row in self.feature_importance_.head(3).iterrows():
            logger.info(f"     {row['feature']}: {row['coefficient']:.4f}")

        return self

    def predict(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ä½¿ç”¨Ridgeæ¨¡å‹è¿›è¡Œé¢„æµ‹

        Args:
            df: åŒ…å«ç¬¬ä¸€å±‚é¢„æµ‹çš„DataFrame

        Returns:
            åŒ…å«é¢„æµ‹åˆ†æ•°å’Œæ’åçš„DataFrame
        """
        if not self.fitted_:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fit()")

        logger.info("ğŸ“Š Ridge Stacker å¼€å§‹é¢„æµ‹...")

        # éªŒè¯è¾“å…¥å¹¶ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
        df_clean = self._validate_input(df)

        # ğŸ”§ Critical Fix: ä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„å®é™…ç‰¹å¾åˆ—
        if self.actual_feature_cols_ is None:
            raise RuntimeError("actual_feature_cols_æœªåˆå§‹åŒ–ï¼Œæ¨¡å‹å¯èƒ½æœªæ­£ç¡®è®­ç»ƒ")

        # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—å­˜åœ¨
        missing_cols = [col for col in self.actual_feature_cols_ if col not in df_clean.columns]
        if missing_cols:
            raise ValueError(f"é¢„æµ‹æ•°æ®ç¼ºå°‘ç‰¹å¾åˆ—: {missing_cols}")

        # ä½¿ç”¨è®­ç»ƒæ—¶çš„å®é™…ç‰¹å¾åˆ—
        X = df_clean[self.actual_feature_cols_].values

        # å¤„ç†NaN
        valid_mask = ~np.isnan(X).any(axis=1)
        X_valid = X[valid_mask]

        if len(X_valid) == 0:
            raise ValueError("æ‰€æœ‰æ ·æœ¬éƒ½åŒ…å«NaNï¼Œæ— æ³•é¢„æµ‹")

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨è®­ç»ƒæ—¶æ‹Ÿåˆçš„æ ‡å‡†åŒ–å™¨
        if self.scaler is None:
            raise RuntimeError("æ ‡å‡†åŒ–å™¨æœªåˆå§‹åŒ–ï¼Œæ¨¡å‹å¯èƒ½æœªæ­£ç¡®è®­ç»ƒ")

        X_scaled = self.scaler.transform(X_valid)

        logger.info(f"   ç‰¹å¾æå–: {len(X_valid)} æœ‰æ•ˆæ ·æœ¬, {X_scaled.shape[1]} ç‰¹å¾")
        logger.info(f"   å®é™…ç‰¹å¾é¡ºåº: {self.actual_feature_cols_}")
        logger.info(f"   é¢„æµ‹æ ‡å‡†åŒ–: ç‰¹å¾å‡å€¼={X_scaled.mean(axis=0)[:3]}, ç‰¹å¾æ ‡å‡†å·®={X_scaled.std(axis=0)[:3]}")

        # ğŸ”§ éªŒè¯ç‰¹å¾ç»´åº¦ä¸€è‡´æ€§
        if X_scaled.shape[1] != len(self.actual_feature_cols_):
            raise RuntimeError(f"ç‰¹å¾ç»´åº¦ä¸ä¸€è‡´: é¢„æµ‹æ—¶{X_scaled.shape[1]}åˆ—ï¼Œè®­ç»ƒæ—¶{len(self.actual_feature_cols_)}åˆ—")

        # é¢„æµ‹
        raw_predictions = self.ridge_model.predict(X_scaled)

        # åˆ›å»ºå®Œæ•´é¢„æµ‹æ•°ç»„
        full_predictions = np.full(len(X), np.nan)
        full_predictions[valid_mask] = raw_predictions

        # æ„å»ºç»“æœDataFrame
        result = df_clean.copy()
        result['score'] = full_predictions

        # æŒ‰æ—¥æœŸè®¡ç®—æ’åï¼ˆä½¿ç”¨transformä¿æŒç´¢å¼•å¯¹é½ï¼Œé¿å…applyäº§ç”Ÿé”™ä½ï¼‰
        result['score_rank'] = result.groupby(level='date')['score'].transform(
            lambda s: s.rank(method='average', ascending=False)
        )

        # æ ‡å‡†åŒ–åˆ†æ•°
        def _zscore_by_date(group):
            scores = group['score']
            valid_scores = scores.dropna()
            if len(valid_scores) <= 1:
                return pd.Series(0.0, index=scores.index)

            mean_score = valid_scores.mean()
            std_score = valid_scores.std()
            if std_score < 1e-12:
                return pd.Series(0.0, index=scores.index)

            zscores = (valid_scores - mean_score) / std_score
            full_zscores = pd.Series(0.0, index=scores.index)
            full_zscores.loc[valid_scores.index] = zscores
            return full_zscores

        # ä½¿ç”¨transformç¡®ä¿ä¸åŸç´¢å¼•å¯¹é½
        result['score_z'] = result.groupby(level='date')['score'].transform(
            lambda s: (s - s.mean()) / s.std() if s.dropna().size > 1 and s.std() >= 1e-12 else 0.0
        )

        logger.info(f"âœ… Ridgeé¢„æµ‹å®Œæˆ: {len(result)}æ ·æœ¬")
        logger.info(f"   æœ‰æ•ˆé¢„æµ‹: {(~pd.isna(result['score'])).sum()}")

        return result[['score', 'score_rank', 'score_z']]

    def replace_ewa_in_pipeline(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ›¿ä»£EWAçš„ç®¡é“æ¥å£ - å…¼å®¹ç°æœ‰è°ƒç”¨

        Args:
            df: è¾“å…¥æ•°æ®

        Returns:
            é¢„æµ‹ç»“æœ
        """
        return self.predict(df)

    def get_model_info(self) -> Dict:
        """
        è·å–æ¨¡å‹ä¿¡æ¯ - å…¼å®¹LTRæ¥å£

        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        if not self.fitted_:
            return {}

        return {
            'model_type': 'Ridge Regression (å¼€ç®±å³ç”¨ç‰ˆ)',
            'n_features': len(self.feature_names_) if self.feature_names_ else 0,
            'alpha': self.best_alpha_,  # ä½¿ç”¨æœ€ä¼˜alpha
            'alpha_grid': list(self.alpha_grid),
            'alpha_scores': dict(self.alpha_scores_),
            'train_score': self.train_score_,
            'solver': self.solver,
            'tol': self.tol,
            'fit_intercept': self.fit_intercept,
            'auto_tune_alpha': self.auto_tune_alpha,
            'intercept': self.ridge_model.intercept_ if self.fit_intercept and self.ridge_model else 0.0,
            'feature_importance': self.feature_importance_.to_dict('records') if self.feature_importance_ is not None else None,
            'feature_names': self.feature_names_,
            'configuration': 'Optimized for 2600 stocks Ã— 3 years, Tâ†’T+5 horizon'
        }

    @property
    def best_iteration_(self):
        """å…¼å®¹LTRæ¥å£çš„å±æ€§"""
        return 1 if self.fitted_ else None

# å…¼å®¹å¯¼å…¥
LtrIsotonicStacker = RidgeStacker  # æä¾›å‘åå…¼å®¹çš„åˆ«å