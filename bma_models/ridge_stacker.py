#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ridge Regression Stacker - æ›¿æ¢LTR Isotonic Stacker
ç®€æ´é«˜æ•ˆçš„äºŒå±‚çº¿æ€§å›žå½’æ¨¡åž‹ï¼Œç›´æŽ¥ä¼˜åŒ–è¿žç»­æ”¶ç›ŠçŽ‡
âœ… å¢žå¼ºCVéªŒè¯ï¼šä½¿ç”¨æ—¶é—´åºåˆ—éªŒè¯é˜²æ­¢è¿‡æ‹Ÿåˆ
"""

import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import logging
from sklearn.linear_model import Ridge, RidgeCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from scipy.stats import spearmanr
from sklearn.metrics import mean_absolute_error

# ðŸ”¥ å¯¼å…¥ç»Ÿä¸€PurgedCVï¼Œé˜²æ­¢æ•°æ®æ³„éœ²
try:
    from bma_models.unified_purged_cv_factory import create_unified_cv
    from bma_models.unified_config_loader import get_time_config
    PURGED_CV_AVAILABLE = True
except ImportError:
    PURGED_CV_AVAILABLE = False

logger = logging.getLogger(__name__)

class RidgeStacker:
    """
    Ridgeå›žå½’äºŒå±‚Stacker - ç®€æ´æ›¿ä»£LTR

    æ ¸å¿ƒä¼˜åŠ¿ï¼š
    - ç›´æŽ¥ä¼˜åŒ–è¿žç»­æ”¶ç›ŠçŽ‡ï¼Œæ— ä¿¡æ¯æŸå¤±
    - çº¿æ€§æ¨¡åž‹ï¼Œè§£é‡Šæ€§å¼º
    - è®­ç»ƒå¿«é€Ÿï¼Œç¨³å®šæ€§å¥½
    - è‡ªåŠ¨ç‰¹å¾æ ‡å‡†åŒ–
    - âœ… æ—¶é—´åºåˆ—CVï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæå‡æ³›åŒ–æ€§èƒ½
    """

    def __init__(self,
                 base_cols: Tuple[str, ...] = ('pred_catboost', 'pred_elastic', 'pred_xgb'),
                 alpha: float = 1.0,
                 fit_intercept: bool = False,
                 solver: str = "auto",
                 tol: float = 1e-6,
                 auto_tune_alpha: bool = False,
                 alpha_grid: Tuple[float, ...] = (0.5, 1.0, 2.0, 3.0, 5.0, 8.0),
                 use_cv: bool = True,
                 use_purged_cv: bool = True,  # ðŸ”¥ ä½¿ç”¨PurgedCVé˜²æ­¢æ•°æ®æ³„éœ²
                 cv_splits: int = 6,  # ðŸ”¥ T+5: 6æŠ˜CV
                 cv_gap_days: int = 5,  # ðŸ”¥ T+5: gap=5
                 cv_embargo_days: int = 5,  # ðŸ”¥ T+5: embargo=5
                 cv_test_size: float = 0.2,
                 use_lambda_percentile: bool = True,  # æ–°å¢žï¼šä½¿ç”¨Lambda percentileç‰¹å¾
                 random_state: int = 42,
                 # ---- Nested CV & constraints (new) ----
                 use_nested_cv: bool = False,
                 nested_outer_splits: int = 4,
                 nested_gap_days: int = 5,
                 nested_embargo_days: int = 5,
                 aggregate_alpha: str = 'median',  # 'median' | 'ir_weighted'
                 use_convex_constraint: bool = True,  # éžè´Ÿä¸”å’Œä¸º1
                 **kwargs):
        """
        åˆå§‹åŒ–Ridge Stacker

        Args:
            base_cols: ç¬¬ä¸€å±‚æ¨¡åž‹é¢„æµ‹åˆ—å
            alpha: Ridgeæ­£åˆ™åŒ–å¼ºåº¦ (é»˜è®¤1.0ï¼Œç®€æ´ç‰ˆ)
            fit_intercept: æ˜¯å¦æ‹Ÿåˆæˆªè· (é»˜è®¤Falseï¼Œå› ä¸ºå·²åšz-score)
            solver: æ±‚è§£å™¨ (é»˜è®¤auto)
            tol: æ”¶æ•›å®¹å·® (é»˜è®¤1e-6)
            auto_tune_alpha: æ˜¯å¦è‡ªåŠ¨è°ƒå‚ (é»˜è®¤Falseï¼Œä¿æŒç®€æ´)
            alpha_grid: è°ƒå‚ç½‘æ ¼ (é»˜è®¤[0.5,1,2,3,5,8])
            use_cv: æ˜¯å¦ä½¿ç”¨äº¤å‰éªŒè¯ (é»˜è®¤True)
            use_purged_cv: ðŸ”¥ æ˜¯å¦ä½¿ç”¨PurgedCV (é»˜è®¤Trueï¼Œé˜²æ­¢æ•°æ®æ³„éœ²)
            cv_splits: CVæŠ˜æ•° (é»˜è®¤6ï¼ŒT+5ä¼˜åŒ–)
            cv_gap_days: ðŸ”¥ CV gapå¤©æ•° (é»˜è®¤5ï¼ŒT+5)
            cv_embargo_days: ðŸ”¥ CV embargoå¤©æ•° (é»˜è®¤5ï¼ŒT+5)
            cv_test_size: æ¯æŠ˜éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤0.2)
            use_lambda_percentile: æ˜¯å¦ä½¿ç”¨Lambda percentileç‰¹å¾ (é»˜è®¤True)
            random_state: éšæœºç§å­
        """
        if not use_cv:
            raise ValueError("RidgeStacker requires cross-validation; disabling CV violates the enforced T+5 protocol.")
        if not use_purged_cv:
            raise ValueError("RidgeStacker requires purged CV to protect the T+5 horizon; do not disable it.")
        if not PURGED_CV_AVAILABLE:
            raise RuntimeError("Unified Purged CV factory is unavailable. Install the required components to enable T+5 training.")
        if (cv_splits, cv_gap_days, cv_embargo_days) != (6, 5, 5):
            raise ValueError("RidgeStacker enforces T+5 CV settings: splits=6, gap=5, embargo=5.")

        self.base_cols = base_cols
        self.alpha = alpha
        self.fit_intercept = fit_intercept
        self.solver = solver
        self.tol = tol
        self.auto_tune_alpha = auto_tune_alpha
        self.alpha_grid = alpha_grid
        self.use_cv = True
        self.use_purged_cv = True
        self.cv_splits = 6
        self.cv_gap_days = 5
        self.cv_embargo_days = 5
        self.cv_test_size = cv_test_size
        self.use_lambda_percentile = use_lambda_percentile
        self.random_state = random_state
        self.actual_feature_cols_ = None  # ðŸ”§ è®­ç»ƒæ—¶å®žé™…ä½¿ç”¨çš„ç‰¹å¾åˆ—ï¼ˆCritical Fixï¼‰

        # Nested CV & constraints
        self.use_nested_cv = use_nested_cv
        self.nested_outer_splits = nested_outer_splits
        self.nested_gap_days = nested_gap_days
        self.nested_embargo_days = nested_embargo_days
        self.aggregate_alpha = aggregate_alpha
        self.use_convex_constraint = use_convex_constraint

        # æ¨¡åž‹ç»„ä»¶
        self.ridge_model = None
        self.scaler = None
        self.feature_importance_ = None
        self.fitted_ = False

        # è°ƒå‚ç›¸å…³
        self.best_alpha_ = alpha
        self.alpha_scores_ = {}

        # è®­ç»ƒç»Ÿè®¡
        self.train_score_ = None
        self.feature_names_ = None

        logger.info(f"âœ… Ridge Stacker åˆå§‹åŒ–å®Œæˆ (Percentileå¢žå¼ºç‰ˆ)")
        logger.info(f"   åŸºç¡€ç‰¹å¾: {self.base_cols}")
        logger.info(f"   Lambda Percentile: {'å¯ç”¨' if self.use_lambda_percentile else 'ç¦ç”¨'}")
        logger.info(f"   æ­£åˆ™åŒ–å¼ºåº¦Î±: {self.alpha}")
        logger.info(f"   æ‹Ÿåˆæˆªè·: {self.fit_intercept} (å·²åšz-score)")
        logger.info(f"   æ±‚è§£å™¨: {self.solver}, å®¹å·®: {self.tol}")
        logger.info(f"   è‡ªåŠ¨è°ƒå‚: {self.auto_tune_alpha}")
        logger.info(f"   ä½¿ç”¨CV: {self.use_cv}, æŠ˜æ•°: {self.cv_splits}")
        logger.info(f"   åµŒå¥—CV: {'å¯ç”¨' if self.use_nested_cv else 'ç¦ç”¨'} (outer_splits={self.nested_outer_splits})")
        logger.info(f"   ç‰¹å¾æ ‡å‡†åŒ–: æ¨ªæˆªé¢z-score")

    def _validate_input(self, df: pd.DataFrame) -> pd.DataFrame:
        """éªŒè¯è¾“å…¥æ•°æ®æ ¼å¼å¹¶ç¡®ä¿åˆ—é¡ºåºä¸€è‡´"""
        if not isinstance(df.index, pd.MultiIndex):
            raise ValueError("è¾“å…¥æ•°æ®å¿…é¡»å…·æœ‰MultiIndex(date, ticker)")

        if df.index.names != ['date', 'ticker']:
            raise ValueError(f"Index nameså¿…é¡»æ˜¯['date', 'ticker'], å®žé™…: {df.index.names}")

        missing_cols = [col for col in self.base_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")

        # ðŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿åˆ—é¡ºåºä¸Žè®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
        return df[list(self.base_cols) + [col for col in df.columns if col not in self.base_cols]]

    def _prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾"""
        # æå–åŸºç¡€ç‰¹å¾
        feature_cols = list(self.base_cols)

        # å¦‚æžœå¯ç”¨lambda percentileä¸”æ•°æ®ä¸­æœ‰è¯¥åˆ—ï¼ŒåŠ å…¥ç‰¹å¾
        if self.use_lambda_percentile and 'lambda_percentile' in df.columns:
            feature_cols.append('lambda_percentile')
            logger.debug("âœ“ åŠ å…¥Lambda Percentileç‰¹å¾")

        # ðŸ”§ Critical Fix: ä¿å­˜å®žé™…ä½¿ç”¨çš„ç‰¹å¾åˆ—ï¼ˆä»…åœ¨è®­ç»ƒæ—¶ï¼Œå³é¦–æ¬¡è°ƒç”¨ï¼‰
        if self.actual_feature_cols_ is None:
            self.actual_feature_cols_ = feature_cols
            logger.info(f"ðŸ”§ ä¿å­˜å®žé™…ç‰¹å¾åˆ—: {self.actual_feature_cols_}")

        X = df[feature_cols].values

        # æå–æ ‡ç­¾ï¼ˆå‡è®¾æ ‡ç­¾åˆ—ä»¥ret_fwdå¼€å¤´ï¼‰
        label_cols = [col for col in df.columns if col.startswith('ret_fwd')]
        if not label_cols:
            raise ValueError("æœªæ‰¾åˆ°æ ‡ç­¾åˆ— (ret_fwd_*)")

        label_col = label_cols[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ ‡ç­¾åˆ—
        y = df[label_col].values

        # ç§»é™¤NaNæ ·æœ¬
        valid_mask = ~(np.isnan(X).any(axis=1) | np.isnan(y))
        X_clean = X[valid_mask]
        y_clean = y[valid_mask]

        logger.info(f"   åŽŸå§‹æ ·æœ¬: {len(X)}")
        logger.info(f"   æœ‰æ•ˆæ ·æœ¬: {len(X_clean)}")
        logger.info(f"   æ•°æ®è¦†ç›–çŽ‡: {len(X_clean)/len(X)*100:.1f}%")

        return X_clean, y_clean

    def _winsorize_labels(self, y: np.ndarray, lower_pct: float = 1.0, upper_pct: float = 99.0) -> np.ndarray:
        """Winsorizeæ ‡ç­¾ï¼Œå¤„ç†æžç«¯å€¼"""
        lower_bound = np.percentile(y, lower_pct)
        upper_bound = np.percentile(y, upper_pct)
        y_winsorized = np.clip(y, lower_bound, upper_bound)

        n_clipped = np.sum((y != y_winsorized))
        if n_clipped > 0:
            logger.info(f"   Winsorize: {n_clipped}/{len(y)} ({n_clipped/len(y)*100:.1f}%) æ ·æœ¬è¢«è£å‰ª")

        return y_winsorized

    def _calculate_rank_ic(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """è®¡ç®—RankIC"""
        try:
            return spearmanr(y_true, y_pred)[0]
        except:
            return 0.0

    def _create_purged_cv_split(self, dates: pd.Series):
        """ðŸ”¥ ä½¿ç”¨ç»Ÿä¸€çš„ PurgedCV åˆ†å‰²ï¼Œé˜²æ­¢æ•°æ®æ³„éœ²"""
        if not self.use_purged_cv:
            raise RuntimeError('PurgedCV is mandatory for RidgeStacker T+5 training.')

        cv = create_unified_cv(
            n_splits=self.cv_splits,
            gap=self.cv_gap_days,
            embargo=self.cv_embargo_days
        )

        unique_dates = sorted(dates.unique())
        if not unique_dates:
            raise ValueError('PurgedCV requires non-empty date information.')
        date_to_idx = {date: i for i, date in enumerate(unique_dates)}
        groups = dates.map(date_to_idx).values

        logger.info(f"âœ… ä½¿ç”¨PurgedCV: splits={self.cv_splits}, gap={self.cv_gap_days}, embargo={self.cv_embargo_days}")
        logger.info(f"   æ—¶é—´èŒƒå›´: {unique_dates[0]} ~ {unique_dates[-1]} ({len(unique_dates)}å¤©)")

        return cv.split(X=np.zeros((len(dates), 1)), y=None, groups=groups)

    def _create_purged_cv_split_params(self, dates: pd.Series, n_splits: int, gap: int, embargo: int):
        """æ ¹æ®æŒ‡å®šå‚æ•°åˆ›å»ºPurgedCVåˆ†å‰²ã€‚"""
        if not PURGED_CV_AVAILABLE:
            raise RuntimeError('Unified Purged CV factory unavailable.')
        cv = create_unified_cv(n_splits=n_splits, gap=gap, embargo=embargo)
        unique_dates = sorted(dates.unique())
        if not unique_dates:
            raise ValueError('PurgedCV requires non-empty date information.')
        date_to_idx = {date: i for i, date in enumerate(unique_dates)}
        groups = dates.map(date_to_idx).values
        logger.info(f"âœ… ä½¿ç”¨PurgedCV(n_splits={n_splits}, gap={gap}, embargo={embargo})")
        return cv.split(X=np.zeros((len(dates), 1)), y=None, groups=groups)

    def _auto_tune_alpha(self, X: np.ndarray, y: np.ndarray, df: pd.DataFrame, dates: pd.Series = None) -> float:
        """
        è‡ªåŠ¨è°ƒå‚é€‰æ‹©æœ€ä¼˜alpha - å¢žå¼ºCVç‰ˆæœ¬
        """
        if not self.auto_tune_alpha:
            return self.alpha

        logger.info(f"ðŸŽ¯ å¼€å§‹è‡ªåŠ¨è°ƒå‚ï¼Œç½‘æ ¼: {self.alpha_grid}")

        if not self.use_cv:
            raise RuntimeError("RidgeStacker requires PurgedCV-based CV for T+5 training.")

        return self._auto_tune_alpha_with_cv(X, y, dates)

    def _auto_tune_alpha_with_cv(self, X: np.ndarray, y: np.ndarray, dates: pd.Series = None) -> float:
        """
        ä½¿ç”¨CVè¿›è¡Œè°ƒå‚ï¼ˆä¸¥æ ¼PurgedCVï¼‰
        """
        if dates is None:
            raise ValueError("PurgedCV tuning requires aligned date information.")
        if not self.use_purged_cv:
            raise RuntimeError("PurgedCV must be enabled for alpha tuning.")

        logger.info(f"   ä½¿ç”¨PurgedCVï¼ŒæŠ˜æ•°: {self.cv_splits}")

        best_alpha = self.alpha
        best_score = -999

        cv_splits = list(self._create_purged_cv_split(dates))

        for alpha in self.alpha_grid:
            cv_scores = []

            for fold, (train_idx, test_idx) in enumerate(cv_splits):
                X_train, X_test = X[train_idx], X[test_idx]
                y_train, y_test = y[train_idx], y[test_idx]

                model = Ridge(
                    alpha=alpha,
                    fit_intercept=self.fit_intercept,
                    solver=self.solver,
                    tol=self.tol,
                    random_state=self.random_state
                )
                model.fit(X_train, y_train)

                y_pred = model.predict(X_test)
                rank_ic = self._calculate_rank_ic(y_test, y_pred)
                cv_scores.append(rank_ic)

            avg_score = np.mean(cv_scores)
            std_score = np.std(cv_scores)
            self.alpha_scores_[alpha] = avg_score

            logger.info(f"   Î±={alpha}: CV RankIC={avg_score:.4f} (Â±{std_score:.4f})")

            if avg_score > best_score:
                best_score = avg_score
                best_alpha = alpha

        logger.info(f"âœ… æœ€ä¼˜Î±: {best_alpha}, CV RankIC: {best_score:.4f}")
        return best_alpha

    def _project_to_simplex(self, w: np.ndarray) -> np.ndarray:
        """å°†å‘é‡æŠ•å½±åˆ°æ¦‚çŽ‡å•çº¯å½¢ï¼šw>=0, sum(w)=1ã€‚"""
        if w.ndim != 1:
            w = w.flatten()
        # Algorithm from: Efficient Projections onto the l1-Ball for Learning in High Dimensions (Duchi et al.)
        u = np.sort(np.maximum(w, 0))[::-1]
        cssv = np.cumsum(u)
        rho = np.nonzero(u * np.arange(1, len(u) + 1) > (cssv - 1))[0]
        if len(rho) == 0:
            # all zeros, return uniform
            m = len(w)
            return np.full(m, 1.0 / m)
        rho = rho[-1]
        theta = (cssv[rho] - 1) / (rho + 1.0)
        w_proj = np.maximum(w - theta, 0)
        s = w_proj.sum()
        if s <= 0:
            m = len(w)
            return np.full(m, 1.0 / m)
        return w_proj / s

    def _fit_ridge_with_constraint(self, X: np.ndarray, y: np.ndarray, alpha: float, fit_intercept: bool = False) -> Tuple[np.ndarray, float]:
        """æ‹ŸåˆRidgeå¹¶å¯¹æƒé‡æŠ•å½±åˆ°éžè´Ÿä¸”å’Œä¸º1çš„å•çº¯å½¢ã€‚"""
        model = Ridge(
            alpha=alpha,
            fit_intercept=fit_intercept,
            solver=self.solver,
            tol=self.tol,
            random_state=self.random_state
        )
        model.fit(X, y)
        w = np.asarray(model.coef_).flatten()
        b = float(model.intercept_) if fit_intercept else 0.0
        if self.use_convex_constraint:
            w = self._project_to_simplex(np.maximum(w, 0))
            b = 0.0  # ä¿æŒå¯è§£é‡Šæ€§ï¼šå‡¸ç»„åˆé€šå¸¸ä¸éœ€è¦æˆªè·
        return w, b

    def _inner_tune_alpha(self, X_train: np.ndarray, y_train: np.ndarray, dates_train: pd.Series) -> float:
        """åœ¨å¤–å±‚è®­ç»ƒæ®µä¸Šè¿›è¡Œå†…å±‚PurgedCVè°ƒå‚ã€‚"""
        best_alpha = None
        best_score = -1e9
        # dates_train å¯èƒ½æ˜¯ Series/Index/ndarrayï¼Œç»Ÿä¸€ä¸ºSeries
        if not isinstance(dates_train, pd.Series):
            dates_train = pd.Series(np.asarray(dates_train))
        inner_splits = list(self._create_purged_cv_split_params(dates_train, self.cv_splits, self.cv_gap_days, self.cv_embargo_days))
        for alpha in self.alpha_grid:
            scores = []
            for (tr_idx, te_idx) in inner_splits:
                # ä½¿ç”¨ä»…åŸºäºŽè®­ç»ƒçš„æ ‡å‡†åŒ–ï¼Œé¿å…æ³„éœ²
                scaler = StandardScaler()
                X_tr = scaler.fit_transform(X_train[tr_idx])
                X_te = scaler.transform(X_train[te_idx])
                w, b = self._fit_ridge_with_constraint(X_tr, y_train[tr_idx], alpha, fit_intercept=self.fit_intercept)
                y_hat = X_te @ w + b
                scores.append(self._calculate_rank_ic(y_train[te_idx], y_hat))
            avg = float(np.nanmean(scores)) if scores else -1e9
            self.alpha_scores_[alpha] = avg
            if avg > best_score:
                best_score = avg
                best_alpha = alpha
        return best_alpha if best_alpha is not None else self.alpha

    def _nested_cv_oof(self, X: np.ndarray, y: np.ndarray, dates: pd.Series) -> Tuple[np.ndarray, List[float], List[float]]:
        """
        å¤–å±‚PurgedCVç”ŸæˆOOFé¢„æµ‹ï¼›å†…å±‚åœ¨å¤–å±‚è®­ç»ƒæ®µæ—¶é—´CVé€‰æ‹©alphaã€‚
        è¿”å›ž: y_oof_pred, alphas_per_fold, ic_per_fold
        """
        n = len(y)
        y_oof = np.full(n, np.nan, dtype=float)
        alphas = []
        ics = []
        outer_splits = list(self._create_purged_cv_split_params(dates, self.nested_outer_splits, self.nested_gap_days, self.nested_embargo_days))
        for k, (train_idx, test_idx) in enumerate(outer_splits):
            # å†…å±‚è°ƒå‚ï¼ˆdates å¯èƒ½æ˜¯ DatetimeIndex/Seriesï¼Œç»Ÿä¸€è½¬æ¢ä¸ºnumpyæ•°ç»„å†æŒ‰ä½ç½®ç´¢å¼•ï¼‰
            dates_array = np.asarray(dates)
            dates_train = pd.Series(dates_array[train_idx])
            alpha_k = self._inner_tune_alpha(X[train_idx], y[train_idx], dates_train)
            alphas.append(alpha_k)
            # åŸºäºŽè®­ç»ƒæ®µæ‹Ÿåˆï¼Œå¹¶åœ¨æµ‹è¯•æ®µé¢„æµ‹
            scaler = StandardScaler()
            X_tr = scaler.fit_transform(X[train_idx])
            X_te = scaler.transform(X[test_idx])
            w, b = self._fit_ridge_with_constraint(X_tr, y[train_idx], alpha_k, fit_intercept=self.fit_intercept)
            y_hat = X_te @ w + b
            y_oof[test_idx] = y_hat
            ic_k = self._calculate_rank_ic(y[test_idx], y_hat)
            ics.append(float(ic_k))
            logger.info(f"[NestedCV] outer_fold={k+1}/{len(outer_splits)} alpha={alpha_k} OOS RankIC={ic_k:.4f}")
        return y_oof, alphas, ics

    def fit(self, df: pd.DataFrame, max_train_to_today: bool = True) -> "RidgeStacker":
        """
        è®­ç»ƒRidgeäºŒå±‚Stackerã€‚è¾“å…¥ä¸ºåŒ…å«ç¬¬ä¸€å±‚é¢„æµ‹ä¸Žæ ‡ç­¾çš„DataFrameã€‚

        å¿…éœ€åˆ—: self.base_cols ä¸­çš„åˆ—ï¼Œä»¥åŠä¸€ä¸ªä»¥ 'ret_fwd' å¼€å¤´çš„æ ‡ç­¾åˆ—ã€‚
        ç´¢å¼•: éœ€è¦MultiIndex(date, ticker)ã€‚
        """
        df_validated = self._validate_input(df)
        X, y = self._prepare_features(df_validated)

        # æ ‡ç­¾Winsorizeï¼Œå¢žå¼ºç¨³å¥æ€§
        y_proc = self._winsorize_labels(y, lower_pct=1.0, upper_pct=99.0)

        # ç‰¹å¾æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)

        dates = df_validated.index.get_level_values('date')

        if self.use_nested_cv:
            # åµŒå¥—CVï¼šç”ŸæˆOOFå¹¶èšåˆalphaï¼Œç„¶åŽåœ¨å…¨é‡ä¸Šå†è®­ç»ƒä¸€æ¬¡
            logger.info("ðŸš€ ä½¿ç”¨ä¸¥è°¨åµŒå¥—CVè®­ç»ƒäºŒå±‚ (å¤–å±‚æ»šåŠ¨ + å†…å±‚PurgedCVè°ƒå‚)")
            # ä½¿ç”¨æœªç¼©æ”¾Xè¿›è¡ŒåµŒå¥—CVï¼Œå†…éƒ¨å„foldå•ç‹¬æ ‡å‡†åŒ–
            y_oof, alphas, ics = self._nested_cv_oof(X, y_proc, dates)
            # é€‰æ‹©èšåˆalpha
            if self.aggregate_alpha == 'ir_weighted' and len(ics) > 1 and np.nanstd(ics) > 0:
                weights = np.maximum(np.array(ics), 0)
                if weights.sum() == 0:
                    self.best_alpha_ = float(np.median(alphas))
                else:
                    self.best_alpha_ = float(np.average(alphas, weights=weights))
            else:
                self.best_alpha_ = float(np.median(alphas)) if len(alphas) > 0 else self.alpha
            logger.info(f"[NestedCV] èšåˆalpha={self.best_alpha_} (strategy={self.aggregate_alpha})")
            # åœ¨å…¨é‡ä¸Šæ‹Ÿåˆæœ€ç»ˆæ¨¡åž‹ï¼ˆå…ˆå…¨é‡æ ‡å‡†åŒ–ï¼Œå†æ‹Ÿåˆ+æŠ•å½±ï¼‰
            self.scaler = StandardScaler()
            X_scaled = self.scaler.fit_transform(X)
            w, b = self._fit_ridge_with_constraint(X_scaled, y_proc, self.best_alpha_, fit_intercept=self.fit_intercept)
            # å­˜å…¥sklearn Ridgeå¯¹è±¡ä»¥ä¿æŒæŽ¥å£ä¸€è‡´
            self.ridge_model = Ridge(alpha=self.best_alpha_, fit_intercept=False)
            # ç¡®ä¿coef_ä¸º1Dï¼Œé¿å…predictè¾“å‡º(n,1)é€ æˆä¸‹æ¸¸"Data must be 1-dimensional"é”™è¯¯
            self.ridge_model.coef_ = w.reshape(-1)
            self.ridge_model.intercept_ = 0.0
            # æ˜¾å¼è®¾ç½®ç‰¹å¾ç»´åº¦ï¼Œæå‡sklearnä¸€è‡´æ€§
            try:
                self.ridge_model.n_features_in_ = X_scaled.shape[1]
            except Exception:
                pass
        else:
            # å¯é€‰ï¼šè‡ªåŠ¨è°ƒå‚ï¼ˆå•å±‚CVï¼‰
            if self.auto_tune_alpha:
                self.best_alpha_ = self._auto_tune_alpha(X_scaled, y_proc, df_validated, dates)
            else:
                self.best_alpha_ = self.alpha

            # ä½¿ç”¨Ridgeè®­ç»ƒ
            self.ridge_model = Ridge(
                alpha=self.best_alpha_,
                fit_intercept=self.fit_intercept,
                solver=self.solver,
                tol=self.tol,
                random_state=self.random_state
            )
            self.ridge_model.fit(X_scaled, y_proc)
            # è®­ç»ƒåŽè¿›è¡Œå‡¸çº¦æŸæŠ•å½±ï¼ˆå¯é€‰ï¼‰
            if self.use_convex_constraint:
                w = np.asarray(self.ridge_model.coef_).flatten()
                w = self._project_to_simplex(np.maximum(w, 0))
                self.ridge_model.coef_ = w.reshape(-1)
                if self.fit_intercept:
                    self.ridge_model.intercept_ = 0.0
            try:
                self.ridge_model.n_features_in_ = X_scaled.shape[1]
            except Exception:
                pass

        # è®­ç»ƒåˆ†æ•°ä¸Žç‰¹å¾é‡è¦æ€§
        y_pred_train = self.ridge_model.predict(X_scaled)
        self.train_score_ = float(self._calculate_rank_ic(y_proc, y_pred_train))
        self.feature_names_ = list(self.actual_feature_cols_ or self.base_cols)
        # çº¿æ€§æ¨¡åž‹çš„é‡è¦æ€§å¯ç”¨ç³»æ•°ç»å¯¹å€¼
        try:
            coefs = np.asarray(self.ridge_model.coef_).flatten()
            self.feature_importance_ = {name: float(abs(w)) for name, w in zip(self.feature_names_, coefs)}
        except Exception:
            self.feature_importance_ = None

        self.fitted_ = True
        return self

    def predict(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„RidgeStackerè¿›è¡Œé¢„æµ‹ã€‚
        è¿”å›žåŒ…å«'score'åˆ—çš„DataFrameï¼ˆç´¢å¼•ä¸Žè¾“å…¥å¯¹é½ï¼‰ã€‚
        """
        if not self.fitted_ or self.ridge_model is None or self.scaler is None:
            raise RuntimeError("RidgeStacker not fitted")

        # ç¡®ä¿åˆ—é¡ºåºä¸Žè®­ç»ƒä¸€è‡´ï¼›å…è®¸ç¼ºå¤±æ ‡ç­¾åˆ—
        if not isinstance(df.index, pd.MultiIndex) or df.index.names != ['date', 'ticker']:
            raise ValueError("é¢„æµ‹æ•°æ®å¿…é¡»å…·æœ‰MultiIndex(date, ticker)")

        feature_cols = list(self.actual_feature_cols_ or self.base_cols)
        missing = [c for c in feature_cols if c not in df.columns]
        if missing:
            raise ValueError(f"é¢„æµ‹ç¼ºå°‘å¿…éœ€ç‰¹å¾åˆ—: {missing}")

        X = df[feature_cols].values
        X_scaled = self.scaler.transform(X)
        y_pred = self.ridge_model.predict(X_scaled)
        return pd.DataFrame({'score': y_pred}, index=df.index)

    def replace_ewa_in_pipeline(self, df: pd.DataFrame) -> pd.DataFrame:
        """Compatibility shim for the legacy EWA interface used in the pipeline."""
        if not self.fitted_ or self.ridge_model is None or self.scaler is None:
            raise RuntimeError('RidgeStacker not fitted; call fit before replace_ewa_in_pipeline.')
        if not isinstance(df, pd.DataFrame):
            raise TypeError('replace_ewa_in_pipeline expects a pandas DataFrame.')
        if df.empty:
            raise ValueError('replace_ewa_in_pipeline received an empty DataFrame.')
        if not isinstance(df.index, pd.MultiIndex) or df.index.names != ['date', 'ticker']:
            raise ValueError('Input to replace_ewa_in_pipeline must have MultiIndex(date, ticker).')

        feature_cols = list(self.actual_feature_cols_ or self.base_cols)
        sanitized = df.copy()
        label_cols = [col for col in sanitized.columns if col.startswith('ret_fwd')]
        if label_cols:
            sanitized = sanitized.drop(columns=label_cols)

        missing = [col for col in feature_cols if col not in sanitized.columns]
        if missing:
            raise ValueError(f'replace_ewa_in_pipeline is missing required features: {missing}')

        sanitized = sanitized[feature_cols]
        valid_mask = ~sanitized.isna().any(axis=1)
        if not valid_mask.any():
            raise ValueError('replace_ewa_in_pipeline found no valid rows after dropping NaNs.')

        if not valid_mask.all():
            logger.debug('replace_ewa_in_pipeline dropping %d rows with NaN features', (~valid_mask).sum())

        valid_features = sanitized.loc[valid_mask]
        predictions = self.predict(valid_features)

        if valid_mask.all():
            return predictions

        full_output = pd.DataFrame(index=sanitized.index, columns=predictions.columns, dtype=float)
        full_output.loc[valid_mask] = predictions.values
        return full_output

    def get_model_info(self) -> Dict[str, any]:
        return {
            'alpha': float(self.best_alpha_),
            'use_lambda_percentile': bool(self.use_lambda_percentile),
            'train_rank_ic': float(self.train_score_) if self.train_score_ is not None else None,
            'feature_importance': self.feature_importance_,
            'features': list(self.feature_names_ or []),
            'n_iterations': 1
        }