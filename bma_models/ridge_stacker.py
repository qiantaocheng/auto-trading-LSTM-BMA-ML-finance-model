#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ridge Regression Stacker - æ›¿æ¢LTR Isotonic Stacker
ç®€æ´é«˜æ•ˆçš„äºŒå±‚çº¿æ€§å›å½’æ¨¡å‹ï¼Œç›´æ¥ä¼˜åŒ–è¿ç»­æ”¶ç›Šç‡
âœ… å¢å¼ºCVéªŒè¯ï¼šä½¿ç”¨æ—¶é—´åºåˆ—éªŒè¯é˜²æ­¢è¿‡æ‹Ÿåˆ
"""

import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import logging
from sklearn.linear_model import Ridge, RidgeCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from scipy.stats import spearmanr
from sklearn.metrics import mean_absolute_error

# ğŸ”¥ å¯¼å…¥ç»Ÿä¸€PurgedCVï¼Œé˜²æ­¢æ•°æ®æ³„éœ²
try:
    from bma_models.unified_purged_cv_factory import create_unified_cv
    from bma_models.unified_config_loader import get_time_config
    PURGED_CV_AVAILABLE = True
except ImportError:
    PURGED_CV_AVAILABLE = False

logger = logging.getLogger(__name__)

class RidgeStacker:
    """
    Ridgeå›å½’äºŒå±‚Stacker - ç®€æ´æ›¿ä»£LTR

    æ ¸å¿ƒä¼˜åŠ¿ï¼š
    - ç›´æ¥ä¼˜åŒ–è¿ç»­æ”¶ç›Šç‡ï¼Œæ— ä¿¡æ¯æŸå¤±
    - çº¿æ€§æ¨¡å‹ï¼Œè§£é‡Šæ€§å¼º
    - è®­ç»ƒå¿«é€Ÿï¼Œç¨³å®šæ€§å¥½
    - è‡ªåŠ¨ç‰¹å¾æ ‡å‡†åŒ–
    - âœ… æ—¶é—´åºåˆ—CVï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæå‡æ³›åŒ–æ€§èƒ½
    """

    def __init__(self,
                 base_cols: Tuple[str, ...] = ('pred_catboost', 'pred_elastic', 'pred_xgb', 'pred_lightgbm_ranker'),
                 alpha: float = 1.0,
                 fit_intercept: bool = False,
                 solver: str = "auto",
                 tol: float = 1e-6,
                 auto_tune_alpha: bool = False,
                 alpha_grid: Tuple[float, ...] = (0.5, 1.0, 2.0, 3.0, 5.0, 8.0),
                 use_cv: bool = True,
                 use_purged_cv: bool = True,  # ğŸ”¥ ä½¿ç”¨PurgedCVé˜²æ­¢æ•°æ®æ³„éœ²
                 cv_splits: int = 6,  # ğŸ”¥ T+5: 6æŠ˜CV
                 cv_gap_days: int = 5,  # ğŸ”¥ T+5: gap=5
                 cv_embargo_days: int = 5,  # ğŸ”¥ T+5: embargo=5
                 cv_test_size: float = 0.2,
                 use_lambda_percentile: bool = True,  # æ–°å¢ï¼šä½¿ç”¨Lambda percentileç‰¹å¾
                 # ---- Direction calibration & feature augmentation (new) ----
                 direction_calibration: bool = False,  # ğŸ”¥ DISABLED: IC signs reverse across regime changes
                 direction_calibration_min_n: int = 30,
                 add_rank_features: bool = True,
                 random_state: int = 42,
                 # ---- Nested CV & constraints (new) ----
                 use_nested_cv: bool = False,
                 nested_outer_splits: int = 4,
                 nested_gap_days: int = 5,
                 nested_embargo_days: int = 5,
                 aggregate_alpha: str = 'median',  # 'median' | 'ir_weighted'
                 use_convex_constraint: bool = True,  # éè´Ÿä¸”å’Œä¸º1
                 # ğŸ”¥ æ–°å¢: æç«¯targetè¿‡æ»¤
                 filter_extreme_targets: bool = True,  # æ˜¯å¦è¿‡æ»¤æç«¯target
                 extreme_lower_pct: float = 0.5,       # è¿‡æ»¤ä¸‹ç•Œç™¾åˆ†ä½ (ç§»é™¤WW -88%ç­‰)
                 extreme_upper_pct: float = 99.5,      # è¿‡æ»¤ä¸Šç•Œç™¾åˆ†ä½ (ç§»é™¤WW 9900%ç­‰)
                 **kwargs):
        """
        åˆå§‹åŒ–Ridge Stacker

        Args:
            base_cols: ç¬¬ä¸€å±‚æ¨¡å‹é¢„æµ‹åˆ—å
            alpha: Ridgeæ­£åˆ™åŒ–å¼ºåº¦ (é»˜è®¤1.0ï¼Œç®€æ´ç‰ˆ)
            fit_intercept: æ˜¯å¦æ‹Ÿåˆæˆªè· (é»˜è®¤Falseï¼Œå› ä¸ºå·²åšz-score)
            solver: æ±‚è§£å™¨ (é»˜è®¤auto)
            tol: æ”¶æ•›å®¹å·® (é»˜è®¤1e-6)
            auto_tune_alpha: æ˜¯å¦è‡ªåŠ¨è°ƒå‚ (é»˜è®¤Falseï¼Œä¿æŒç®€æ´)
            alpha_grid: è°ƒå‚ç½‘æ ¼ (é»˜è®¤[0.5,1,2,3,5,8])
            use_cv: æ˜¯å¦ä½¿ç”¨äº¤å‰éªŒè¯ (é»˜è®¤True)
            use_purged_cv: ğŸ”¥ æ˜¯å¦ä½¿ç”¨PurgedCV (é»˜è®¤Trueï¼Œé˜²æ­¢æ•°æ®æ³„éœ²)
            cv_splits: CVæŠ˜æ•° (é»˜è®¤6ï¼ŒT+5ä¼˜åŒ–)
            cv_gap_days: ğŸ”¥ CV gapå¤©æ•° (é»˜è®¤5ï¼ŒT+5)
            cv_embargo_days: ğŸ”¥ CV embargoå¤©æ•° (é»˜è®¤5ï¼ŒT+5)
            cv_test_size: æ¯æŠ˜éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤0.2)
            use_lambda_percentile: æ˜¯å¦ä½¿ç”¨Lambda percentileç‰¹å¾ (é»˜è®¤True)
            random_state: éšæœºç§å­
        """
        if not use_cv:
            raise ValueError("RidgeStacker requires cross-validation; disabling CV violates the enforced T+5 protocol.")
        if not use_purged_cv:
            raise ValueError("RidgeStacker requires purged CV to protect the T+5 horizon; do not disable it.")
        if not PURGED_CV_AVAILABLE:
            raise RuntimeError("Unified Purged CV factory is unavailable. Install the required components to enable T+5 training.")
        if (cv_splits, cv_gap_days, cv_embargo_days) != (6, 5, 5):
            raise ValueError("RidgeStacker enforces T+5 CV settings: splits=6, gap=5, embargo=5.")

        self.base_cols = base_cols
        self.alpha = alpha
        self.fit_intercept = fit_intercept
        self.solver = solver
        self.tol = tol
        self.auto_tune_alpha = auto_tune_alpha
        self.alpha_grid = alpha_grid
        self.use_cv = True
        self.use_purged_cv = True
        self.cv_splits = 6
        self.cv_gap_days = 5
        self.cv_embargo_days = 5
        self.cv_test_size = cv_test_size
        self.use_lambda_percentile = use_lambda_percentile
        self.direction_calibration = bool(direction_calibration)
        self.direction_calibration_min_n = int(direction_calibration_min_n)
        self.add_rank_features = bool(add_rank_features)
        self.random_state = random_state
        self.actual_feature_cols_ = None  # ğŸ”§ è®­ç»ƒæ—¶å®é™…ä½¿ç”¨çš„ç‰¹å¾åˆ—ï¼ˆCritical Fixï¼‰
        self.direction_sign_map_ = {}  # col -> +1 / -1
        self.direction_ic_mean_ = {}   # col -> mean per-date RankIC (train window)
        self.output_sign_ = 1.0        # final output direction (score sign)

        # Nested CV & constraints
        self.use_nested_cv = use_nested_cv
        self.nested_outer_splits = nested_outer_splits
        self.nested_gap_days = nested_gap_days
        self.nested_embargo_days = nested_embargo_days
        self.aggregate_alpha = aggregate_alpha
        self.use_convex_constraint = use_convex_constraint

        # ğŸ”¥ æç«¯å€¼è¿‡æ»¤
        self.filter_extreme_targets = filter_extreme_targets
        self.extreme_lower_pct = extreme_lower_pct
        self.extreme_upper_pct = extreme_upper_pct

        # æ¨¡å‹ç»„ä»¶
        self.ridge_model = None
        self.scaler = None
        self.feature_importance_ = None
        self.fitted_ = False

        # è°ƒå‚ç›¸å…³
        self.best_alpha_ = alpha
        self.alpha_scores_ = {}

        # è®­ç»ƒç»Ÿè®¡
        self.train_score_ = None
        self.feature_names_ = None

        logger.info(f"âœ… Ridge Stacker åˆå§‹åŒ–å®Œæˆ (Percentileå¢å¼ºç‰ˆ)")
        logger.info(f"   åŸºç¡€ç‰¹å¾: {self.base_cols}")
        logger.info(f"   Lambda Percentile: {'å¯ç”¨' if self.use_lambda_percentile else 'ç¦ç”¨'}")
        logger.info(f"   æ–¹å‘æ ¡æ­£(IC<0ç¿»è½¬): {'å¯ç”¨' if self.direction_calibration else 'ç¦ç”¨'} (min_n={self.direction_calibration_min_n})")
        logger.info(f"   Rankç‰¹å¾å¢å¼º(æˆªé¢rank_pct): {'å¯ç”¨' if self.add_rank_features else 'ç¦ç”¨'}")
        logger.info(f"   æ­£åˆ™åŒ–å¼ºåº¦Î±: {self.alpha}")
        logger.info(f"   æç«¯targetè¿‡æ»¤: {'å¯ç”¨' if self.filter_extreme_targets else 'ç¦ç”¨'} "
                   f"(é˜ˆå€¼: {self.extreme_lower_pct}%-{self.extreme_upper_pct}%)")
        logger.info(f"   æ‹Ÿåˆæˆªè·: {self.fit_intercept} (å·²åšz-score)")
        logger.info(f"   æ±‚è§£å™¨: {self.solver}, å®¹å·®: {self.tol}")
        logger.info(f"   è‡ªåŠ¨è°ƒå‚: {self.auto_tune_alpha}")
        logger.info(f"   ä½¿ç”¨CV: {self.use_cv}, æŠ˜æ•°: {self.cv_splits}")
        logger.info(f"   åµŒå¥—CV: {'å¯ç”¨' if self.use_nested_cv else 'ç¦ç”¨'} (outer_splits={self.nested_outer_splits})")
        logger.info(f"   ç‰¹å¾æ ‡å‡†åŒ–: æ¨ªæˆªé¢z-score")

    def _validate_input(self, df: pd.DataFrame) -> pd.DataFrame:
        """éªŒè¯è¾“å…¥æ•°æ®æ ¼å¼å¹¶ç¡®ä¿åˆ—é¡ºåºä¸€è‡´"""
        if not isinstance(df.index, pd.MultiIndex):
            raise ValueError("è¾“å…¥æ•°æ®å¿…é¡»å…·æœ‰MultiIndex(date, ticker)")

        if df.index.names != ['date', 'ticker']:
            raise ValueError(f"Index nameså¿…é¡»æ˜¯['date', 'ticker'], å®é™…: {df.index.names}")

        missing_cols = [col for col in self.base_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿åˆ—é¡ºåºä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
        return df[list(self.base_cols) + [col for col in df.columns if col not in self.base_cols]]

    def _prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆå«æ–¹å‘æ ¡æ­£ + rankå¢å¼ºï¼‰"""
        if not isinstance(df.index, pd.MultiIndex) or df.index.names != ['date', 'ticker']:
            raise ValueError("RidgeStacker expects MultiIndex(date, ticker)")

        # ---- label ----
        label_cols = [col for col in df.columns if col.startswith('ret_fwd')]
        if not label_cols:
            raise ValueError("æœªæ‰¾åˆ°æ ‡ç­¾åˆ— (ret_fwd_*)")
        label_col = label_cols[0]

        # ---- raw base features ----
        base_feature_cols = list(self.base_cols)
        if self.use_lambda_percentile and 'lambda_percentile' in df.columns:
            base_feature_cols.append('lambda_percentile')

        # Work on a copy to avoid mutating caller data
        feat = df[base_feature_cols + [label_col]].copy()

        # Drop NaNs early for stable IC estimation & fit
        valid_mask = ~feat.isna().any(axis=1)
        feat = feat.loc[valid_mask].copy()
        if feat.empty:
            raise ValueError("No valid rows after dropping NaNs for ridge stacker features/label.")

        # ---- direction calibration: ensure higher score => higher return ----
        if self.direction_calibration:
            # Compute per-date RankIC means for each base column
            self.direction_sign_map_ = {}
            self.direction_ic_mean_ = {}
            for col in base_feature_cols:
                # per-date Spearman
                ics = []
                for _, g in feat[[col, label_col]].groupby(level='date'):
                    if len(g) < self.direction_calibration_min_n:
                        continue
                    ic = spearmanr(g[col].values, g[label_col].values)[0]
                    if ic is None or np.isnan(ic):
                        continue
                    ics.append(float(ic))
                if len(ics) == 0:
                    # fallback to global Spearman
                    ic_global = spearmanr(feat[col].values, feat[label_col].values)[0]
                    ic_mean = float(ic_global) if ic_global is not None and not np.isnan(ic_global) else 0.0
                else:
                    ic_mean = float(np.nanmean(ics))
                sign = -1.0 if ic_mean < 0 else 1.0
                self.direction_sign_map_[col] = float(sign)
                self.direction_ic_mean_[col] = float(ic_mean)
                if sign < 0:
                    feat[col] = -feat[col]
            neg = [k for k, v in self.direction_sign_map_.items() if v < 0]
            logger.info(f"ğŸ§­ æ–¹å‘æ ¡æ­£å®Œæˆ: flipped={neg} (n={len(neg)})")

        # ---- feature augmentation: cross-sectional ranks ----
        feature_cols = list(base_feature_cols)
        if self.add_rank_features:
            for col in base_feature_cols:
                rank_col = f"{col}__rank_pct"
                feat[rank_col] = feat.groupby(level='date')[col].rank(pct=True)
                feature_cols.append(rank_col)

        # ğŸ”§ Critical Fix: ä¿å­˜å®é™…ä½¿ç”¨çš„ç‰¹å¾åˆ—ï¼ˆä»…åœ¨è®­ç»ƒæ—¶ï¼Œå³é¦–æ¬¡è°ƒç”¨ï¼‰
        if self.actual_feature_cols_ is None:
            self.actual_feature_cols_ = list(feature_cols)
            logger.info(f"ğŸ”§ ä¿å­˜å®é™…ç‰¹å¾åˆ—: {self.actual_feature_cols_}")

        X = feat[feature_cols].values
        y = feat[label_col].values

        logger.info(f"   åŸå§‹æ ·æœ¬: {len(df)}")
        logger.info(f"   æœ‰æ•ˆæ ·æœ¬: {len(feat)}")
        logger.info(f"   æ•°æ®è¦†ç›–ç‡: {len(feat)/max(len(df),1)*100:.1f}%")

        return X, y

    def _winsorize_labels(self, y: np.ndarray, lower_pct: float = 1.0, upper_pct: float = 99.0) -> np.ndarray:
        """Winsorizeæ ‡ç­¾ï¼Œå¤„ç†æç«¯å€¼"""
        lower_bound = np.percentile(y, lower_pct)
        upper_bound = np.percentile(y, upper_pct)
        y_winsorized = np.clip(y, lower_bound, upper_bound)

        n_clipped = np.sum((y != y_winsorized))
        if n_clipped > 0:
            logger.info(f"   Winsorize: {n_clipped}/{len(y)} ({n_clipped/len(y)*100:.1f}%) æ ·æœ¬è¢«è£å‰ª")

        return y_winsorized

    def _filter_extreme_targets(
        self,
        df: pd.DataFrame,
        X: np.ndarray,
        y: np.ndarray,
        lower_pct: float = 0.5,
        upper_pct: float = 99.5
    ) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:
        """
        è¿‡æ»¤targetæç«¯å€¼çš„æ ·æœ¬ï¼Œå®Œå…¨ç§»é™¤è€Œä¸æ˜¯winsorize

        ç”¨äºç§»é™¤ç±»ä¼¼WWè¿™ç§æç«¯outliers (9900%æ¶¨å¹…æˆ–-88%è·Œå¹…)ï¼Œ
        é˜²æ­¢æ¨¡å‹è®­ç»ƒè¢«æç«¯å€¼æ‰­æ›²ã€‚

        Parameters:
        -----------
        df : pd.DataFrame
            åŸå§‹DataFrameï¼ˆå«MultiIndexï¼‰
        X : np.ndarray
            ç‰¹å¾çŸ©é˜µ
        y : np.ndarray
            targetæ•°ç»„
        lower_pct : float
            ä¸‹ç•Œç™¾åˆ†ä½æ•°ï¼ˆé»˜è®¤0.5%ï¼Œç§»é™¤æœ€æç«¯ä¸‹è·Œï¼‰
        upper_pct : float
            ä¸Šç•Œç™¾åˆ†ä½æ•°ï¼ˆé»˜è®¤99.5%ï¼Œç§»é™¤æœ€æç«¯ä¸Šæ¶¨ï¼‰

        Returns:
        --------
        df_filtered : pd.DataFrame
            è¿‡æ»¤åçš„DataFrame
        X_filtered : np.ndarray
            è¿‡æ»¤åçš„ç‰¹å¾çŸ©é˜µ
        y_filtered : np.ndarray
            è¿‡æ»¤åçš„targetæ•°ç»„

        Examples:
        ---------
        >>> # ç§»é™¤WWè¿™ç§9900%çš„æç«¯æ¶¨å¹…å’Œ-88%çš„æç«¯è·Œå¹…
        >>> df_filt, X_filt, y_filt = self._filter_extreme_targets(
        ...     df, X, y, lower_pct=0.5, upper_pct=99.5
        ... )
        """
        # è®¡ç®—é˜ˆå€¼
        lower_bound = np.percentile(y, lower_pct)
        upper_bound = np.percentile(y, upper_pct)

        # åˆ›å»ºmask: ä¿ç•™åœ¨é˜ˆå€¼èŒƒå›´å†…çš„æ ·æœ¬
        mask = (y >= lower_bound) & (y <= upper_bound)

        # è¿‡æ»¤
        df_filtered = df.iloc[mask].copy()
        X_filtered = X[mask]
        y_filtered = y[mask]

        # ç»Ÿè®¡
        n_removed = np.sum(~mask)
        n_total = len(y)
        pct_removed = n_removed / n_total * 100

        logger.info(f"ğŸ” è¿‡æ»¤æç«¯target:")
        logger.info(f"   é˜ˆå€¼èŒƒå›´: [{lower_bound*100:.2f}%, {upper_bound*100:.2f}%]")
        logger.info(f"   ç§»é™¤æ ·æœ¬: {n_removed}/{n_total} ({pct_removed:.2f}%)")

        if n_removed > 0:
            # æ˜¾ç¤ºè¢«ç§»é™¤çš„æç«¯å€¼ç»Ÿè®¡
            extreme_targets = y[~mask]
            extreme_min = extreme_targets.min() * 100
            extreme_max = extreme_targets.max() * 100
            logger.info(f"   æç«¯å€¼èŒƒå›´: [{extreme_min:.2f}%, {extreme_max:.2f}%]")

            # æ˜¾ç¤ºæœ€æç«¯çš„å‡ ä¸ª
            sorted_extremes = np.sort(extreme_targets)
            n_show = min(5, len(sorted_extremes))
            if n_show > 0:
                bottom_extremes = sorted_extremes[:n_show] * 100
                top_extremes = sorted_extremes[-n_show:] * 100
                logger.info(f"   æœ€ä½{n_show}ä¸ª: [{', '.join([f'{x:.2f}%' for x in bottom_extremes])}]")
                logger.info(f"   æœ€é«˜{n_show}ä¸ª: [{', '.join([f'{x:.2f}%' for x in top_extremes])}]")

        return df_filtered, X_filtered, y_filtered

    def _calculate_rank_ic(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """è®¡ç®—RankIC"""
        try:
            return spearmanr(y_true, y_pred)[0]
        except:
            return 0.0

    def _create_purged_cv_split(self, dates: pd.Series):
        """ğŸ”¥ ä½¿ç”¨ç»Ÿä¸€çš„ PurgedCV åˆ†å‰²ï¼Œé˜²æ­¢æ•°æ®æ³„éœ²"""
        if not self.use_purged_cv:
            raise RuntimeError('PurgedCV is mandatory for RidgeStacker T+5 training.')

        cv = create_unified_cv(
            n_splits=self.cv_splits,
            gap=self.cv_gap_days,
            embargo=self.cv_embargo_days
        )

        unique_dates = sorted(dates.unique())
        if not unique_dates:
            raise ValueError('PurgedCV requires non-empty date information.')
        date_to_idx = {date: i for i, date in enumerate(unique_dates)}
        groups = dates.map(date_to_idx).values

        logger.info(f"âœ… ä½¿ç”¨PurgedCV: splits={self.cv_splits}, gap={self.cv_gap_days}, embargo={self.cv_embargo_days}")
        logger.info(f"   æ—¶é—´èŒƒå›´: {unique_dates[0]} ~ {unique_dates[-1]} ({len(unique_dates)}å¤©)")

        return cv.split(X=np.zeros((len(dates), 1)), y=None, groups=groups)

    def _create_purged_cv_split_params(self, dates: pd.Series, n_splits: int, gap: int, embargo: int):
        """æ ¹æ®æŒ‡å®šå‚æ•°åˆ›å»ºPurgedCVåˆ†å‰²ã€‚"""
        if not PURGED_CV_AVAILABLE:
            raise RuntimeError('Unified Purged CV factory unavailable.')
        cv = create_unified_cv(n_splits=n_splits, gap=gap, embargo=embargo)
        unique_dates = sorted(dates.unique())
        if not unique_dates:
            raise ValueError('PurgedCV requires non-empty date information.')
        date_to_idx = {date: i for i, date in enumerate(unique_dates)}
        groups = dates.map(date_to_idx).values
        logger.info(f"âœ… ä½¿ç”¨PurgedCV(n_splits={n_splits}, gap={gap}, embargo={embargo})")
        return cv.split(X=np.zeros((len(dates), 1)), y=None, groups=groups)

    def _auto_tune_alpha(self, X: np.ndarray, y: np.ndarray, df: pd.DataFrame, dates: pd.Series = None) -> float:
        """
        è‡ªåŠ¨è°ƒå‚é€‰æ‹©æœ€ä¼˜alpha - å¢å¼ºCVç‰ˆæœ¬
        """
        if not self.auto_tune_alpha:
            return self.alpha

        logger.info(f"ğŸ¯ å¼€å§‹è‡ªåŠ¨è°ƒå‚ï¼Œç½‘æ ¼: {self.alpha_grid}")

        if not self.use_cv:
            raise RuntimeError("RidgeStacker requires PurgedCV-based CV for T+5 training.")

        return self._auto_tune_alpha_with_cv(X, y, dates)

    def _auto_tune_alpha_with_cv(self, X: np.ndarray, y: np.ndarray, dates: pd.Series = None) -> float:
        """
        ä½¿ç”¨CVè¿›è¡Œè°ƒå‚ï¼ˆä¸¥æ ¼PurgedCVï¼‰
        """
        if dates is None:
            raise ValueError("PurgedCV tuning requires aligned date information.")
        if not self.use_purged_cv:
            raise RuntimeError("PurgedCV must be enabled for alpha tuning.")

        logger.info(f"   ä½¿ç”¨PurgedCVï¼ŒæŠ˜æ•°: {self.cv_splits}")

        best_alpha = self.alpha
        best_score = -999

        cv_splits = list(self._create_purged_cv_split(dates))

        for alpha in self.alpha_grid:
            cv_scores = []

            for fold, (train_idx, test_idx) in enumerate(cv_splits):
                X_train, X_test = X[train_idx], X[test_idx]
                y_train, y_test = y[train_idx], y[test_idx]

                model = Ridge(
                    alpha=alpha,
                    fit_intercept=self.fit_intercept,
                    solver=self.solver,
                    tol=self.tol,
                    random_state=self.random_state
                )
                model.fit(X_train, y_train)

                y_pred = model.predict(X_test)
                rank_ic = self._calculate_rank_ic(y_test, y_pred)
                cv_scores.append(rank_ic)

            avg_score = np.mean(cv_scores)
            std_score = np.std(cv_scores)
            self.alpha_scores_[alpha] = avg_score

            logger.info(f"   Î±={alpha}: CV RankIC={avg_score:.4f} (Â±{std_score:.4f})")

            if avg_score > best_score:
                best_score = avg_score
                best_alpha = alpha

        logger.info(f"âœ… æœ€ä¼˜Î±: {best_alpha}, CV RankIC: {best_score:.4f}")
        return best_alpha

    def _project_to_simplex(self, w: np.ndarray) -> np.ndarray:
        """å°†å‘é‡æŠ•å½±åˆ°æ¦‚ç‡å•çº¯å½¢ï¼šw>=0, sum(w)=1ã€‚"""
        if w.ndim != 1:
            w = w.flatten()
        # Algorithm from: Efficient Projections onto the l1-Ball for Learning in High Dimensions (Duchi et al.)
        u = np.sort(np.maximum(w, 0))[::-1]
        cssv = np.cumsum(u)
        rho = np.nonzero(u * np.arange(1, len(u) + 1) > (cssv - 1))[0]
        if len(rho) == 0:
            # all zeros, return uniform
            m = len(w)
            return np.full(m, 1.0 / m)
        rho = rho[-1]
        theta = (cssv[rho] - 1) / (rho + 1.0)
        w_proj = np.maximum(w - theta, 0)
        s = w_proj.sum()
        if s <= 0:
            m = len(w)
            return np.full(m, 1.0 / m)
        return w_proj / s

    def _fit_ridge_with_constraint(self, X: np.ndarray, y: np.ndarray, alpha: float, fit_intercept: bool = False) -> Tuple[np.ndarray, float]:
        """æ‹ŸåˆRidgeå¹¶å¯¹æƒé‡æŠ•å½±åˆ°éè´Ÿä¸”å’Œä¸º1çš„å•çº¯å½¢ã€‚"""
        model = Ridge(
            alpha=alpha,
            fit_intercept=fit_intercept,
            solver=self.solver,
            tol=self.tol,
            random_state=self.random_state
        )
        model.fit(X, y)
        w = np.asarray(model.coef_).flatten()
        b = float(model.intercept_) if fit_intercept else 0.0
        if self.use_convex_constraint:
            w = self._project_to_simplex(np.maximum(w, 0))
            b = 0.0  # ä¿æŒå¯è§£é‡Šæ€§ï¼šå‡¸ç»„åˆé€šå¸¸ä¸éœ€è¦æˆªè·
        return w, b

    def _inner_tune_alpha(self, X_train: np.ndarray, y_train: np.ndarray, dates_train: pd.Series) -> float:
        """åœ¨å¤–å±‚è®­ç»ƒæ®µä¸Šè¿›è¡Œå†…å±‚PurgedCVè°ƒå‚ã€‚"""
        best_alpha = None
        best_score = -1e9
        # dates_train å¯èƒ½æ˜¯ Series/Index/ndarrayï¼Œç»Ÿä¸€ä¸ºSeries
        if not isinstance(dates_train, pd.Series):
            dates_train = pd.Series(np.asarray(dates_train))
        inner_splits = list(self._create_purged_cv_split_params(dates_train, self.cv_splits, self.cv_gap_days, self.cv_embargo_days))
        for alpha in self.alpha_grid:
            scores = []
            for (tr_idx, te_idx) in inner_splits:
                # ä½¿ç”¨ä»…åŸºäºè®­ç»ƒçš„æ ‡å‡†åŒ–ï¼Œé¿å…æ³„éœ²
                scaler = StandardScaler()
                X_tr = scaler.fit_transform(X_train[tr_idx])
                X_te = scaler.transform(X_train[te_idx])
                w, b = self._fit_ridge_with_constraint(X_tr, y_train[tr_idx], alpha, fit_intercept=self.fit_intercept)
                y_hat = X_te @ w + b
                scores.append(self._calculate_rank_ic(y_train[te_idx], y_hat))
            avg = float(np.nanmean(scores)) if scores else -1e9
            self.alpha_scores_[alpha] = avg
            if avg > best_score:
                best_score = avg
                best_alpha = alpha
        return best_alpha if best_alpha is not None else self.alpha

    def _nested_cv_oof(self, X: np.ndarray, y: np.ndarray, dates: pd.Series) -> Tuple[np.ndarray, List[float], List[float]]:
        """
        å¤–å±‚PurgedCVç”ŸæˆOOFé¢„æµ‹ï¼›å†…å±‚åœ¨å¤–å±‚è®­ç»ƒæ®µæ—¶é—´CVé€‰æ‹©alphaã€‚
        è¿”å›: y_oof_pred, alphas_per_fold, ic_per_fold
        """
        n = len(y)
        y_oof = np.full(n, np.nan, dtype=float)
        alphas = []
        ics = []
        outer_splits = list(self._create_purged_cv_split_params(dates, self.nested_outer_splits, self.nested_gap_days, self.nested_embargo_days))
        for k, (train_idx, test_idx) in enumerate(outer_splits):
            # å†…å±‚è°ƒå‚ï¼ˆdates å¯èƒ½æ˜¯ DatetimeIndex/Seriesï¼Œç»Ÿä¸€è½¬æ¢ä¸ºnumpyæ•°ç»„å†æŒ‰ä½ç½®ç´¢å¼•ï¼‰
            dates_array = np.asarray(dates)
            dates_train = pd.Series(dates_array[train_idx])
            alpha_k = self._inner_tune_alpha(X[train_idx], y[train_idx], dates_train)
            alphas.append(alpha_k)
            # åŸºäºè®­ç»ƒæ®µæ‹Ÿåˆï¼Œå¹¶åœ¨æµ‹è¯•æ®µé¢„æµ‹
            scaler = StandardScaler()
            X_tr = scaler.fit_transform(X[train_idx])
            X_te = scaler.transform(X[test_idx])
            w, b = self._fit_ridge_with_constraint(X_tr, y[train_idx], alpha_k, fit_intercept=self.fit_intercept)
            y_hat = X_te @ w + b
            y_oof[test_idx] = y_hat
            ic_k = self._calculate_rank_ic(y[test_idx], y_hat)
            ics.append(float(ic_k))
            logger.info(f"[NestedCV] outer_fold={k+1}/{len(outer_splits)} alpha={alpha_k} OOS RankIC={ic_k:.4f}")
        return y_oof, alphas, ics

    def fit(self, df: pd.DataFrame, max_train_to_today: bool = True) -> "RidgeStacker":
        """
        è®­ç»ƒRidgeäºŒå±‚Stackerã€‚è¾“å…¥ä¸ºåŒ…å«ç¬¬ä¸€å±‚é¢„æµ‹ä¸æ ‡ç­¾çš„DataFrameã€‚

        å¿…éœ€åˆ—: self.base_cols ä¸­çš„åˆ—ï¼Œä»¥åŠä¸€ä¸ªä»¥ 'ret_fwd' å¼€å¤´çš„æ ‡ç­¾åˆ—ã€‚
        ç´¢å¼•: éœ€è¦MultiIndex(date, ticker)ã€‚
        """
        df_validated = self._validate_input(df)
        X, y = self._prepare_features(df_validated)

        # ğŸ”¥ æ–°å¢: å…ˆè¿‡æ»¤æç«¯targetæ ·æœ¬ï¼ˆå¦‚WWçš„9900%æ¶¨å¹…ï¼‰
        if self.filter_extreme_targets:
            df_filtered, X_filtered, y_filtered = self._filter_extreme_targets(
                df_validated,
                X,
                y,
                lower_pct=self.extreme_lower_pct,
                upper_pct=self.extreme_upper_pct
            )
        else:
            df_filtered, X_filtered, y_filtered = df_validated, X, y
            logger.info("âš ï¸ æç«¯targetè¿‡æ»¤å·²ç¦ç”¨")

        # æ ‡ç­¾Winsorizeï¼Œå¢å¼ºç¨³å¥æ€§ï¼ˆåœ¨è¿‡æ»¤åçš„æ•°æ®ä¸Šæ“ä½œï¼‰
        y_proc = self._winsorize_labels(y_filtered, lower_pct=1.0, upper_pct=99.0)

        # ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆä½¿ç”¨è¿‡æ»¤åçš„æ•°æ®ï¼‰
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X_filtered)

        # ä½¿ç”¨è¿‡æ»¤åçš„dates
        dates = df_filtered.index.get_level_values('date')

        if self.use_nested_cv:
            # åµŒå¥—CVï¼šç”ŸæˆOOFå¹¶èšåˆalphaï¼Œç„¶ååœ¨å…¨é‡ä¸Šå†è®­ç»ƒä¸€æ¬¡
            logger.info("ğŸš€ ä½¿ç”¨ä¸¥è°¨åµŒå¥—CVè®­ç»ƒäºŒå±‚ (å¤–å±‚æ»šåŠ¨ + å†…å±‚PurgedCVè°ƒå‚)")
            # ä½¿ç”¨æœªç¼©æ”¾Xè¿›è¡ŒåµŒå¥—CVï¼Œå†…éƒ¨å„foldå•ç‹¬æ ‡å‡†åŒ–
            y_oof, alphas, ics = self._nested_cv_oof(X_filtered, y_proc, dates)
            # é€‰æ‹©èšåˆalpha
            if self.aggregate_alpha == 'ir_weighted' and len(ics) > 1 and np.nanstd(ics) > 0:
                weights = np.maximum(np.array(ics), 0)
                if weights.sum() == 0:
                    self.best_alpha_ = float(np.median(alphas))
                else:
                    self.best_alpha_ = float(np.average(alphas, weights=weights))
            else:
                self.best_alpha_ = float(np.median(alphas)) if len(alphas) > 0 else self.alpha
            logger.info(f"[NestedCV] èšåˆalpha={self.best_alpha_} (strategy={self.aggregate_alpha})")
            # åœ¨å…¨é‡ä¸Šæ‹Ÿåˆæœ€ç»ˆæ¨¡å‹ï¼ˆå…ˆå…¨é‡æ ‡å‡†åŒ–ï¼Œå†æ‹Ÿåˆ+æŠ•å½±ï¼‰
            self.scaler = StandardScaler()
            X_scaled = self.scaler.fit_transform(X_filtered)
            w, b = self._fit_ridge_with_constraint(X_scaled, y_proc, self.best_alpha_, fit_intercept=self.fit_intercept)
            # å­˜å…¥sklearn Ridgeå¯¹è±¡ä»¥ä¿æŒæ¥å£ä¸€è‡´
            self.ridge_model = Ridge(alpha=self.best_alpha_, fit_intercept=False)
            # ç¡®ä¿coef_ä¸º1Dï¼Œé¿å…predictè¾“å‡º(n,1)é€ æˆä¸‹æ¸¸"Data must be 1-dimensional"é”™è¯¯
            self.ridge_model.coef_ = w.reshape(-1)
            self.ridge_model.intercept_ = 0.0
            # æ˜¾å¼è®¾ç½®ç‰¹å¾ç»´åº¦ï¼Œæå‡sklearnä¸€è‡´æ€§
            try:
                self.ridge_model.n_features_in_ = X_scaled.shape[1]
            except Exception:
                pass
        else:
            # å¯é€‰ï¼šè‡ªåŠ¨è°ƒå‚ï¼ˆå•å±‚CVï¼‰
            if self.auto_tune_alpha:
                self.best_alpha_ = self._auto_tune_alpha(X_scaled, y_proc, df_validated, dates)
            else:
                self.best_alpha_ = self.alpha

            # ä½¿ç”¨Ridgeè®­ç»ƒ
            self.ridge_model = Ridge(
                alpha=self.best_alpha_,
                fit_intercept=self.fit_intercept,
                solver=self.solver,
                tol=self.tol,
                random_state=self.random_state
            )
            self.ridge_model.fit(X_scaled, y_proc)
            # è®­ç»ƒåè¿›è¡Œå‡¸çº¦æŸæŠ•å½±ï¼ˆå¯é€‰ï¼‰
            if self.use_convex_constraint:
                w = np.asarray(self.ridge_model.coef_).flatten()
                w = self._project_to_simplex(np.maximum(w, 0))
                self.ridge_model.coef_ = w.reshape(-1)
                if self.fit_intercept:
                    self.ridge_model.intercept_ = 0.0
            try:
                self.ridge_model.n_features_in_ = X_scaled.shape[1]
            except Exception:
                pass

        # è®­ç»ƒåˆ†æ•°ä¸ç‰¹å¾é‡è¦æ€§
        y_pred_train = self.ridge_model.predict(X_scaled)
        train_ic = float(self._calculate_rank_ic(y_proc, y_pred_train))
        # Final direction calibration: ensure Ridge score higher => higher return on train window.
        # This is robust even if base preds look aligned but the learned combination is inverted.
        self.output_sign_ = -1.0 if (self.direction_calibration and train_ic < 0) else 1.0
        if self.output_sign_ < 0:
            y_pred_train = -y_pred_train
            train_ic = float(self._calculate_rank_ic(y_proc, y_pred_train))
            logger.info("ğŸ§­ Ridgeè¾“å‡ºæ–¹å‘æ ¡æ­£: output_sign=-1 (train RankIC flipped to %.4f)", train_ic)
        self.train_score_ = train_ic
        self.feature_names_ = list(self.actual_feature_cols_ or self.base_cols)
        # çº¿æ€§æ¨¡å‹çš„é‡è¦æ€§å¯ç”¨ç³»æ•°ç»å¯¹å€¼
        try:
            coefs = np.asarray(self.ridge_model.coef_).flatten()
            self.feature_importance_ = {name: float(abs(w)) for name, w in zip(self.feature_names_, coefs)}
        except Exception:
            self.feature_importance_ = None

        self.fitted_ = True
        return self

    def predict(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„RidgeStackerè¿›è¡Œé¢„æµ‹ã€‚
        è¿”å›åŒ…å«'score'åˆ—çš„DataFrameï¼ˆç´¢å¼•ä¸è¾“å…¥å¯¹é½ï¼‰ã€‚
        """
        if not self.fitted_ or self.ridge_model is None or self.scaler is None:
            raise RuntimeError("RidgeStacker not fitted")

        # ç¡®ä¿åˆ—é¡ºåºä¸è®­ç»ƒä¸€è‡´ï¼›å…è®¸ç¼ºå¤±æ ‡ç­¾åˆ—
        if not isinstance(df.index, pd.MultiIndex) or df.index.names != ['date', 'ticker']:
            raise ValueError("é¢„æµ‹æ•°æ®å¿…é¡»å…·æœ‰MultiIndex(date, ticker)")

        # Build required features (supports derived cols like "__rank_pct")
        if not isinstance(df.index, pd.MultiIndex) or df.index.names != ['date', 'ticker']:
            raise ValueError("é¢„æµ‹æ•°æ®å¿…é¡»å…·æœ‰MultiIndex(date, ticker)")

        feature_cols = list(self.actual_feature_cols_ or self.base_cols)
        base_needed = list(self.base_cols)
        if self.use_lambda_percentile and 'lambda_percentile' in feature_cols and 'lambda_percentile' not in base_needed:
            base_needed.append('lambda_percentile')

        missing_base = [c for c in base_needed if c not in df.columns]
        if missing_base:
            raise ValueError(f"é¢„æµ‹ç¼ºå°‘å¿…éœ€åŸºç¡€ç‰¹å¾åˆ—: {missing_base}")

        feat = df[base_needed].copy()

        # Apply persisted direction sign-map if available
        if getattr(self, 'direction_sign_map_', None):
            for col, s in self.direction_sign_map_.items():
                if col in feat.columns and float(s) < 0:
                    feat[col] = -feat[col]

        # Generate derived features if required by trained model
        for col in feature_cols:
            if col in feat.columns:
                continue
            if col.endswith("__rank_pct"):
                raw = col.replace("__rank_pct", "")
                if raw not in feat.columns:
                    raise ValueError(f"æ— æ³•ç”Ÿæˆæ´¾ç”Ÿç‰¹å¾ {col}: ç¼ºå°‘åŸå§‹åˆ— {raw}")
                feat[col] = feat.groupby(level='date')[raw].rank(pct=True)
                continue
            raise ValueError(f"é¢„æµ‹ç¼ºå°‘å¿…éœ€ç‰¹å¾åˆ—: {col}")

        X = feat[feature_cols].values
        X_scaled = self.scaler.transform(X)
        y_pred = self.ridge_model.predict(X_scaled)
        if float(getattr(self, "output_sign_", 1.0) or 1.0) < 0:
            y_pred = -y_pred
        return pd.DataFrame({'score': y_pred}, index=df.index)

    def replace_ewa_in_pipeline(self, df: pd.DataFrame) -> pd.DataFrame:
        """Compatibility shim for the legacy EWA interface used in the pipeline."""
        if not self.fitted_ or self.ridge_model is None or self.scaler is None:
            raise RuntimeError('RidgeStacker not fitted; call fit before replace_ewa_in_pipeline.')
        if not isinstance(df, pd.DataFrame):
            raise TypeError('replace_ewa_in_pipeline expects a pandas DataFrame.')
        if df.empty:
            raise ValueError('replace_ewa_in_pipeline received an empty DataFrame.')
        if not isinstance(df.index, pd.MultiIndex) or df.index.names != ['date', 'ticker']:
            raise ValueError('Input to replace_ewa_in_pipeline must have MultiIndex(date, ticker).')

        feature_cols = list(self.actual_feature_cols_ or self.base_cols)
        sanitized = df.copy()
        label_cols = [col for col in sanitized.columns if col.startswith('ret_fwd')]
        if label_cols:
            sanitized = sanitized.drop(columns=label_cols)

        missing = [col for col in feature_cols if col not in sanitized.columns]
        if missing:
            raise ValueError(f'replace_ewa_in_pipeline is missing required features: {missing}')

        sanitized = sanitized[feature_cols]
        valid_mask = ~sanitized.isna().any(axis=1)
        if not valid_mask.any():
            raise ValueError('replace_ewa_in_pipeline found no valid rows after dropping NaNs.')

        if not valid_mask.all():
            logger.debug('replace_ewa_in_pipeline dropping %d rows with NaN features', (~valid_mask).sum())

        valid_features = sanitized.loc[valid_mask]
        predictions = self.predict(valid_features)

        if valid_mask.all():
            return predictions

        full_output = pd.DataFrame(index=sanitized.index, columns=predictions.columns, dtype=float)
        full_output.loc[valid_mask] = predictions.values
        return full_output

    def get_model_info(self) -> Dict[str, any]:
        return {
            'alpha': float(self.best_alpha_),
            'use_lambda_percentile': bool(self.use_lambda_percentile),
            'direction_calibration': bool(self.direction_calibration),
            'direction_sign_map': dict(getattr(self, 'direction_sign_map_', {}) or {}),
            'direction_ic_mean': dict(getattr(self, 'direction_ic_mean_', {}) or {}),
            'output_sign': float(getattr(self, 'output_sign_', 1.0) or 1.0),
            'add_rank_features': bool(self.add_rank_features),
            'train_rank_ic': float(self.train_score_) if self.train_score_ is not None else None,
            'feature_importance': self.feature_importance_,
            'features': list(self.feature_names_ or []),
            'n_iterations': 1
        }