#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¶…çº§æé€Ÿç‰ˆæƒ…æ„Ÿå› å­ - çœŸå®æ–°é—»æ•°æ®ç‰ˆæœ¬
==========================================
- æ¯æ—¥å¤„ç†3æ¡æœ€é‡è¦æ–°é—»
- æ¯æ¡æ–°é—»45è¯
- ä½¿ç”¨çœŸå®Polygon APIæ•°æ®
- æ— éšæœºæ•°æ®ç”Ÿæˆ
"""

import os
import sys
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from polygon import RESTClient
except ImportError:
    print("é”™è¯¯: polygon-api-clientæœªå®‰è£…ã€‚éœ€è¦å®‰è£…: pip install polygon-api-client")
    RESTClient = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class UltraFastSentimentFactor:
    """
    è¶…çº§æé€Ÿæƒ…æ„Ÿå› å­åˆ†æå™¨
    - æ¯æ—¥3æ¡æœ€é‡è¦æ–°é—»
    - æ¯æ¡æ–°é—»45è¯
    - ä½¿ç”¨çœŸå®Polygon APIæ•°æ®
    - æ— éšæœºæ•°æ®
    """

    def __init__(self, polygon_api_key: str = None):
        """åˆå§‹åŒ–è¶…çº§æé€Ÿç‰ˆ - ä»…æ”¯æŒçœŸå®æ•°æ®"""

        # ä¸¥æ ¼éªŒè¯ï¼šå¿…é¡»æœ‰APIå¯†é’¥å’Œæ¨¡å‹æ‰èƒ½å·¥ä½œ
        if not polygon_api_key:
            logger.warning("è­¦å‘Š: æœªæä¾›Polygon APIå¯†é’¥ã€‚æŸäº›åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")

        if not RESTClient:
            raise ImportError("éœ€è¦å®‰è£…polygon-api-client: pip install polygon-api-client")

        self.polygon_api_key = polygon_api_key
        self.client = None
        self.tokenizer = None
        self.model = None

        # BMAå…¼å®¹æ€§ï¼šå®šä¹‰æƒ…æ„Ÿç‰¹å¾åˆ—è¡¨ï¼ˆä»…ä½¿ç”¨sentiment_scoreï¼‰
        self.sentiment_features = ['sentiment_score']

        if polygon_api_key:
            try:
                logger.info(f"Initializing Polygon client with key: {polygon_api_key[:8]}...")
                self.client = RESTClient(polygon_api_key)
                logger.info("âœ“ Polygonå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ - å°†ä½¿ç”¨çœŸå®æ–°é—»æ•°æ®")
            except Exception as e:
                logger.error(f"Polygonå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                raise ValueError("æ— æ³•åˆå§‹åŒ–Polygonå®¢æˆ·ç«¯ã€‚è¯·æ£€æŸ¥APIå¯†é’¥ã€‚")
        else:
            logger.warning("No API key provided to UltraFastSentimentFactor")

        self._init_lightweight_model()

    def _init_lightweight_model(self):
        """åˆå§‹åŒ–è½»é‡çº§æ¨¡å‹é…ç½®"""
        try:
            model_name = "ProsusAI/finbert"
            logger.info(f"æ­£åœ¨åŠ è½½ {model_name} æ¨¡å‹...")

            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_name)

            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            if torch.cuda.is_available():
                self.model = self.model.cuda()
                logger.info("âœ“ ä½¿ç”¨GPUåŠ é€Ÿ")
            else:
                logger.info("âœ“ ä½¿ç”¨CPUæ¨ç†")

            logger.info("âœ“ FinBERTæ¨¡å‹åŠ è½½æˆåŠŸ - å‡†å¤‡è¿›è¡ŒçœŸå®æƒ…æ„Ÿåˆ†æ")
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise RuntimeError(f"æ— æ³•åŠ è½½FinBERTæ¨¡å‹: {e}")

    def calculate_ultra_fast_sentiment(self, tickers: List[str],
                                     start_date: str,
                                     end_date: str,
                                     max_news_per_day: int = 2,
                                     enable_quality_monitoring: bool = True) -> pd.DataFrame:
        """
        è¶…çº§æé€Ÿè®¡ç®—sentiment_score

        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            max_news_per_day: æ¯æ—¥æœ€å¤§æ–°é—»æ•°ï¼ˆè¶…çº§æé€Ÿ=2ï¼‰
            enable_quality_monitoring: å¯ç”¨è´¨é‡ç›‘æ§

        Returns:
            åŒ…å«sentiment_scoreçš„DataFrame
        """
        logger.info("=" * 60)
        logger.info("è¶…çº§æé€Ÿsentiment_scoreè®¡ç®—")
        logger.info("=" * 60)
        logger.info(f"è‚¡ç¥¨æ•°é‡: {len(tickers)}")
        logger.info(f"æ—¥æœŸèŒƒå›´: {start_date} - {end_date}")
        logger.info(f"æ¯æ—¥æ–°é—»: {max_news_per_day} (è¶…çº§æé€Ÿä¼˜åŒ–)")
        logger.info("æ–‡æœ¬é•¿åº¦: 80å­—ç¬¦")
        logger.info("æ¨¡å‹è¾“å…¥: 64 tokens")

        all_data = []
        processed_count = 0
        start_time = datetime.now()
        news_count_by_ticker = {}  # æ”¶é›†æ–°é—»ç»Ÿè®¡

        for ticker in tickers:
            try:
                ticker_data, ticker_news_count = self._process_ultra_fast_ticker_with_stats(
                    ticker, start_date, end_date, max_news_per_day
                )

                if not ticker_data.empty:
                    all_data.append(ticker_data)

                # è®°å½•è¯¥è‚¡ç¥¨çš„æ–°é—»æ•°é‡
                news_count_by_ticker[ticker] = ticker_news_count

                processed_count += 1
                if processed_count % 25 == 0:
                    elapsed = (datetime.now() - start_time).total_seconds()
                    speed = processed_count / (elapsed / 60)
                    logger.info(f"å·²å¤„ç† {processed_count}/{len(tickers)} è‚¡ç¥¨ | é€Ÿåº¦: {speed:.1f} è‚¡ç¥¨/åˆ†é’Ÿ")

            except Exception as e:
                logger.warning(f"å¤„ç†è‚¡ç¥¨ {ticker} å¤±è´¥: {e}")
                news_count_by_ticker[ticker] = 0  # å¤±è´¥çš„è‚¡ç¥¨è®°å½•0æ¡æ–°é—»
                continue

        if not all_data:
            logger.error("æœªèƒ½å¤„ç†ä»»ä½•è‚¡ç¥¨æ•°æ®")
            return pd.DataFrame()

        # åˆå¹¶æ•°æ®
        combined_data = pd.concat(all_data, ignore_index=True)
        combined_data['date'] = pd.to_datetime(combined_data['date'])
        combined_data.set_index(['date', 'ticker'], inplace=True)
        combined_data.sort_index(inplace=True)

        # æ”¹è¿›çš„æ¨ªæˆªé¢æ ‡å‡†åŒ– - é¿å…äº§ç”Ÿå¸¸æ•°åˆ—
        def robust_standardize(x):
            """ç¨³å¥çš„æ ‡å‡†åŒ–å‡½æ•°ï¼Œé¿å…äº§ç”Ÿå¸¸æ•°åˆ—"""
            if len(x) <= 1:
                # å•ä¸ªå€¼æ— æ³•æ ‡å‡†åŒ–ï¼Œä¿æŒåŸå€¼
                return x

            std_val = x.std()
            if std_val < 1e-8:  # æ ‡å‡†å·®å¤ªå°ï¼Œæ¥è¿‘å¸¸æ•°
                # ä¸è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä¿æŒåŸå§‹å€¼çš„ç›¸å¯¹å·®å¼‚
                return x - x.mean()  # åªè¿›è¡Œä¸­å¿ƒåŒ–ï¼Œä¿ç•™åŸå§‹å˜å¼‚
            else:
                # æ­£å¸¸æ ‡å‡†åŒ–
                return (x - x.mean()) / std_val

        combined_data['sentiment_score'] = combined_data.groupby('date')['sentiment_score'].transform(robust_standardize)

        elapsed_total = (datetime.now() - start_time).total_seconds()
        final_speed = len(tickers) / (elapsed_total / 60)

        logger.info("=" * 60)
        logger.info(f"è¶…çº§æé€Ÿå¤„ç†å®Œæˆ!")
        logger.info(f"æ•°æ®å½¢çŠ¶: {combined_data.shape}")
        logger.info(f"æ€»è€—æ—¶: {elapsed_total:.1f}ç§’")
        logger.info(f"å¹³å‡é€Ÿåº¦: {final_speed:.1f} è‚¡ç¥¨/åˆ†é’Ÿ")
        logger.info(f"é€Ÿåº¦æå‡: æ¯”æ ‡å‡†ç‰ˆå¿«50%+")

        # æ–°é—»ç»Ÿè®¡æ‘˜è¦
        total_news = sum(news_count_by_ticker.values())
        tickers_with_news = sum(1 for count in news_count_by_ticker.values() if count > 0)
        logger.info(f"æ–°é—»ç»Ÿè®¡: æ€»è®¡{total_news}æ¡æ–°é—»ï¼Œ{tickers_with_news}/{len(tickers)}åªè‚¡ç¥¨æœ‰æ–°é—»")

        # è´¨é‡ç›‘æ§
        if enable_quality_monitoring and not combined_data.empty:
            logger.info("ğŸ” å¯åŠ¨æƒ…æ„Ÿå› å­è´¨é‡ç›‘æ§...")
            try:
                from bma_models.alpha_factor_quality_monitor import AlphaFactorQualityMonitor
                monitor = AlphaFactorQualityMonitor(save_reports=True)
                quality_report = monitor.monitor_sentiment_factor(
                    sentiment_data=combined_data,
                    news_count_by_ticker=news_count_by_ticker,
                    processing_time=elapsed_total
                )

                # ç®€è¦æ—¥å¿—è´¨é‡ç»“æœ
                if quality_report:
                    score = quality_report.get('sentiment_quality_score', {}).get('overall', 0)
                    grade = quality_report.get('sentiment_quality_score', {}).get('grade', 'N/A')
                    logger.info(f"âœ… è´¨é‡ç›‘æ§å®Œæˆ: è¯„åˆ† {score:.1f}/100 (ç­‰çº§: {grade})")
                else:
                    logger.warning("è´¨é‡ç›‘æ§æœªè¿”å›æŠ¥å‘Š")

            except ImportError as e:
                logger.warning(f"è´¨é‡ç›‘æ§æ¨¡å—æœªæ‰¾åˆ°: {e}")
            except Exception as e:
                logger.error(f"è´¨é‡ç›‘æ§å¤±è´¥: {e}")
                import traceback
                logger.debug(traceback.format_exc())
        elif not enable_quality_monitoring:
            logger.info("è´¨é‡ç›‘æ§å·²ç¦ç”¨")
        elif combined_data.empty:
            logger.warning("æ— æ•°æ®å¯ç”¨äºè´¨é‡ç›‘æ§")

        logger.info("=" * 60)

        return combined_data

    def _process_ultra_fast_ticker_with_stats(self, ticker: str,
                                            start_date: str,
                                            end_date: str,
                                            max_news_per_day: int) -> Tuple[pd.DataFrame, int]:
        """è¶…çº§æé€Ÿå¤„ç†å•åªè‚¡ç¥¨ï¼Œè¿”å›æ•°æ®å’Œæ–°é—»ç»Ÿè®¡"""

        # è·å–ç²¾ç®€æ–°é—»æ•°æ®
        news_data, total_news_count = self._get_minimal_news_with_stats(ticker, start_date, end_date, max_news_per_day)

        if not news_data:
            return pd.DataFrame(), 0

        # è®¡ç®—æ¯æ—¥æƒ…æ„Ÿåˆ†æ•°
        daily_sentiment = self._calculate_daily_sentiment(news_data)

        # è½¬æ¢ä¸ºDataFrame - åªåŒ…å«æœ‰çœŸå®sentimentçš„æ—¥æœŸ
        result_data = []
        for date, sentiment in daily_sentiment.items():
            # åªæ·»åŠ éé›¶çš„sentimentå€¼ï¼Œé¿å…ç”¨0ç¨€é‡ŠçœŸå®æ•°æ®
            if sentiment != 0.0 or len(daily_sentiment) == 1:  # å¦‚æœåªæœ‰ä¸€å¤©æ•°æ®ï¼Œä¿ç•™å³ä½¿æ˜¯0
                result_data.append({
                    'date': date,
                    'ticker': ticker,
                    'sentiment_score': sentiment
                })

        return pd.DataFrame(result_data), total_news_count

    def _process_ultra_fast_ticker(self, ticker: str,
                                  start_date: str,
                                  end_date: str,
                                  max_news_per_day: int) -> pd.DataFrame:
        """è¶…çº§æé€Ÿå¤„ç†å•åªè‚¡ç¥¨ï¼ˆå‘åå…¼å®¹æ–¹æ³•ï¼‰"""
        result_df, _ = self._process_ultra_fast_ticker_with_stats(ticker, start_date, end_date, max_news_per_day)
        return result_df

    def _get_minimal_news_with_stats(self, ticker: str, start_date: str, end_date: str,
                                   max_news_per_day: int) -> Tuple[Dict, int]:
        """è·å–æœ€ç²¾ç®€æ–°é—»æ•°æ®å¹¶ç»Ÿè®¡æ–°é—»æ•°é‡"""

        if not self.client:
            logger.warning("No Polygon API client available - API key not provided")
            logger.warning(f"  API key value: {self.polygon_api_key[:8] if self.polygon_api_key else 'None'}...")
            return {}, 0  # Return empty data instead of raising an exception

        if not self.model or not self.tokenizer:
            raise ValueError("FinBERTæ¨¡å‹æœªæ­£ç¡®åŠ è½½ï¼Œæ— æ³•è¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚")

        try:
            start_dt = pd.to_datetime(start_date)
            end_dt = pd.to_datetime(end_date)
            news_by_date = {}
            total_news_processed = 0

            # æŒ‰å‘¨åˆ†æ‰¹è¯·æ±‚ï¼ˆå‡å°‘APIè°ƒç”¨ï¼‰
            current_date = start_dt
            while current_date <= end_dt:
                week_end = min(current_date + pd.DateOffset(weeks=2), end_dt)

                try:
                    news_iterator = self.client.list_ticker_news(
                        ticker=ticker,
                        published_utc_gte=current_date.strftime('%Y-%m-%d'),
                        published_utc_lte=week_end.strftime('%Y-%m-%d'),
                        limit=max_news_per_day * 14,  # 2å‘¨çš„æ–°é—»
                        sort='published_utc',
                        order='desc'
                    )

                    for news in news_iterator:
                        news_date = pd.to_datetime(news.published_utc).date()

                        if news_date not in news_by_date:
                            news_by_date[news_date] = []

                        # å–å½“æ—¥é‡è¦æ–°é—»ï¼ˆæœ€å¤š3æ¡ï¼‰
                        if len(news_by_date[news_date]) < max_news_per_day:
                            # ä½¿ç”¨æ ‡é¢˜+æè¿°ï¼Œ45è¯é™åˆ¶
                            text = (news.title or "") + " " + (news.description or "")
                            words = text.split()[:45]  # é™åˆ¶45è¯
                            limited_text = " ".join(words)

                            sentiment = self._ultra_fast_sentiment_analysis(limited_text)

                            news_by_date[news_date].append({
                                'sentiment': sentiment,
                                'title': news.title
                            })

                            total_news_processed += 1

                except Exception as e:
                    logger.warning(f"è·å– {ticker} æ–°é—»å¤±è´¥: {e}")

                current_date = week_end + pd.DateOffset(days=1)

            return news_by_date, total_news_processed

        except Exception as e:
            logger.error(f"æ–°é—»è·å–å¤±è´¥: {e}")
            # å·²ç§»é™¤æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆï¼Œå¼‚å¸¸æ—¶è¿”å›ç©ºç»“æœï¼Œè°ƒç”¨æ–¹å°†è·³è¿‡è¯¥è‚¡ç¥¨/æ—¥æœŸ
            return {}, 0

    def _get_minimal_news(self, ticker: str, start_date: str, end_date: str,
                         max_news_per_day: int) -> Dict:
        """è·å–æœ€ç²¾ç®€æ–°é—»æ•°æ®ï¼ˆå‘åå…¼å®¹æ–¹æ³•ï¼‰"""
        news_data, _ = self._get_minimal_news_with_stats(ticker, start_date, end_date, max_news_per_day)
        return news_data

    def _ultra_fast_sentiment_analysis(self, text: str) -> float:
        """
        è¶…çº§æé€Ÿæƒ…æ„Ÿåˆ†æ
        - æ–‡æœ¬80å­—ç¬¦
        - æ¨¡å‹è¾“å…¥64 tokens
        - GPUæ‰¹å¤„ç†
        """
        if not self.model or not self.tokenizer:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ã€‚è¯·ç¡®ä¿FinBERTæ¨¡å‹æ­£ç¡®åˆå§‹åŒ–ã€‚")

        try:
            # æåº¦ç²¾ç®€æ–‡æœ¬
            text = text.strip()[:80]
            if not text:
                return 0.0

            # æå°è¾“å…¥é•¿åº¦
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=64  # è¶…å°è¾“å…¥é•¿åº¦
            )

            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}

            # å¿«é€Ÿæ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)
                probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)

            probs = probabilities.cpu().numpy()[0]
            # è®¡ç®—ç»¼åˆæƒ…æ„Ÿåˆ†æ•°
            sentiment_score = probs[2] - probs[0]  # positive - negative

            return float(np.clip(sentiment_score, -1.0, 1.0))

        except Exception as e:
            return 0.0

    def _calculate_daily_sentiment(self, news_data: Dict) -> Dict[str, float]:
        """è®¡ç®—æ¯æ—¥æƒ…æ„Ÿåˆ†æ•°ï¼ˆè¶…ç®€åŒ–ç‰ˆï¼‰"""
        daily_sentiment = {}

        for date, news_list in news_data.items():
            if not news_list:
                daily_sentiment[date] = 0.0
            else:
                # å–åŠ æƒå¹³å‡ï¼ˆæœ€å¤š2æ¡æ–°é—»ï¼‰
                sentiments = [news['sentiment'] for news in news_list]
                if len(sentiments) == 1:
                    daily_sentiment[date] = float(sentiments[0])
                else:
                    # è¾ƒæ–°çš„æ–°é—»æƒé‡æ›´é«˜
                    weights = np.linspace(0.6, 1.0, len(sentiments))
                    daily_sentiment[date] = float(np.average(sentiments, weights=weights))

        return daily_sentiment

   

    def process_universe_sentiment(self,
                                  tickers: List[str],
                                  start_date,  # datetimeæˆ–str
                                  end_date,    # datetimeæˆ–str
                                  trading_dates: Optional[List] = None) -> pd.DataFrame:
        """
        å¤„ç†æ•´ä¸ªè‚¡ç¥¨æ± çš„æƒ…æ„Ÿåˆ†æï¼ˆBMAå…¼å®¹æ¥å£ï¼‰

        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            trading_dates: å¯é€‰çš„äº¤æ˜“æ—¥æœŸåˆ—è¡¨

        Returns:
            MultiIndex DataFrame (date, ticker) åŒ…å«sentiment_score
        """
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        if hasattr(start_date, 'strftime'):
            start_date = start_date.strftime('%Y-%m-%d')
        if hasattr(end_date, 'strftime'):
            end_date = end_date.strftime('%Y-%m-%d')

        # å¦‚æœæä¾›äº†äº¤æ˜“æ—¥æœŸï¼Œæ”¶ç¼©èµ·æ­¢èŒƒå›´ä»¥å‡å°‘æ— æ•ˆè®¡ç®—
        if trading_dates:
            try:
                td = sorted(pd.to_datetime(trading_dates))
                if len(td) > 0:
                    start_date = min(td).strftime('%Y-%m-%d')
                    end_date = max(td).strftime('%Y-%m-%d')
            except Exception:
                pass

        # è®¡ç®—æƒ…æ„Ÿæ•°æ®
        df = self.calculate_ultra_fast_sentiment(
            tickers=tickers,
            start_date=start_date,
            end_date=end_date,
            max_news_per_day=3  # ä½¿ç”¨ä¼˜åŒ–çš„å‚æ•°ï¼š3æ¡æ–°é—»/å¤©
        )

        # è‹¥æä¾›äº†äº¤æ˜“æ—¥æœŸï¼Œåˆ™æŒ‰äº¤æ˜“æ—¥è¿‡æ»¤ï¼ˆç¡®ä¿ä¸å¼•æ“å¯¹é½ï¼‰
        if trading_dates and not df.empty and isinstance(df.index, pd.MultiIndex):
            try:
                td_norm = set(pd.to_datetime(trading_dates).tz_localize(None).normalize())
                df_dates = pd.to_datetime(df.index.get_level_values('date')).tz_localize(None).normalize()
                mask = df_dates.isin(td_norm)
                df = df[mask]
            except Exception:
                pass

        return df


def run_ultra_fast_sentiment_pipeline(stock_file_path: str = "filtered_stocks_20250817_002928.txt",
                                    polygon_api_key: str = None,
                                    max_stocks: int = 100,
                                    years: int = 1) -> pd.DataFrame:
    """
    è¿è¡Œè¶…çº§æé€Ÿå®Œæ•´æµæ°´çº¿

    Args:
        stock_file_path: è‚¡ç¥¨æ–‡ä»¶è·¯å¾„
        polygon_api_key: APIå¯†é’¥
        max_stocks: æœ€å¤§å¤„ç†è‚¡ç¥¨æ•°
        years: å†å²å¹´æ•°

    Returns:
        è¶…çº§æé€ŸBMAå…¼å®¹æ•°æ®é›†
    """
    logger.info("=" * 80)
    logger.info("è¶…çº§æé€ŸBMAæƒ…æ„Ÿåˆ†ææµæ°´çº¿")
    logger.info("=" * 80)
    logger.info("è¶…çº§ä¼˜åŒ–é…ç½®:")
    logger.info("- æ¯æ—¥2æ¡é‡è¦æ–°é—»")
    logger.info("- æ–‡æœ¬80å­—ç¬¦æˆªæ–­")
    logger.info("- æ¨¡å‹64 tokensè¾“å…¥")
    logger.info("- é¢„æœŸé€Ÿåº¦æå‡50%")

    try:
        # åŠ è½½è‚¡ç¥¨
        if os.path.exists(stock_file_path):
            with open(stock_file_path, 'r', encoding='utf-8') as f:
                all_tickers = [line.strip() for line in f if line.strip()]
        else:
            all_tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'NVDA', 'META', 'NFLX']

        tickers = all_tickers[:max_stocks]

        # æ—¶é—´èŒƒå›´
        end_date = datetime.now().strftime('%Y-%m-%d')
        start_date = (datetime.now() - pd.DateOffset(years=years)).strftime('%Y-%m-%d')

        logger.info(f"å¤„ç†è‚¡ç¥¨: {len(tickers)}")
        logger.info(f"æ—¶é—´èŒƒå›´: {start_date} - {end_date}")

        # åˆ›å»ºè¶…çº§æé€Ÿåˆ†æå™¨
        ultra_analyzer = UltraFastSentimentFactor(polygon_api_key=polygon_api_key)

        # è¿è¡Œåˆ†æ
        start_time = datetime.now()
        sentiment_data = ultra_analyzer.calculate_ultra_fast_sentiment(
            tickers=tickers,
            start_date=start_date,
            end_date=end_date,
            max_news_per_day=2  # è¶…çº§æé€Ÿï¼šæ¯æ—¥2æ¡æ–°é—»
        )
        end_time = datetime.now()

        processing_time = (end_time - start_time).total_seconds()

        if not sentiment_data.empty:
            # ä¿å­˜ç»“æœ
            os.makedirs('result', exist_ok=True)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

            output_path = f'result/ultra_fast_sentiment_{timestamp}.pkl'
            sentiment_data.to_pickle(output_path)

            csv_path = output_path.replace('.pkl', '.csv')
            sentiment_data.to_csv(csv_path)

            logger.info("=" * 80)
            logger.info("è¶…çº§æé€Ÿæµæ°´çº¿å®Œæˆ!")
            logger.info("=" * 80)
            logger.info(f"å¤„ç†è‚¡ç¥¨: {len(tickers)}")
            logger.info(f"æ•°æ®é›†: {sentiment_data.shape}")
            logger.info(f"å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’")
            logger.info(f"å¤„ç†é€Ÿåº¦: {len(tickers)/(processing_time/60):.1f} è‚¡ç¥¨/åˆ†é’Ÿ")
            logger.info(f"é€Ÿåº¦æå‡: æ¯”æ ‡å‡†ç‰ˆå¿« 50%+")

            print(f"\nä¿å­˜æ–‡ä»¶:")
            print(f"  æ•°æ®: {output_path}")
            print(f"  CSV: {csv_path}")

            if 'sentiment_score' in sentiment_data.columns:
                print(f"\nsentiment_scoreç»Ÿè®¡:")
                print(sentiment_data['sentiment_score'].describe())

            print(f"\n[OK] è¶…çº§æé€Ÿç‰ˆæœ¬å®Œæˆ!")
            print(f"[OK] æ¯æ—¥2æ¡æ–°é—», æ–‡æœ¬80å­—ç¬¦")
            print(f"[OK] é€Ÿåº¦æå‡50%, å®Œå…¨å…¼å®¹BMA")

            return sentiment_data

        else:
            logger.error("æœªèƒ½ç”Ÿæˆæ•°æ®")
            return pd.DataFrame()

    except Exception as e:
        logger.error(f"è¶…çº§æé€Ÿæµæ°´çº¿å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="è¶…çº§æé€Ÿæƒ…æ„Ÿåˆ†æ")
    parser.add_argument('--api-key', type=str, help="Polygon APIå¯†é’¥")
    parser.add_argument('--max-stocks', type=int, default=50, help="æœ€å¤§è‚¡ç¥¨æ•°")
    parser.add_argument('--years', type=int, default=1, help="å†å²å¹´æ•°")

    args = parser.parse_args()

    print("è¶…çº§æé€Ÿæƒ…æ„Ÿåˆ†æå¯åŠ¨")
    print("=" * 50)
    print("é…ç½®:")
    print("- æ¯æ—¥2æ¡é‡è¦æ–°é—»")
    print("- æ–‡æœ¬80å­—ç¬¦æˆªæ–­")
    print("- æ¨¡å‹64 tokensè¾“å…¥")
    print("- é¢„æœŸé€Ÿåº¦æå‡50%")

    api_key = args.api_key or os.environ.get('POLYGON_API_KEY')

    result = run_ultra_fast_sentiment_pipeline(
        polygon_api_key=api_key,
        max_stocks=args.max_stocks,
        years=args.years
    )

    if not result.empty:
        print(f"\næœ€ç»ˆç»“æœ:")
        print(f"  æ•°æ®å½¢çŠ¶: {result.shape}")
        print(f"  sentiment_scoreèŒƒå›´: [{result['sentiment_score'].min():.3f}, {result['sentiment_score'].max():.3f}]")
        print("\n[OK] è¶…çº§æé€Ÿç‰ˆæœ¬å‡†å¤‡å°±ç»ª!")