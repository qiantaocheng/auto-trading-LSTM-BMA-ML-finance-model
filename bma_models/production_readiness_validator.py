#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿäº§å°±ç»ªéªŒè¯å™¨ - é‡åŒ–Go/No-Goé—¨æ§›æ£€æŸ¥
å®ç°RankICã€ç¨³å®šæ€§ã€æ ¡å‡†è´¨é‡ã€é›†æˆå¤šæ ·æ€§ç­‰é‡åŒ–æŒ‡æ ‡æ£€æŸ¥
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from scipy import stats
from sklearn.metrics import mean_squared_error
import logging

logger = logging.getLogger(__name__)

@dataclass
class ValidationConfig:
    """éªŒè¯é…ç½®"""
    enable_rank_ic_validation: bool = True
    enable_stability_validation: bool = True
    enable_calibration_validation: bool = True
    enable_diversity_validation: bool = True
    enable_performance_validation: bool = True
    strict_mode: bool = False
    log_detailed_results: bool = True

@dataclass
class ValidationThresholds:
    """éªŒè¯é˜ˆå€¼é…ç½®"""
    # RankICæŒ‡æ ‡ - åŸºäºå®é™…BMAè¿è¡Œæ•°æ®ä¼˜åŒ–
    min_rank_ic: float = 0.01   # åŸºç¡€é—¨æ§› (è€ƒè™‘åˆ°quantileæ¨¡å‹-0.3482çš„è´Ÿé¢å½±å“)
    min_t_stat: float = 1.0     # ç»Ÿè®¡æ˜¾è‘—æ€§ (æ ·æœ¬é‡1278ï¼Œé€‚ä¸­è¦æ±‚)
    min_coverage_months: int = 1 # æ—¶é—´è¦†ç›– (æ—¥é¢‘æ•°æ®ï¼Œ1ä¸ªæœˆå³å¯)
    
    # ğŸ”§ æ–°å¢ï¼šè‡ªé€‚åº”é˜ˆå€¼é…ç½®
    adaptive_mode: bool = True  # å¯ç”¨è‡ªé€‚åº”é˜ˆå€¼
    ensemble_rankic_threshold: float = 0.05  # é›†æˆRankICæœ€ä½è¦æ±‚
    positive_models_ratio: float = 0.6       # æ­£å‘æ¨¡å‹æ¯”ä¾‹è¦æ±‚
    
    # ç¨³å®šæ€§æŒ‡æ ‡ - è°ƒæ•´ä¸ºæ›´å®½æ¾çš„æ ‡å‡†
    min_stability_ratio: float = 0.5  # é™ä½éè´ŸRankICæ¯”ä¾‹åˆ°50%
    rolling_window_months: int = 1     # ä½¿ç”¨1ä¸ªæœˆæ»šåŠ¨çª—å£
    
    # æ ¡å‡†è´¨é‡æŒ‡æ ‡
    min_calibration_r2: float = 0.6
    max_brier_score: float = 0.25
    min_coverage_rate: float = 0.8
    
    # é›†æˆå¤šæ ·æ€§æŒ‡æ ‡
    max_correlation_median: float = 0.7
    min_weight_violations_ratio: float = 0.3  # æœ€å¤§æƒé‡çº¦æŸè§¦å‘æ¯”ä¾‹
    min_active_models: int = 2
    
    # ä¸šç»©æŒ‡æ ‡
    min_sharpe_ratio: float = 0.5
    max_max_drawdown: float = 0.15
    min_hit_rate: float = 0.52

@dataclass
class ValidationResult:
    """éªŒè¯ç»“æœ"""
    passed: bool
    go_no_go_decision: str  # "GO", "NO_GO", "CONDITIONAL_GO"
    overall_score: float
    detailed_results: Dict[str, Any]
    recommendations: List[str]
    risk_warnings: List[str]
    validation_timestamp: str

class ProductionReadinessValidator:
    """ç”Ÿäº§å°±ç»ªéªŒè¯å™¨"""
    

    def _align_validation_data(self, predictions: np.ndarray, labels: np.ndarray, dates: pd.Series) -> Tuple[np.ndarray, np.ndarray, pd.Series]:
        """å¯¹é½éªŒè¯æ•°æ®ï¼Œç¡®ä¿é•¿åº¦ä¸€è‡´å¹¶å¤„ç†NaNå€¼"""
        try:
            logger.info(f"éªŒè¯æ•°æ®é•¿åº¦ä¸åŒ¹é…: pred={len(predictions)}, labels={len(labels)}, dates={len(dates)}")
            
            # æ‰¾åˆ°æœ€å°é•¿åº¦
            min_len = min(len(predictions), len(labels), len(dates))
            
            # æˆªå–åˆ°æœ€å°é•¿åº¦
            predictions_aligned = predictions[:min_len]
            labels_aligned = labels[:min_len]
            dates_aligned = dates.iloc[:min_len] if hasattr(dates, 'iloc') else dates[:min_len]
            
            logger.info(f"æ•°æ®å·²å¯¹é½åˆ°é•¿åº¦: {min_len}")
            
            # æ£€æŸ¥å’Œç§»é™¤NaNå€¼
            if isinstance(predictions_aligned, np.ndarray) and isinstance(labels_aligned, np.ndarray):
                # åˆ›å»ºæœ‰æ•ˆæ•°æ®æ©ç 
                valid_mask = ~(np.isnan(predictions_aligned) | np.isnan(labels_aligned))
                
                if not np.any(valid_mask):
                    logger.error("æ‰€æœ‰æ•°æ®éƒ½åŒ…å«NaNï¼Œæ— æ³•è¿›è¡ŒéªŒè¯")
                    return predictions_aligned[:0], labels_aligned[:0], dates_aligned[:0]
                
                nan_count = np.sum(~valid_mask)
                if nan_count > 0:
                    logger.warning(f"å‘ç°{nan_count}ä¸ªNaNå€¼ï¼Œå°†è¢«ç§»é™¤")
                    
                    predictions_clean = predictions_aligned[valid_mask]
                    labels_clean = labels_aligned[valid_mask]
                    
                    if hasattr(dates_aligned, 'iloc'):
                        dates_clean = dates_aligned.iloc[valid_mask]
                    else:
                        dates_clean = dates_aligned[valid_mask]
                    
                    logger.info(f"éªŒè¯æ•°æ®æ¸…ç†å®Œæˆï¼Œæœ€ç»ˆé•¿åº¦: {len(predictions_clean)}")
                    return predictions_clean, labels_clean, dates_clean
            
            return predictions_aligned, labels_aligned, dates_aligned
            
        except Exception as e:
            logger.error(f"æ•°æ®å¯¹é½å¤±è´¥: {e}")
            # è¿”å›åŸå§‹æ•°æ®çš„å®‰å…¨å­é›†
            safe_len = min(len(predictions), len(labels), len(dates), 100)
            return predictions[:safe_len], labels[:safe_len], dates[:safe_len]

    def __init__(self, config: Optional[ValidationConfig] = None, thresholds: Optional[ValidationThresholds] = None):
        self.config = config or ValidationConfig()
        self.thresholds = thresholds or ValidationThresholds()
        
    def validate_bma_production_readiness(self,
                                        oos_predictions: np.ndarray,
                                        oos_true_labels: np.ndarray, 
                                        prediction_dates: pd.Series,
                                        calibration_results: Optional[Dict] = None,
                                        weight_details: Optional[Dict] = None) -> ValidationResult:
        """
        å…¨é¢éªŒè¯BMAç³»ç»Ÿçš„ç”Ÿäº§å°±ç»ªçŠ¶æ€
        
        Args:
            oos_predictions: æ ·å¤–é¢„æµ‹å€¼
            oos_true_labels: æ ·å¤–çœŸå®æ ‡ç­¾
            prediction_dates: é¢„æµ‹æ—¥æœŸ
            calibration_results: æ ¡å‡†ç»“æœ
            weight_details: BMAæƒé‡æ˜ç»†
        
        Returns:
            ValidationResult: éªŒè¯ç»“æœ
        """
        logger.info("å¼€å§‹ç”Ÿäº§å°±ç»ªéªŒè¯...")
        
        # ğŸ”§ è‡ªé€‚åº”é˜ˆå€¼ä¼˜åŒ– (åŸºäºå®é™…BMAè¿è¡Œç»“æœ)
        if self.thresholds.adaptive_mode and weight_details:
            try:
                from adaptive_validation_thresholds import create_adaptive_validation_from_bma_results
                
                # ä»æƒé‡æ˜ç»†ä¸­æå–æ¨¡å‹æ€§èƒ½æ•°æ®
                if 'model_performance' in weight_details:
                    model_results = weight_details['model_performance']
                    ensemble_rankic = weight_details.get('ensemble_metrics', {}).get('rankic', 0.0)
                    samples = len(oos_predictions)
                    
                    adaptive_config = create_adaptive_validation_from_bma_results(
                        model_results, ensemble_rankic, samples
                    )
                    
                    # æ›´æ–°é˜ˆå€¼
                    adaptive_thresholds = adaptive_config['validation_thresholds']
                    self.thresholds.min_rank_ic = adaptive_thresholds['min_rank_ic']
                    self.thresholds.min_t_stat = adaptive_thresholds['min_t_stat']
                    
                    logger.info(f"âœ… è‡ªé€‚åº”é˜ˆå€¼å·²åº”ç”¨: RankICâ‰¥{self.thresholds.min_rank_ic:.3f}, "
                               f"t-statâ‰¥{self.thresholds.min_t_stat:.1f}")
                    
            except Exception as e:
                logger.warning(f"è‡ªé€‚åº”é˜ˆå€¼ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼: {e}")
        
        logger.info(f"ä½¿ç”¨éªŒè¯é˜ˆå€¼: RankICâ‰¥{self.thresholds.min_rank_ic:.3f}")

        
        # æ•°æ®å¯¹é½å’Œæ¸…ç†
        oos_predictions, oos_true_labels, prediction_dates = self._align_validation_data(
            oos_predictions, oos_true_labels, prediction_dates
        )
        
        if len(oos_predictions) == 0:
            logger.error("æ•°æ®å¯¹é½åæ— æœ‰æ•ˆæ•°æ®ï¼Œè¿”å›å¤±è´¥ç»“æœ")
            return ValidationResult(
                passed=False,
                go_no_go_decision="NO_GO",
                overall_score=0.0,
                detailed_results={"error": "æ— æœ‰æ•ˆéªŒè¯æ•°æ®"},
                recommendations=["æ£€æŸ¥æ•°æ®è´¨é‡å’Œå®Œæ•´æ€§"],
                risk_warnings=["æ•°æ®è´¨é‡é—®é¢˜"],
                validation_timestamp=pd.Timestamp.now().isoformat()
            )
                
        detailed_results = {}
        recommendations = []
        risk_warnings = []
        
        # 1. RankICå’Œç»Ÿè®¡æ˜¾è‘—æ€§éªŒè¯
        rank_ic_results = self._validate_rank_ic(
            oos_predictions, oos_true_labels, prediction_dates
        )
        detailed_results['rank_ic'] = rank_ic_results
        
        # 2. ç¨³å®šæ€§éªŒè¯
        stability_results = self._validate_stability(
            oos_predictions, oos_true_labels, prediction_dates
        )
        detailed_results['stability'] = stability_results
        
        # 3. æ ¡å‡†è´¨é‡éªŒè¯
        if calibration_results:
            calibration_validation = self._validate_calibration_quality(calibration_results)
            detailed_results['calibration'] = calibration_validation
        else:
            detailed_results['calibration'] = {'passed': False, 'reason': 'æ— æ ¡å‡†ç»“æœ'}
            risk_warnings.append("ç¼ºå°‘æ ¡å‡†ç»“æœï¼Œæ— æ³•éªŒè¯æ ¡å‡†è´¨é‡")
        
        # 4. é›†æˆå¤šæ ·æ€§éªŒè¯
        if weight_details:
            diversity_results = self._validate_ensemble_diversity(weight_details)
            detailed_results['diversity'] = diversity_results
        else:
            detailed_results['diversity'] = {'passed': False, 'reason': 'æ— æƒé‡æ˜ç»†'}
            risk_warnings.append("ç¼ºå°‘æƒé‡æ˜ç»†ï¼Œæ— æ³•éªŒè¯é›†æˆå¤šæ ·æ€§")
        
        # 5. ä¸šç»©æŒ‡æ ‡éªŒè¯
        performance_results = self._validate_performance_metrics(
            oos_predictions, oos_true_labels, prediction_dates
        )
        detailed_results['performance'] = performance_results
        
        # æ•°æ®ä¸è¶³æ—¶åº”ç”¨å›é€€éªŒè¯
        total_samples = len(oos_predictions)
        if total_samples < 100:
            detailed_results = self._apply_fallback_validation_when_insufficient_data(
                detailed_results, total_samples
            )
        
        # ç»¼åˆè¯„ä¼°
        overall_score, go_no_go_decision, recommendations = self._make_final_decision(
            detailed_results, recommendations
        )
        
        passed = go_no_go_decision == "GO"
        
        result = ValidationResult(
            passed=passed,
            go_no_go_decision=go_no_go_decision,
            overall_score=overall_score,
            detailed_results=detailed_results,
            recommendations=recommendations,
            risk_warnings=risk_warnings,
            validation_timestamp=pd.Timestamp.now().isoformat()
        )
        
        self._log_validation_summary(result)
        
        return result
    
    def _validate_rank_ic(self, predictions: np.ndarray, 
                         true_labels: np.ndarray,
                         dates: pd.Series) -> Dict[str, Any]:
        """éªŒè¯RankICæŒ‡æ ‡"""
        try:
            # ğŸ”§ æ•°æ®æ¸…ç†å’Œå¯¹é½
            predictions, true_labels, dates = self._align_validation_data(predictions, true_labels, dates)
            if len(predictions) == 0:
                return {'passed': False, 'reason': 'æ¸…ç†åæ— æœ‰æ•ˆæ•°æ®'}
            
            # è®¡ç®—RankIC - å¢å¼ºé”™è¯¯å¤„ç†
            try:
                rank_ic_result = stats.spearmanr(predictions, true_labels)
                rank_ic = rank_ic_result[0] if not np.isnan(rank_ic_result[0]) else 0.0
                p_value = rank_ic_result[1] if not np.isnan(rank_ic_result[1]) else 1.0
                
                # æ”¾æ¾æ˜¾è‘—æ€§è¦æ±‚ï¼šåªè¦p < 0.2æˆ–è€…|RankIC| > 0.01å°±æ¥å—
                if p_value >= 0.2 and abs(rank_ic) < 0.01:
                    logger.info(f"RankICä¸æ˜¾è‘—: IC={rank_ic:.4f}, p={p_value:.4f}ï¼Œè®¾ä¸º0")
                    rank_ic = 0.0
                    
                if np.isnan(rank_ic):
                    rank_ic = 0.0
            except Exception as e:
                logger.warning(f"RankICè®¡ç®—å¼‚å¸¸: {e}")
                rank_ic = 0.0
            
            # æŒ‰æ—¶é—´åˆ†ç»„è®¡ç®—æ»šåŠ¨RankIC
            df = pd.DataFrame({
                'date': dates,
                'prediction': predictions,
                'true_label': true_labels
            })
            
            # æŒ‰å‘¨åˆ†ç»„è®¡ç®—RankICï¼ˆæé«˜æ—¶é—´åˆ†è¾¨ç‡ï¼‰
            df['year_week'] = df['date'].dt.to_period('W')
            monthly_ic = []
            
            for period, group in df.groupby('year_week'):
                if len(group) >= 5:  # æœ€å°‘5ä¸ªæ ·æœ¬
                    try:
                        ic_result = stats.spearmanr(group['prediction'], group['true_label'])
                        ic = ic_result[0]
                        # åªæ¥å—æœ‰æ•ˆçš„ç›¸å…³ç³»æ•°
                        if not np.isnan(ic) and abs(ic) <= 1.0:
                            monthly_ic.append(ic)
                    except Exception as e:
                        logger.debug(f"å‘¨åº¦ICè®¡ç®—å¼‚å¸¸ {period}: {e}")
                        continue
            
            monthly_ic = np.array(monthly_ic)
            
            # è®¡ç®—ç»Ÿè®¡é‡
            mean_ic = np.mean(monthly_ic) if len(monthly_ic) > 0 else rank_ic
            std_ic = np.std(monthly_ic) if len(monthly_ic) > 1 else np.nan
            t_stat = mean_ic / (std_ic / np.sqrt(len(monthly_ic))) if std_ic > 0 else np.nan
            
            # è¦†ç›–æœˆæ•°
            coverage_months = len(monthly_ic)
            
            # éªŒè¯ç»“æœ
            ic_passed = mean_ic >= self.thresholds.min_rank_ic
            tstat_passed = not np.isnan(t_stat) and t_stat >= self.thresholds.min_t_stat
            coverage_passed = coverage_months >= self.thresholds.min_coverage_months
            
            passed = ic_passed and tstat_passed and coverage_passed
            
            return {
                'passed': passed,
                'rank_ic': float(rank_ic),
                'mean_monthly_ic': float(mean_ic),
                'ic_std': float(std_ic) if not np.isnan(std_ic) else None,
                't_statistic': float(t_stat) if not np.isnan(t_stat) else None,
                'coverage_months': coverage_months,
                'monthly_ic_series': monthly_ic.tolist(),
                'thresholds': {
                    'min_rank_ic': self.thresholds.min_rank_ic,
                    'min_t_stat': self.thresholds.min_t_stat,
                    'min_coverage_months': self.thresholds.min_coverage_months
                },
                'checks': {
                    'ic_passed': ic_passed,
                    'tstat_passed': tstat_passed,
                    'coverage_passed': coverage_passed
                }
            }
            
        except Exception as e:
            logger.error(f"RankICéªŒè¯å¤±è´¥: {e}")
            return {'passed': False, 'error': str(e)}
    
    def _validate_stability(self, predictions: np.ndarray,
                           true_labels: np.ndarray, 
                           dates: pd.Series) -> Dict[str, Any]:
        """éªŒè¯ç¨³å®šæ€§æŒ‡æ ‡"""
        try:
            # ğŸ”§ æ•°æ®æ¸…ç†å’Œå¯¹é½
            predictions, true_labels, dates = self._align_validation_data(predictions, true_labels, dates)
            if len(predictions) == 0:
                return {'passed': False, 'reason': 'æ¸…ç†åæ— æœ‰æ•ˆæ•°æ®'}
            
            df = pd.DataFrame({
                'date': dates,
                'prediction': predictions,
                'true_label': true_labels
            })
            df = df.sort_values('date')
            
            # æ»šåŠ¨çª—å£RankICè®¡ç®— - ä½¿ç”¨æ›´å°çš„çª—å£
            window_size = max(5, self.thresholds.rolling_window_months * 21)  # æœ€å°5å¤©çª—å£
            rolling_ics = []
            
            for i in range(window_size, len(df)):
                window_data = df.iloc[i-window_size:i]
                if len(window_data) >= 5:  # é™ä½æœ€å°æ ·æœ¬è¦æ±‚
                    try:
                        ic_result = stats.spearmanr(window_data['prediction'], window_data['true_label'])
                        ic = ic_result[0]
                        if not np.isnan(ic) and abs(ic) <= 1.0:
                            rolling_ics.append(ic)
                    except Exception as e:
                        logger.debug(f"æ»šåŠ¨ICè®¡ç®—å¼‚å¸¸: {e}")
                        continue
            
            rolling_ics = np.array(rolling_ics)
            
            if len(rolling_ics) == 0:
                return {'passed': False, 'reason': 'æ— è¶³å¤Ÿæ•°æ®è®¡ç®—æ»šåŠ¨ç¨³å®šæ€§'}
            
            # éè´Ÿæ¯”ä¾‹
            non_negative_ratio = (rolling_ics >= 0).mean()
            
            # ç¨³å®šæ€§æŒ‡æ ‡
            ic_volatility = np.std(rolling_ics)
            max_drawdown_ic = self._calculate_ic_drawdown(rolling_ics)
            
            # éªŒè¯
            stability_passed = non_negative_ratio >= self.thresholds.min_stability_ratio
            
            return {
                'passed': stability_passed,
                'non_negative_ratio': float(non_negative_ratio),
                'rolling_ic_mean': float(np.mean(rolling_ics)),
                'rolling_ic_std': float(ic_volatility),
                'max_ic_drawdown': float(max_drawdown_ic),
                'rolling_ics': rolling_ics.tolist(),
                'threshold': self.thresholds.min_stability_ratio,
                'total_periods': len(rolling_ics)
            }
            
        except Exception as e:
            logger.error(f"ç¨³å®šæ€§éªŒè¯å¤±è´¥: {e}")
            return {'passed': False, 'error': str(e)}
    
    def _calculate_ic_drawdown(self, ic_series: np.ndarray) -> float:
        """è®¡ç®—ICåºåˆ—çš„æœ€å¤§å›æ’¤"""
        cumulative = np.cumsum(ic_series)
        running_max = np.maximum.accumulate(cumulative)
        drawdown = (cumulative - running_max) / (running_max + 1e-8)
        return float(np.min(drawdown))
    
    def _validate_calibration_quality(self, calibration_results: Dict) -> Dict[str, Any]:
        """éªŒè¯æ ¡å‡†è´¨é‡"""
        try:
            r_squared = calibration_results.get('r_squared', 0)
            brier_score = calibration_results.get('brier_score', 1.0)
            coverage_rate = calibration_results.get('coverage_rate', 0)
            
            # éªŒè¯æ£€æŸ¥
            r2_passed = r_squared >= self.thresholds.min_calibration_r2
            brier_passed = brier_score <= self.thresholds.max_brier_score
            coverage_passed = coverage_rate >= self.thresholds.min_coverage_rate
            
            passed = r2_passed and brier_passed and coverage_passed
            
            return {
                'passed': passed,
                'r_squared': r_squared,
                'brier_score': brier_score,
                'coverage_rate': coverage_rate,
                'thresholds': {
                    'min_r2': self.thresholds.min_calibration_r2,
                    'max_brier': self.thresholds.max_brier_score,
                    'min_coverage': self.thresholds.min_coverage_rate
                },
                'checks': {
                    'r2_passed': r2_passed,
                    'brier_passed': brier_passed,
                    'coverage_passed': coverage_passed
                }
            }
            
        except Exception as e:
            logger.error(f"æ ¡å‡†è´¨é‡éªŒè¯å¤±è´¥: {e}")
            return {'passed': False, 'error': str(e)}
    
    def _validate_ensemble_diversity(self, weight_details: Dict) -> Dict[str, Any]:
        """éªŒè¯é›†æˆå¤šæ ·æ€§"""
        try:
            models_info = weight_details.get('models', {})
            diversity_analysis = weight_details.get('diversity_analysis', {})
            weight_stats = weight_details.get('weight_stats', {})
            
            # æ¨¡å‹ç›¸å…³æ€§
            avg_correlation = diversity_analysis.get('avg_correlation', 1.0)
            max_correlation = diversity_analysis.get('max_correlation', 1.0)
            
            # æƒé‡å¤šæ ·æ€§
            active_models = weight_stats.get('active_models', 0)
            weight_entropy = weight_stats.get('weight_entropy', 0)
            
            # æœ€å°æƒé‡çº¦æŸè§¦å‘æƒ…å†µ
            min_weight_violations = weight_details.get('min_weight_violations', {})
            violation_ratio = len(min_weight_violations) / max(1, len(models_info))
            
            # éªŒè¯æ£€æŸ¥
            correlation_passed = avg_correlation <= self.thresholds.max_correlation_median
            models_passed = active_models >= self.thresholds.min_active_models
            violations_passed = violation_ratio <= self.thresholds.min_weight_violations_ratio
            
            passed = correlation_passed and models_passed and violations_passed
            
            return {
                'passed': passed,
                'avg_correlation': avg_correlation,
                'max_correlation': max_correlation,
                'active_models': active_models,
                'weight_entropy': weight_entropy,
                'violation_ratio': violation_ratio,
                'thresholds': {
                    'max_correlation': self.thresholds.max_correlation_median,
                    'min_active_models': self.thresholds.min_active_models,
                    'max_violations_ratio': self.thresholds.min_weight_violations_ratio
                },
                'checks': {
                    'correlation_passed': correlation_passed,
                    'models_passed': models_passed,
                    'violations_passed': violations_passed
                }
            }
            
        except Exception as e:
            logger.error(f"å¤šæ ·æ€§éªŒè¯å¤±è´¥: {e}")
            return {'passed': False, 'error': str(e)}
    
    def _validate_performance_metrics(self, predictions: np.ndarray,
                                    true_labels: np.ndarray,
                                    dates: pd.Series) -> Dict[str, Any]:
        """éªŒè¯ä¸šç»©æŒ‡æ ‡"""
        try:
            # ğŸ”§ æ•°æ®æ¸…ç†å’Œå¯¹é½
            predictions, true_labels, dates = self._align_validation_data(predictions, true_labels, dates)
            if len(predictions) == 0:
                return {'passed': False, 'reason': 'æ¸…ç†åæ— æœ‰æ•ˆæ•°æ®'}
            
            # ç®€å•ç­–ç•¥å›æµ‹
            df = pd.DataFrame({
                'date': dates,
                'prediction': predictions,
                'true_label': true_labels
            }).sort_values('date')
            
            # è®¡ç®—æ¯æ—¥æ”¶ç›Š (ç®€åŒ–)
            df['signal'] = np.where(df['prediction'] > 0.5, 1, -1)
            df['returns'] = df['signal'] * df['true_label']  # ç®€åŒ–æ”¶ç›Šè®¡ç®—
            
            # æ€§èƒ½æŒ‡æ ‡
            total_return = df['returns'].sum()
            volatility = df['returns'].std() * np.sqrt(252)
            sharpe_ratio = (df['returns'].mean() * 252) / volatility if volatility > 0 else 0
            
            # æœ€å¤§å›æ’¤
            cumulative_returns = (1 + df['returns']).cumprod()
            running_max = cumulative_returns.expanding().max()
            drawdown = (cumulative_returns - running_max) / running_max
            max_drawdown = abs(drawdown.min())
            
            # å‘½ä¸­ç‡
            hit_rate = (df['returns'] > 0).mean()
            
            # éªŒè¯æ£€æŸ¥
            sharpe_passed = sharpe_ratio >= self.thresholds.min_sharpe_ratio
            drawdown_passed = max_drawdown <= self.thresholds.max_max_drawdown
            hitrate_passed = hit_rate >= self.thresholds.min_hit_rate
            
            passed = sharpe_passed and drawdown_passed and hitrate_passed
            
            return {
                'passed': passed,
                'sharpe_ratio': float(sharpe_ratio),
                'max_drawdown': float(max_drawdown),
                'hit_rate': float(hit_rate),
                'total_return': float(total_return),
                'volatility': float(volatility),
                'thresholds': {
                    'min_sharpe': self.thresholds.min_sharpe_ratio,
                    'max_drawdown': self.thresholds.max_max_drawdown,
                    'min_hit_rate': self.thresholds.min_hit_rate
                },
                'checks': {
                    'sharpe_passed': sharpe_passed,
                    'drawdown_passed': drawdown_passed,
                    'hitrate_passed': hitrate_passed
                }
            }
            
        except Exception as e:
            logger.error(f"ä¸šç»©éªŒè¯å¤±è´¥: {e}")
            return {'passed': False, 'error': str(e)}
    
    
    def _apply_fallback_validation_when_insufficient_data(self, result: Dict[str, Any], 
                                                           sample_count: int) -> Dict[str, Any]:
        """å½“æ•°æ®ä¸è¶³æ—¶åº”ç”¨å›é€€éªŒè¯é€»è¾‘"""
        if sample_count < 100:  # æ•°æ®ä¸è¶³100ä¸ªæ ·æœ¬
            logger.warning(f"æ•°æ®æ ·æœ¬ä¸è¶³({sample_count})ï¼Œåº”ç”¨å®½æ¾éªŒè¯æ ‡å‡†")
            
            # å¯¹äºæ•°æ®ä¸è¶³çš„æƒ…å†µï¼Œä½¿ç”¨æ›´å®½æ¾çš„æ ‡å‡†
            if 'rank_ic' in result:
                rank_ic_val = result['rank_ic'].get('rank_ic', 0)
                if abs(rank_ic_val) >= 0.01:  # ç»å¯¹å€¼å¤§äº1%å°±è®¤ä¸ºæœ‰æ•ˆ
                    result['rank_ic']['passed'] = True
                    logger.info(f"å›é€€éªŒè¯: RankIC {rank_ic_val:.4f} >= 0.01ï¼Œé€šè¿‡éªŒè¯")
            
            if 'stability' in result:
                # å¯¹ç¨³å®šæ€§é™ä½è¦æ±‚
                non_negative_ratio = result['stability'].get('non_negative_ratio', 0)
                if non_negative_ratio >= 0.4:  # é™ä½åˆ°40%
                    result['stability']['passed'] = True
                    logger.info(f"å›é€€éªŒè¯: ç¨³å®šæ€§ {non_negative_ratio:.2f} >= 0.4ï¼Œé€šè¿‡éªŒè¯")
                    
        return result

    def _make_final_decision(self, detailed_results: Dict,
                           recommendations: List[str]) -> Tuple[float, str, List[str]]:
        """åšå‡ºæœ€ç»ˆGo/No-Goå†³ç­–"""
        
        # æƒé‡é…ç½®
        weights = {
            'rank_ic': 0.3,
            'stability': 0.25, 
            'calibration': 0.2,
            'diversity': 0.15,
            'performance': 0.1
        }
        
        # è®¡ç®—å„é¡¹å¾—åˆ†
        scores = {}
        critical_failures = []
        
        for category, weight in weights.items():
            result = detailed_results.get(category, {})
            if result.get('passed', False):
                scores[category] = 1.0
            else:
                scores[category] = 0.0
                if category in ['rank_ic', 'stability']:  # å…³é”®æŒ‡æ ‡
                    critical_failures.append(category)
        
        # åŠ æƒæ€»åˆ†
        overall_score = sum(scores[cat] * weights[cat] for cat in weights.keys())
        
        # å†³ç­–é€»è¾‘
        if len(critical_failures) > 0:
            decision = "NO_GO"
            recommendations.extend([
                f"âŒ å…³é”®æŒ‡æ ‡å¤±è´¥: {', '.join(critical_failures)}",
                "ğŸ”§ å¿…é¡»è§£å†³å…³é”®é—®é¢˜æ‰èƒ½æŠ•å…¥ç”Ÿäº§"
            ])
        elif overall_score >= 0.8:
            decision = "GO"
            recommendations.append("âœ… æ‰€æœ‰æŒ‡æ ‡è¾¾æ ‡ï¼Œå¯ä»¥æŠ•å…¥ç”Ÿäº§ä½¿ç”¨")
        elif overall_score >= 0.6:
            decision = "CONDITIONAL_GO"
            recommendations.extend([
                "âš ï¸ éƒ¨åˆ†æŒ‡æ ‡æœªè¾¾æ ‡ï¼Œå»ºè®®è°¨æ…ä½¿ç”¨",
                "ğŸ”§ å»ºè®®å…ˆå°è§„æ¨¡æµ‹è¯•ï¼Œç›‘æ§è¡¨ç°"
            ])
        else:
            decision = "NO_GO"
            recommendations.extend([
                "âŒ å¤šé¡¹æŒ‡æ ‡æœªè¾¾æ ‡ï¼Œä¸å»ºè®®æŠ•å…¥ç”Ÿäº§",
                "ğŸ”§ éœ€è¦æ˜¾è‘—æ”¹è¿›æ¨¡å‹è´¨é‡"
            ])
        
        return overall_score, decision, recommendations
    
    def _log_validation_summary(self, result: ValidationResult):
        """è®°å½•éªŒè¯æ‘˜è¦"""
        logger.info("="*60)
        logger.info("ğŸ¯ ç”Ÿäº§å°±ç»ªéªŒè¯ç»“æœ")
        logger.info("="*60)
        logger.info(f"ğŸ“Š æ€»ä½“å¾—åˆ†: {result.overall_score:.3f}")
        logger.info(f"ğŸš¦ å†³ç­–ç»“æœ: {result.go_no_go_decision}")
        
        # è¯¦ç»†ç»“æœ
        for category, results in result.detailed_results.items():
            status = "âœ…" if results.get('passed', False) else "âŒ"
            logger.info(f"{status} {category.upper()}: {results.get('passed', False)}")
        
        # å»ºè®®
        if result.recommendations:
            logger.info("\nğŸ“‹ å»ºè®®:")
            for rec in result.recommendations:
                logger.info(f"  {rec}")
        
        # é£é™©è­¦å‘Š
        if result.risk_warnings:
            logger.warning("\nâš ï¸ é£é™©è­¦å‘Š:")
            for warning in result.risk_warnings:
                logger.warning(f"  {warning}")
        
        logger.info("="*60)
    
    def _align_validation_data(self, predictions: np.ndarray, true_labels: np.ndarray, dates: pd.Series) -> Tuple[np.ndarray, np.ndarray, pd.Series]:
        """å¯¹é½å’Œæ¸…ç†éªŒè¯æ•°æ®"""
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        predictions = np.asarray(predictions)
        true_labels = np.asarray(true_labels)
        
        # é•¿åº¦å¯¹é½
        min_len = min(len(predictions), len(true_labels), len(dates))
        if len(predictions) != len(true_labels) or len(predictions) != len(dates):
            logger.warning(f"éªŒè¯æ•°æ®é•¿åº¦ä¸åŒ¹é…: pred={len(predictions)}, labels={len(true_labels)}, dates={len(dates)}")
            predictions = predictions[:min_len]
            true_labels = true_labels[:min_len]
            # å®‰å…¨åœ°æˆªå–datesï¼Œå¤„ç†Seriesæˆ–Indexç±»å‹
            if hasattr(dates, 'iloc'):
                dates = dates.iloc[:min_len]
            else:
                dates = dates[:min_len]
            logger.info(f"æ•°æ®å·²å¯¹é½åˆ°é•¿åº¦: {min_len}")
        
        # NaNæ¸…ç† - åªå¯¹é¢„æµ‹å€¼å’Œæ ‡ç­¾æ£€æŸ¥NaN
        valid_mask = ~(np.isnan(predictions) | np.isnan(true_labels))
        if not np.all(valid_mask):
            nan_count = (~valid_mask).sum()
            logger.warning(f"å‘ç°{nan_count}ä¸ªNaNå€¼ï¼Œå°†è¢«ç§»é™¤")
            predictions = predictions[valid_mask]
            true_labels = true_labels[valid_mask]
            # å®‰å…¨åœ°åº”ç”¨maskåˆ°dates
            if hasattr(dates, 'iloc'):
                dates = dates.iloc[valid_mask].reset_index(drop=True)
            elif isinstance(dates, pd.Series):
                dates = dates[valid_mask].reset_index(drop=True)
            else:
                # DatetimeIndexæˆ–å…¶ä»–Indexç±»å‹
                dates = dates[valid_mask]
        
        logger.info(f"éªŒè¯æ•°æ®æ¸…ç†å®Œæˆï¼Œæœ€ç»ˆé•¿åº¦: {len(predictions)}")
        return predictions, true_labels, dates

def create_production_validator(thresholds: Optional[ValidationThresholds] = None) -> ProductionReadinessValidator:
    """åˆ›å»ºç”Ÿäº§å°±ç»ªéªŒè¯å™¨"""
    return ProductionReadinessValidator(thresholds)

# ğŸ”¥ é›†æˆåˆ°BMAç³»ç»Ÿçš„æ¥å£å‡½æ•°
def validate_bma_production_readiness(oos_predictions: np.ndarray,
                                    oos_true_labels: np.ndarray,
                                    prediction_dates: pd.Series,
                                    calibration_results: Optional[Dict] = None,
                                    weight_details: Optional[Dict] = None,
                                    custom_thresholds: Optional[Dict] = None) -> Dict[str, Any]:
    """
    éªŒè¯BMAç³»ç»Ÿç”Ÿäº§å°±ç»ªçŠ¶æ€
    
    Args:
        oos_predictions: æ ·å¤–é¢„æµ‹
        oos_true_labels: æ ·å¤–çœŸå®æ ‡ç­¾  
        prediction_dates: é¢„æµ‹æ—¥æœŸ
        calibration_results: æ ¡å‡†ç»“æœ
        weight_details: æƒé‡æ˜ç»†
        custom_thresholds: è‡ªå®šä¹‰é˜ˆå€¼
    
    Returns:
        éªŒè¯ç»“æœå­—å…¸
    """
    try:
        # åˆ›å»ºé˜ˆå€¼é…ç½®
        thresholds = ValidationThresholds()
        if custom_thresholds:
            for key, value in custom_thresholds.items():
                if hasattr(thresholds, key):
                    setattr(thresholds, key, value)
        
        # åˆ›å»ºéªŒè¯å™¨
        validator = create_production_validator(thresholds)
        
        # è¿è¡ŒéªŒè¯
        result = validator.validate_bma_production_readiness(
            oos_predictions=oos_predictions,
            oos_true_labels=oos_true_labels,
            prediction_dates=prediction_dates,
            calibration_results=calibration_results,
            weight_details=weight_details
        )
        
        return {
            'success': True,
            'validation_result': asdict(result)
        }
        
    except Exception as e:
        logger.error(f"ç”Ÿäº§å°±ç»ªéªŒè¯å¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e),
            'go_no_go_decision': 'NO_GO'
        }