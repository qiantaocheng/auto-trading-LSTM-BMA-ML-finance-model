#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€ç‰¹å¾ç®¡é“ - è§£å†³è®­ç»ƒ-é¢„æµ‹ç‰¹å¾ç»´åº¦ä¸åŒ¹é…é—®é¢˜
ç¡®ä¿è®­ç»ƒå’Œé¢„æµ‹ä½¿ç”¨å®Œå…¨ç›¸åŒçš„ç‰¹å¾å¤„ç†æµç¨‹
"""

import pandas as pd
import numpy as np
import pickle
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from sklearn.preprocessing import StandardScaler, RobustScaler
# PCA COMPLETELY REMOVED - NO DIMENSIONALITY REDUCTION
from sklearn.impute import SimpleImputer

logger = logging.getLogger(__name__)

@dataclass
class FeaturePipelineConfig:
    """ç‰¹å¾ç®¡é“é…ç½® - ğŸš« NO PCA - DIRECT FEATURE USAGE"""
    enable_alpha_summary: bool = False
    # ğŸš« PCA COMPLETELY REMOVED - NO DIMENSIONALITY REDUCTION
    enable_scaling: bool = True
    scaler_type: str = 'robust'  # 'standard' or 'robust'
    imputation_strategy: str = 'median'
    feature_selection_threshold: float = 0.001
    save_pipeline: bool = True
    cache_dir: str = "cache/feature_pipeline"

class UnifiedFeaturePipeline:
    """
    ç»Ÿä¸€ç‰¹å¾ç®¡é“ - ç¡®ä¿è®­ç»ƒå’Œé¢„æµ‹æ—¶ç‰¹å¾å®Œå…¨ä¸€è‡´
    
    æ ¸å¿ƒåŠŸèƒ½:
    1. ç»Ÿä¸€ç‰¹å¾ç”Ÿæˆæµç¨‹
    2. ä¿å­˜/åŠ è½½ç‰¹å¾è½¬æ¢å™¨çŠ¶æ€
    3. ç»´åº¦ä¸€è‡´æ€§æ£€æŸ¥
    4. å¯é‡ç°çš„ç‰¹å¾å¤„ç†
    """
    
    def __init__(self, config: FeaturePipelineConfig):
        self.config = config
        self.is_fitted = False
        self.feature_names = None
        self.feature_dim = None
        
        # ç‰¹å¾å¤„ç†ç»„ä»¶ - ğŸš« NO PCA
        self.scaler = None
        # self.pca = None  # ğŸš« REMOVED - NO PCA
        self.imputer = None
        self.alpha_summary_generator = None
        
        # ç¼“å­˜ç›®å½•
        self.cache_dir = Path(config.cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ç‰¹å¾è½¬æ¢å†å²
        self.transform_history = {}
        
    def fit_transform(self, 
                     base_features: pd.DataFrame, 
                     targets: Optional[pd.Series] = None,
                     dates: Optional[pd.Series] = None,
                     alpha_data: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        æ‹Ÿåˆç‰¹å¾ç®¡é“å¹¶è½¬æ¢æ•°æ®
        
        Args:
            base_features: åŸºç¡€ç‰¹å¾æ•°æ®
            targets: ç›®æ ‡å˜é‡ (ç”¨äºæœ‰ç›‘ç£ç‰¹å¾é€‰æ‹©)
            dates: æ—¥æœŸåºåˆ—
            alpha_data: Alphaå› å­æ•°æ®
            
        Returns:
            (è½¬æ¢åç‰¹å¾, è½¬æ¢ä¿¡æ¯)
        """
        logger.info(f"å¼€å§‹æ‹Ÿåˆç‰¹å¾ç®¡é“ï¼ŒåŸºç¡€ç‰¹å¾ç»´åº¦: {base_features.shape}")
        
        # 1. æ•°æ®é¢„å¤„ç†å’Œæ¸…æ´—
        X_clean = self._clean_features(base_features)
        logger.info(f"ç‰¹å¾æ¸…æ´—å®Œæˆ: {base_features.shape} -> {X_clean.shape}")
        
        # 2. ç”ŸæˆAlphaæ‘˜è¦ç‰¹å¾ (å¦‚æœå¯ç”¨)
        if self.config.enable_alpha_summary and alpha_data is not None:
            alpha_summary = self._generate_alpha_summary_features(alpha_data)
            logger.info(f"Alphaæ‘˜è¦ç‰¹å¾ç”Ÿæˆ: {alpha_summary.shape}")
            
            # èåˆç‰¹å¾ - ç¡®ä¿ç´¢å¼•å¯¹é½
            X_fused = self._fuse_features(X_clean, alpha_summary)
            logger.info(f"ç‰¹å¾èåˆå®Œæˆ: {X_fused.shape}")
        else:
            X_fused = X_clean
            logger.info("è·³è¿‡Alphaæ‘˜è¦ç‰¹å¾ï¼Œä½¿ç”¨åŸºç¡€ç‰¹å¾")
        
        # 3. æ‹Ÿåˆæ•°æ®æ’è¡¥å™¨
        self.imputer = SimpleImputer(strategy=self.config.imputation_strategy)
        X_imputed = pd.DataFrame(
            self.imputer.fit_transform(X_fused),
            index=X_fused.index,
            columns=X_fused.columns
        )
        logger.info(f"æ•°æ®æ’è¡¥å®Œæˆï¼ŒNaNå¤„ç†ç­–ç•¥: {self.config.imputation_strategy}")
        
        # 4. æ‹Ÿåˆæ ‡å‡†åŒ–å™¨
        if self.config.enable_scaling:
            if self.config.scaler_type == 'robust':
                self.scaler = RobustScaler()
            else:
                self.scaler = StandardScaler()
                
            X_scaled = pd.DataFrame(
                self.scaler.fit_transform(X_imputed),
                index=X_imputed.index,
                columns=X_imputed.columns
            )
            logger.info(f"ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆï¼Œæ–¹æ³•: {self.config.scaler_type}")
        else:
            X_scaled = X_imputed
        
        # 5. ğŸš« NO PCA - DIRECT FEATURE USAGE
        # PCA COMPLETELY REMOVED - USE FEATURES DIRECTLY
        X_final = X_scaled
        logger.info(f"ğŸš« NO PCA - ç›´æ¥ä½¿ç”¨ç‰¹å¾: {X_scaled.shape[1]} ä¸ªç‰¹å¾")
        logger.info("âœ… ç‰¹å¾ç»´åº¦ä¿æŒå®Œå…¨ä¸€è‡´ï¼Œç¡®ä¿è®­ç»ƒé¢„æµ‹æ— å·®å¼‚")
        
        # 6. ä¿å­˜ç®¡é“çŠ¶æ€
        self.feature_names = list(X_final.columns)
        self.feature_dim = X_final.shape[1]
        self.is_fitted = True
        
        # 7. ç”Ÿæˆè½¬æ¢ä¿¡æ¯
        transform_info = {
            'input_shape': base_features.shape,
            'output_shape': X_final.shape,
            'feature_names': self.feature_names,
            'alpha_summary_enabled': self.config.enable_alpha_summary and alpha_data is not None,
            'pca_enabled': False,  # ğŸš« ALWAYS FALSE - NO PCA
            'pca_components': 0,   # ğŸš« NO PCA COMPONENTS
            'explained_variance_ratio': None,  # ğŸš« NO PCA ANALYSIS
            'scaling_enabled': self.config.enable_scaling,
            'scaler_type': self.config.scaler_type if self.config.enable_scaling else None
        }
        
        # 8. ä¿å­˜ç®¡é“ (å¦‚æœå¯ç”¨)
        if self.config.save_pipeline:
            self.save_pipeline()
        
        logger.info(f"ç‰¹å¾ç®¡é“æ‹Ÿåˆå®Œæˆï¼Œæœ€ç»ˆç‰¹å¾ç»´åº¦: {X_final.shape}")
        return X_final, transform_info
    
    def transform(self, 
                 base_features: pd.DataFrame,
                 alpha_data: Optional[pd.DataFrame] = None) -> pd.DataFrame:
        """
        ä½¿ç”¨å·²æ‹Ÿåˆçš„ç®¡é“è½¬æ¢æ–°æ•°æ®
        
        Args:
            base_features: åŸºç¡€ç‰¹å¾æ•°æ®
            alpha_data: Alphaå› å­æ•°æ®
            
        Returns:
            è½¬æ¢åçš„ç‰¹å¾æ•°æ®
        """
        if not self.is_fitted:
            raise ValueError("ç‰¹å¾ç®¡é“å°šæœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨fit_transform()")
        
        logger.info(f"ä½¿ç”¨å·²æ‹Ÿåˆç®¡é“è½¬æ¢ç‰¹å¾ï¼Œè¾“å…¥ç»´åº¦: {base_features.shape}")
        
        # 1. æ¸…æ´—ç‰¹å¾
        X_clean = self._clean_features(base_features)
        
        # 2. ç”ŸæˆAlphaæ‘˜è¦ç‰¹å¾ (å¦‚æœåœ¨è®­ç»ƒæ—¶å¯ç”¨äº†)
        if self.config.enable_alpha_summary and alpha_data is not None:
            alpha_summary = self._generate_alpha_summary_features(alpha_data)
            X_fused = self._fuse_features(X_clean, alpha_summary)
        else:
            X_fused = X_clean
        
        # 3. åº”ç”¨æ•°æ®æ’è¡¥
        if self.imputer is not None:
            X_imputed = pd.DataFrame(
                self.imputer.transform(X_fused),
                index=X_fused.index,
                columns=X_fused.columns
            )
        else:
            X_imputed = X_fused
        
        # 4. åº”ç”¨æ ‡å‡†åŒ–
        if self.scaler is not None:
            X_scaled = pd.DataFrame(
                self.scaler.transform(X_imputed),
                index=X_imputed.index,
                columns=X_imputed.columns
            )
        else:
            X_scaled = X_imputed
        
        # 5. ğŸš« NO PCA - DIRECT FEATURE USAGE
        # PCA COMPLETELY REMOVED - USE FEATURES DIRECTLY
        X_final = X_scaled
        logger.info(f"ğŸš« NO PCA è½¬æ¢ - ç›´æ¥ä½¿ç”¨ {X_scaled.shape[1]} ä¸ªç‰¹å¾")
        
        # 6. ç»´åº¦ä¸€è‡´æ€§æ£€æŸ¥
        if X_final.shape[1] != self.feature_dim:
            logger.error(f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.feature_dim}, å®é™…{X_final.shape[1]}")
            raise ValueError(f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.feature_dim}, å®é™…{X_final.shape[1]}")
        
        # 7. ç¡®ä¿åˆ—åä¸€è‡´
        X_final.columns = self.feature_names
        
        logger.info(f"ç‰¹å¾è½¬æ¢å®Œæˆï¼Œè¾“å‡ºç»´åº¦: {X_final.shape}")
        return X_final
    
    def _clean_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—ç‰¹å¾æ•°æ®"""
        # åªä¿ç•™æ•°å€¼ç‰¹å¾
        numeric_features = features.select_dtypes(include=[np.number])
        
        # ç§»é™¤å…¨NaNåˆ—
        numeric_features = numeric_features.dropna(axis=1, how='all')
        
        # ç§»é™¤å¸¸æ•°åˆ—
        numeric_features = numeric_features.loc[:, numeric_features.std() > 1e-8]
        
        return numeric_features
    
    def _generate_alpha_summary_features(self, alpha_data: pd.DataFrame) -> pd.DataFrame:
        """ç”ŸæˆAlphaæ‘˜è¦ç‰¹å¾ - ä¸åšPCAé¢„å¤„ç†ï¼Œç•™ç»™ç»Ÿä¸€PCAå¤„ç†"""
        # æ¸…æ´—Alphaæ•°æ®ä½†ä¸åšPCAå‹ç¼©
        alpha_clean = self._clean_features(alpha_data)
        
        # ç›´æ¥è¿”å›æ¸…æ´—åçš„Alphaç‰¹å¾ï¼Œè®©å®ƒä»¬ä¸ä¼ ç»Ÿç‰¹å¾ä¸€èµ·åšç»Ÿä¸€PCA
        return alpha_clean
    
    def _fuse_features(self, base_features: pd.DataFrame, alpha_features: pd.DataFrame) -> pd.DataFrame:
        """èåˆåŸºç¡€ç‰¹å¾å’ŒAlphaç‰¹å¾ - ä¿®å¤ç´¢å¼•å¯¹é½é—®é¢˜"""
        
        # ğŸ”¥ CRITICAL FIX: ç¡®ä¿ä¸¤è€…éƒ½æ˜¯MultiIndexæ ¼å¼
        logger.info(f"ç‰¹å¾åˆå¹¶å¼€å§‹: base_features{base_features.shape}, alpha_features{alpha_features.shape}")
        logger.info(f"baseç´¢å¼•ç±»å‹: {type(base_features.index)}, alphaç´¢å¼•ç±»å‹: {type(alpha_features.index)}")
        
        # 1. æ ‡å‡†åŒ–ç´¢å¼•æ ¼å¼
        base_standardized = self._ensure_multiindex_format(base_features, "base_features")
        alpha_standardized = self._ensure_multiindex_format(alpha_features, "alpha_features") 
        
        if base_standardized is None or alpha_standardized is None:
            logger.error("âŒ ç‰¹å¾æ ‡å‡†åŒ–å¤±è´¥")
            return base_features  # å›é€€åˆ°åŸºç¡€ç‰¹å¾
        
        # 2. ä½¿ç”¨æ™ºèƒ½ç´¢å¼•å¯¹é½
        try:
            common_index = base_standardized.index.intersection(alpha_standardized.index)
            logger.info(f"å…¬å…±ç´¢å¼•æ•°é‡: {len(common_index)} / base:{len(base_standardized)} / alpha:{len(alpha_standardized)}")
            
            if len(common_index) > 0:
                # ä½¿ç”¨å…¬å…±ç´¢å¼•å¯¹é½
                base_aligned = base_standardized.loc[common_index]
                alpha_aligned = alpha_standardized.loc[common_index]
                
                # æ£€æŸ¥åˆ—åé‡å 
                overlapping_cols = set(base_aligned.columns) & set(alpha_aligned.columns)
                if overlapping_cols:
                    logger.warning(f"å‘ç°é‡å åˆ—: {overlapping_cols}")
                    # é‡å‘½åalphaç‰¹å¾ä»¥é¿å…å†²çª
                    alpha_renamed = alpha_aligned.rename(columns={col: f"alpha_{col}" for col in overlapping_cols})
                    fused = pd.concat([base_aligned, alpha_renamed], axis=1)
                else:
                    fused = pd.concat([base_aligned, alpha_aligned], axis=1)
                    
                logger.info(f"âœ… ç´¢å¼•å¯¹é½åˆå¹¶æˆåŠŸ: {fused.shape}")
                
            else:
                # å›é€€ç­–ç•¥ï¼šæ—¶é—´æˆ³è¿‘ä¼¼å¯¹é½
                logger.warning("âš ï¸ æ— å…¬å…±ç´¢å¼•ï¼Œå°è¯•æ—¶é—´æˆ³è¿‘ä¼¼å¯¹é½")
                fused = self._approximate_time_alignment(base_standardized, alpha_standardized)
                
                if fused is None:
                    # æœ€åå›é€€ï¼šé•¿åº¦æˆªæ–­å¯¹é½
                    logger.warning("âš ï¸ æ—¶é—´å¯¹é½å¤±è´¥ï¼Œä½¿ç”¨é•¿åº¦æˆªæ–­")
                    min_len = min(len(base_standardized), len(alpha_standardized))
                    fused = pd.concat([
                        base_standardized.iloc[:min_len], 
                        alpha_standardized.iloc[:min_len]
                    ], axis=1)
                    logger.info(f"âš ï¸ é•¿åº¦å¯¹é½å®Œæˆ: {fused.shape}")
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾åˆå¹¶å¤±è´¥: {e}")
            logger.info("ğŸ”„ å›é€€åˆ°åŸºç¡€ç‰¹å¾")
            return base_features
        
        logger.info(f"ğŸ¯ ç‰¹å¾åˆå¹¶å®Œæˆ: {base_features.shape} + {alpha_features.shape} â†’ {fused.shape}")
        return fused
    
    def _ensure_multiindex_format(self, df: pd.DataFrame, data_name: str) -> pd.DataFrame:
        """ç¡®ä¿DataFrameæ˜¯MultiIndexæ ¼å¼"""
        if isinstance(df.index, pd.MultiIndex):
            logger.debug(f"âœ… {data_name} å·²æ˜¯MultiIndexæ ¼å¼")
            return df
            
        if 'date' in df.columns and 'ticker' in df.columns:
            try:
                dates = pd.to_datetime(df['date'])
                tickers = df['ticker']
                multi_idx = pd.MultiIndex.from_arrays([dates, tickers], names=['date', 'ticker'])
                
                df_multiindex = df.drop(['date', 'ticker'], axis=1).copy()
                df_multiindex.index = multi_idx
                
                logger.info(f"âœ… {data_name} è½¬æ¢ä¸ºMultiIndex: {df_multiindex.shape}")
                return df_multiindex
                
            except Exception as e:
                logger.error(f"âŒ {data_name} MultiIndexè½¬æ¢å¤±è´¥: {e}")
                return None
        else:
            logger.warning(f"âš ï¸ {data_name} ç¼ºå°‘date/tickeråˆ—ï¼Œæ— æ³•åˆ›å»ºMultiIndex")
            return None
    
    def _approximate_time_alignment(self, base_df: pd.DataFrame, alpha_df: pd.DataFrame) -> pd.DataFrame:
        """æ—¶é—´æˆ³è¿‘ä¼¼å¯¹é½ç­–ç•¥"""
        try:
            # è·å–æ—¥æœŸçº§åˆ«çš„ç´¢å¼•
            base_dates = base_df.index.get_level_values('date').unique()
            alpha_dates = alpha_df.index.get_level_values('date').unique()
            
            # æ‰¾åˆ°æ—¶é—´é‡å èŒƒå›´
            date_overlap = pd.Index(base_dates).intersection(pd.Index(alpha_dates))
            
            if len(date_overlap) > 0:
                # æŒ‰æ—¥æœŸè¿‡æ»¤
                base_filtered = base_df[base_df.index.get_level_values('date').isin(date_overlap)]
                alpha_filtered = alpha_df[alpha_df.index.get_level_values('date').isin(date_overlap)]
                
                # å†æ¬¡å°è¯•ç´¢å¼•äº¤é›†
                common_index = base_filtered.index.intersection(alpha_filtered.index)
                if len(common_index) > 0:
                    fused = pd.concat([
                        base_filtered.loc[common_index],
                        alpha_filtered.loc[common_index]
                    ], axis=1)
                    logger.info(f"âœ… æ—¶é—´è¿‘ä¼¼å¯¹é½æˆåŠŸ: {len(date_overlap)}ä¸ªé‡å æ—¥æœŸ, æœ€ç»ˆ{fused.shape}")
                    return fused
                    
        except Exception as e:
            logger.warning(f"æ—¶é—´è¿‘ä¼¼å¯¹é½å¤±è´¥: {e}")
            
        return None
    
    def save_pipeline(self, filepath: Optional[str] = None) -> str:
        """ä¿å­˜ç‰¹å¾ç®¡é“"""
        if filepath is None:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = self.cache_dir / f"feature_pipeline_{timestamp}.pkl"
        
        pipeline_state = {
            'config': self.config,
            'scaler': self.scaler,
            # 'pca': None,  # ğŸš« REMOVED - NO PCA
            'imputer': self.imputer,
            'feature_names': self.feature_names,
            'feature_dim': self.feature_dim,
            'is_fitted': self.is_fitted,
            'transform_history': self.transform_history
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(pipeline_state, f)
        
        logger.info(f"ç‰¹å¾ç®¡é“å·²ä¿å­˜: {filepath}")
        return str(filepath)
    
    def load_pipeline(self, filepath: str) -> None:
        """åŠ è½½ç‰¹å¾ç®¡é“"""
        with open(filepath, 'rb') as f:
            pipeline_state = pickle.load(f)
        
        self.config = pipeline_state['config']
        self.scaler = pipeline_state['scaler']
        # self.pca = None  # ğŸš« REMOVED - NO PCA
        self.imputer = pipeline_state['imputer']
        self.feature_names = pipeline_state['feature_names']
        self.feature_dim = pipeline_state['feature_dim']
        self.is_fitted = pipeline_state['is_fitted']
        self.transform_history = pipeline_state.get('transform_history', {})
        
        logger.info(f"ç‰¹å¾ç®¡é“å·²åŠ è½½: {filepath}")
        logger.info(f"ç®¡é“ç‰¹å¾ç»´åº¦: {self.feature_dim}")
        
    @classmethod
    def create_production_pipeline(cls) -> 'UnifiedFeaturePipeline':
        """åˆ›å»ºç”Ÿäº§çº§ç‰¹å¾ç®¡é“"""
        config = FeaturePipelineConfig(
            enable_alpha_summary=True,
            enable_pca=False,  # ğŸš« ALWAYS FALSE - NO PCA
            # pca_variance_threshold=0.95,  # ğŸš« REMOVED - NO PCA
            enable_scaling=True,
            scaler_type='robust',
            imputation_strategy='median',
            save_pipeline=True
        )
        return cls(config)
    
    def get_feature_info(self) -> Dict[str, Any]:
        """è·å–ç‰¹å¾ç®¡é“ä¿¡æ¯"""
        return {
            'is_fitted': self.is_fitted,
            'feature_dim': self.feature_dim,
            'feature_names': self.feature_names[:10] if self.feature_names else None,  # åªæ˜¾ç¤ºå‰10ä¸ª
            'total_features': len(self.feature_names) if self.feature_names else 0,
            'has_pca': False,  # ğŸš« ALWAYS FALSE - NO PCA
            'has_scaler': self.scaler is not None,
            'has_imputer': self.imputer is not None,
            'config': {
                'enable_alpha_summary': self.config.enable_alpha_summary,
                'enable_pca': False,  # ğŸš« ALWAYS FALSE - NO PCA
                'enable_scaling': self.config.enable_scaling,
                'scaler_type': self.config.scaler_type
            }
        }