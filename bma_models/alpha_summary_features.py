#!/usr/bin/env python3
"""
Alpha Summary Features Integration Module
Route A: Representation-level Alpha injection into LTR/ML pipeline

Key Design Principles:
1. Minimal code invasion (18 columns max)
2. Strict time alignment and leakage prevention  
3. Cross-sectional winsorization and standardization
4. Dimensionality reduction (PCA/PLS) for Alpha compression
5. Robust summary statistics generation
6. Integration with existing X_clean -> X_fused pipeline
"""

import pandas as pd
import numpy as np
import warnings
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import logging

# Scientific computing
from scipy import stats
from scipy.stats import spearmanr
from sklearn.decomposition import PCA
from sklearn.cross_decomposition import PLSRegression
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.covariance import LedoitWolf

# Configure logging
logger = logging.getLogger(__name__)

@dataclass
class AlphaSummaryConfig:
    """Configuration for Alpha Summary Feature Generation"""
    # A1: Data cleaning
    winsorize_lower: float = 0.01  # 1% lower tail
    winsorize_upper: float = 0.99  # 99% upper tail
    use_mad_winsorize: bool = True  # Use median Â± 3MAD instead of percentile
    neutralize_by_industry: bool = False  # DISABLED to avoid MultiIndex errors
    neutralize_by_market_cap: bool = False
    
    # A2: Dimensionality reduction (PROFESSIONAL: 40+ -> 18 features)
    max_alpha_features: int = 18  # Professional standard: 15-20 features
    min_alpha_features: int = 15  # Minimum for robustness
    pca_variance_explained: float = 0.85  # PCA variance threshold
    pls_n_components: int = 8  # Increased PLS components
    use_ic_weighted: bool = True  # PRIMARY: Use professional IC-weighted method
    use_pca: bool = False  # DEPRECATED: Use as fallback only
    use_pls: bool = False
    use_ic_weighted_composite: bool = False  # DEPRECATED: Old simple version
    include_alpha_strategy_signal: bool = True  # Include composite Alpha strategy signal as feature
    
    # A3: Summary statistics
    include_dispersion: bool = True
    include_agreement: bool = True  
    include_quality: bool = True
    ic_lookback_days: int = 120  # IC quality lookback
    
    # A4: Integration
    fill_method: str = 'cross_median'  # cross_median, forward_fill, zero
    data_type: str = 'float32'  # Memory optimization
    
    # A5: Time alignment
    strict_time_validation: bool = True
    min_history_days: int = 60  # Minimum history for quality metrics

class AlphaSummaryProcessor:
    """
    Alpha Summary Feature Processor
    Converts 45+ Alpha signals -> 5-10 summary features for ML injection
    """
    
    def __init__(self, config: AlphaSummaryConfig = None):
        self.config = config or AlphaSummaryConfig()
        self.scaler = RobustScaler()
        self.imputer = SimpleImputer(strategy='median')
        
        # Cache for IC calculations and fitted components
        self.ic_cache = {}
        self.pca_fitted = None
        self.pls_fitted = None
        self.neutralization_models = {}
        
        # Statistics tracking
        self.stats = {
            'total_alphas_processed': 0,
            'features_generated': 0,
            'time_violations': 0,
            'neutralization_r2': {},
            'compression_variance_explained': 0.0
        }
        
        # Initialize logging
        logger.info(f"Alphaæ‘˜è¦ç‰¹å¾ç”Ÿæˆå™¨åˆå§‹åŒ–: ç›®æ ‡{self.config.max_alpha_features}ä¸ªç‰¹å¾")
        logger.info(f"  - PCAå‹ç¼©: {self.config.enable_pca_compression}")
        logger.info(f"  - ICæƒé‡å‹ç¼©: {self.config.enable_ic_compression}")
        logger.info(f"  - ç»Ÿè®¡ç‰¹å¾: dispersion={self.config.include_dispersion}, agreement={self.config.include_agreement}, quality={self.config.include_quality}")
        logger.info(f"  - Alphaç­–ç•¥ä¿¡å·: {self.config.include_alpha_strategy}")
        logger.info(f"  - æ—¶é—´è¿è§„æ£€æŸ¥: {self.config.prevent_lookahead}")
        logger.info(f"  - è¡Œä¸šä¸­æ€§åŒ–: {self.config.neutralize_by_industry}")
        logger.info(f"  - MAD Winsorize: {self.config.use_mad_winsorize}")
        logger.info(f"  - PCAæ–¹å·®é˜ˆå€¼: {self.config.pca_variance_explained}")
        
    def _log_data_quality_info(self, alpha_df: pd.DataFrame, market_data: pd.DataFrame = None):
        """è®°å½•æ•°æ®è´¨é‡ä¿¡æ¯ç”¨äºè°ƒè¯•"""
        logger.info("ğŸ“Š æ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š:")
        
        # Alphaæ•°æ®åŸºæœ¬ä¿¡æ¯
        logger.info(f"  Alphaæ•°æ®å½¢çŠ¶: {alpha_df.shape}")
        logger.info(f"  Alphaæ•°æ®ç´¢å¼•ç±»å‹: {type(alpha_df.index)}")
        if isinstance(alpha_df.index, pd.MultiIndex):
            logger.info(f"  MultiIndexå±‚çº§: {alpha_df.index.names}")
        
        # æ•°å€¼åˆ—ç»Ÿè®¡
        numeric_cols = alpha_df.select_dtypes(include=[np.number]).columns
        logger.info(f"  æ•°å€¼åˆ—æ•°é‡: {len(numeric_cols)}")
        
        if len(numeric_cols) > 0:
            # æ•°æ®èŒƒå›´æ£€æŸ¥
            numeric_data = alpha_df[numeric_cols]
            all_zeros = (numeric_data == 0).all()
            constant_cols = numeric_data.nunique() == 1
            
            if all_zeros.any():
                zero_cols = all_zeros[all_zeros].index.tolist()
                logger.warning(f"  âš ï¸ å…¨é›¶åˆ—({len(zero_cols)}ä¸ª): {zero_cols[:5]}")
            
            if constant_cols.any():
                const_cols = constant_cols[constant_cols].index.tolist()
                logger.warning(f"  âš ï¸ å¸¸æ•°åˆ—({len(const_cols)}ä¸ª): {const_cols[:5]}")
            
            # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            means = numeric_data.mean()
            stds = numeric_data.std()
            
            logger.debug(f"  Alphaå‡å€¼èŒƒå›´: [{means.min():.6f}, {means.max():.6f}]")
            logger.debug(f"  Alphaæ ‡å‡†å·®èŒƒå›´: [{stds.min():.6f}, {stds.max():.6f}]")
            logger.debug(f"  éé›¶æ ‡å‡†å·®åˆ—æ•°: {(stds > 1e-10).sum()}")
            
            # ç¼ºå¤±å€¼æ£€æŸ¥
            missing_ratio = numeric_data.isnull().mean()
            high_missing = missing_ratio[missing_ratio > 0.5]
            if not high_missing.empty:
                logger.warning(f"  âš ï¸ é«˜ç¼ºå¤±ç‡åˆ—({len(high_missing)}ä¸ª): {high_missing.index.tolist()[:5]}")
        
        # æ—¶é—´èŒƒå›´æ£€æŸ¥
        try:
            if isinstance(alpha_df.index, pd.MultiIndex) and 'date' in alpha_df.index.names:
                dates = alpha_df.index.get_level_values('date')
                unique_dates = pd.Series(dates).drop_duplicates()
                logger.info(f"  æ—¶é—´èŒƒå›´: {unique_dates.min()} åˆ° {unique_dates.max()}")
                logger.info(f"  äº¤æ˜“æ—¥æ•°é‡: {len(unique_dates)}")
                
            elif 'date' in alpha_df.columns:
                dates = pd.to_datetime(alpha_df['date'])
                logger.info(f"  æ—¶é—´èŒƒå›´: {dates.min()} åˆ° {dates.max()}")
                logger.info(f"  äº¤æ˜“æ—¥æ•°é‡: {dates.nunique()}")
        except Exception as e:
            logger.debug(f"æ—¶é—´èŒƒå›´æ£€æŸ¥å¤±è´¥: {e}")
        
        # è‚¡ç¥¨æ•°é‡ç»Ÿè®¡
        try:
            if isinstance(alpha_df.index, pd.MultiIndex) and 'ticker' in alpha_df.index.names:
                tickers = alpha_df.index.get_level_values('ticker')
                logger.info(f"  è‚¡ç¥¨æ•°é‡: {pd.Series(tickers).nunique()}")
            elif 'ticker' in alpha_df.columns:
                logger.info(f"  è‚¡ç¥¨æ•°é‡: {alpha_df['ticker'].nunique()}")
        except Exception as e:
            logger.debug(f"è‚¡ç¥¨æ•°é‡ç»Ÿè®¡å¤±è´¥: {e}")
        
        # Market dataæ£€æŸ¥
        if market_data is not None and not market_data.empty:
            logger.info(f"  å¸‚åœºæ•°æ®å½¢çŠ¶: {market_data.shape}")
        else:
            logger.warning("  âš ï¸ æœªæä¾›å¸‚åœºæ•°æ®")
        
    def process_alpha_to_summary(self, 
                               alpha_df: pd.DataFrame,
                               market_data: pd.DataFrame,
                               target_dates: pd.Series = None) -> pd.DataFrame:
        """
        Main processing pipeline: Alpha signals -> Summary features
        
        Args:
            alpha_df: Raw Alpha signals (date, ticker, alpha_001, alpha_002, ...)
            market_data: Market context data (date, ticker, market_cap, industry, ...)
            target_dates: Target prediction dates (for time validation)
            
        Returns:
            Summary features DataFrame (date, ticker, alpha_pc1, alpha_pc2, ..., alpha_quality)
        """
        logger.info(f"å¼€å§‹Alphaæ‘˜è¦ç‰¹å¾å¤„ç†ï¼Œè¾“å…¥å½¢çŠ¶: {alpha_df.shape}")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥å’Œè°ƒè¯•ä¿¡æ¯
        self._log_data_quality_info(alpha_df, market_data)
        
        if alpha_df.empty:
            logger.warning("è¾“å…¥Alphaæ•°æ®ä¸ºç©º")
            return pd.DataFrame()
        
        # A1: Data cleaning and alignment
        alpha_cleaned = self._clean_and_align_data(alpha_df, market_data, target_dates)
        if alpha_cleaned.empty:
            logger.warning("æ•°æ®æ¸…æ´—åä¸ºç©º")
            return pd.DataFrame()
        
        # A2: Dimensionality reduction
        alpha_compressed = self._compress_alpha_dimensions(alpha_cleaned)
        
        # A3: Robust summary statistics
        alpha_stats = self._compute_summary_statistics(alpha_cleaned)
        
        # A3.5: Alpha strategy composite signal
        alpha_strategy_signal = self._compute_alpha_strategy_signal(alpha_cleaned) if self.config.include_alpha_strategy_signal else None
        
        # A4: Combine and prepare final features
        summary_features = self._combine_and_finalize_features(
            alpha_compressed, alpha_stats, alpha_strategy_signal, alpha_cleaned.index
        )
        
        # ğŸ”§ å¥åº·æ£€æŸ¥3: æ—¶é—´å¯¹é½è¿è§„é—¨æ§›æ”¶ç´§ (åœ¨æœ€ç»ˆè¾“å‡ºå‰æ£€æŸ¥)
        violations_result = self._validate_time_alignment_detailed(summary_features, target_dates) if target_dates is not None else {'total_violations': 0, 'bad_columns': []}
        violation_rate = violations_result['total_violations'] / (summary_features.shape[0] + 1e-8)
        
        if violation_rate > 0.05:  # æ”¶ç´§é˜ˆå€¼ï¼šè¿è§„ç‡è¶…è¿‡5%
            bad_columns = violations_result.get('bad_columns', [])
            logger.warning(f"[SELECTIVE_CLEANUP] æ—¶é—´å¯¹é½è¿è§„è¿‡å¤š({violations_result['total_violations']}é¡¹, {violation_rate:.1%})")
            logger.warning(f"[SELECTIVE_CLEANUP] é—®é¢˜åˆ—: {bad_columns}")
            
            # é€‰æ‹©æ€§æ¸…ç†ï¼šä»…ç§»é™¤é—®é¢˜åˆ—ï¼Œè€Œéå…¨ä½“å›é€€
            return self._selective_column_cleanup(summary_features, bad_columns)
        
        logger.info(f"âœ… Alphaæ‘˜è¦ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {summary_features.shape}")
        self.stats['features_generated'] = summary_features.shape[1]
        
        return summary_features
    
    def _fallback_to_traditional_features(self, alpha_df: pd.DataFrame) -> pd.DataFrame:
        """ä¼ ç»Ÿç‰¹å¾å›é€€å®ç°"""
        try:
            # é€‰æ‹©æ•°å€¼åˆ—ï¼Œåº”ç”¨ç®€å•ç»Ÿè®¡èšåˆ
            numeric_cols = alpha_df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) == 0:
                return pd.DataFrame()
            
            # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
            fallback_features = pd.DataFrame(index=alpha_df.index)
            for col in numeric_cols[:10]:  # é™åˆ¶å‰10ä¸ªé¿å…è¿‡åº¦ç»´åº¦
                fallback_features[f'mean_{col}'] = alpha_df[col].rolling(5).mean()
                fallback_features[f'std_{col}'] = alpha_df[col].rolling(10).std()
            
            logger.info(f"[FALLBACK] ä¼ ç»Ÿç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œå½¢çŠ¶: {fallback_features.shape}")
            return fallback_features.fillna(0)
        except Exception as e:
            logger.error(f"[FALLBACK] ä¼ ç»Ÿç‰¹å¾ç”Ÿæˆå¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _clean_and_align_data(self, 
                            alpha_df: pd.DataFrame,
                            market_data: pd.DataFrame,
                            target_dates: pd.Series = None) -> pd.DataFrame:
        """A1: Cross-sectional cleaning, neutralization, time alignment"""
        
        # âœ… FIX: æ™ºèƒ½ç´¢å¼•æ ¼å¼å¤„ç†
        # æ£€æŸ¥å¹¶å°è¯•åˆ›å»ºåˆé€‚çš„ç´¢å¼•ç»“æ„
        try:
            if not isinstance(alpha_df.index, pd.MultiIndex):
                # å°è¯•åˆ›å»ºMultiIndexï¼Œä½†å…ˆæ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
                if 'date' in alpha_df.columns and 'ticker' in alpha_df.columns:
                    alpha_df = alpha_df.set_index(['date', 'ticker'])
                elif 'date' in alpha_df.columns:
                    # åªæœ‰dateåˆ—ï¼Œåˆ›å»ºç®€å•çš„æ—¶é—´ç´¢å¼•
                    alpha_df = alpha_df.set_index('date')
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œä¿æŒåŸå§‹ç´¢å¼•
        except Exception as e:
            logger.warning(f"ç´¢å¼•è®¾ç½®å¤±è´¥: {e}ï¼Œä¿æŒåŸå§‹ç´¢å¼•æ ¼å¼")
        
        # âœ… FIX: æ ¹æ®æ ‡ç­¾æœŸç¡®å®šæ­£ç¡®çš„æ»åå¤©æ•°ï¼ˆé€‚åº”æ€§è°ƒæ•´ï¼‰
        # ä»target_datesæ¨æ–­æ ‡ç­¾æœŸï¼Œæˆ–ä»åˆ—åè§£æ
        label_horizon = 5  # ğŸ”§ FIX: é»˜è®¤æ”¹ä¸º5å¤©æ ‡ç­¾æœŸ
        if target_dates is not None and len(target_dates) > 1:
            # å°è¯•ä»target_datesé—´éš”æ¨æ–­æ ‡ç­¾æœŸ
            try:
                target_dates_dt = pd.to_datetime(target_dates)
                if len(target_dates_dt) > 1:
                    avg_interval = (target_dates_dt.max() - target_dates_dt.min()).days / max(1, len(target_dates_dt) - 1)
                    if avg_interval > 1:
                        label_horizon = min(int(avg_interval), 10)  # ğŸ”§ FIX: Cap label horizon at 10 days
            except Exception as e:
                logger.debug(f"Failed to infer label horizon from target_dates: {e}")
                pass
        
        # ğŸ”§ FIX: Use adaptive lag based on dataset size  
        # Adaptive lag based on data characteristics
        if len(alpha_df) < 500:
            default_lag = max(1, label_horizon // 2)  # Very small datasets: minimal lag
        elif len(alpha_df) < 1000:
            default_lag = max(2, label_horizon)  # Small datasets: use label horizon
        else:
            default_lag = max(label_horizon, 3)  # ğŸ”§ FIX: Reduced minimum lag to 3 days
        
        # âœ… FIX: å¼ºåˆ¶ç´¢å¼•æ ‡å‡†åŒ– - ç¡®ä¿datetimeç´¢å¼•
        try:
            if not isinstance(alpha_df.index, pd.MultiIndex):
                if 'date' in alpha_df.columns and 'ticker' in alpha_df.columns:
                    alpha_df['date'] = pd.to_datetime(alpha_df['date'])
                    alpha_df = alpha_df.set_index(['date', 'ticker']).sort_index()
                elif 'date' in alpha_df.columns:
                    alpha_df['date'] = pd.to_datetime(alpha_df['date'])
                    alpha_df = alpha_df.set_index('date').sort_index()
            else:
                # ç¡®ä¿dateçº§åˆ«æ˜¯datetimeç±»å‹
                if 'date' in alpha_df.index.names:
                    alpha_df = alpha_df.reset_index()
                    alpha_df['date'] = pd.to_datetime(alpha_df['date'])
                    if 'ticker' in alpha_df.columns:
                        alpha_df = alpha_df.set_index(['date', 'ticker']).sort_index()
                    else:
                        alpha_df = alpha_df.set_index('date').sort_index()
        except Exception as e:
            logger.warning(f"ç´¢å¼•æ ‡å‡†åŒ–å¤±è´¥: {e}ï¼Œä¿æŒåŸå§‹ç´¢å¼•æ ¼å¼")
        
        # âœ… FIX: åº”ç”¨æ­£ç¡®çš„æ»å
        alpha_df_shifted = alpha_df.copy()
        try:
            if isinstance(alpha_df.index, pd.MultiIndex) and 'ticker' in alpha_df.index.names:
                # MultiIndex with ticker - æŒ‰tickeråˆ†ç»„shift
                numeric_cols = alpha_df.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 0:
                    # æŒ‰tickeråˆ†ç»„ï¼Œæ¯ä¸ªtickerçš„alphaä¿¡å·å³ç§»default_lagå¤©
                    for ticker_group in alpha_df.groupby(level='ticker'):
                        ticker_name = ticker_group[0]
                        ticker_data = ticker_group[1]
                        shifted_data = ticker_data[numeric_cols].shift(default_lag)
                        alpha_df_shifted.loc[ticker_data.index, numeric_cols] = shifted_data
            elif 'ticker' in alpha_df.columns:
                # tickerä½œä¸ºåˆ—
                numeric_cols = alpha_df.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 0:
                    shifted_data = alpha_df.groupby('ticker')[numeric_cols].shift(default_lag)
                    alpha_df_shifted.loc[:, numeric_cols] = shifted_data
            else:
                # ç®€å•æ—¶é—´åºåˆ—
                numeric_cols = alpha_df.select_dtypes(include=[np.number]).columns
                alpha_df_shifted[numeric_cols] = alpha_df[numeric_cols].shift(default_lag)
                
            # åˆ é™¤å› æ»åäº§ç”Ÿçš„NaNè¡Œ
            pre_dropna_len = len(alpha_df_shifted)
            alpha_df_shifted = alpha_df_shifted.dropna()
            post_dropna_len = len(alpha_df_shifted)
            
            # éªŒè¯æ»åæ•ˆæœ
            if post_dropna_len < pre_dropna_len * 0.1:
                logger.warning(f"æ»åå¤„ç†å¯¼è‡´æ•°æ®å¤§å¹…å‡å°‘: {pre_dropna_len} -> {post_dropna_len}")
            else:
                logger.info(f"æ»åå¤„ç†æˆåŠŸ: {pre_dropna_len} -> {post_dropna_len} è¡Œ, lag={default_lag}å¤©")
                
        except Exception as e:
            logger.error(f"æ»åå¤„ç†å¤±è´¥: {e}ï¼Œè¿”å›ç©ºæ•°æ®ä»¥é¿å…æ—¶é—´æ³„æ¼é£é™©")
            alpha_df_shifted = pd.DataFrame()  # ä¸¥æ ¼æ¨¡å¼ï¼šå¤±è´¥æ—¶è¿”å›ç©ºæ•°æ®
        
        logger.info(f"åº”ç”¨äº†{default_lag}å¤©æ»åï¼ˆåŸºäº{label_horizon}å¤©æ ‡ç­¾æœŸï¼‰ï¼Œé¿å…æ—¶é—´å¯¹é½è¿è§„")
        
        # âœ… FIX: æ›´æ™ºèƒ½çš„æ—¶é—´éªŒè¯ï¼Œå‡å°‘è¯¯æŠ¥
        if target_dates is not None and self.config.strict_time_validation:
            violations = self._validate_time_alignment(alpha_df_shifted, target_dates)
            # âœ… FIX: æ­£ç¡®çš„ç™¾åˆ†æ¯”è®¡ç®— - æ€»å•å…ƒæ ¼æ•°è€Œä¸æ˜¯è¡Œæ•°
            numeric_cols = alpha_df_shifted.select_dtypes(include=[np.number]).columns
            total_cells = len(alpha_df_shifted) * len(numeric_cols)
            violation_rate = violations / (total_cells + 1e-8)
            
            if violation_rate > 0.15:  # è¿è§„ç‡è¶…è¿‡15%æŠ¥è­¦
                logger.warning(f"å‘ç°è¾ƒå¤šæ—¶é—´å¯¹é½è¿è§„: {violations} ä¸ªå•å…ƒæ ¼ ({violation_rate:.1%}) / æ€»è®¡{total_cells}")
                self.stats['time_violations'] = violations
            elif violations > 0:
                logger.debug(f"å‘ç°å°‘é‡æ—¶é—´å¯¹é½è¿è§„: {violations} ä¸ªå•å…ƒæ ¼ ({violation_rate:.1%})ï¼Œåœ¨å¯æ¥å—èŒƒå›´å†…")
                self.stats['time_violations'] = violations
            else:
                logger.info("æ—¶é—´å¯¹é½éªŒè¯é€šè¿‡ï¼Œæ— æ³„éœ²é£é™©")
        
        # ä½¿ç”¨æ»ååçš„æ•°æ®ç»§ç»­å¤„ç†
        alpha_df = alpha_df_shifted
        
        # ğŸ”§ å¥åº·æ£€æŸ¥1: åˆ—ååŒ¹é…åº¦ (æ‰©å±•åˆ—åç­›é€‰)
        alpha_cols = []
        exclude_cols = ['date', 'ticker', 'target', 'target_10d', 'Close', 'High', 'Low', 'Open', 'amount']
        
        for col in alpha_df.columns:
            # Skip excluded columns
            if col in exclude_cols:
                continue
                
            # Include numeric columns that are likely alpha factors
            if alpha_df[col].dtype in [np.float32, np.float64, np.int32, np.int64]:
                # More flexible pattern matching - include any column that looks like a feature
                if (col.startswith('alpha_') or 
                    col.startswith('volume') or   # Include all volume-based features
                    col.startswith('momentum') or  # Include all momentum features
                    col.startswith('volatility') or # Include all volatility features
                    col.startswith('mean_reversion') or # Include mean reversion features
                    col.startswith('rsi') or  # Include RSI features
                    col.startswith('price_position') or # Include price position features
                    any(pattern in col.lower() for pattern in ['factor', 'reversal', 
                                                               'turnover', 'amihud', 'bid_ask', 'yield',
                                                               'ohlson', 'altman', 'qmj', 'earnings', 'beta',
                                                               'ratio', 'rsi', 'macd', 'ma_', '_ma', '_1d', '_5d', '_20d', '_14d'])):
                    alpha_cols.append(col)
        
        # Log what columns were detected
        if not alpha_cols:
            logger.warning(f"No alpha columns found. Available columns: {list(alpha_df.columns)[:20]}")
            logger.warning(f"Total columns: {len(alpha_df.columns)}, Data shape: {alpha_df.shape}")
            # Log column types
            numeric_cols = alpha_df.select_dtypes(include=[np.float32, np.float64, np.int32, np.int64]).columns
            logger.warning(f"Numeric columns found: {len(numeric_cols)}: {list(numeric_cols)[:10]}")
        
        # ğŸ”§ å¥åº·æ£€æŸ¥2: æœ€ä½åˆ—æ•°é—¨æ§›
        if len(alpha_cols) < 3:
            logger.warning(f"[FALLBACK] Alphaåˆ—æ•°è¿‡å°‘({len(alpha_cols)} < 3)ï¼Œè§¦å‘ä¼ ç»Ÿç‰¹å¾å›é€€")
            return self._fallback_to_traditional_features(alpha_df)
        
        if not alpha_cols:
            logger.warning(f"[FALLBACK] æœªæ‰¾åˆ°æœ‰æ•ˆçš„Alphaåˆ—ï¼Œè§¦å‘ä¼ ç»Ÿç‰¹å¾å›é€€ï¼Œå¯ç”¨åˆ—: {list(alpha_df.columns)[:10]}...")
            return self._fallback_to_traditional_features(alpha_df)
        
        # Include date column for groupby
        if 'date' in alpha_df.columns:
            cols_to_process = ['date'] + alpha_cols
        else:
            # If date is in index, reset it temporarily
            alpha_df = alpha_df.reset_index()
            cols_to_process = ['date'] + alpha_cols
        
        alpha_only = alpha_df[cols_to_process].copy()
        self.stats['total_alphas_processed'] = len(alpha_cols)
        
        # Cross-sectional processing by date
        cleaned_data = []
        
        for date, group in alpha_only.groupby('date'):
            # Drop date column for processing (will be in index after groupby)
            group_for_processing = group.drop(columns=['date'], errors='ignore')
            
            # Cross-sectional winsorization
            group_clean = self._cross_sectional_winsorize(group_for_processing)
            
            # Cross-sectional standardization
            group_clean = self._cross_sectional_standardize(group_clean)
            
            # Industry/factor neutralization
            if self.config.neutralize_by_industry and market_data is not None:
                group_clean = self._neutralize_factors(group_clean, market_data, date)
            
            cleaned_data.append(group_clean)
        
        # Check if we have any cleaned data
        if not cleaned_data:
            logger.warning("[FALLBACK] No data after cleaning, returning fallback features")
            return self._fallback_to_traditional_features(alpha_df)
        
        result = pd.concat(cleaned_data)
        
        # Safe way to get trading days count
        try:
            if isinstance(result.index, pd.MultiIndex) and 'date' in result.index.names:
                trading_days = len(result.index.get_level_values('date').unique())
            elif 'date' in result.columns:
                trading_days = len(result['date'].unique())
            else:
                trading_days = len(result)
            logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆï¼Œå¤„ç†äº† {trading_days} ä¸ªäº¤æ˜“æ—¥")
        except Exception as e:
            logger.warning(f"æ— æ³•è®¡ç®—äº¤æ˜“æ—¥æ•°é‡: {e}, ä½¿ç”¨æ€»è¡Œæ•°: {len(result)}")
            logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆï¼Œå¤„ç†äº† {len(result)} è¡Œæ•°æ®")
        
        return result
    
    def _cross_sectional_winsorize(self, group: pd.DataFrame) -> pd.DataFrame:
        """Cross-sectional winsorization by date"""
        group_winsor = group.copy()
        
        for col in group.columns:
            if self.config.use_mad_winsorize:
                # Use median Â± 3MAD method (more robust)
                median_val = group[col].median()
                mad_val = stats.median_abs_deviation(group[col].dropna())
                lower_bound = median_val - 3 * mad_val
                upper_bound = median_val + 3 * mad_val
            else:
                # Use percentile method
                lower_bound = group[col].quantile(self.config.winsorize_lower)
                upper_bound = group[col].quantile(self.config.winsorize_upper)
            
            group_winsor[col] = group[col].clip(lower_bound, upper_bound)
        
        return group_winsor
    
    def _cross_sectional_standardize(self, group: pd.DataFrame) -> pd.DataFrame:
        """Cross-sectional standardization (z-score) by date"""
        group_std = group.copy()
        
        for col in group.columns:
            mean_val = group[col].mean()
            std_val = group[col].std()
            
            if std_val > 1e-8:  # Avoid division by zero
                group_std[col] = (group[col] - mean_val) / std_val
            else:
                group_std[col] = 0.0
        
        return group_std
    
    def _neutralize_factors(self, 
                          alpha_group: pd.DataFrame, 
                          market_data: pd.DataFrame, 
                          date: str) -> pd.DataFrame:
        """Industry/factor neutralization using regression residuals"""
        try:
            # Get market context for this date
            date_market = market_data[market_data['date'] == date]
            if date_market.empty:
                return alpha_group
            
            # Align with alpha_group tickers
            tickers = alpha_group.index.get_level_values('ticker')
            market_aligned = date_market[date_market['ticker'].isin(tickers)].set_index('ticker')
            
            if market_aligned.empty:
                return alpha_group
            
            # Prepare neutralization factors
            neutralize_factors = []
            if self.config.neutralize_by_industry and 'industry' in market_aligned.columns:
                # One-hot encode industry
                industry_dummies = pd.get_dummies(market_aligned['industry'], prefix='ind')
                neutralize_factors.append(industry_dummies)
            
            if self.config.neutralize_by_market_cap and 'market_cap' in market_aligned.columns:
                # Log market cap
                market_cap_log = np.log(market_aligned['market_cap'].replace(0, np.nan)).fillna(0)
                neutralize_factors.append(pd.DataFrame({'log_market_cap': market_cap_log}))
            
            if not neutralize_factors:
                return alpha_group
            
            X_neutralize = pd.concat(neutralize_factors, axis=1).fillna(0)
            
            # Neutralize each alpha
            alpha_neutralized = alpha_group.copy()
            neutralization_stats = {}
            
            for col in alpha_group.columns:
                y = alpha_group[col].reindex(X_neutralize.index).dropna()
                X_aligned = X_neutralize.reindex(y.index)
                
                if len(y) > X_aligned.shape[1] + 5:  # Minimum samples required
                    try:
                        reg = LinearRegression().fit(X_aligned, y)
                        residuals = y - reg.predict(X_aligned)
                        alpha_neutralized.loc[y.index, col] = residuals
                        neutralization_stats[col] = reg.score(X_aligned, y)
                    except:
                        pass  # Keep original values if neutralization fails
            
            self.stats['neutralization_r2'][str(date)] = neutralization_stats
            return alpha_neutralized
            
        except Exception as e:
            logger.warning(f"å› å­ä¸­æ€§åŒ–å¤±è´¥ (æ—¥æœŸ {date}): {e}")
            return alpha_group
    
    def _compress_alpha_dimensions(self, alpha_df: pd.DataFrame, 
                                  returns: Optional[pd.DataFrame] = None,
                                  market_data: Optional[pd.DataFrame] = None) -> Optional[pd.DataFrame]:
        """A2: Compress Alpha dimensions using IC-weighted professional method
        
        ä¼˜å…ˆä½¿ç”¨ICåŠ æƒæ–¹æ¡ˆï¼ŒPCAä½œä¸ºå¤‡é€‰
        """
        
        alpha_values = alpha_df.select_dtypes(include=[np.number])
        if alpha_values.empty or alpha_values.shape[1] < 3:
            logger.warning("Alphaæ•°æ®ä¸è¶³ï¼Œè·³è¿‡é™ç»´")
            return None
        
        # ä¼˜å…ˆå°è¯•ä½¿ç”¨ä¸“ä¸šçš„ICåŠ æƒå¤„ç†å™¨
        try:
            from alpha_ic_weighted_processor import ICWeightedAlphaProcessor, ICWeightedConfig
            
            # åˆ›å»ºICåŠ æƒå¤„ç†å™¨
            ic_config = ICWeightedConfig()
            ic_processor = ICWeightedAlphaProcessor(ic_config)
            
            # å¦‚æœæ²¡æœ‰returnsæ•°æ®ï¼Œåˆ›å»ºæ¨¡æ‹Ÿreturnsï¼ˆå®é™…åº”è¯¥ä¼ å…¥çœŸå®returnsï¼‰
            if returns is None:
                # ä½¿ç”¨ç®€å•çš„åŠ¨é‡ä½œä¸ºä»£ç†returns
                if 'close' in alpha_df.columns:
                    returns = alpha_df['close'].pct_change().shift(-10)  # 10å¤©å‰å‘æ”¶ç›Š
                else:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªalphaå› å­çš„å˜åŒ–ä½œä¸ºä»£ç†
                    returns = alpha_values.iloc[:, 0].pct_change().shift(-10)
            
            # å¤„ç†alphaå› å­
            compressed_features = ic_processor.process_alpha_factors(
                alpha_values,
                returns,
                market_data
            )
            
            if compressed_features is not None and not compressed_features.empty:
                logger.info(f"ICåŠ æƒå¤„ç†æˆåŠŸ: {alpha_values.shape[1]} -> {compressed_features.shape[1]} ä¸ªç‰¹å¾")
                return compressed_features
                
        except ImportError:
            logger.warning("ICåŠ æƒå¤„ç†å™¨æœªæ‰¾åˆ°ï¼Œå›é€€åˆ°PCAæ–¹æ¡ˆ")
        except Exception as e:
            logger.warning(f"ICåŠ æƒå¤„ç†å¤±è´¥: {e}ï¼Œå›é€€åˆ°PCAæ–¹æ¡ˆ")
        
        # å›é€€åˆ°åŸå§‹PCAæ–¹æ¡ˆ
        compressed_features = []
        feature_names = []
        
        # Method 1: PCA (unsupervised)
        if self.config.use_pca:
            pca_features, pca_names = self._apply_pca_compression(alpha_values)
            if pca_features is not None:
                compressed_features.append(pca_features)
                feature_names.extend(pca_names)
        
        # Method 2: PLS (supervised - requires target)
        # Note: PLS would require target variable, implementing as placeholder
        if self.config.use_pls:
            logger.info("PLS compressionéœ€è¦ç›®æ ‡å˜é‡ï¼Œå½“å‰ç‰ˆæœ¬æš‚æœªå®ç°")
        
        # Method 3: IC-weighted composite (åŸå§‹ç®€å•ç‰ˆæœ¬)
        if self.config.use_ic_weighted_composite:
            ic_features, ic_names = self._apply_ic_weighted_compression(alpha_values)
            if ic_features is not None:
                compressed_features.append(ic_features)
                feature_names.extend(ic_names)
        
        if not compressed_features:
            logger.warning("æ‰€æœ‰å‹ç¼©æ–¹æ³•éƒ½å¤±è´¥")
            return None
        
        # Combine all compressed features
        combined_features = pd.concat(compressed_features, axis=1)
        combined_features.columns = feature_names[:len(combined_features.columns)]
        
        logger.info(f"Alphaé™ç»´å®Œæˆ: {alpha_values.shape[1]} -> {combined_features.shape[1]} ä¸ªç‰¹å¾")
        
        return combined_features
    
    def _apply_pca_compression(self, alpha_values: pd.DataFrame) -> Tuple[Optional[pd.DataFrame], List[str]]:
        """Apply TIME-SAFE PCA compression with fallback mechanisms"""
        
        # æ•°æ®è´¨é‡é¢„æ£€æŸ¥
        if alpha_values.empty:
            logger.warning("è¾“å…¥Alphaæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡PCAå‹ç¼©")
            return None, []
        
        # å»é™¤éæ•°å€¼åˆ—
        numeric_cols = alpha_values.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) == 0:
            logger.warning("æ²¡æœ‰æ•°å€¼ç±»å‹çš„Alphaç‰¹å¾ï¼Œè·³è¿‡PCAå‹ç¼©")
            return None, []
        
        alpha_numeric = alpha_values[numeric_cols].copy()
        
        # æ£€æŸ¥æ•°æ®å˜åŒ–æ€§
        col_std = alpha_numeric.std()
        valid_cols = col_std[col_std > 1e-8].index  # ç§»é™¤æ–¹å·®è¿‡å°çš„åˆ—
        
        if len(valid_cols) == 0:
            logger.warning("æ‰€æœ‰Alphaç‰¹å¾æ–¹å·®è¿‡å°ï¼Œè·³è¿‡PCAå‹ç¼©")
            return None, []
        
        if len(valid_cols) < len(numeric_cols):
            logger.info(f"ç§»é™¤äº†{len(numeric_cols) - len(valid_cols)}ä¸ªä½æ–¹å·®Alphaç‰¹å¾")
            alpha_numeric = alpha_numeric[valid_cols]
        
        try:
            # å°è¯•ä½¿ç”¨æ—¶é—´å®‰å…¨çš„PCA
            from bma_models.time_safe_pca import TimeSeriesSafePCA
            
            logger.info("ğŸ”§ ä½¿ç”¨æ—¶é—´å®‰å…¨PCAï¼Œé˜²æ­¢æ—¶é—´æ³„éœ²")
            
            # åˆ›å»ºæ—¶é—´å®‰å…¨PCA
            safe_pca = TimeSeriesSafePCA(
                n_components=min(self.config.pca_variance_explained, 0.95),  # é™åˆ¶æœ€å¤§è§£é‡Šæ–¹å·®
                min_history_days=30,  # é™ä½æœ€å°å†å²å¤©æ•°è¦æ±‚
                refit_frequency=21,   # 21å¤©é‡æ–°æ‹Ÿåˆ
                max_components=min(len(valid_cols), self.config.max_alpha_features - 3, 8)  # é™åˆ¶æœ€å¤§ç»„ä»¶æ•°
            )
            
            # æ—¶é—´å®‰å…¨çš„æ‹Ÿåˆè½¬æ¢
            pca_features_df, pca_stats = safe_pca.fit_transform_safe(alpha_numeric)
            
            if not pca_features_df.empty:
                # è·å–PCAç‰¹å¾æ•°æ®ï¼ˆæ’é™¤æ—¥æœŸå’Œtickeråˆ—ï¼‰
                pca_feature_cols = [col for col in pca_features_df.columns 
                                  if col.startswith('alpha_pca_')]
                
                if pca_feature_cols:
                    pca_features = pca_features_df[pca_feature_cols].values
                    
                    # å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
                    self.pca_fitted = safe_pca  # å­˜å‚¨æ—¶é—´å®‰å…¨PCAå¯¹è±¡
                    self.stats['compression_variance_explained'] = pca_stats.get('avg_components', 0)
                    self.stats['time_safe_pca_stats'] = pca_stats
                    
                    logger.info(f"âœ… æ—¶é—´å®‰å…¨PCAæˆåŠŸï¼Œç”Ÿæˆ{len(pca_feature_cols)}ä¸ªå‹ç¼©ç‰¹å¾")
                    return pca_features_df[pca_feature_cols], pca_feature_cols
            
            logger.warning("æ—¶é—´å®‰å…¨PCAå¤„ç†å¤±è´¥ï¼Œå°è¯•ç®€å•PCAå›é€€")
            
        except Exception as e:
            logger.warning(f"æ—¶é—´å®‰å…¨PCAå¤±è´¥: {e}ï¼Œå°è¯•ç®€å•PCAå›é€€")
        
        # å›é€€åˆ°ç®€å•PCA
        try:
            from sklearn.decomposition import PCA
            from sklearn.impute import SimpleImputer
            
            logger.info("ä½¿ç”¨ç®€å•PCAä½œä¸ºå›é€€æ–¹æ¡ˆ")
            
            # å¡«å……ç¼ºå¤±å€¼
            imputer = SimpleImputer(strategy='median')
            alpha_filled = pd.DataFrame(
                imputer.fit_transform(alpha_numeric),
                columns=alpha_numeric.columns,
                index=alpha_numeric.index
            )
            
            # åº”ç”¨ç®€å•PCA
            max_components = min(len(valid_cols), 8, alpha_filled.shape[0] // 10)  # ç¡®ä¿è¶³å¤Ÿçš„æ ·æœ¬
            if max_components < 1:
                logger.warning("æ ·æœ¬æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡ŒPCAå‹ç¼©")
                return None, []
            
            pca = PCA(n_components=max_components)
            pca_features = pca.fit_transform(alpha_filled)
            
            # åˆ›å»ºç‰¹å¾DataFrame
            pca_feature_names = [f'alpha_pca_{i+1}' for i in range(pca_features.shape[1])]
            pca_features_df = pd.DataFrame(
                pca_features,
                columns=pca_feature_names,
                index=alpha_numeric.index
            )
            
            # å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
            self.stats['compression_variance_explained'] = pca.explained_variance_ratio_.sum()
            self.stats['pca_components'] = len(pca_feature_names)
            
            logger.info(f"âœ… ç®€å•PCAæˆåŠŸï¼Œç”Ÿæˆ{len(pca_feature_names)}ä¸ªå‹ç¼©ç‰¹å¾ï¼Œè§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.3f}")
            return pca_features_df, pca_feature_names
            
        except Exception as e:
            logger.warning(f"ç®€å•PCAä¹Ÿå¤±è´¥: {e}")
            return None, []
    
    def _apply_ic_weighted_compression(self, alpha_values: pd.DataFrame) -> Tuple[Optional[pd.DataFrame], List[str]]:
        """Apply IC-weighted composite compression"""
        try:
            # This is a simplified version - in practice, you'd compute IC using historical target returns
            # For now, we create equal-weighted and volatility-adjusted composites
            
            # Equal-weighted composite
            alpha_composite = alpha_values.mean(axis=1)
            
            # Volatility-adjusted composite (weight inversely by volatility)
            alpha_vols = alpha_values.rolling(window=20, min_periods=5).std().fillna(1.0)
            vol_weights = (1.0 / alpha_vols).div((1.0 / alpha_vols).sum(axis=1), axis=0)
            alpha_vol_weighted = (alpha_values * vol_weights).sum(axis=1)
            
            # Create composite DataFrame
            composite_df = pd.DataFrame({
                'alpha_composite_ew': alpha_composite,
                'alpha_composite_vw': alpha_vol_weighted
            }, index=alpha_values.index)
            
            # Orthogonalize composites (remove correlation)
            composite_values = composite_df.fillna(0).values
            if composite_values.shape[1] > 1:
                # Simple Gram-Schmidt orthogonalization
                composite_orth = self._orthogonalize_features(composite_values)
                composite_df = pd.DataFrame(
                    composite_orth, 
                    index=alpha_values.index,
                    columns=['alpha_composite_orth1', 'alpha_composite_orth2']
                )
            
            feature_names = list(composite_df.columns)
            logger.info(f"ICåŠ æƒåˆæˆ: ç”Ÿæˆ {len(feature_names)} ä¸ªåˆæˆç‰¹å¾")
            
            return composite_df, feature_names
            
        except Exception as e:
            logger.warning(f"ICåŠ æƒåˆæˆå¤±è´¥: {e}")
            return None, []
    
    def _orthogonalize_features(self, features: np.ndarray) -> np.ndarray:
        """Simple Gram-Schmidt orthogonalization"""
        try:
            orth_features = features.copy()
            n_features = features.shape[1]
            
            for i in range(1, n_features):
                for j in range(i):
                    # Project feature i onto feature j and subtract
                    projection = np.dot(orth_features[:, i], orth_features[:, j]) / np.dot(orth_features[:, j], orth_features[:, j])
                    orth_features[:, i] -= projection * orth_features[:, j]
                
                # Normalize
                norm = np.linalg.norm(orth_features[:, i])
                if norm > 1e-8:
                    orth_features[:, i] /= norm
            
            return orth_features
        except:
            return features  # Return original if orthogonalization fails
    
    def _compute_summary_statistics(self, alpha_df: pd.DataFrame) -> Optional[pd.DataFrame]:
        """A3: Compute robust summary statistics"""
        
        alpha_numeric = alpha_df.select_dtypes(include=[np.number])
        if alpha_numeric.empty:
            return None
        
        stats_df = pd.DataFrame(index=alpha_df.index)
        
        try:
            # 1. Alpha dispersion (cross-sectional volatility)
            if self.config.include_dispersion:
                stats_df['alpha_dispersion'] = alpha_numeric.std(axis=1)
            
            # 2. Alpha agreement (directional consistency)
            if self.config.include_agreement:
                positive_count = (alpha_numeric > 0).sum(axis=1)
                total_count = alpha_numeric.notna().sum(axis=1)
                stats_df['alpha_agreement'] = positive_count / total_count.replace(0, 1)
            
            # 3. Alpha quality (rolling IC proxy - simplified)
            if self.config.include_quality:
                # Simplified quality measure: rolling correlation stability
                quality_scores = []
                try:
                    # ç¡®ä¿æœ‰dateå­—æ®µç”¨äºåˆ†ç»„
                    if isinstance(alpha_df.index, pd.MultiIndex) and 'date' in alpha_df.index.names:
                        # ä½¿ç”¨MultiIndexä¸­çš„date
                        date_groups = alpha_numeric.groupby(level='date')
                    elif 'date' in alpha_df.columns:
                        # ä½¿ç”¨åˆ—ä¸­çš„date
                        date_groups = alpha_numeric.groupby(alpha_df['date'])
                    else:
                        # å¦‚æœæ²¡æœ‰dateå­—æ®µï¼Œä½¿ç”¨æ•´ä½“ç›¸å…³æ€§
                        logger.warning("æ— æ³•æ‰¾åˆ°dateå­—æ®µï¼Œä½¿ç”¨æ•´ä½“ç›¸å…³æ€§è®¡ç®—è´¨é‡æŒ‡æ ‡")
                        if len(alpha_numeric.columns) > 1:
                            corr_matrix = alpha_numeric.T.corr()
                            avg_corr = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)].mean()
                            if np.isfinite(avg_corr):
                                stats_df['alpha_quality'] = avg_corr
                            else:
                                stats_df['alpha_quality'] = 0.0
                        else:
                            stats_df['alpha_quality'] = 0.0
                        quality_scores = None
                    
                    if quality_scores is not None:
                        for date, group in date_groups:
                            if len(group) > 1 and len(group.columns) > 1:
                                # Compute average pairwise correlation as quality proxy
                                corr_matrix = group.T.corr()
                                upper_tri_indices = np.triu_indices_from(corr_matrix.values, k=1)
                                if len(upper_tri_indices[0]) > 0:
                                    avg_corr = corr_matrix.values[upper_tri_indices].mean()
                                    if np.isfinite(avg_corr):
                                        quality_scores.extend([avg_corr] * len(group))
                                    else:
                                        quality_scores.extend([0.0] * len(group))
                                else:
                                    quality_scores.extend([0.0] * len(group))
                            else:
                                quality_scores.extend([0.0] * len(group))
                        
                        if len(quality_scores) == len(stats_df):
                            stats_df['alpha_quality'] = quality_scores
                        else:
                            # é•¿åº¦ä¸åŒ¹é…æ—¶ä½¿ç”¨é»˜è®¤å€¼
                            stats_df['alpha_quality'] = 0.0
                except Exception as e:
                    logger.warning(f"è´¨é‡æŒ‡æ ‡è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
                    stats_df['alpha_quality'] = 0.0
            
            # Handle infinite values and NaN
            stats_df = stats_df.replace([np.inf, -np.inf], np.nan)
            stats_df = stats_df.fillna(stats_df.median())
            
            logger.info(f"æ‘˜è¦ç»Ÿè®¡ç‰¹å¾ç”Ÿæˆå®Œæˆ: {stats_df.shape[1]} ä¸ªç»Ÿè®¡ç‰¹å¾")
            return stats_df
            
        except Exception as e:
            logger.warning(f"æ‘˜è¦ç»Ÿè®¡è®¡ç®—å¤±è´¥: {e}")
            return None
    
    def _compute_alpha_strategy_signal(self, alpha_df: pd.DataFrame) -> Optional[pd.DataFrame]:
        """A3.5: Compute Alpha strategy composite signal with enhanced data quality checks
        
        Alphaæ¥æºåˆ†é…:
        - è´¨é‡ç­›é€‰(40%): QMJ, Piotroski, Altman, Ohlsonè´¨é‡å› å­
        - åŠ¨é‡æ•è·(25%): åŠ¨é‡ã€æ®‹å·®åŠ¨é‡ã€åŠ¨é‡é—¨æ§å› å­  
        - æƒ…ç»ªä¼˜åŠ¿(20%): æ–°é—»æƒ…ç»ªã€å¸‚åœºæƒ…ç»ªã€ææƒ§è´ªå©ªæŒ‡æ•°
        - æµåŠ¨æ€§æº¢ä»·(15%): AmihudéæµåŠ¨æ€§ã€ä¹°å–ä»·å·®å› å­
        """
        
        alpha_numeric = alpha_df.select_dtypes(include=[np.number])
        if alpha_numeric.empty:
            logger.warning("æ— Alphaæ•°æ®ç”¨äºç­–ç•¥ä¿¡å·è®¡ç®—")
            return None
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        logger.debug(f"Alphaæ•°æ®å½¢çŠ¶: {alpha_numeric.shape}")
        logger.debug(f"Alphaæ•°æ®ç»Ÿè®¡:")
        for col in alpha_numeric.columns[:5]:  # åªæ˜¾ç¤ºå‰5åˆ—çš„ç»Ÿè®¡ä¿¡æ¯
            col_stats = alpha_numeric[col].describe()
            logger.debug(f"  {col}: mean={col_stats['mean']:.6f}, std={col_stats['std']:.6f}, "
                        f"min={col_stats['min']:.6f}, max={col_stats['max']:.6f}")
        
        # ç§»é™¤å…¨ä¸º0æˆ–å¸¸æ•°çš„åˆ—
        col_std = alpha_numeric.std()
        non_zero_cols = col_std[col_std > 1e-10].index
        
        if len(non_zero_cols) == 0:
            logger.warning("æ‰€æœ‰Alphaå› å­éƒ½æ˜¯å¸¸æ•°æˆ–é›¶ï¼Œæ— æ³•ç”Ÿæˆç­–ç•¥ä¿¡å·")
            return None
        
        if len(non_zero_cols) < len(alpha_numeric.columns):
            logger.info(f"ç§»é™¤äº†{len(alpha_numeric.columns) - len(non_zero_cols)}ä¸ªå¸¸æ•°Alphaå› å­")
            alpha_numeric = alpha_numeric[non_zero_cols]
        
        try:
            strategy_df = pd.DataFrame(index=alpha_df.index)
            
            # æ ¹æ®Alphaå› å­åç§°åˆ†ç±»ï¼ˆåŸºäºæ‚¨çš„alphas_config.yamlé…ç½®ï¼‰
            quality_factors = []
            momentum_factors = []
            sentiment_factors = []
            liquidity_factors = []
            other_factors = []
            
            for col in alpha_numeric.columns:
                col_lower = col.lower()
                if any(q in col_lower for q in ['qmj', 'piotroski', 'altman', 'ohlson', 'quality', 'roe', 'roic', 'margin', 'profitability', 'earnings_stability']):
                    quality_factors.append(col)
                elif any(m in col_lower for m in ['momentum', 'reversal', 'residual', 'hump']):
                    momentum_factors.append(col)
                elif any(s in col_lower for s in ['sentiment', 'news', 'fear', 'greed', 'market_sentiment']):
                    sentiment_factors.append(col)
                elif any(l in col_lower for l in ['amihud', 'bid_ask', 'illiq', 'spread', 'volume', 'turnover']):
                    liquidity_factors.append(col)
                else:
                    other_factors.append(col)
            
            # è®¡ç®—å„ç±»åˆ«çš„ç»„åˆä¿¡å·
            signals = {}
            
            if quality_factors:
                signals['quality'] = alpha_numeric[quality_factors].mean(axis=1) * 0.40
                logger.info(f"è´¨é‡å› å­ ({len(quality_factors)}ä¸ª): æƒé‡40%")
            
            if momentum_factors:
                signals['momentum'] = alpha_numeric[momentum_factors].mean(axis=1) * 0.25
                logger.info(f"åŠ¨é‡å› å­ ({len(momentum_factors)}ä¸ª): æƒé‡25%")
            
            if sentiment_factors:
                signals['sentiment'] = alpha_numeric[sentiment_factors].mean(axis=1) * 0.20
                logger.info(f"æƒ…ç»ªå› å­ ({len(sentiment_factors)}ä¸ª): æƒé‡20%")
            
            if liquidity_factors:
                signals['liquidity'] = alpha_numeric[liquidity_factors].mean(axis=1) * 0.15
                logger.info(f"æµåŠ¨æ€§å› å­ ({len(liquidity_factors)}ä¸ª): æƒé‡15%")
            
            if other_factors:
                # å…¶ä»–å› å­å¹³å‡åˆ†é…å‰©ä½™æƒé‡
                remaining_weight = 1.0 - sum([0.40, 0.25, 0.20, 0.15]) if not all([quality_factors, momentum_factors, sentiment_factors, liquidity_factors]) else 0.0
                if remaining_weight > 0:
                    signals['other'] = alpha_numeric[other_factors].mean(axis=1) * remaining_weight
                    logger.info(f"å…¶ä»–å› å­ ({len(other_factors)}ä¸ª): æƒé‡{remaining_weight:.1%}")
            
            # åˆæˆæœ€ç»ˆçš„Alphaç­–ç•¥ä¿¡å·
            if signals:
                alpha_strategy_raw = sum(signals.values())
                logger.info(f"æˆåŠŸåˆæˆ{len(signals)}ç±»Alphaä¿¡å·")
            else:
                # å¦‚æœæ²¡æœ‰åˆ†ç±»ä¿¡å·ï¼Œä½¿ç”¨ç®€å•å¹³å‡
                alpha_strategy_raw = alpha_numeric.mean(axis=1)
                logger.info("ä½¿ç”¨ç®€å•å¹³å‡ä½œä¸ºAlphaç­–ç•¥ä¿¡å·")
            
            # æ£€æŸ¥åŸå§‹ä¿¡å·è´¨é‡
            raw_std = alpha_strategy_raw.std()
            raw_mean = alpha_strategy_raw.mean()
            logger.debug(f"åŸå§‹ç­–ç•¥ä¿¡å·: mean={raw_mean:.6f}, std={raw_std:.6f}, "
                        f"min={alpha_strategy_raw.min():.6f}, max={alpha_strategy_raw.max():.6f}")
            
            if raw_std < 1e-10:
                logger.warning(f"åŸå§‹ç­–ç•¥ä¿¡å·æ–¹å·®è¿‡å°({raw_std:.2e})ï¼Œç”Ÿæˆéšæœºæ‰°åŠ¨")
                # æ·»åŠ å¾®å°çš„éšæœºæ‰°åŠ¨ä»¥é¿å…å…¨é›¶ä¿¡å·
                noise_scale = max(abs(raw_mean) * 0.01, 1e-6)
                alpha_strategy_raw += np.random.normal(0, noise_scale, len(alpha_strategy_raw))
                raw_std = alpha_strategy_raw.std()
                logger.info(f"æ·»åŠ æ‰°åŠ¨åä¿¡å·æ–¹å·®: {raw_std:.6f}")
            
            # åº”ç”¨æ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼ˆä¸å…¶ä»–æ‘˜è¦ç‰¹å¾ä¿æŒä¸€è‡´ï¼‰
            if self.config.neutralize_by_industry and isinstance(alpha_df.index, pd.MultiIndex):
                # ç®€åŒ–çš„è¡Œä¸šä¸­æ€§åŒ–ï¼ˆè¿™é‡Œä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–ï¼‰
                try:
                    alpha_strategy_normalized = (alpha_strategy_raw.groupby(alpha_df.index.get_level_values('date'))
                                               .apply(lambda x: (x - x.mean()) / (x.std() if x.std() > 1e-10 else 1e-6)))
                    if alpha_strategy_normalized.isna().all():
                        raise ValueError("åˆ†ç»„æ ‡å‡†åŒ–äº§ç”Ÿå…¨NaNç»“æœ")
                except Exception as e:
                    logger.warning(f"åˆ†ç»„æ ‡å‡†åŒ–å¤±è´¥({e})ï¼Œä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–")
                    # å¦‚æœåˆ†ç»„æ ‡å‡†åŒ–å¤±è´¥ï¼Œä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–
                    alpha_strategy_normalized = (alpha_strategy_raw - alpha_strategy_raw.mean()) / (alpha_strategy_raw.std() if alpha_strategy_raw.std() > 1e-10 else 1e-6)
            else:
                alpha_strategy_normalized = (alpha_strategy_raw - alpha_strategy_raw.mean()) / (alpha_strategy_raw.std() if alpha_strategy_raw.std() > 1e-10 else 1e-6)
            
            # æ£€æŸ¥æ ‡å‡†åŒ–åçš„ä¿¡å·
            norm_std = alpha_strategy_normalized.std()
            norm_mean = alpha_strategy_normalized.mean()
            logger.debug(f"æ ‡å‡†åŒ–åä¿¡å·: mean={norm_mean:.6f}, std={norm_std:.6f}")
            
            # Winsorizeå¤„ç†å¼‚å¸¸å€¼
            if self.config.use_mad_winsorize:
                median = alpha_strategy_normalized.median()
                mad = np.median(np.abs(alpha_strategy_normalized - median))
                if mad > 1e-10:
                    alpha_strategy_winsorized = np.clip(alpha_strategy_normalized, 
                                                      median - 3*mad, median + 3*mad)
                else:
                    alpha_strategy_winsorized = alpha_strategy_normalized.copy()
            else:
                try:
                    q01, q99 = alpha_strategy_normalized.quantile([0.01, 0.99])
                    if abs(q99 - q01) > 1e-10:
                        alpha_strategy_winsorized = np.clip(alpha_strategy_normalized, q01, q99)
                    else:
                        alpha_strategy_winsorized = alpha_strategy_normalized.copy()
                except:
                    alpha_strategy_winsorized = alpha_strategy_normalized.copy()
            
            strategy_df['alpha_strategy_signal'] = alpha_strategy_winsorized
            
            final_min, final_max = alpha_strategy_winsorized.min(), alpha_strategy_winsorized.max()
            logger.info(f"Alphaç­–ç•¥ç»¼åˆä¿¡å·ç”Ÿæˆå®Œæˆ: èŒƒå›´[{final_min:.6f}, {final_max:.6f}]")
            
            if abs(final_max - final_min) < 1e-8:
                logger.warning("âš ï¸ æœ€ç»ˆä¿¡å·èŒƒå›´è¿‡å°ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®è´¨é‡é—®é¢˜")
            
            return strategy_df
            
        except Exception as e:
            logger.warning(f"Alphaç­–ç•¥ä¿¡å·è®¡ç®—å¤±è´¥: {e}")
            return None
    
    def _combine_and_finalize_features(self, 
                                     alpha_compressed: Optional[pd.DataFrame],
                                     alpha_stats: Optional[pd.DataFrame],
                                     alpha_strategy_signal: Optional[pd.DataFrame],
                                     target_index: pd.Index) -> pd.DataFrame:
        """A4: Combine and finalize features for ML integration"""
        
        # Collect all feature components
        feature_components = []
        
        if alpha_compressed is not None:
            feature_components.append(alpha_compressed)
        
        if alpha_stats is not None:
            feature_components.append(alpha_stats)
        
        if alpha_strategy_signal is not None:
            feature_components.append(alpha_strategy_signal)
        
        if not feature_components:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„Alphaæ‘˜è¦ç‰¹å¾")
            return pd.DataFrame(index=target_index)
        
        # Combine all features
        combined_features = pd.concat(feature_components, axis=1)
        
        # Ensure we don't exceed max features limit
        if combined_features.shape[1] > self.config.max_alpha_features:
            # Keep most informative features (highest variance)
            feature_vars = combined_features.var()
            top_features = feature_vars.nlargest(self.config.max_alpha_features).index
            combined_features = combined_features[top_features]
            logger.info(f"ç‰¹å¾æ•°é‡é™åˆ¶: ä¿ç•™å‰ {self.config.max_alpha_features} ä¸ªé«˜æ–¹å·®ç‰¹å¾")
        
        # Final data type optimization
        if self.config.data_type == 'float32':
            combined_features = combined_features.astype(np.float32)
        
        # Handle remaining missing values
        if self.config.fill_method == 'cross_median':
            # Fill with cross-sectional median by date
            filled_features = []
            # Check if date is in index or columns
            if isinstance(combined_features.index, pd.MultiIndex) and 'date' in combined_features.index.names:
                # Date is in MultiIndex
                for date, group in combined_features.groupby(level='date'):
                    group_filled = group.fillna(group.median())
                    filled_features.append(group_filled)
                combined_features = pd.concat(filled_features)
            elif 'date' in combined_features.columns:
                # Date is in columns
                for date, group in combined_features.groupby('date'):
                    group_filled = group.fillna(group.median())
                    filled_features.append(group_filled)
                combined_features = pd.concat(filled_features)
            else:
                # No date column, use simple fill
                combined_features = combined_features.fillna(combined_features.median())
        elif self.config.fill_method == 'forward_fill':
            combined_features = combined_features.fillna(method='ffill').fillna(0)
        else:  # zero fill
            combined_features = combined_features.fillna(0)
        
        logger.info(f"Alphaæ‘˜è¦ç‰¹å¾æœ€ç»ˆç”Ÿæˆ: {combined_features.shape}")
        
        return combined_features
    
    def _validate_time_alignment(self, alpha_df: pd.DataFrame, target_dates: pd.Series) -> int:
        """A5: Validate time alignment to prevent leakage - æ”¹è¿›ç‰ˆ"""
        violations = 0
        
        try:
            # âœ… FIX: æ›´æ™ºèƒ½çš„æ—¶é—´å¯¹é½éªŒè¯
            if isinstance(alpha_df.index, pd.MultiIndex):
                # MultiIndexæƒ…å†µ - å°è¯•è·å–dateçº§åˆ«
                if 'date' in alpha_df.index.names:
                    alpha_dates = alpha_df.index.get_level_values('date')
                else:
                    # å¦‚æœæ²¡æœ‰dateçº§åˆ«ï¼Œå°è¯•ç¬¬ä¸€ä¸ªçº§åˆ«
                    alpha_dates = alpha_df.index.get_level_values(0)
                    try:
                        alpha_dates = pd.to_datetime(alpha_dates)
                    except:
                        logger.debug("æ— æ³•å°†ç´¢å¼•è½¬æ¢ä¸ºæ—¥æœŸï¼Œè·³è¿‡æ—¶é—´å¯¹é½éªŒè¯")
                        return 0
            else:
                # æ™®é€šç´¢å¼• - å°è¯•è½¬æ¢ä¸ºæ—¥æœŸ
                try:
                    alpha_dates = pd.to_datetime(alpha_df.index)
                except:
                    logger.debug("æ— æ³•å°†ç´¢å¼•è½¬æ¢ä¸ºæ—¥æœŸï¼Œè·³è¿‡æ—¶é—´å¯¹é½éªŒè¯")
                    return 0
            
            # è½¬æ¢target_datesä¸ºdatetime
            if target_dates is not None:
                try:
                    target_dates = pd.to_datetime(target_dates)
                except:
                    logger.debug("æ— æ³•è½¬æ¢target_datesï¼Œè·³è¿‡æ—¶é—´å¯¹é½éªŒè¯")
                    return 0
                
                # âœ… FIX: æ­£ç¡®çš„æ—¶é—´æ³„æ¼éªŒè¯é€»è¾‘
                # æ£€æŸ¥æ¯ä¸ªalphaæ•°æ®ç‚¹æ˜¯å¦è¿åæ—¶é—´é¡ºåº
                max_target_date = target_dates.max()
                min_target_date = target_dates.min()
                
                # è®¡ç®—å®é™…çš„æ—¶é—´æ³„æ¼ï¼šalphaæ•°æ®æ™šäºæœ€æ–°ç›®æ ‡æ—¥æœŸ + å®¹å¿æœŸ
                tolerance_days = 1  # å®¹å¿1å¤©çš„å·®å¼‚
                cutoff_date = max_target_date + pd.Timedelta(days=tolerance_days)
                
                # ç»Ÿè®¡è¿è§„æ•°æ®ç‚¹ï¼ˆä¸é‡å¤è®¡ç®—ï¼‰
                future_data_mask = alpha_dates > cutoff_date
                violations = future_data_mask.sum()
                
                if violations > 0:
                    logger.debug(f"å‘ç° {violations} ä¸ªæ•°æ®ç‚¹æ™šäºæˆªæ­¢æ—¥æœŸ {cutoff_date}")
                    # é¢å¤–æ£€æŸ¥ï¼šä¸¥é‡è¿è§„ï¼ˆè¶…è¿‡7å¤©ï¼‰
                    severe_cutoff = max_target_date + pd.Timedelta(days=7)
                    severe_violations = (alpha_dates > severe_cutoff).sum()
                    if severe_violations > 0:
                        logger.warning(f"ä¸¥é‡æ—¶é—´è¿è§„: {severe_violations} ä¸ªæ•°æ®ç‚¹è¶…è¿‡7å¤©æˆªæ­¢æœŸ")
            
        except Exception as e:
            logger.debug(f"æ—¶é—´å¯¹é½éªŒè¯å¼‚å¸¸: {e}")
            violations = 0
        
        return violations
    
    def _validate_time_alignment_detailed(self, alpha_df: pd.DataFrame, 
                                        target_dates: pd.Series) -> Dict[str, Any]:
        """è¯¦ç»†çš„æ—¶é—´å¯¹é½éªŒè¯ï¼Œè¿”å›é—®é¢˜åˆ—æ¸…å•"""
        result = {
            'total_violations': 0,
            'bad_columns': [],
            'column_violations': {},
            'validation_summary': {}
        }
        
        try:
            if alpha_df.empty or target_dates is None:
                return result
            
            # è·å–æ—¥æœŸç´¢å¼•
            if isinstance(alpha_df.index, pd.MultiIndex) and 'date' in alpha_df.index.names:
                alpha_dates = alpha_df.index.get_level_values('date').unique()
            elif 'date' in alpha_df.columns:
                alpha_dates = alpha_df['date'].unique()
            else:
                alpha_dates = alpha_df.index if isinstance(alpha_df.index, pd.DatetimeIndex) else []
            
            if len(alpha_dates) == 0:
                return result
            
            alpha_dates = pd.to_datetime(alpha_dates)
            target_dates = pd.to_datetime(target_dates)
            
            # é€åˆ—æ£€æŸ¥æ—¶é—´å¯¹é½é—®é¢˜
            for col in alpha_df.columns:
                if col in ['date', 'ticker']:
                    continue
                
                col_violations = 0
                
                # æ£€æŸ¥è¯¥åˆ—çš„æ•°æ®æ—¥æœŸæ˜¯å¦è¶…å‰ç›®æ ‡æ—¥æœŸ
                col_data = alpha_df[col].dropna()
                if col_data.empty:
                    continue
                
                # è·å–è¯¥åˆ—æ•°æ®çš„æ—¥æœŸ
                if isinstance(alpha_df.index, pd.MultiIndex):
                    col_dates = col_data.index.get_level_values('date')
                else:
                    col_dates = col_data.index if isinstance(col_data.index, pd.DatetimeIndex) else alpha_dates
                
                # æ£€æŸ¥æœªæ¥ä¿¡æ¯æ³„æ¼
                future_leakage = 0
                for target_date in target_dates:
                    future_data = col_dates[col_dates > target_date]
                    future_leakage += len(future_data)
                
                col_violations += future_leakage
                
                # å¦‚æœè¯¥åˆ—è¿è§„è¾ƒå¤šï¼Œæ ‡è®°ä¸ºé—®é¢˜åˆ—
                if col_violations > len(col_data) * 0.1:  # è¶…è¿‡10%çš„æ•°æ®æœ‰é—®é¢˜
                    result['bad_columns'].append(col)
                    result['column_violations'][col] = col_violations
                
                result['total_violations'] += col_violations
            
            # ç”ŸæˆéªŒè¯æ‘˜è¦
            result['validation_summary'] = {
                'total_columns_checked': len([c for c in alpha_df.columns if c not in ['date', 'ticker']]),
                'problematic_columns': len(result['bad_columns']),
                'clean_columns': len(alpha_df.columns) - len(result['bad_columns']) - 2,  # å‡å»dateå’Œticker
                'worst_column': max(result['column_violations'], key=result['column_violations'].get) if result['column_violations'] else None,
                'worst_violation_count': max(result['column_violations'].values()) if result['column_violations'] else 0
            }
            
            logger.debug(f"è¯¦ç»†æ—¶é—´éªŒè¯å®Œæˆ: {result['total_violations']} æ€»è¿è§„, "
                        f"{len(result['bad_columns'])} é—®é¢˜åˆ—")
            
            return result
            
        except Exception as e:
            logger.error(f"è¯¦ç»†æ—¶é—´å¯¹é½éªŒè¯å¼‚å¸¸: {e}")
            return result
    
    def _selective_column_cleanup(self, features_df: pd.DataFrame, 
                                bad_columns: List[str]) -> pd.DataFrame:
        """é€‰æ‹©æ€§åˆ—æ¸…ç†ï¼šä»…ç§»é™¤é—®é¢˜åˆ—ï¼Œä¿ç•™å…¶ä»–ç‰¹å¾"""
        if not bad_columns:
            return features_df
        
        try:
            # ç§»é™¤é—®é¢˜åˆ—
            clean_columns = [col for col in features_df.columns if col not in bad_columns]
            
            if len(clean_columns) < self.config.min_alpha_features:
                logger.warning(f"æ¸…ç†åç‰¹å¾æ•°ä¸è¶³({len(clean_columns)} < {self.config.min_alpha_features})ï¼Œå›é€€åˆ°ä¼ ç»Ÿç‰¹å¾")
                # è¿™ç§æƒ…å†µä¸‹ä»ç„¶å›é€€ï¼Œä½†è®°å½•å…·ä½“åŸå› 
                self.stats['selective_cleanup_failed'] = {
                    'removed_columns': bad_columns,
                    'remaining_columns': len(clean_columns),
                    'min_required': self.config.min_alpha_features
                }
                return self._fallback_to_traditional_features_with_log(features_df, bad_columns)
            
            cleaned_features = features_df[clean_columns].copy()
            
            # è®°å½•æ¸…ç†ç»Ÿè®¡
            self.stats['selective_cleanup_applied'] = {
                'removed_columns': bad_columns,
                'removed_count': len(bad_columns),
                'retained_columns': clean_columns,
                'retained_count': len(clean_columns),
                'cleanup_rate': len(bad_columns) / len(features_df.columns)
            }
            
            logger.info(f"[SELECTIVE_CLEANUP] ç§»é™¤ {len(bad_columns)} é—®é¢˜åˆ—ï¼Œä¿ç•™ {len(clean_columns)} ç‰¹å¾")
            logger.info(f"[SELECTIVE_CLEANUP] ç§»é™¤çš„åˆ—: {bad_columns}")
            
            return cleaned_features
            
        except Exception as e:
            logger.error(f"é€‰æ‹©æ€§æ¸…ç†å¤±è´¥: {e}")
            return self._fallback_to_traditional_features_with_log(features_df, bad_columns)
    
    def _fallback_to_traditional_features_with_log(self, original_df: pd.DataFrame, 
                                                 bad_columns: List[str]) -> pd.DataFrame:
        """å¸¦æ—¥å¿—è®°å½•çš„ä¼ ç»Ÿç‰¹å¾å›é€€"""
        logger.warning(f"[FALLBACK] é€‰æ‹©æ€§æ¸…ç†å¤±è´¥ï¼Œå®Œå…¨å›é€€åˆ°ä¼ ç»Ÿç‰¹å¾")
        logger.warning(f"[FALLBACK] åŸå§‹é—®é¢˜åˆ—: {bad_columns}")
        
        # è®°å½•å®Œæ•´å›é€€ç»Ÿè®¡
        self.stats['full_fallback_triggered'] = {
            'reason': 'selective_cleanup_insufficient',
            'original_columns': len(original_df.columns),
            'problematic_columns': bad_columns,
            'timestamp': datetime.now().isoformat()
        }
        
        return self._fallback_to_traditional_features(original_df)
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """Get processing statistics"""
        return self.stats.copy()

# Factory function for easy integration
def create_alpha_summary_processor(config: Dict[str, Any] = None) -> AlphaSummaryProcessor:
    """Create Alpha Summary Processor with configuration"""
    if config:
        alpha_config = AlphaSummaryConfig(**config)
    else:
        alpha_config = AlphaSummaryConfig()
    
    return AlphaSummaryProcessor(alpha_config)