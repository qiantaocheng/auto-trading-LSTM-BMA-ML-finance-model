#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LambdaRank Stacker - ä¸“é—¨ä¼˜åŒ–æ’åºçš„äºŒå±‚æ¨¡å‹

æ ¸å¿ƒç‰¹æ€§ï¼š
- ä½¿ç”¨LightGBMçš„LambdaRankç›®æ ‡ï¼Œä¸“é—¨ä¼˜åŒ–NDCG@K
- è¿ç»­ç›®æ ‡ â†’ ç»„å†…åˆ†ä½æ•° â†’ æ•´æ•°ç­‰çº§è½¬æ¢
- äº¤æ˜“æ—¥ä½œä¸ºæ’åºç»„ï¼Œç¬¦åˆå®é™…é€‰è‚¡åœºæ™¯
- ä¸Ridge Stackerå¹¶è¡Œè®­ç»ƒï¼Œäº’ä¸ºè¡¥å……

è®¾è®¡ç†å¿µï¼š
Ridgeå›å½’ -> è¿ç»­é¢„æµ‹ï¼Œä¿ç•™åˆ»åº¦ä¿¡æ¯
LambdaRank -> æ’åºä¼˜åŒ–ï¼Œæå‡Top-Kæ€§èƒ½
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, Any, Tuple, Optional
from sklearn.preprocessing import StandardScaler
from scipy.stats import norm
import warnings
warnings.filterwarnings('ignore')

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

# å¯¼å…¥PurgedCVé˜²æ­¢æ•°æ®æ³„éœ²
try:
    from bma_models.unified_purged_cv_factory import create_unified_cv
    PURGED_CV_AVAILABLE = True
except ImportError:
    PURGED_CV_AVAILABLE = False

logger = logging.getLogger(__name__)


def per_day_rank_normalize(predictions: np.ndarray, dates: pd.Series, use_gauss_rank: bool = True) -> np.ndarray:
    """
    å¯¹é¢„æµ‹å€¼æŒ‰å¤©åšæ¨ªæˆªé¢rankæ ‡å‡†åŒ–ï¼ˆ0~1ï¼‰æˆ–Gauss-rank
    
    ğŸ”¥ å…³é”®ï¼šrankå¿…é¡»æ˜¯åŒä¸€å¤©å†…ï¼Œä¸èƒ½è·¨å¤©rank
    
    å®ç°é€»è¾‘ï¼š
    1. éå†æ¯ä¸€å¤©ï¼ˆunique_datesï¼‰
    2. å¯¹æ¯ä¸€å¤©çš„é¢„æµ‹å€¼ï¼Œåœ¨åŒä¸€å¤©å†…è¿›è¡Œrankï¼ˆæ¨ªæˆªé¢rankï¼‰
    3. å°†rankè½¬æ¢ä¸º[0,1]æˆ–Gauss-rank
    4. è¿™æ ·meta rankeråƒçš„æ˜¯"ç¨³å®šçš„æ’åºå°ºåº¦"ï¼Œä¸å—æŠ˜é—´æ¨¡å‹æˆç†Ÿåº¦å½±å“
    
    Args:
        predictions: é¢„æµ‹å€¼æ•°ç»„
        dates: å¯¹åº”çš„æ—¥æœŸSeriesï¼ˆå¿…é¡»ä¸predictionsé•¿åº¦ä¸€è‡´ï¼‰
        use_gauss_rank: æ˜¯å¦ä½¿ç”¨Gauss-rankï¼ˆæ¨èï¼‰ï¼Œå¦åˆ™ä½¿ç”¨æ™®é€šrank (0~1)
    
    Returns:
        æ ‡å‡†åŒ–åçš„é¢„æµ‹å€¼æ•°ç»„ï¼ˆæŒ‰å¤©rankåçš„ç»“æœï¼‰
    """
    if len(predictions) != len(dates):
        raise ValueError(f"predictionsé•¿åº¦({len(predictions)})ä¸datesé•¿åº¦({len(dates)})ä¸åŒ¹é…")
    
    normalized = np.zeros_like(predictions)
    # ç¡®ä¿datesæ˜¯pd.Seriesï¼ˆå¦‚æœå·²ç»æ˜¯Seriesï¼Œunique()ä¼šä¿æŒé¡ºåºï¼‰
    if not isinstance(dates, pd.Series):
        dates = pd.Series(dates)
    unique_dates = dates.unique()
    
    # ğŸ”¥ æŒ‰å¤©éå†ï¼šå¯¹æ¯ä¸€å¤©çš„é¢„æµ‹å€¼è¿›è¡Œæ¨ªæˆªé¢rank
    for date in unique_dates:
        # æå–å½“å¤©çš„æ‰€æœ‰é¢„æµ‹å€¼ï¼ˆæ¨ªæˆªé¢ï¼‰
        date_mask = (dates == date).values if hasattr(dates, 'values') else (dates == date)
        date_preds = predictions[date_mask]
        
        if len(date_preds) == 0:
            continue
        
        # ğŸ”¥ å…³é”®ï¼šåœ¨åŒä¸€å¤©å†…è¿›è¡Œrankï¼ˆæ¨ªæˆªé¢rankï¼‰ï¼Œä¸è·¨å¤©
        if len(date_preds) == 1:
            # å•ä¸ªæ ·æœ¬æ—¶ï¼Œrankè®¾ä¸º0.5
            normalized[date_mask] = 0.5
        else:
            # ä½¿ç”¨pandasçš„rankæ–¹æ³•ï¼Œpct=Trueå¾—åˆ°0~1çš„rank
            # è¿™æ˜¯åŒä¸€å¤©å†…çš„æ¨ªæˆªé¢rankï¼Œç¡®ä¿meta rankeråƒçš„æ˜¯ç¨³å®šçš„æ’åºå°ºåº¦
            rank_series = pd.Series(date_preds).rank(method='average', pct=True)
            ranks = rank_series.values
            
            if use_gauss_rank:
                # Gauss-rank: rank -> é€†æ­£æ€CDF -> æ ‡å‡†åŒ–
                # å°†0~1çš„rankæ˜ å°„åˆ°æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„é€†CDF
                # é¿å…è¾¹ç•Œæ•ˆåº”ï¼ˆ0å’Œ1æ˜ å°„åˆ°-infå’Œinfï¼‰
                eps = 1e-6
                ranks_clipped = np.clip(ranks, eps, 1 - eps)
                gauss_ranks = norm.ppf(ranks_clipped)
                # æ ‡å‡†åŒ–åˆ°0~1èŒƒå›´ï¼ˆå¯é€‰ï¼Œä¹Ÿå¯ä»¥ä¿æŒgaussåˆ†å¸ƒï¼‰
                gauss_ranks_normalized = (gauss_ranks - gauss_ranks.min()) / (gauss_ranks.max() - gauss_ranks.min() + 1e-10)
                normalized[date_mask] = gauss_ranks_normalized
            else:
                # æ™®é€šrank (0~1)
                normalized[date_mask] = ranks
    
    return normalized


def calculate_topk_return_proxy(predictions: np.ndarray, y_true: np.ndarray, dates: pd.Series, k: int = 10) -> Dict[str, float]:
    """
    è®¡ç®—Top-Kæ”¶ç›ŠproxyæŒ‡æ ‡
    
    å¯¹æ¯ä¸ªéªŒè¯æ—¥å–é¢„æµ‹TopKï¼Œç”¨çœŸå®T+10 returnè®¡ç®—å¹³å‡æ”¶ç›Šï¼›æ±‡æ€»æˆå‡å€¼/IR/t-statã€‚
    è¿™æ˜¯æœ€ç»ˆç­–ç•¥çš„æœ€ç›´æ¥proxyï¼Œèƒ½æŠ“ä½"æ’åºæŒ‡æ ‡æå‡ä½†æ”¶ç›Šä¸åŠ¨"çš„é—®é¢˜ã€‚
    
    Args:
        predictions: é¢„æµ‹å€¼æ•°ç»„
        y_true: çœŸå®æ”¶ç›Šæ•°ç»„ï¼ˆT+10 returnï¼‰
        dates: å¯¹åº”çš„æ—¥æœŸSeries
        k: Top-Kæ•°é‡ï¼ˆé»˜è®¤10ï¼‰
    
    Returns:
        åŒ…å«å‡å€¼ã€IRã€t-statçš„å­—å…¸
    """
    if len(predictions) != len(y_true) or len(predictions) != len(dates):
        raise ValueError("predictions, y_true, datesé•¿åº¦å¿…é¡»ä¸€è‡´")
    
    daily_returns = []
    # ç¡®ä¿datesæ˜¯pd.Series
    if not isinstance(dates, pd.Series):
        dates = pd.Series(dates)
    unique_dates = dates.unique()
    
    for date in unique_dates:
        date_mask = (dates == date).values if hasattr(dates, 'values') else (dates == date)
        date_preds = predictions[date_mask]
        date_returns = y_true[date_mask]
        
        if len(date_preds) < k:
            # å¦‚æœå½“å¤©æ ·æœ¬æ•°å°‘äºKï¼Œä½¿ç”¨å…¨éƒ¨æ ·æœ¬
            topk_mask = np.ones(len(date_preds), dtype=bool)
        else:
            # å–Top-K
            topk_indices = np.argsort(date_preds)[-k:]
            topk_mask = np.zeros(len(date_preds), dtype=bool)
            topk_mask[topk_indices] = True
        
        # è®¡ç®—Top-Kçš„å¹³å‡æ”¶ç›Š
        topk_returns = date_returns[topk_mask]
        if len(topk_returns) > 0:
            daily_returns.append(np.mean(topk_returns))
    
    if len(daily_returns) == 0:
        return {
            'mean_return': 0.0,
            'ir': 0.0,
            't_stat': 0.0,
            'n_days': 0
        }
    
    daily_returns = np.array(daily_returns)
    
    # è®¡ç®—å‡å€¼ã€IRã€t-stat
    mean_return = np.mean(daily_returns)
    std_return = np.std(daily_returns)
    ir = mean_return / (std_return + 1e-10)  # Information Ratio
    t_stat = mean_return / (std_return / np.sqrt(len(daily_returns)) + 1e-10)  # t-statistic
    
    return {
        'mean_return': float(mean_return),
        'ir': float(ir),
        't_stat': float(t_stat),
        'n_days': len(daily_returns),
        'std_return': float(std_return)
    }


class LambdaRankStacker:
    """
    LambdaRankæ’åºæ¨¡å‹ - ç›´æ¥ä½¿ç”¨Alpha Factorsä¼˜åŒ–æ’åº

    æ ¸å¿ƒä¼˜åŠ¿ï¼š
    - ç›´æ¥ä½¿ç”¨åŸå§‹Alpha factorsè®­ç»ƒï¼Œä¸ä¾èµ–ç¬¬ä¸€å±‚OOF
    - ä¸“é—¨ä¼˜åŒ–æ’åºæŒ‡æ ‡ï¼ˆNDCG@Kï¼‰
    - ç»„å†…ç­‰çº§è½¬æ¢ï¼Œé€‚åˆTop-Ké€‰è‚¡
    - ä¸Stackingæ¨¡å‹å½¢æˆäº’è¡¥èåˆ
    """

    def __init__(self,
                 base_cols: Tuple[str, ...] = None,  # å°†è‡ªåŠ¨ä½¿ç”¨alpha factor columns
                 n_quantiles: int = 32,  # ğŸ”§ 128 -> 32ï¼ˆå›ºå®šæ¡£ä½æ•°é‡ï¼‰
                 winsorize_quantiles: Tuple[float, float] = (0.01, 0.99),  # å¼‚å¸¸å€¼æˆªæ–­
                 label_gain_power: float = 2.1,  # ğŸ”§ 1.0 -> 2.1ï¼ˆæ ‡ç­¾å¢ç›Šå¹‚æ¬¡ï¼‰
                 lgb_params: Optional[Dict[str, Any]] = None,
                 num_boost_round: int = 1200,  # ğŸ”§ 700 -> 1200ï¼ˆæ›´å¤šè½®æ¬¡ï¼‰
                 early_stopping_rounds: int = 100,  # ğŸ”§ 70 -> 100ï¼ˆæ›´ä¿å®ˆæ—©åœï¼‰
                 use_purged_cv: bool = True,  # å¼ºåˆ¶ä½¿ç”¨PurgedCVé˜²æ­¢æ•°æ®æ³„éœ²ï¼ˆå½“internal CVå¯ç”¨æ—¶ï¼‰
                 use_internal_cv: bool = True,  # æ˜¯å¦åœ¨fitå†…éƒ¨æ‰§è¡ŒPurgedCVï¼ˆå¤–å±‚å·²æœ‰CVæ—¶å¯ç¦ç”¨ä»¥é¿å…fold-in-foldï¼‰
                 cv_n_splits: int = 6,        # ğŸ”¥ CVæŠ˜æ•°ï¼ˆT+5: 6æŠ˜ï¼Œæé«˜æ•°æ®åˆ©ç”¨ç‡ï¼‰
                 cv_gap_days: int = 5,        # ğŸ”¥ T+5é¢„æµ‹ï¼šgap=5ï¼ˆä¸horizonå¯¹é½ï¼‰
                 cv_embargo_days: int = 5,    # ğŸ”¥ T+5é¢„æµ‹ï¼šembargo=5ï¼ˆä¸horizonå¯¹é½ï¼‰
                 random_state: int = 42):
        """
        åˆå§‹åŒ–LambdaRankæ’åºæ¨¡å‹

        Args:
            base_cols: ç‰¹å¾åˆ—åï¼ˆNoneæ—¶è‡ªåŠ¨æ£€æµ‹alpha factoråˆ—ï¼‰
            n_quantiles: å›ºå®šæ¡£ä½æ•°é‡ï¼ˆ64/128ï¼Œç¨³å®šæ ‡ç­¾æ„é€ ï¼‰
            winsorize_quantiles: å¼‚å¸¸å€¼æˆªæ–­åˆ†ä½æ•°ï¼ˆé˜²æå€¼å½±å“ï¼‰
            label_gain_power: æ ‡ç­¾å¢ç›Šå¹‚æ¬¡ï¼ˆ1.0=çº¿æ€§ï¼Œ1.5=å¼ºåŒ–é«˜æ¡£ä½ï¼‰
            lgb_params: LightGBMå‚æ•°
            num_boost_round: è®­ç»ƒè½®æ•°
            early_stopping_rounds: æ—©åœè½®æ•°ï¼ˆå¢å¼ºé˜²è¿‡æ‹Ÿåˆï¼‰
            use_purged_cv: æ˜¯å¦ä½¿ç”¨PurgedCVï¼ˆå¼ºçƒˆæ¨èTrueï¼‰
            cv_n_splits: äº¤å‰éªŒè¯æŠ˜æ•°ï¼ˆT+5: 6æŠ˜ï¼‰
            cv_gap_days: CVé—´éš™å¤©æ•°ï¼ˆT+5: 5å¤©ï¼Œé˜²æ•°æ®æ³„éœ²ï¼‰
            cv_embargo_days: CVç¦è¿å¤©æ•°ï¼ˆT+5: 5å¤©ï¼Œé˜²å‰è§†åè¯¯ï¼‰
            random_state: éšæœºç§å­
        """
        if not LIGHTGBM_AVAILABLE:
            raise ImportError("LightGBM is required for LambdaRankStacker")
        if use_internal_cv and not use_purged_cv:
            raise ValueError("When internal CV is enabled, purged CV must be enabled for T+5 training.")
        if not PURGED_CV_AVAILABLE:
            raise RuntimeError("Unified Purged CV factory is unavailable. Install the required components to enable T+5 training.")
        if use_internal_cv and (cv_n_splits, cv_gap_days, cv_embargo_days) != (6, 5, 5):
            raise ValueError("LambdaRankStacker enforces T+5 CV settings when internal CV is enabled: splits=6, gap=5, embargo=5.")

        # base_colsåœ¨fitæ—¶ä¼šè‡ªåŠ¨æ£€æµ‹ä¸ºalpha factor columns
        self.base_cols = base_cols
        self._alpha_factor_cols = None  # å­˜å‚¨å®é™…ä½¿ç”¨çš„alpha factoråˆ—
        self.n_quantiles = n_quantiles
        self.winsorize_quantiles = winsorize_quantiles
        self.label_gain_power = label_gain_power
        self.num_boost_round = num_boost_round
        # è®­ç»ƒè¿‡å°æ ·æœ¬æ—¶é™åˆ¶ï¼Œé˜²æ­¢ best_iteration è¿‡é«˜
        self.early_stopping_rounds = max(early_stopping_rounds, 100)
        self.use_internal_cv = bool(use_internal_cv)
        self.use_purged_cv = bool(use_purged_cv)
        self.cv_n_splits = 6  # ğŸ”¥ ä½¿ç”¨å›ºå®šçš„n_splitsï¼ˆT+5: 6ï¼‰
        self.cv_gap_days = 5
        self.cv_embargo_days = 5
        self.random_state = random_state

        # ç”Ÿæˆlabel_gainåºåˆ—ï¼ˆæ”¯æŒå¹‚æ¬¡å¢å¼ºï¼‰
        if label_gain_power == 1.0:
            self.label_gain = list(range(n_quantiles))  # çº¿æ€§å¢ç›Š: [0,1,2,...,63]
        else:
            # å¹‚æ¬¡å¢ç›Šå¼ºåŒ–é«˜æ¡£ä½: [(i/N)^power * N for i in range(N)]
            self.label_gain = [(i / (n_quantiles - 1)) ** label_gain_power * (n_quantiles - 1)
                              for i in range(n_quantiles)]

        # ä¸“ä¸šçº§LambdaRankå‚æ•°ï¼ˆä½¿ç”¨ä¼ å…¥çš„lgb_paramsè¦†ç›–é»˜è®¤å€¼ï¼Œç¡®ä¿YAMLé…ç½®ç”Ÿæ•ˆï¼‰
        # é»˜è®¤å€¼ä»…ä½œä¸ºfallbackï¼Œå®é™…å€¼åº”ä»YAMLé…ç½®ä¼ å…¥
        # ğŸ”§ Updated 2025-01-19: æ›´ç¨³çš„å‚æ•°é…ç½®
        default_lgb_params = {
            'objective': 'lambdarank',
            'metric': 'ndcg',
            'ndcg_eval_at': [10],  # ğŸ”§ èšç„¦Top10
            'label_gain': self.label_gain,  # å…³é”®ï¼šå›ºå®šæ¡£ä½å¢ç›Š
            'num_leaves': 31,  # ğŸ”§ 63 -> 31ï¼ˆæ›´ä¿å®ˆï¼‰
            'max_depth': 5,   # ğŸ”§ 6 -> 5ï¼ˆæ›´æµ…ï¼‰
            'learning_rate': 0.01,  # ğŸ”§ 0.02 -> 0.01ï¼ˆæ›´ç¨³ï¼‰
            'feature_fraction': 0.9,  # ğŸ”§ 0.85 -> 0.9ï¼ˆç‰¹å¾å°‘å°±åˆ«dropå¤ªç‹ ï¼‰
            'bagging_fraction': 0.75,  # ğŸ”§ 0.8 -> 0.75
            'bagging_freq': 3,  # ğŸ”§ 5 -> 3
            'min_data_in_leaf': 800,  # ğŸ”§ 650 -> 800ï¼ˆæ›´ç¨³ï¼‰
            'lambda_l1': 0.0,  # Disabled L1 regularization
            'lambda_l2': 30.0,  # ğŸ”§ 22.0 -> 30.0ï¼ˆæ›´å¼ºæ­£åˆ™åŒ–ï¼‰
            'lambdarank_truncation_level': 80,  # ğŸ”§ 100 -> 80
            'sigmoid': 1.1,  # ğŸ”§ 1.05 -> 1.1
            'verbose': -1,
            'random_state': random_state,
            'force_col_wise': True
        }
        
        # å…ˆè®¾ç½®é»˜è®¤å€¼
        self.lgb_params = default_lgb_params.copy()
        
        # ç„¶åç”¨ä¼ å…¥çš„lgb_paramsè¦†ç›–ï¼ˆç¡®ä¿YAMLé…ç½®ç”Ÿæ•ˆï¼‰
        if lgb_params:
            self.lgb_params.update(lgb_params)

        # æ¨¡å‹çŠ¶æ€
        self.model = None
        self.scaler = None
        self.fitted_ = False
        self._oof_predictions = None  # OOFé¢„æµ‹ï¼ˆé˜²æ•°æ®æ³„æ¼ï¼‰
        self._oof_index = None  # OOFç´¢å¼•ï¼ˆç”¨äºå¯¹é½ï¼‰
        self._first_val_date = None  # ç¬¬ä¸€ä¸ªéªŒè¯é›†çš„æ—¥æœŸï¼ˆç”¨äºOOFå†·å¯åŠ¨è¿‡æ»¤ï¼‰

        logger.info("ğŸ† LambdaRank æ’åºæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ç‰¹å¾æ¨¡å¼: {'Alpha Factors' if self.base_cols is None else 'Custom'}")
        logger.info(f"   åˆ†ä½æ•°ç­‰çº§: {self.n_quantiles}")
        logger.info(f"   Label gain power: {self.label_gain_power}")
        logger.info(f"   NDCGè¯„ä¼°: {self.lgb_params['ndcg_eval_at']}")
        logger.info(f"   è®­ç»ƒè½®æ•°: {self.num_boost_round}, æ—©åœ: {self.early_stopping_rounds}")
        logger.info(f"   æ¨¡å‹å®¹é‡: num_leaves={self.lgb_params.get('num_leaves')}, max_depth={self.lgb_params.get('max_depth')}, min_data_in_leaf={self.lgb_params.get('min_data_in_leaf')}")
        logger.info(f"   é‡‡æ ·: feature_fraction={self.lgb_params.get('feature_fraction')}, bagging_fraction={self.lgb_params.get('bagging_fraction')}")
        logger.info(f"   LambdaRank: truncation_level={self.lgb_params.get('lambdarank_truncation_level')}, sigmoid={self.lgb_params.get('sigmoid')}")
        logger.info(f"   å†…éƒ¨CV: {'å¯ç”¨' if self.use_internal_cv else 'ç¦ç”¨'}")
        if self.use_internal_cv and self.use_purged_cv:
            logger.info(f"   CVå‚æ•°: splits={self.cv_n_splits}, gap={self.cv_gap_days}å¤©, embargo={self.cv_embargo_days}å¤©")

    def _convert_to_rank_labels(self, df: pd.DataFrame, target_col: str = 'ret_fwd_10d') -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        å°†è¿ç»­ç›®æ ‡å˜é‡è½¬æ¢ä¸ºç¨³å®šçš„å›ºå®šæ¡£ä½ç­‰çº§ï¼ˆ64/128æ¡£è½¯ç¦»æ•£ï¼‰

        æ ¸å¿ƒæ”¹è¿›ï¼š
        1. å¼‚å¸¸å€¼æˆªæ–­ (winsorize) æå‡é²æ£’æ€§
        2. rank(pct=True) â†’ floor(rank_pct * N) å›ºå®šæ¡£ä½
        3. é¿å…pd.qcutçš„binåˆå¹¶å’Œä¸ç¨³å®šé—®é¢˜
        4. å®Œå…¨ç¡®å®šæ€§çš„ç­‰çº§åˆ†é…

        Args:
            df: åŒ…å«ç›®æ ‡å˜é‡çš„DataFrame
            target_col: ç›®æ ‡å˜é‡åˆ—å

        Returns:
            å¤„ç†åçš„DataFrameï¼Œè½¬æ¢æŠ¥å‘Š
        """
        logger.info(f"ğŸ”„ å¼€å§‹ç¨³å®šæ ‡ç­¾æ„é€ : {target_col} â†’ å›ºå®š{self.n_quantiles}æ¡£è½¯ç¦»æ•£")

        df_processed = df.copy()

        def _stable_group_rank_transform(group):
            """ç¨³å®šçš„ç»„å†…å›ºå®šæ¡£ä½è½¬æ¢"""
            target_values = group[target_col].dropna()
            if len(target_values) <= 1:
                # å•ä¸ªæ ·æœ¬ç»„ï¼Œè®¾ä¸ºä¸­ä½ç­‰çº§
                group[f'{target_col}_rank'] = self.n_quantiles // 2
                return group

            # æ­¥éª¤1: Winsorizeå¼‚å¸¸å€¼æˆªæ–­ï¼ˆæå‡é²æ£’æ€§ï¼‰
            lower_q, upper_q = self.winsorize_quantiles
            lower_bound = target_values.quantile(lower_q)
            upper_bound = target_values.quantile(upper_q)
            target_winsorized = target_values.clip(lower_bound, upper_bound)

            # æ­¥éª¤2: ç»„å†…ç™¾åˆ†ä½æ’åº [0, 1]
            rank_pct = target_winsorized.rank(pct=True, method='average')

            # æ­¥éª¤3: å›ºå®šæ¡£ä½æ˜ å°„ [0, N-1]
            # rank_pct âˆˆ (0, 1] â†’ label âˆˆ [0, N-1]
            labels = np.floor(rank_pct * self.n_quantiles).astype(int)
            labels[labels == self.n_quantiles] = self.n_quantiles - 1  # å¤„ç†rank_pct=1çš„è¾¹ç•Œæƒ…å†µ

            # æ­¥éª¤4: æ˜ å°„å›åŸDataFrameç»“æ„
            full_ranks = pd.Series(self.n_quantiles // 2, index=group.index)  # é»˜è®¤ä¸­ä½ç­‰çº§
            full_ranks.loc[target_values.index] = labels
            group[f'{target_col}_rank'] = full_ranks.astype(int)

            return group

        # æŒ‰äº¤æ˜“æ—¥åˆ†ç»„è¿›è¡Œç¨³å®šç­‰çº§è½¬æ¢ï¼ˆé¿å…åˆ—å/ç´¢å¼•æ­§ä¹‰ï¼Œä½¿ç”¨ç´¢å¼•å±‚åï¼‰
        df_processed = df_processed.groupby(level='date', group_keys=False).apply(_stable_group_rank_transform)

        # éªŒè¯è½¬æ¢ç»“æœ
        rank_col = f'{target_col}_rank'
        unique_ranks = df_processed[rank_col].nunique()
        rank_distribution = df_processed[rank_col].value_counts().sort_index()

        # æ£€æŸ¥å¼‚å¸¸å€¼æˆªæ–­æ•ˆæœ
        original_values = df[target_col].dropna()
        winsorized_count = 0
        if len(original_values) > 0:
            lower_q, upper_q = self.winsorize_quantiles
            lower_bound = original_values.quantile(lower_q)
            upper_bound = original_values.quantile(upper_q)
            winsorized_count = ((original_values < lower_bound) | (original_values > upper_bound)).sum()

        logger.info(f"âœ… ç¨³å®šæ ‡ç­¾æ„é€ å®Œæˆ: å›ºå®š{unique_ranks}æ¡£ä½")
        logger.info(f"   å¼‚å¸¸å€¼æˆªæ–­: {winsorized_count}/{len(original_values)} ({winsorized_count/len(original_values)*100:.1f}%)")
        logger.info(f"   ç­‰çº§åˆ†å¸ƒ: {dict(list(rank_distribution.items())[:5])}...")

        conversion_report = {
            'n_quantiles_configured': self.n_quantiles,
            'n_quantiles_used': unique_ranks,
            'rank_distribution': dict(rank_distribution),
            'winsorized_count': winsorized_count,
            'winsorized_rate': winsorized_count / len(original_values) if len(original_values) > 0 else 0.0,
            'conversion_coverage': 1.0 - df_processed[rank_col].isna().mean(),
            'label_gain_type': 'linear' if self.label_gain_power == 1.0 else f'power_{self.label_gain_power}'
        }

        return df_processed, conversion_report

    def fit(self, df: pd.DataFrame, target_col: str = 'ret_fwd_10d', alpha_factors: pd.DataFrame = None) -> 'LambdaRankStacker':
        """
        è®­ç»ƒLambdaRankæ¨¡å‹

        Args:
            df: è®­ç»ƒæ•°æ®ï¼Œå¿…é¡»åŒ…å«MultiIndex(date, ticker)å’Œtarget
            target_col: ç›®æ ‡å˜é‡åˆ—å
            alpha_factors: Alphaå› å­DataFrameï¼ˆå¦‚æœä¸ºNoneï¼Œå°†ä»dfä¸­è‡ªåŠ¨æ£€æµ‹ï¼‰

        Returns:
            self
        """
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒLambdaRankæ’åºæ¨¡å‹ï¼ˆä½¿ç”¨Alpha Factorsï¼‰...")

        # ğŸ”§ ç»Ÿä¸€è¾“å…¥å¤„ç†ï¼šç¡®ä¿ä½¿ç”¨æ£€æµ‹åˆ°çš„MultiIndex
        # éªŒè¯è¾“å…¥æ ¼å¼
        if not isinstance(df.index, pd.MultiIndex):
            raise ValueError("DataFrameå¿…é¡»æœ‰MultiIndex(date, ticker)")
        
        # ğŸ”§ éªŒè¯MultiIndexæ ¼å¼æ­£ç¡®ï¼ˆdate, tickerï¼‰
        if df.index.names != ['date', 'ticker']:
            logger.warning(f"MultiIndexåç§°ä¸åŒ¹é…: {df.index.names}ï¼ŒæœŸæœ›: ['date', 'ticker']")
            # å°è¯•ä¿®å¤ï¼šå¦‚æœåªæœ‰ä¸¤å±‚ï¼Œé‡å‘½å
            if df.index.nlevels == 2:
                df.index.names = ['date', 'ticker']
                logger.info("âœ… å·²ä¿®å¤MultiIndexåç§°")
            else:
                raise ValueError(f"MultiIndexæ ¼å¼ä¸æ­£ç¡®: names={df.index.names}, levels={df.index.nlevels}")

        if target_col not in df.columns:
            # ä¸¥æ ¼æ¨¡å¼ï¼šä¸è¿›è¡Œä»»ä½•å›é€€ï¼Œè¦æ±‚ä¸Šæ¸¸æ˜ç¡®æä¾›æ­£ç¡®çš„ç›®æ ‡åˆ—
            raise ValueError(f"ç›®æ ‡å˜é‡ {target_col} ä¸å­˜åœ¨")

        # ğŸ”§ ç»Ÿä¸€è¾“å…¥å¤„ç†ï¼šä¸å…¶ä»–æ¨¡å‹ï¼ˆElasticNet/XGBoost/CatBoostï¼‰ä¿æŒä¸€è‡´
        # ä¼˜å…ˆçº§ï¼šbase_cols > alpha_factors > è‡ªåŠ¨æ£€æµ‹
        if self.base_cols is not None and len(self.base_cols) > 0:
            # ä½¿ç”¨æŒ‡å®šçš„åˆ—ï¼ˆä¸å…¶ä»–æ¨¡å‹ä¸€è‡´çš„ç‰¹å¾é€‰æ‹©ï¼‰
            self._alpha_factor_cols = [col for col in self.base_cols if col in df.columns]
            missing_cols = [col for col in self.base_cols if col not in df.columns]
            if missing_cols:
                raise ValueError(f"ç‰¹å¾åˆ—ä¸å­˜åœ¨: {missing_cols}")
            logger.info(f"   ä½¿ç”¨æŒ‡å®šçš„ç‰¹å¾åˆ—: {len(self._alpha_factor_cols)}ä¸ªå› å­ï¼ˆä¸å…¶ä»–æ¨¡å‹ä¸€è‡´ï¼‰")
        elif alpha_factors is not None:
            # ä½¿ç”¨æä¾›çš„alpha factors
            if not isinstance(alpha_factors, pd.DataFrame):
                raise ValueError("alpha_factorså¿…é¡»æ˜¯pandas DataFrame")

            self._alpha_factor_cols = [col for col in alpha_factors.columns if col != target_col]

            if len(self._alpha_factor_cols) == 0:
                raise ValueError("alpha_factorsä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç‰¹å¾åˆ—")

            # ç¡®ä¿ç´¢å¼•å¯¹é½
            try:
                df = pd.concat([df[[target_col]], alpha_factors[self._alpha_factor_cols]], axis=1)
            except Exception as e:
                raise ValueError(f"æ— æ³•åˆå¹¶DataFrameå’Œalpha_factors: {e}")

            logger.info(f"   ä½¿ç”¨æä¾›çš„Alpha Factors: {len(self._alpha_factor_cols)}ä¸ªå› å­")
        else:
            # è‡ªåŠ¨æ£€æµ‹alpha factoråˆ—ï¼ˆæ’é™¤targetå’Œpred_å¼€å¤´çš„åˆ—ï¼‰
            exclude_patterns = [target_col, 'pred_', 'lambda_', 'ridge_', 'final_', 'rank', 'weight']
            self._alpha_factor_cols = [col for col in df.columns
                                      if not any(pattern in col.lower() for pattern in exclude_patterns)]
            logger.info(f"   è‡ªåŠ¨æ£€æµ‹åˆ°{len(self._alpha_factor_cols)}ä¸ªAlpha Factors")
            logger.info(f"   å‰5ä¸ªå› å­: {self._alpha_factor_cols[:5]}")

        # æ›´æ–°base_colsä¸ºå®é™…ä½¿ç”¨çš„åˆ—
        self.base_cols = tuple(self._alpha_factor_cols)

        # ğŸ”§ ç»Ÿä¸€è¾“å…¥å¤„ç†ï¼šç¡®ä¿ä½¿ç”¨ä¸å…¶ä»–æ¨¡å‹ç›¸åŒçš„æ ·æœ¬å’Œç‰¹å¾
        # æ³¨æ„ï¼šä¸Šæ¸¸è®­ç»ƒå¾ªç¯å·²ç»å¤„ç†äº†NaNå’Œç´¢å¼•å¯¹é½ï¼Œè¿™é‡Œåªåšæœ€å°å¿…è¦çš„éªŒè¯
        
        # è½¬æ¢ä¸ºç»„å†…ç­‰çº§æ ‡ç­¾
        df_processed, conversion_report = self._convert_to_rank_labels(df, target_col)
        rank_col = f'{target_col}_rank'

        # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆä½¿ç”¨æŒ‡å®šçš„ç‰¹å¾åˆ—ï¼Œç¡®ä¿ä¸å…¶ä»–æ¨¡å‹ä¸€è‡´ï¼‰
        # ç¡®ä¿ç‰¹å¾åˆ—é¡ºåºä¸base_colsä¸€è‡´
        feature_cols = [col for col in self._alpha_factor_cols if col in df_processed.columns]
        if len(feature_cols) != len(self._alpha_factor_cols):
            missing = set(self._alpha_factor_cols) - set(feature_cols)
            raise ValueError(f"ç‰¹å¾åˆ—ç¼ºå¤±: {missing}")
        
        X = df_processed[feature_cols].values
        y = df_processed[rank_col].values

        # å‡†å¤‡åˆ†ç»„ä¿¡æ¯ï¼ˆæ¯ä¸ªäº¤æ˜“æ—¥ä¸ºä¸€ä¸ªç»„ï¼‰
        date_index = df_processed.index.get_level_values('date')
        unique_dates = date_index.unique()
        group_sizes = [len(df_processed.loc[date]) for date in unique_dates]

        logger.info(f"   è®­ç»ƒæ ·æœ¬: {len(X)}")
        logger.info(f"   ç‰¹å¾ç»´åº¦: {X.shape[1]} (ç‰¹å¾åˆ—: {feature_cols[:3]}...)")
        logger.info(f"   äº¤æ˜“æ—¥ç»„æ•°: {len(group_sizes)}")
        logger.info(f"   å¹³å‡ç»„å¤§å°: {np.mean(group_sizes):.1f}")

        # ğŸ”§ ç»Ÿä¸€NaNå¤„ç†ï¼šä¸å…¶ä»–æ¨¡å‹ä¿æŒä¸€è‡´
        # ä¸Šæ¸¸è®­ç»ƒå¾ªç¯å·²ç»å¤„ç†äº†NaNï¼Œä½†è¿™é‡Œä»éœ€è¦å¤„ç†å¯èƒ½çš„NaNï¼ˆæ¥è‡ªæ ‡ç­¾è½¬æ¢ï¼‰
        valid_mask = ~np.isnan(X).any(axis=1) & ~np.isnan(y)
        X_valid = X[valid_mask]
        y_valid = y[valid_mask]
        
        # è®°å½•è¿‡æ»¤çš„æ ·æœ¬æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if not valid_mask.all():
            logger.info(f"   è¿‡æ»¤NaNæ ·æœ¬: {len(X)} -> {len(X_valid)} ({len(X_valid)/len(X)*100:.1f}%)")

        # å°æ ·æœ¬è‡ªé€‚åº”ï¼šæ”¾å®½æœ€å°æ ·æœ¬é™åˆ¶å¹¶åŠ¨æ€è°ƒæ•´LightGBMå‚æ•°
        min_required = max(30, self.cv_n_splits * 2)
        if len(X_valid) < min_required:
            logger.warning(
                f"æœ‰æ•ˆè®­ç»ƒæ ·æœ¬è¿‡å°‘: {len(X_valid)} < {min_required}ï¼Œå¯ç”¨å°æ ·æœ¬è‡ªé€‚åº”å‚æ•°ä»¥ç»§ç»­è®­ç»ƒ"
            )
            small_n = int(len(X_valid))
            # åŠ¨æ€é™ä½å¤æ‚åº¦ï¼Œé¿å…å¶å­æ ·æœ¬è¦æ±‚è¿‡é«˜å¯¼è‡´è®­ç»ƒå¤±è´¥
            self.lgb_params['min_data_in_leaf'] = max(1, small_n // 5)
            self.lgb_params['num_leaves'] = min(self.lgb_params.get('num_leaves', 31), max(7, small_n // 2))
            self.lgb_params['max_depth'] = min(self.lgb_params.get('max_depth', 6), 6)
            self.lgb_params['learning_rate'] = min(self.lgb_params.get('learning_rate', 0.1), 0.1)
            # ç¼©çŸ­è®­ç»ƒè½®æ•°ä»¥é˜²è¿‡æ‹Ÿåˆå’Œè¿‡é•¿è®­ç»ƒ
            self.num_boost_round = min(self.num_boost_round, 100)
        elif len(X_valid) < 200:
            # ä¸­ç­‰å°æ ·æœ¬çš„æ¸©å’Œè‡ªé€‚åº”
            small_n = int(len(X_valid))
            self.lgb_params['min_data_in_leaf'] = max(5, min(self.lgb_params.get('min_data_in_leaf', 50), small_n // 4))

        # é‡æ–°è®¡ç®—ç»„å¤§å°ï¼ˆåŸºäºæœ‰æ•ˆæ ·æœ¬ï¼‰
        df_valid = df_processed.iloc[valid_mask]
        valid_date_index = df_valid.index.get_level_values('date')
        valid_unique_dates = valid_date_index.unique()
        valid_group_sizes = [len(df_valid.loc[date]) for date in valid_unique_dates]

        logger.info(f"   æœ‰æ•ˆæ ·æœ¬: {len(X_valid)} ({len(X_valid)/len(X)*100:.1f}%)")

        # ç‰¹å¾æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X_valid)

        logger.info(f"   ç‰¹å¾æ ‡å‡†åŒ–: å‡å€¼={X_scaled.mean(axis=0)[:3]}, æ ‡å‡†å·®={X_scaled.std(axis=0)[:3]}")

        if self.use_internal_cv:
            logger.info("ğŸ‹ï¸ å¼€å§‹PurgedCV LambdaRankè®­ç»ƒï¼ˆé˜²æ•°æ®æ³„éœ²ï¼‰...")
            self.model = self._train_with_purged_cv(
                X_scaled, y_valid, df_valid, valid_group_sizes
            )
        else:
            logger.info("ğŸ‹ï¸ å†…éƒ¨CVå·²ç¦ç”¨ï¼šåœ¨å¤–å±‚CVçš„è®­ç»ƒå­é›†ä¸Šè¿›è¡Œå…¨é‡è®­ç»ƒï¼ˆæ— å†…éƒ¨åˆ†æŠ˜ï¼‰")
            # æ„å»ºLightGBMæ•°æ®é›†ï¼ˆå°†æ‰€æœ‰æ ·æœ¬è§†ä¸ºä¸€ä¸ªå¤§ç»„ï¼›æ’åºå­¦ä¹ ä»å¯è¿è¡Œï¼Œä½†ä¸»è¦ä¾èµ–å¤–å±‚CVè¯„ä¼°ï¼‰
            # æ›´ä¸¥è°¨åšæ³•æ˜¯æŒ‰dateåˆ†ç»„æ„å»ºå•ä¸ªè®­ç»ƒæ•°æ®çš„groupæ•°ç»„
            dates = df_valid.index.get_level_values('date')
            unique_dates = dates.unique()
            train_group_sizes = [len(dates[dates == d]) for d in unique_dates]
            train_data = lgb.Dataset(
                X_scaled, label=y_valid, group=train_group_sizes,
                feature_name=[f'f_{i}' for i in range(X_scaled.shape[1])]
            )
            callbacks = [lgb.log_evaluation(period=0)]
            if self.early_stopping_rounds > 0:
                # æ— éªŒè¯é›†æ—¶ä¸ä½¿ç”¨æ—©åœ
                pass
            logger.info(f"ğŸ”§ [LambdaRankæœ€ç»ˆè®­ç»ƒ] ä½¿ç”¨å‚æ•°: feature_fraction={self.lgb_params.get('feature_fraction')}, min_data_in_leaf={self.lgb_params.get('min_data_in_leaf')}, lambdarank_truncation_level={self.lgb_params.get('lambdarank_truncation_level')}, label_gain_power={self.label_gain_power}")
            self.model = lgb.train(
                self.lgb_params,
                train_data,
                num_boost_round=self.num_boost_round,
                valid_sets=[],
                callbacks=callbacks
            )

        # è®­ç»ƒåè¯„ä¼°
        train_predictions = self.model.predict(X_scaled)

        # è®¡ç®—NDCGæŒ‡æ ‡ - æ‰©å±•Kå€¼ä»¥é€‚åº”2600è‚¡ç¥¨å®‡å®™
        ndcg_scores = {}
        # åˆ†å±‚è¯„ä¼°ï¼šæ ¸å¿ƒé€‰è‚¡ + æŠ•èµ„ç»„åˆæ„å»º + é£é™©åˆ†æ•£
        k_values = [5, 10, 20, 50, 100]  # æœ€å¤§åˆ°100ï¼Œè€ƒè™‘è®¡ç®—æ•ˆç‡
        for k in k_values:
            if k <= max(valid_group_sizes):
                ndcg_scores[f'NDCG@{k}'] = self._calculate_ndcg(y_valid, train_predictions, valid_group_sizes, k)

        self.fitted_ = True

        logger.info("âœ… LambdaRankæ’åºæ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆåŸºäºAlpha Factorsï¼‰")
        logger.info(f"   æœ€ä½³è¿­ä»£: {self.model.best_iteration}")
        logger.info(f"   NDCGæŒ‡æ ‡: {ndcg_scores}")
        logger.info(f"   ç‰¹å¾é‡è¦æ€§: {dict(zip(self.base_cols, self.model.feature_importance()[:len(self.base_cols)]))}")

        return self

    def _train_with_purged_cv(self, X_scaled: np.ndarray, y_valid: np.ndarray,
                             df_valid: pd.DataFrame, group_sizes: list) -> lgb.Booster:
        """
        ä½¿ç”¨PurgedCVè®­ç»ƒLambdaRankæ¨¡å‹ï¼ˆé˜²æ­¢æ•°æ®æ³„éœ²ï¼‰

        Args:
            X_scaled: æ ‡å‡†åŒ–åçš„ç‰¹å¾
            y_valid: æœ‰æ•ˆçš„ç›®æ ‡å˜é‡ï¼ˆç­‰çº§ï¼‰
            df_valid: æœ‰æ•ˆçš„DataFrameï¼ˆåŒ…å«æ—¥æœŸç´¢å¼•ï¼‰
            group_sizes: æ¯ä¸ªäº¤æ˜“æ—¥çš„ç»„å¤§å°

        Returns:
            è®­ç»ƒå¥½çš„LightGBMæ¨¡å‹
        """
        # åˆ›å»ºPurgedCV
        cv_splitter = create_unified_cv(
            n_splits=self.cv_n_splits,
            gap=self.cv_gap_days,
            embargo=self.cv_embargo_days
        )

        # è·å–æ—¥æœŸåºåˆ—ç”¨äºCVåˆ†å‰²
        dates = df_valid.index.get_level_values('date')
        unique_dates = sorted(dates.unique())

        logger.info(f"   PurgedCV: {self.cv_n_splits}æŠ˜, gap={self.cv_gap_days}å¤©, embargo={self.cv_embargo_days}å¤©")
        logger.info(f"   æ•°æ®æ—¶é—´èŒƒå›´: {unique_dates[0]} ~ {unique_dates[-1]} ({len(unique_dates)}å¤©)")

        # æ‰§è¡ŒCVè®­ç»ƒ
        cv_models = []
        cv_scores = []
        oof_predictions = np.zeros(len(X_scaled))  # åˆå§‹åŒ–OOFæ•°ç»„ï¼ˆé˜²æ•°æ®æ³„æ¼ï¼‰

        # ä¸ºCVåˆ›å»ºæ—¥æœŸç´¢å¼•æ˜ å°„
        date_to_idx = {date: i for i, date in enumerate(unique_dates)}
        sample_date_indices = [date_to_idx[date] for date in dates]

        cv_splits = list(cv_splitter.split(X_scaled, y_valid, groups=sample_date_indices))
        if not cv_splits:
            raise RuntimeError('PurgedCV did not yield any splits for LambdaRankStacker.')
        logger.info(f"   æˆåŠŸç”Ÿæˆ{len(cv_splits)}ä¸ªCVåˆ†å‰²")

        # ğŸ”§ OOFå†·å¯åŠ¨ä¿®å¤ï¼šè®°å½•ç¬¬ä¸€ä¸ªéªŒè¯é›†çš„æ—¥æœŸ
        first_val_date = None
        if cv_splits:
            first_train_idx, first_val_idx = cv_splits[0]
            first_val_dates = dates[first_val_idx]
            if len(first_val_dates) > 0:
                first_val_date = pd.to_datetime(first_val_dates.min()).normalize()
                self._first_val_date = first_val_date
                logger.info(f"   ğŸ”§ OOFå†·å¯åŠ¨ä¿®å¤: ç¬¬ä¸€ä¸ªéªŒè¯é›†æ—¥æœŸ = {first_val_date.date()}")
                logger.info(f"   âš ï¸  å°†è¿‡æ»¤æ‰æ­¤æ—¥æœŸä¹‹å‰çš„æ‰€æœ‰æ ·æœ¬ï¼ˆé¿å…åˆ†å¸ƒæ–­å±‚ï¼‰")

        # ğŸ”§ æœ€å°è®­ç»ƒçª—é™åˆ¶ï¼šè‡³å°‘2å¹´äº¤æ˜“æ—¥ï¼ˆçº¦500å¤©ï¼‰æ‰èƒ½è®¡å…¥OOFå’Œbest_iteration
        # è·å–æœ€å°è®­ç»ƒçª—é…ç½®ï¼ˆé»˜è®¤500å¤© = 2å¹´äº¤æ˜“æ—¥ï¼‰
        try:
            from bma_models.unified_config_loader import get_time_config
            time_config = get_time_config()
            min_train_window_days = getattr(time_config, 'min_train_window_days', 252)
        except:
            min_train_window_days = 252  # é»˜è®¤1å¹´äº¤æ˜“æ—¥
        
        logger.info(f"   ğŸ”§ æœ€å°è®­ç»ƒçª—é™åˆ¶: {min_train_window_days}å¤©ï¼ˆçº¦{min_train_window_days/252:.1f}å¹´ï¼‰")
        logger.info(f"   âš ï¸  è®­ç»ƒçª—å°äº{min_train_window_days}å¤©çš„foldå°†è¢«è·³è¿‡ï¼ˆé¿å…å™ªå£°çˆ†ç‚¸ï¼‰")
        
        # éå†CVåˆ†å‰²è¿›è¡Œè®­ç»ƒ
        valid_fold_start_idx = None  # è®°å½•ç¬¬ä¸€ä¸ªæœ‰æ•ˆfoldçš„ç´¢å¼•
        for fold_idx, (train_idx, val_idx) in enumerate(cv_splits):
            # è®¡ç®—è®­ç»ƒçª—å¤©æ•°
            train_dates = dates[train_idx]
            train_unique_dates = train_dates.unique()
            train_window_days = len(train_unique_dates)
            
            logger.info(f"   CV Fold {fold_idx + 1}/{len(cv_splits)}: è®­ç»ƒ={len(train_idx)}, éªŒè¯={len(val_idx)}, è®­ç»ƒçª—={train_window_days}å¤©")
            
            # ğŸ”§ æ£€æŸ¥è®­ç»ƒçª—æ˜¯å¦æ»¡è¶³æœ€å°è¦æ±‚
            if train_window_days < min_train_window_days:
                logger.warning(
                    f"   âš ï¸  CV Fold {fold_idx + 1} è®­ç»ƒçª—({train_window_days}å¤©) < æœ€å°è¦æ±‚({min_train_window_days}å¤©)ï¼Œè·³è¿‡"
                )
                logger.info(f"   ğŸ”§ æ­¤foldçš„OOFå’Œbest_iterationå°†ä¸è®¡å…¥ç»Ÿè®¡ï¼ˆé¿å…å™ªå£°æ±¡æŸ“ï¼‰")
                continue
            
            # è®°å½•ç¬¬ä¸€ä¸ªæœ‰æ•ˆfold
            if valid_fold_start_idx is None:
                valid_fold_start_idx = fold_idx
                logger.info(f"   âœ… ä»Fold {fold_idx + 1}å¼€å§‹è®¡å…¥OOFå’Œbest_iterationç»Ÿè®¡")

            # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯æ•°æ®
            X_train_fold, X_val_fold = X_scaled[train_idx], X_scaled[val_idx]
            y_train_fold, y_val_fold = y_valid[train_idx], y_valid[val_idx]

            # é‡æ–°è®¡ç®—è®­ç»ƒé›†çš„ç»„å¤§å°
            train_group_sizes = [len(train_dates[train_dates == date]) for date in train_unique_dates]

            # é‡æ–°è®¡ç®—éªŒè¯é›†çš„ç»„å¤§å°
            val_dates = dates[val_idx]
            val_unique_dates = val_dates.unique()
            val_group_sizes = [len(val_dates[val_dates == date]) for date in val_unique_dates]

            if len(train_group_sizes) == 0 or len(val_group_sizes) == 0:
                logger.warning(f"   CV Fold {fold_idx + 1}: è®­ç»ƒæˆ–éªŒè¯é›†ä¸ºç©ºï¼Œè·³è¿‡")
                continue

            # åˆ›å»ºLightGBMæ•°æ®é›†
            train_data = lgb.Dataset(
                X_train_fold, label=y_train_fold, group=train_group_sizes,
                feature_name=[f'f_{i}' for i in range(X_scaled.shape[1])]
            )
            val_data = lgb.Dataset(
                X_val_fold, label=y_val_fold, group=val_group_sizes,
                reference=train_data
            )

            # è®­ç»ƒæ¨¡å‹
            callbacks = [lgb.log_evaluation(period=0)]  # é™é»˜è®­ç»ƒ
            if self.early_stopping_rounds > 0:
                callbacks.append(lgb.early_stopping(self.early_stopping_rounds))

            logger.info(f"ğŸ”§ [LambdaRank CV Fold {fold_idx+1}] ä½¿ç”¨å‚æ•°: feature_fraction={self.lgb_params.get('feature_fraction')}, min_data_in_leaf={self.lgb_params.get('min_data_in_leaf')}, lambdarank_truncation_level={self.lgb_params.get('lambdarank_truncation_level')}, label_gain_power={self.label_gain_power}")
            model = lgb.train(
                self.lgb_params,
                train_data,
                num_boost_round=self.num_boost_round,
                valid_sets=[val_data],
                valid_names=['val'],
                callbacks=callbacks
            )

            cv_models.append(model)

            # è®¡ç®—éªŒè¯é›†NDCG - ä½¿ç”¨NDCG@50ä½œä¸ºä¸»è¦CVæŒ‡æ ‡
            val_pred = model.predict(X_val_fold)

            # ğŸ”§ OOFæ ‡å‡†åŒ–ï¼šå¯¹åŒä¸€å¤©çš„é¢„æµ‹åšæ¨ªæˆªé¢rankï¼ˆè§£å†³æŠ˜é—´æ¨¡å‹æˆç†Ÿåº¦ä¸åŒå¯¼è‡´çš„å°ºåº¦æ¼‚ç§»ï¼‰
            # å…³é”®ï¼šå¿…é¡»æ˜¯"æŒ‰å¤©"åšï¼Œå¯¹æ¯ä¸€å¤©å†…çš„é¢„æµ‹å€¼è¿›è¡Œrankï¼Œä¸èƒ½è·¨å¤©rank
            # è¿™æ ·meta rankeråƒçš„æ˜¯"ç¨³å®šçš„æ’åºå°ºåº¦"
            # ç¡®ä¿val_datesæ˜¯pd.Seriesæ ¼å¼ï¼ˆper_day_rank_normalizeéœ€è¦ï¼‰
            if isinstance(val_dates, pd.Index):
                val_dates_series = pd.Series(val_dates)
            elif isinstance(val_dates, np.ndarray):
                val_dates_series = pd.Series(val_dates)
            else:
                val_dates_series = pd.Series(val_dates) if not isinstance(val_dates, pd.Series) else val_dates
            
            val_pred_normalized = per_day_rank_normalize(val_pred, val_dates_series, use_gauss_rank=True)

            # ä¿å­˜OOFé¢„æµ‹ï¼ˆå…³é”®ï¼šé˜²æ­¢æ•°æ®æ³„æ¼ï¼Œä½¿ç”¨æ ‡å‡†åŒ–åçš„é¢„æµ‹ï¼‰
            oof_predictions[val_idx] = val_pred_normalized
            if len(val_group_sizes) > 0:
                # æ ¹æ®æ•°æ®é‡é€‰æ‹©åˆé€‚çš„ä¸»è¦è¯„ä¼°æŒ‡æ ‡
                max_group_size = max(val_group_sizes)
                primary_k = min(50, max_group_size) if max_group_size >= 50 else min(20, max_group_size)

                ndcg_score = self._calculate_ndcg(y_val_fold, val_pred, val_group_sizes, primary_k)
                cv_scores.append(ndcg_score)
                
                # ğŸ”§ Top-Kæ”¶ç›ŠproxyæŒ‡æ ‡ï¼ˆæœ€ç»ˆç­–ç•¥çš„æœ€ç›´æ¥proxyï¼‰
                # æ³¨æ„ï¼šTop-Kæ”¶ç›Šproxyä½¿ç”¨åŸå§‹é¢„æµ‹ï¼ˆæœªæ ‡å‡†åŒ–ï¼‰ï¼Œå› ä¸ºæˆ‘ä»¬è¦çœ‹çœŸå®çš„æ”¶ç›Šæ’åº
                topk_metrics = calculate_topk_return_proxy(val_pred, y_val_fold, val_dates_series, k=10)
                
                # å¤šå±‚æ¬¡è¯„ä¼°æŠ¥å‘Š
                if max_group_size >= 50:
                    ndcg5 = self._calculate_ndcg(y_val_fold, val_pred, val_group_sizes, 5)
                    ndcg20 = self._calculate_ndcg(y_val_fold, val_pred, val_group_sizes, 20)
                    logger.info(
                        f"   CV Fold {fold_idx + 1}: NDCG@5={ndcg5:.4f}, @20={ndcg20:.4f}, @50={ndcg_score:.4f} | "
                        f"Top10æ”¶ç›Šproxy: mean={topk_metrics['mean_return']:.4f}, IR={topk_metrics['ir']:.2f}, t-stat={topk_metrics['t_stat']:.2f}"
                    )
                else:
                    ndcg5 = self._calculate_ndcg(y_val_fold, val_pred, val_group_sizes, 5)
                    logger.info(
                        f"   CV Fold {fold_idx + 1}: NDCG@5={ndcg5:.4f}, @{primary_k}={ndcg_score:.4f} | "
                        f"Top10æ”¶ç›Šproxy: mean={topk_metrics['mean_return']:.4f}, IR={topk_metrics['ir']:.2f}, t-stat={topk_metrics['t_stat']:.2f}"
                    )

        if cv_models:
            # ğŸ”§ åªä½¿ç”¨æ»¡è¶³æœ€å°è®­ç»ƒçª—è¦æ±‚çš„foldçš„åˆ†æ•°
            if len(cv_scores) > 0:
                primary_k_desc = "50" if max([max(fold_sizes) for fold_sizes in [val_group_sizes] if fold_sizes]) >= 50 else "20"
                logger.info(f"   CVå¹³å‡NDCG@{primary_k_desc}: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f} (åŸºäº{len(cv_scores)}ä¸ªæœ‰æ•ˆfold)")
                
                if valid_fold_start_idx is not None and valid_fold_start_idx > 0:
                    skipped_folds = valid_fold_start_idx
                    logger.info(f"   ğŸ”§ è·³è¿‡äº†å‰{skipped_folds}ä¸ªfoldï¼ˆè®­ç»ƒçª—ä¸è¶³{min_train_window_days}å¤©ï¼‰")

            # ä¿å­˜OOFé¢„æµ‹å’Œç´¢å¼•ï¼ˆç”¨äºåç»­èåˆï¼‰
            self._oof_predictions = oof_predictions
            self._oof_index = pd.Series(dates).reset_index(drop=True)  # ä¿å­˜åŸå§‹ç´¢å¼•
            logger.info(f"   âœ“ OOFé¢„æµ‹å·²ç”Ÿæˆ: {len(oof_predictions)} ä¸ªæ ·æœ¬")

            # è¿”å›æœ€åä¸€ä¸ªæ¨¡å‹ï¼ˆè§è¿‡æœ€å¤šæ•°æ®ï¼‰
            return cv_models[-1]
        else:
            raise RuntimeError("æ‰€æœ‰CV foldéƒ½å¤±è´¥ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")

    def get_oof_predictions(self, df: pd.DataFrame) -> pd.Series:
        """
        è·å–OOFé¢„æµ‹ï¼ˆOut-of-Fold predictionsï¼‰

        é‡è¦ï¼šè¿™æ˜¯çœŸæ­£çš„OOFé¢„æµ‹ï¼Œæ¯ä¸ªæ ·æœ¬åªè¢«æœªè§è¿‡å®ƒçš„æ¨¡å‹é¢„æµ‹ï¼Œé˜²æ­¢æ•°æ®æ³„æ¼ã€‚
        
        ğŸ”§ OOFå†·å¯åŠ¨ä¿®å¤ï¼šè‡ªåŠ¨è¿‡æ»¤æ‰ç¬¬ä¸€ä¸ªéªŒè¯é›†æ—¥æœŸä¹‹å‰çš„æ ·æœ¬ï¼ˆè¿™äº›æ ·æœ¬çš„OOFé¢„æµ‹ä¸º0æˆ–ç¼ºå¤±ï¼‰

        Args:
            df: åŸå§‹è®­ç»ƒæ•°æ®ï¼ˆç”¨äºæå–MultiIndexï¼‰

        Returns:
            OOFé¢„æµ‹Seriesï¼ˆå¸¦MultiIndex: date, tickerï¼‰ï¼ŒåªåŒ…å«æœ‰æœ‰æ•ˆOOFé¢„æµ‹çš„æ ·æœ¬

        Raises:
            RuntimeError: å¦‚æœOOFé¢„æµ‹æœªç”Ÿæˆï¼ˆæ¨¡å‹æœªä½¿ç”¨CVè®­ç»ƒï¼‰
            ValueError: å¦‚æœdfæ²¡æœ‰MultiIndexæˆ–ç´¢å¼•é•¿åº¦ä¸åŒ¹é…
        """
        if self._oof_predictions is None:
            raise RuntimeError("OOFé¢„æµ‹æœªç”Ÿæˆï¼Œå¯èƒ½æ¨¡å‹æœªä½¿ç”¨CVè®­ç»ƒ")

        if not isinstance(df.index, pd.MultiIndex):
            raise ValueError("dfå¿…é¡»æœ‰MultiIndex(date, ticker)")

        if len(self._oof_predictions) != len(df):
            raise ValueError(
                f"OOFé¢„æµ‹é•¿åº¦({len(self._oof_predictions)})ä¸dfé•¿åº¦({len(df)})ä¸åŒ¹é…"
            )

        # åˆ›å»ºSeriesï¼ˆä½¿ç”¨dfçš„MultiIndexï¼‰
        oof_series = pd.Series(
            self._oof_predictions,
            index=df.index,
            name='lambda_oof'
        )

        # ğŸ”§ OOFå†·å¯åŠ¨ä¿®å¤ï¼šè¿‡æ»¤æ‰ç¬¬ä¸€ä¸ªéªŒè¯é›†æ—¥æœŸä¹‹å‰çš„æ ·æœ¬
        if self._first_val_date is not None:
            df_dates = pd.to_datetime(df.index.get_level_values('date')).normalize()
            valid_mask = df_dates >= self._first_val_date
            
            before_count = (~valid_mask).sum()
            if before_count > 0:
                logger.warning(
                    f"   âš ï¸  OOFå†·å¯åŠ¨ç©ºæ´æ£€æµ‹: è¿‡æ»¤æ‰{before_count}ä¸ªæ ·æœ¬ "
                    f"(æ—¥æœŸ < {self._first_val_date.date()})"
                )
                logger.info(
                    f"   ğŸ”§ è¿™äº›æ ·æœ¬çš„OOFé¢„æµ‹ä¸º0/ç¼ºå¤±ï¼Œä¼šå¯¼è‡´Stackerå­¦åˆ°æ—¶é—´ä¼ªä¿¡å·"
                )
            
            oof_series = oof_series[valid_mask]
            logger.info(
                f"âœ“ è¿”å›Lambda OOFé¢„æµ‹: {len(oof_series)} ä¸ªæœ‰æ•ˆæ ·æœ¬ "
                f"(å·²è¿‡æ»¤{before_count}ä¸ªå†·å¯åŠ¨æ ·æœ¬)"
            )
        else:
            logger.info(f"âœ“ è¿”å›Lambda OOFé¢„æµ‹: {len(oof_series)} ä¸ªæ ·æœ¬ (æœªè®°å½•first_val_date)")

        return oof_series

    def predict(self, df: pd.DataFrame, alpha_factors: pd.DataFrame = None) -> pd.DataFrame:
        """
        ä½¿ç”¨LambdaRankæ¨¡å‹é¢„æµ‹

        Args:
            df: é¢„æµ‹æ•°æ®
            alpha_factors: Alphaå› å­DataFrameï¼ˆå¦‚æœä¸ºNoneï¼Œå°†ä»dfä¸­æå–ï¼‰

        Returns:
            åŒ…å«é¢„æµ‹åˆ†æ•°çš„DataFrame
        """
        if not self.fitted_:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fit()")

        logger.info("ğŸ“Š LambdaRankæ’åºæ¨¡å‹å¼€å§‹é¢„æµ‹ï¼ˆåŸºäºAlpha Factorsï¼‰...")

        # éªŒè¯è¾“å…¥å¹¶ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
        if alpha_factors is not None:
            # ä½¿ç”¨æä¾›çš„alpha factors
            df_clean = alpha_factors.copy()
        else:
            df_clean = df.copy()

        # ä½¿ç”¨è®­ç»ƒæ—¶çš„alpha factoråˆ—
        if self._alpha_factor_cols is None:
            raise RuntimeError("Alpha factoråˆ—æœªè®¾ç½®ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")

        # æ£€æŸ¥å¿…éœ€çš„åˆ—
        missing_cols = [col for col in self._alpha_factor_cols if col not in df_clean.columns]
        if missing_cols:
            logger.warning(f"ç¼ºå°‘{len(missing_cols)}ä¸ªAlpha factoråˆ—: {missing_cols[:5]}")
            # å°è¯•å®¹é”™å¤„ç†
            available_cols = [col for col in self._alpha_factor_cols if col in df_clean.columns]
            if len(available_cols) < len(self._alpha_factor_cols) * 0.5:
                raise ValueError(f"å¯ç”¨Alpha factoråˆ—å¤ªå°‘: {len(available_cols)}/{len(self._alpha_factor_cols)}")
            X = df_clean[available_cols].values
            logger.info(f"   ä½¿ç”¨{len(available_cols)}ä¸ªå¯ç”¨Alpha factorsè¿›è¡Œé¢„æµ‹")
        else:
            # æå–ç‰¹å¾
            X = df_clean[list(self._alpha_factor_cols)].values

        # å¤„ç†NaN
        valid_mask = ~np.isnan(X).any(axis=1)
        X_valid = X[valid_mask]

        if len(X_valid) == 0:
            raise ValueError("æ‰€æœ‰æ ·æœ¬éƒ½åŒ…å«NaNï¼Œæ— æ³•é¢„æµ‹")

        # ä½¿ç”¨è®­ç»ƒæ—¶æ‹Ÿåˆçš„æ ‡å‡†åŒ–å™¨
        X_scaled = self.scaler.transform(X_valid)

        logger.info(f"   ç‰¹å¾æå–: {len(X_valid)} æœ‰æ•ˆæ ·æœ¬")

        # é¢„æµ‹
        raw_predictions = self.model.predict(X_scaled)

        # åˆ›å»ºå®Œæ•´é¢„æµ‹æ•°ç»„
        full_predictions = np.full(len(X), np.nan)
        full_predictions[valid_mask] = raw_predictions

        # æ„å»ºç»“æœDataFrame
        result = df_clean.copy()
        result['lambda_score'] = full_predictions

        # æŒ‰æ—¥æœŸè®¡ç®—æ’åï¼ˆLambdaRankçš„æ ¸å¿ƒè¾“å‡ºï¼‰
        def _rank_by_date(group):
            scores = group['lambda_score']
            valid_scores = scores.dropna()
            if len(valid_scores) <= 1:
                return pd.Series(np.nan, index=scores.index)

            ranks = valid_scores.rank(method='average', ascending=False)
            full_ranks = pd.Series(np.nan, index=scores.index)
            full_ranks.loc[valid_scores.index] = ranks
            return full_ranks

        # ğŸ”§ FIX: ä¿æŒæ­£ç¡®çš„ç´¢å¼•é¡ºåºï¼Œé¿å…groupby.applyäº§ç”Ÿå¤šå±‚ç´¢å¼•
        ranked_series = result.groupby(level='date')['lambda_score'].rank(method='average', ascending=False)
        result['lambda_rank'] = ranked_series

        # è®¡ç®—ç»„å†…ç™¾åˆ†ä½ï¼ˆç”¨äºåç»­Copulaæ­£æ€åŒ–ï¼‰
        pct_series = result.groupby(level='date')['lambda_score'].rank(pct=True)
        result['lambda_pct'] = pct_series

        logger.info(f"âœ… LambdaRanké¢„æµ‹å®Œæˆ: è¦†ç›–ç‡={valid_mask.mean():.1%}")

        return result[['lambda_score', 'lambda_rank', 'lambda_pct']]

    def _calculate_ndcg(self, y_true: np.ndarray, y_pred: np.ndarray, group_sizes: list, k: int) -> float:
        """è®¡ç®—NDCG@KæŒ‡æ ‡"""
        try:
            from sklearn.metrics import ndcg_score

            # å°†é¢„æµ‹åˆ†ç»„
            start_idx = 0
            ndcg_scores = []

            for group_size in group_sizes:
                if group_size < k:
                    continue

                end_idx = start_idx + group_size
                group_true = y_true[start_idx:end_idx]
                group_pred = y_pred[start_idx:end_idx]

                # è®¡ç®—NDCG@K
                ndcg = ndcg_score(
                    group_true.reshape(1, -1),
                    group_pred.reshape(1, -1),
                    k=k
                )
                ndcg_scores.append(ndcg)

                start_idx = end_idx

            return np.mean(ndcg_scores) if ndcg_scores else 0.0

        except ImportError:
            # Fallbackï¼šç®€å•çš„æ’åºç›¸å…³æ€§
            return 0.0

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if not self.fitted_:
            return {'fitted': False}

        return {
            'fitted': True,
            'model_type': 'LambdaRank',
            'best_iteration': self.model.best_iteration,
            'feature_importance': dict(zip(self.base_cols, self.model.feature_importance()[:len(self.base_cols)])),
            'n_quantiles': self.n_quantiles,
            'lgb_params': self.lgb_params
        }
