#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Dual Head Late Fusion - åŒå¤´æ™šèåˆç³»ç»Ÿ
====================================

ä¸“ä¸šçº§åŒå¤´èåˆæ¶æ„ï¼Œé’ˆå¯¹"æœ€å¤§åŒ–é¢„æµ‹æ”¶ç›Šç‡"ä¼˜åŒ–ï¼š
- å›å½’å¤´ï¼ˆRidge stackingï¼‰ï¼šé¢„æµ‹è¿ç»­æ”¶ç›Šç‡magnitude
- æ’åºå¤´ï¼ˆLambdaRankï¼‰ï¼šä¼˜åŒ–Top-Ké€‰è‚¡æ’åº

æ ¸å¿ƒç‰¹æ€§ï¼š
1. ç‹¬ç«‹æ ¡å‡†ï¼šIsotonicï¼ˆå›å½’ï¼‰ + Quantile-Normalï¼ˆæ’åºï¼‰
2. æ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼šç»Ÿä¸€åˆ»åº¦ï¼ˆz-scoreï¼‰
3. å›ºå®šçº¿æ€§åŠ æƒï¼šS = Î± * z_reg + Î² * z_rank
4. OOFè°ƒå‚ï¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜Î±/Î²
5. é˜²æ•°æ®æ³„æ¼ï¼šä¸¥æ ¼æ—¶é—´éš”ç¦»

ä½œè€…: BMA Enhanced System
æ—¥æœŸ: 2025-01-XX
ç‰ˆæœ¬: 1.0.0 (T+5 Optimized)
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, Tuple, Optional, List
import logging
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import mean_squared_error
from scipy.stats import spearmanr, norm
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class DualHeadLateFusion:
    """
    åŒå¤´æ™šèåˆç³»ç»Ÿ - å›ºå®šçº¿æ€§åŠ æƒåŸºçº¿ï¼ˆStrong Stable Baselineï¼‰

    æ¶æ„ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ç¬¬ä¸€å±‚ï¼šåŸºç¡€æ¨¡å‹ï¼ˆPurgedCV OOFï¼‰                          â”‚
    â”‚  â”œâ”€ ElasticNet, XGBoost, CatBoost                        â”‚
    â”‚  â””â”€ ç”ŸæˆOOF predictions                                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼                        â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  å›å½’å¤´ï¼ˆRidgeï¼‰       â”‚  â”‚  æ’åºå¤´ï¼ˆLambdaRankï¼‰ â”‚
    â”‚  ç›®æ ‡ï¼šé¢„æµ‹æ”¶ç›Šç‡      â”‚  â”‚  ç›®æ ‡ï¼šTop-Kæ’åº     â”‚
    â”‚  è¾“å…¥ï¼š3ä¸ªOOF          â”‚  â”‚  è¾“å…¥ï¼šAlpha Factors â”‚
    â”‚  è¾“å‡ºï¼šÅ·_reg          â”‚  â”‚  è¾“å‡ºï¼šs_rank        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                         â”‚
                â”‚  Isotonicæ ¡å‡†           â”‚  Quantileæ˜ å°„
                â”‚  â†“                      â”‚  â†“
                â”‚  æ¨ªæˆªé¢z-score          â”‚  æ­£æ€åŒ–z-score
                â”‚  â†“                      â”‚  â†“
                â”‚  z_reg                  â”‚  z_rank
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  å›ºå®šçº¿æ€§åŠ æƒèåˆ        â”‚
              â”‚  S = Î±*z_reg + Î²*z_rank â”‚
              â”‚  (Î±=0.7, Î²=0.3 é»˜è®¤)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ä½¿ç”¨åœºæ™¯ï¼š
    - é‡åŒ–é€‰è‚¡ï¼šéœ€è¦é¢„æµ‹æ”¶ç›Šç‡ + æ’åº
    - æŠ•èµ„ç»„åˆæ„å»ºï¼šsizingç”¨å›å½’ï¼Œæ’åºç”¨LambdaRank
    - é£é™©ç®¡ç†ï¼šè¿ç»­é¢„æµ‹å€¼ç”¨äºé£æ§è®¡ç®—
    """

    def __init__(self,
                 alpha: float = 0.7,
                 beta: float = 0.3,
                 auto_tune_weights: bool = True,
                 alpha_grid: List[float] = None,
                 tanh_clip_c: float = 2.5,
                 use_isotonic_calibration: bool = False,  # ğŸ”§ é»˜è®¤å…³é—­ï¼šæ•ˆæœå°ä¸”å¢åŠ è¿‡æ‹Ÿåˆé£é™©
                 random_state: int = 42):
        """
        åˆå§‹åŒ–åŒå¤´æ™šèåˆç³»ç»Ÿ

        Args:
            alpha: å›å½’å¤´æƒé‡ï¼ˆé»˜è®¤0.7ï¼Œå›å½’ä¸ºä¸»ï¼‰
            beta: æ’åºå¤´æƒé‡ï¼ˆé»˜è®¤0.3ï¼Œæ’åºä¸ºè¾…ï¼‰
            auto_tune_weights: æ˜¯å¦ä½¿ç”¨OOFè‡ªåŠ¨è°ƒå‚ï¼ˆæ¨èTrueï¼‰
            alpha_grid: è°ƒå‚ç½‘æ ¼ï¼ˆé»˜è®¤[0.5, 0.6, 0.7, 0.8, 0.9]ï¼‰
            tanh_clip_c: tanhé™å¹…å‚æ•°ï¼ˆé˜²æ­¢æç«¯å€¼ï¼Œé»˜è®¤2.5ï¼‰
            use_isotonic_calibration: æ˜¯å¦å¯¹å›å½’å¤´ä½¿ç”¨Isotonicæ ¡å‡†
            random_state: éšæœºç§å­
        """
        # æƒé‡é…ç½®
        self.alpha = alpha
        self.beta = beta
        if abs(alpha + beta - 1.0) > 0.01:
            logger.warning(f"æƒé‡ä¹‹å’Œä¸ä¸º1: Î±={alpha}, Î²={beta}ï¼Œå°†è‡ªåŠ¨å½’ä¸€åŒ–")
            total = alpha + beta
            self.alpha = alpha / total
            self.beta = beta / total

        self.auto_tune_weights = auto_tune_weights
        self.alpha_grid = alpha_grid or [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
        self.tanh_clip_c = tanh_clip_c
        self.use_isotonic_calibration = use_isotonic_calibration
        self.random_state = random_state

        # æ ¡å‡†æ¨¡å‹ï¼ˆè®­ç»ƒååˆå§‹åŒ–ï¼‰
        self.isotonic_reg = None  # å›å½’å¤´æ ¡å‡†å™¨
        self.rank_quantile_map = None  # æ’åºå¤´åˆ†ä½æ•°æ˜ å°„

        # è®­ç»ƒçŠ¶æ€
        self.fitted_ = False
        self.best_alpha_ = alpha
        self.best_beta_ = beta
        self.tuning_results_ = None

        logger.info("âœ… åŒå¤´æ™šèåˆç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   å›å½’å¤´æƒé‡ Î±: {self.alpha:.2f}")
        logger.info(f"   æ’åºå¤´æƒé‡ Î²: {self.beta:.2f}")
        logger.info(f"   è‡ªåŠ¨è°ƒå‚: {self.auto_tune_weights}")
        logger.info(f"   Isotonicæ ¡å‡†: {self.use_isotonic_calibration}")
        logger.info(f"   Tanhé™å¹…: c={self.tanh_clip_c}")

    def calibrate_regression_head(self, y_pred_reg_oof: pd.Series,
                                   y_true_oof: pd.Series) -> None:
        """
        æ ¡å‡†å›å½’å¤´ï¼šIsotonic Regressionï¼ˆå•è°ƒæ˜ å°„ï¼Œä¿ç•™æ’åºï¼‰

        ä¸ºä»€ä¹ˆä½¿ç”¨Isotonicï¼Ÿ
        - å•è°ƒæ€§ï¼šä¿ç•™åŸå§‹æ’åºä¿¡æ¯
        - éå‚æ•°ï¼šè‡ªé€‚åº”æ•°æ®åˆ†å¸ƒ
        - æ ¡å‡†æ€§ï¼šä¿®æ­£ç³»ç»Ÿæ€§åå·®

        Args:
            y_pred_reg_oof: å›å½’å¤´OOFé¢„æµ‹ï¼ˆè¿ç»­å€¼ï¼‰
            y_true_oof: çœŸå®æ ‡ç­¾ï¼ˆè¿ç»­æ”¶ç›Šç‡ï¼‰
        """
        if not self.use_isotonic_calibration:
            logger.info("â­ï¸ Isotonicæ ¡å‡†å·²ç¦ç”¨ï¼Œè·³è¿‡")
            return

        logger.info("ğŸ”§ å¼€å§‹æ ¡å‡†å›å½’å¤´ï¼ˆIsotonic Regressionï¼‰...")

        # ç§»é™¤NaN
        valid_mask = y_pred_reg_oof.notna() & y_true_oof.notna()
        y_pred_clean = y_pred_reg_oof[valid_mask].values
        y_true_clean = y_true_oof[valid_mask].values

        if len(y_pred_clean) < 30:
            logger.warning(f"âš ï¸ æœ‰æ•ˆæ ·æœ¬è¿‡å°‘({len(y_pred_clean)}), è·³è¿‡Isotonicæ ¡å‡†")
            return

        # è®­ç»ƒIsotonic Regression
        self.isotonic_reg = IsotonicRegression(
            y_min=np.percentile(y_true_clean, 1),
            y_max=np.percentile(y_true_clean, 99),
            increasing=True,
            out_of_bounds='clip'
        )
        self.isotonic_reg.fit(y_pred_clean, y_true_clean)

        # è¯„ä¼°æ ¡å‡†æ•ˆæœ
        y_calibrated = self.isotonic_reg.predict(y_pred_clean)
        mse_before = mean_squared_error(y_true_clean, y_pred_clean)
        mse_after = mean_squared_error(y_true_clean, y_calibrated)

        logger.info(f"âœ… Isotonicæ ¡å‡†å®Œæˆ")
        logger.info(f"   MSE: {mse_before:.6f} â†’ {mse_after:.6f} (æ”¹å–„{(1-mse_after/mse_before)*100:.1f}%)")

    def calibrate_ranking_head(self, s_rank_oof: pd.Series,
                               y_true_oof: pd.Series,
                               dates: pd.Series) -> None:
        """
        æ ¡å‡†æ’åºå¤´ï¼šç®€åŒ–ç‰ˆ - ç›´æ¥ä½¿ç”¨rank percentile

        ä¿®å¤åŸå› ï¼š
        - åŸç‰ˆè®¡ç®—çš„æ˜¯"å¹³å‡çœŸå®æ”¶ç›Š"è€Œé"æ”¶ç›Šåˆ†ä½æ•°"
        - å¯¼è‡´æ˜ å°„èŒƒå›´å€’æŒ‚ï¼ˆæœ€å¤§å€¼ < æœ€å°å€¼ï¼‰
        - ç®€åŒ–ä¸ºç›´æ¥ä½¿ç”¨Lambdaåˆ†æ•°çš„rank percentile

        é‡è¦è¯´æ˜ï¼š
        - å¦‚æœLambda OOFä¸æ”¶ç›Šè´Ÿç›¸å…³ï¼Œä¼šè¾“å‡ºERRORçº§åˆ«è­¦å‘Š
        - ä¸åº”åœ¨é¢„æµ‹æ—¶è‡ªåŠ¨åè½¬åˆ†æ•°ï¼ˆä¼šæ©ç›–è®­ç»ƒé—®é¢˜ï¼‰
        - å»ºè®®ï¼šä½¿ç”¨lambda_pctï¼ˆç™¾åˆ†ä½ï¼‰è€Œélambda_scoreï¼ˆåŸå§‹åˆ†æ•°ï¼‰
          å› ä¸ºlambda_pcté€šè¿‡rank(pct=True, ascending=True)è®¡ç®—ï¼Œ
          ä¿è¯äº†åˆ†æ•°é«˜â†’ç™¾åˆ†ä½é«˜çš„ä¸€è‡´æ€§

        Args:
            s_rank_oof: æ’åºå¤´OOFåˆ†æ•°ï¼ˆLambdaRankè¾“å‡ºçš„åŸå§‹åˆ†æ•°ï¼‰
            y_true_oof: çœŸå®æ ‡ç­¾
            dates: æ—¥æœŸåºåˆ—ï¼ˆç”¨äºæ¨ªæˆªé¢åˆ†ä½æ•°è®¡ç®—ï¼‰
        """
        logger.info("ğŸ”§ å¼€å§‹æ ¡å‡†æ’åºå¤´ï¼ˆç®€åŒ–ç‰ˆï¼šç›´æ¥ä½¿ç”¨Rank Percentileï¼‰...")

        # ç§»é™¤NaN
        valid_mask = s_rank_oof.notna() & y_true_oof.notna()
        s_rank_clean = s_rank_oof[valid_mask]
        y_true_clean = y_true_oof[valid_mask]
        dates_clean = dates[valid_mask]

        if len(s_rank_clean) < 30:
            logger.warning(f"âš ï¸ æœ‰æ•ˆæ ·æœ¬è¿‡å°‘({len(s_rank_clean)}), è·³è¿‡æ’åºæ ¡å‡†")
            return

        # éªŒè¯Lambdaä¸æ”¶ç›Šçš„ç›¸å…³æ€§ï¼ˆä»…è¯Šæ–­ï¼Œä¸åè½¬ï¼‰
        from scipy.stats import spearmanr
        avg_rank_ic = 0.0

        try:
            df_temp = pd.DataFrame({
                's_rank': s_rank_clean.values,
                'y_true': y_true_clean.values,
                'date': dates_clean.values
            })

            # è®¡ç®—æ¨ªæˆªé¢Spearmanç›¸å…³ç³»æ•°
            corr_scores = []
            for date, group in df_temp.groupby('date'):
                if len(group) >= 10:  # è‡³å°‘10ä¸ªæ ·æœ¬æ‰è®¡ç®—ç›¸å…³æ€§
                    corr, _ = spearmanr(group['s_rank'], group['y_true'])
                    if not np.isnan(corr):
                        corr_scores.append(corr)

            avg_rank_ic = np.mean(corr_scores) if corr_scores else 0.0

            # ğŸ”§ è¯Šæ–­è­¦å‘Šï¼ˆä¸è‡ªåŠ¨ä¿®æ­£ï¼Œç”¨æˆ·éœ€æ£€æŸ¥è®­ç»ƒé€»è¾‘ï¼‰
            if avg_rank_ic < -0.01:  # è´Ÿç›¸å…³é˜ˆå€¼
                logger.error("="*80)
                logger.error(f"âŒ ä¸¥é‡é—®é¢˜ï¼šLambdaä¸æ”¶ç›Šå‘ˆè´Ÿç›¸å…³({avg_rank_ic:.4f})ï¼")
                logger.error("   è¿™è¡¨æ˜LambdaRankè®­ç»ƒå¯èƒ½æœ‰é—®é¢˜ï¼š")
                logger.error("   1. æ£€æŸ¥ç‰¹å¾ä¸æ”¶ç›Šçš„å…³ç³»æ˜¯å¦æ­£ç¡®")
                logger.error("   2. æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®æ³„æ¼")
                logger.error("   3. æ£€æŸ¥LambdaRankæ˜¯å¦è¿‡æ‹Ÿåˆ")
                logger.error("   å»ºè®®ï¼šä½¿ç”¨lambda_pctï¼ˆç™¾åˆ†ä½ï¼‰è€Œélambda_scoreï¼ˆåŸå§‹åˆ†æ•°ï¼‰")
                logger.error("="*80)
            elif avg_rank_ic < 0:
                logger.warning(f"âš ï¸ Lambdaä¸æ”¶ç›Šç›¸å…³æ€§æ¥è¿‘é›¶({avg_rank_ic:.4f})")
        except Exception as e:
            logger.warning(f"âš ï¸ ç›¸å…³æ€§éªŒè¯å¤±è´¥: {e}")

        # ğŸ”§ ä¿å­˜Lambda OOFçš„ç»Ÿè®¡ä¿¡æ¯ï¼Œç”¨äºé¢„æµ‹æ—¶æ ‡å‡†åŒ–
        self.rank_quantile_map = {
            'method': 'simple_rank_pct',
            'lambda_mean': float(s_rank_clean.mean()),
            'lambda_std': float(s_rank_clean.std()),
            'lambda_min': float(s_rank_clean.min()),
            'lambda_max': float(s_rank_clean.max()),
            'rank_ic': float(avg_rank_ic)
        }

        logger.info(f"âœ… æ’åºå¤´æ ¡å‡†å®Œæˆï¼ˆç®€åŒ–ç‰ˆï¼‰")
        logger.info(f"   Lambdaç»Ÿè®¡: mean={self.rank_quantile_map['lambda_mean']:.4f}, "
                   f"std={self.rank_quantile_map['lambda_std']:.4f}")
        logger.info(f"   LambdaèŒƒå›´: [{self.rank_quantile_map['lambda_min']:.4f}, "
                   f"{self.rank_quantile_map['lambda_max']:.4f}]")
        logger.info(f"   å¹³å‡RankIC: {avg_rank_ic:.4f} (æ¨ªæˆªé¢ç›¸å…³æ€§)")

    def _apply_calibration(self, y_pred_reg: np.ndarray,
                          s_rank: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        åº”ç”¨æ ¡å‡†ï¼šIsotonicï¼ˆå›å½’ï¼‰ + Rank Percentileï¼ˆæ’åºï¼‰

        ä¿®å¤ï¼šç®€åŒ–æ’åºå¤´æ ¡å‡†ï¼Œç›´æ¥ä½¿ç”¨rank percentile

        Args:
            y_pred_reg: å›å½’å¤´åŸå§‹é¢„æµ‹
            s_rank: æ’åºå¤´åŸå§‹åˆ†æ•°

        Returns:
            æ ¡å‡†åçš„ (y_pred_reg_calibrated, s_rank_calibrated)
        """
        # æ ¡å‡†å›å½’å¤´
        if self.isotonic_reg is not None:
            y_pred_reg_cal = self.isotonic_reg.predict(y_pred_reg)
        else:
            y_pred_reg_cal = y_pred_reg

        # ğŸ”§ ç®€åŒ–æ’åºå¤´æ ¡å‡† - ç›´æ¥ä½¿ç”¨rank percentile
        if self.rank_quantile_map is not None and self.rank_quantile_map.get('method') == 'simple_rank_pct':
            # è®¡ç®—rank percentileï¼ˆ0-100ï¼‰
            # argsortä¸¤æ¬¡å¾—åˆ°æ’åï¼Œé™¤ä»¥(n-1)å¾—åˆ°åˆ†ä½æ•°
            # æ³¨æ„ï¼šé»˜è®¤ascending=Trueï¼Œåˆ†æ•°é«˜â†’percentileé«˜
            rank_pct = np.argsort(np.argsort(s_rank)) / max(len(s_rank) - 1, 1)
            s_rank_cal = rank_pct * 100  # è½¬ä¸º0-100èŒƒå›´
        else:
            # å…œåº•ï¼šç›´æ¥ä½¿ç”¨åŸå§‹åˆ†æ•°
            s_rank_cal = s_rank

        return y_pred_reg_cal, s_rank_cal

    def _cross_sectional_zscore(self, values: pd.Series, dates: pd.Series) -> pd.Series:
        """
        æ¨ªæˆªé¢z-scoreæ ‡å‡†åŒ–ï¼ˆæ¯æ—¥å†…æ ‡å‡†åŒ–ï¼‰

        ä¸ºä»€ä¹ˆæ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼Ÿ
        - æ¶ˆé™¤å¸‚åœºæ•´ä½“æ³¢åŠ¨å½±å“
        - ç»Ÿä¸€ä¸åŒæ—¥æœŸçš„é¢„æµ‹åˆ»åº¦
        - ä½¿å›å½’å¤´å’Œæ’åºå¤´åœ¨åŒä¸€æ ‡å°ºä¸Š

        Args:
            values: å¾…æ ‡å‡†åŒ–çš„å€¼
            dates: æ—¥æœŸåºåˆ—

        Returns:
            z-scoreæ ‡å‡†åŒ–åçš„å€¼ï¼ˆé•¿åº¦ä¸è¾“å…¥ç›¸åŒï¼‰
        """
        # Extract raw values and ensure proper alignment
        vals = values.values if hasattr(values, 'values') else values
        dts = dates.values if hasattr(dates, 'values') else dates

        input_len = len(vals)

        # Create DataFrame with explicit integer index
        df_temp = pd.DataFrame({
            'value': vals,
            'date': dts
        }, index=range(input_len))

        # Initialize result array with zeros
        z_scores_array = np.zeros(input_len)

        # Group by date and calculate z-scores
        for date_val, group in df_temp.groupby('date'):
            indices = group.index
            group_values = group['value'].values

            mean_val = np.mean(group_values)
            std_val = np.std(group_values)

            if std_val < 1e-10:
                # If std is too small, set to zero
                z_scores_array[indices] = 0.0
            else:
                # Calculate z-score
                z_scores_array[indices] = (group_values - mean_val) / std_val

        # Return as Series to maintain compatibility
        return pd.Series(z_scores_array, index=range(input_len))

    def _tanh_clip(self, z: np.ndarray, c: float = None) -> np.ndarray:
        """
        Tanhé™å¹…ï¼šé˜²æ­¢æç«¯å€¼ç ´åèåˆ

        áº‘ = tanh(z / c)

        Args:
            z: z-score
            c: é™å¹…å‚æ•°ï¼ˆé»˜è®¤ä½¿ç”¨self.tanh_clip_cï¼‰

        Returns:
            é™å¹…åçš„z-score
        """
        if c is None:
            c = self.tanh_clip_c
        return np.tanh(z / c)

    def tune_weights_on_oof(self, y_pred_reg_oof: pd.Series,
                            s_rank_oof: pd.Series,
                            y_true_oof: pd.Series,
                            dates_oof: pd.Series) -> Tuple[float, float]:
        """
        ä½¿ç”¨OOFæ•°æ®ç½‘æ ¼æœç´¢æœ€ä¼˜æƒé‡Î±, Î²

        ä¼˜åŒ–ç›®æ ‡ï¼šRankICï¼ˆSpearmanç›¸å…³ç³»æ•°ï¼‰
        - ä¸ºä»€ä¹ˆç”¨RankICï¼Ÿæ—¢è€ƒè™‘magnitudeåˆè€ƒè™‘ranking
        - ç®€å•ç¨³å®šï¼Œæ— éœ€å¤æ‚çš„åˆ†å±‚è¯„ä¼°

        Args:
            y_pred_reg_oof: å›å½’å¤´OOFé¢„æµ‹
            s_rank_oof: æ’åºå¤´OOFåˆ†æ•°
            y_true_oof: çœŸå®æ ‡ç­¾
            dates_oof: æ—¥æœŸåºåˆ—

        Returns:
            (best_alpha, best_beta)
        """
        logger.info("ğŸ¯ å¼€å§‹OOFæƒé‡è°ƒå‚...")

        # å…ˆæ ¡å‡†
        valid_mask = (y_pred_reg_oof.notna() &
                     s_rank_oof.notna() &
                     y_true_oof.notna())

        y_pred_reg_val = y_pred_reg_oof[valid_mask].values
        s_rank_val = s_rank_oof[valid_mask].values
        y_true_val = y_true_oof[valid_mask].values
        dates_val = dates_oof[valid_mask]

        # åº”ç”¨æ ¡å‡†
        y_pred_reg_cal, s_rank_cal = self._apply_calibration(y_pred_reg_val, s_rank_val)

        # æ¨ªæˆªé¢z-score - pass as Series to maintain compatibility
        z_reg = self._cross_sectional_zscore(
            pd.Series(y_pred_reg_cal),
            pd.Series(dates_val.values if hasattr(dates_val, 'values') else dates_val)
        ).values
        z_rank = self._cross_sectional_zscore(
            pd.Series(s_rank_cal),
            pd.Series(dates_val.values if hasattr(dates_val, 'values') else dates_val)
        ).values

        # Tanhé™å¹…
        z_reg = self._tanh_clip(z_reg)
        z_rank = self._tanh_clip(z_rank)

        # ç½‘æ ¼æœç´¢
        best_alpha = self.alpha
        best_score = -999
        tuning_results = []

        for alpha_candidate in self.alpha_grid:
            beta_candidate = 1.0 - alpha_candidate

            # èåˆ
            S = alpha_candidate * z_reg + beta_candidate * z_rank

            # è¯„ä¼°RankIC
            try:
                rank_ic, _ = spearmanr(S, y_true_val)
                if np.isnan(rank_ic):
                    rank_ic = 0.0
            except:
                rank_ic = 0.0

            tuning_results.append({
                'alpha': alpha_candidate,
                'beta': beta_candidate,
                'rank_ic': rank_ic
            })

            logger.info(f"   Î±={alpha_candidate:.2f}, Î²={beta_candidate:.2f}: RankIC={rank_ic:.4f}")

            if rank_ic > best_score:
                best_score = rank_ic
                best_alpha = alpha_candidate

        best_beta = 1.0 - best_alpha

        self.tuning_results_ = pd.DataFrame(tuning_results)

        logger.info(f"âœ… æœ€ä¼˜æƒé‡: Î±={best_alpha:.2f}, Î²={best_beta:.2f}, RankIC={best_score:.4f}")

        return best_alpha, best_beta

    def fit(self, ridge_oof_preds: pd.Series,
            lambda_oof_preds: pd.Series,
            y_true: pd.Series,
            dates: pd.Series) -> 'DualHeadLateFusion':
        """
        è®­ç»ƒåŒå¤´èåˆç³»ç»Ÿ

        Args:
            ridge_oof_preds: Ridgeå›å½’å¤´OOFé¢„æµ‹
            lambda_oof_preds: Lambdaæ’åºå¤´OOFé¢„æµ‹
            y_true: çœŸå®æ ‡ç­¾
            dates: æ—¥æœŸåºåˆ—

        Returns:
            self
        """
        logger.info("="*80)
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒåŒå¤´æ™šèåˆç³»ç»Ÿ")
        logger.info("="*80)

        # Step 1: æ ¡å‡†ä¸¤ä¸ªå¤´
        self.calibrate_regression_head(ridge_oof_preds, y_true)
        self.calibrate_ranking_head(lambda_oof_preds, y_true, dates)

        # Step 2: è‡ªåŠ¨è°ƒå‚ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.auto_tune_weights:
            self.best_alpha_, self.best_beta_ = self.tune_weights_on_oof(
                ridge_oof_preds, lambda_oof_preds, y_true, dates
            )
        else:
            self.best_alpha_ = self.alpha
            self.best_beta_ = self.beta
            logger.info(f"ä½¿ç”¨å›ºå®šæƒé‡: Î±={self.best_alpha_:.2f}, Î²={self.best_beta_:.2f}")

        self.fitted_ = True

        logger.info("="*80)
        logger.info("âœ… åŒå¤´æ™šèåˆç³»ç»Ÿè®­ç»ƒå®Œæˆ")
        logger.info(f"   æœ€ç»ˆæƒé‡: Î±={self.best_alpha_:.2f} (å›å½’), Î²={self.best_beta_:.2f} (æ’åº)")
        logger.info("="*80)

        return self

    def predict(self, ridge_preds: pd.Series,
                lambda_preds: pd.Series,
                dates: pd.Series) -> pd.Series:
        """
        ä½¿ç”¨åŒå¤´èåˆç³»ç»Ÿè¿›è¡Œé¢„æµ‹

        Args:
            ridge_preds: Ridgeå›å½’å¤´é¢„æµ‹
            lambda_preds: Lambdaæ’åºå¤´é¢„æµ‹
            dates: æ—¥æœŸåºåˆ—

        Returns:
            èåˆåçš„æœ€ç»ˆåˆ†æ•° S
        """
        if not self.fitted_:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fit()")

        logger.info("ğŸ”® åŒå¤´èåˆé¢„æµ‹å¼€å§‹...")

        # ç§»é™¤NaN
        valid_mask = ridge_preds.notna() & lambda_preds.notna()
        y_reg = ridge_preds[valid_mask].values
        s_rank = lambda_preds[valid_mask].values
        dates_val = dates[valid_mask]

        # Step 1: æ ¡å‡†
        y_reg_cal, s_rank_cal = self._apply_calibration(y_reg, s_rank)

        # Step 2: æ¨ªæˆªé¢z-score - pass as Series to maintain compatibility
        z_reg = self._cross_sectional_zscore(
            pd.Series(y_reg_cal),
            pd.Series(dates_val.values if hasattr(dates_val, 'values') else dates_val)
        ).values
        z_rank = self._cross_sectional_zscore(
            pd.Series(s_rank_cal),
            pd.Series(dates_val.values if hasattr(dates_val, 'values') else dates_val)
        ).values

        # Step 3: Tanhé™å¹…
        z_reg = self._tanh_clip(z_reg)
        z_rank = self._tanh_clip(z_rank)

        # Step 4: å›ºå®šçº¿æ€§åŠ æƒèåˆ
        S = self.best_alpha_ * z_reg + self.best_beta_ * z_rank

        # Verify lengths match before assignment
        n_valid = valid_mask.sum()
        if len(S) != n_valid:
            logger.warning(f"âš ï¸ é•¿åº¦ä¸åŒ¹é…: Sé•¿åº¦={len(S)}, valid_maskè®¡æ•°={n_valid}")
            logger.warning(f"   è¾“å…¥: y_reg={len(y_reg)}, dates_val={len(dates_val)}")
            logger.warning(f"   z-scoreå: z_reg={len(z_reg)}, z_rank={len(z_rank)}")
            # Truncate or pad to match
            if len(S) > n_valid:
                S = S[:n_valid]
            else:
                S_padded = np.full(n_valid, np.nan)
                S_padded[:len(S)] = S
                S = S_padded

        # æ„å»ºå®Œæ•´ç»“æœï¼ˆåŒ…æ‹¬NaNï¼‰
        result = pd.Series(np.nan, index=ridge_preds.index)
        result[valid_mask] = S

        logger.info(f"âœ… èåˆé¢„æµ‹å®Œæˆ")
        logger.info(f"   æœ‰æ•ˆæ ·æœ¬: {len(S)}/{len(ridge_preds)} ({len(S)/len(ridge_preds)*100:.1f}%)")
        logger.info(f"   åˆ†æ•°èŒƒå›´: [{S.min():.4f}, {S.max():.4f}]")

        return result

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'fitted': self.fitted_,
            'best_alpha': float(self.best_alpha_),
            'best_beta': float(self.best_beta_),
            'isotonic_calibration': self.isotonic_reg is not None,
            'rank_quantile_mapping': self.rank_quantile_map is not None,
            'tanh_clip_c': float(self.tanh_clip_c),
            'tuning_results': self.tuning_results_.to_dict() if self.tuning_results_ is not None else None
        }
