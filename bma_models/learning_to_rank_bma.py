#!/usr/bin/env python3
"""
Learning-to-Rankå¢å¼ºBMAæ¨¡å—
å®ç°æ’åºä¼˜åŒ–ã€ä¸ç¡®å®šæ€§æ„ŸçŸ¥ã€Mixture-of-Expertsç­‰é«˜çº§æŠ€æœ¯
"""

import numpy as np
import pandas as pd
import warnings
from typing import Dict, List, Optional, Tuple, Any, Union
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.linear_model import LinearRegression, QuantileRegressor
from sklearn.ensemble import RandomForestRegressor
from scipy.stats import spearmanr, kendalltau, entropy
from scipy.optimize import minimize
import logging

# å°è¯•å¯¼å…¥é«˜çº§æ¨¡å‹
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

# CatBoost removed due to compatibility issues
CATBOOST_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LearningToRankBMA:
    """åŸºäºLearning-to-Rankçš„å¢å¼ºBMAç³»ç»Ÿ"""
    
    def __init__(self, 
                 ranking_objective: str = "rank:pairwise",
                 uncertainty_method: str = "ensemble",
                 temperature: float = 1.2,
                 enable_regime_detection: bool = True):
        """
        åˆå§‹åŒ–Learning-to-Rank BMA
        
        Args:
            ranking_objective: æ’åºç›®æ ‡å‡½æ•°
            uncertainty_method: ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•
            temperature: BMAæ¸©åº¦ç³»æ•°
            enable_regime_detection: æ˜¯å¦å¯ç”¨çŠ¶æ€æ£€æµ‹
        """
        self.ranking_objective = ranking_objective
        self.uncertainty_method = uncertainty_method
        self.temperature = temperature
        self.enable_regime_detection = enable_regime_detection
        
        self.models = {}
        self.model_uncertainties = {}
        self.regime_weights = {}
        self.expert_gates = {}
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'train_metrics': {},
            'oof_metrics': {},
            'ranking_metrics': {},
            'uncertainty_metrics': {}
        }
        
        logger.info(f"LearningToRankBMAåˆå§‹åŒ–å®Œæˆï¼Œæ’åºç›®æ ‡: {ranking_objective}")
    
    def _create_robust_time_groups(self, dates: pd.Series, min_gap_days: int = 1) -> np.ndarray:
        """
        åˆ›å»ºä¸¥æ ¼çš„æ—¶é—´åˆ†ç»„ï¼Œç¡®ä¿è®­ç»ƒæµ‹è¯•é›†ä¹‹é—´æœ‰è¶³å¤Ÿçš„æ—¶é—´éš”ç¦»
        
        Args:
            dates: æ—¥æœŸåºåˆ—
            min_gap_days: æœ€å°é—´éš”å¤©æ•°
            
        Returns:
            ç»„IDæ•°ç»„ï¼Œ-1è¡¨ç¤ºbufferåŒºåŸŸï¼ˆä¸ç”¨äºè®­ç»ƒæˆ–æµ‹è¯•ï¼‰
        """
        unique_dates = sorted(dates.unique())
        if len(unique_dates) < 10:
            logger.warning(f"å”¯ä¸€æ—¥æœŸæ•°é‡è¿‡å°‘({len(unique_dates)})ï¼Œå¯èƒ½å½±å“CVè´¨é‡")
        
        # å°†æ—¥æœŸåˆ†æˆæ—¶é—´å—ï¼ˆç¡®ä¿ä¸¥æ ¼é¡ºåºï¼‰
        n_blocks = min(10, len(unique_dates) // 3)  # æœ€å¤š10ä¸ªå—ï¼Œæ¯å—è‡³å°‘3å¤©
        if n_blocks < 3:
            n_blocks = 3
            
        # åˆ›å»ºæ—¶é—´å—
        date_to_block = {}
        dates_per_block = len(unique_dates) // n_blocks
        
        for i, date in enumerate(unique_dates):
            block_id = min(i // dates_per_block, n_blocks - 1)
            date_to_block[date] = block_id
        
        # åˆ›å»ºbufferåŒºåŸŸé˜²æ­¢è¾¹ç•Œæ³„éœ²
        buffered_mapping = {}
        for date, block_id in date_to_block.items():
            if block_id > 0:
                # æ£€æŸ¥æ˜¯å¦åœ¨å—è¾¹ç•Œçš„bufferåŒºåŸŸå†…
                block_start_idx = block_id * dates_per_block
                if abs(unique_dates.index(date) - block_start_idx) < min_gap_days:
                    buffered_mapping[date] = -1  # æ ‡è®°ä¸ºbuffer
                    continue
            buffered_mapping[date] = block_id
        
        # æ˜ å°„åˆ°åŸå§‹æ—¥æœŸåºåˆ—
        group_ids = np.array([buffered_mapping[date] for date in dates])
        
        # è¿‡æ»¤æ‰bufferåŒºåŸŸ
        valid_mask = group_ids >= 0
        n_buffer = np.sum(~valid_mask)
        n_valid = np.sum(valid_mask)
        
        logger.info(f"æ—¶é—´åˆ†ç»„åˆ›å»ºå®Œæˆ: {n_blocks}ä¸ªå—, {n_valid}ä¸ªæœ‰æ•ˆæ ·æœ¬, {n_buffer}ä¸ªbufferæ ·æœ¬")
        
        return group_ids
    
    def create_ranking_dataset(self, X: pd.DataFrame, y: pd.Series, 
                              dates: pd.Series, group_col: str = 'date') -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        åˆ›å»ºæ’åºæ•°æ®é›†
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: ç›®æ ‡å˜é‡
            dates: æ—¥æœŸåºåˆ—
            group_col: åˆ†ç»„åˆ—
            
        Returns:
            (ç‰¹å¾çŸ©é˜µ, ç›®æ ‡, åˆ†ç»„ID)
        """
        logger.info("åˆ›å»ºæ’åºæ•°æ®é›†")
        
        # ç¡®ä¿æ•°æ®å¯¹é½
        logger.info(f"LTRæ•°æ®å¯¹é½æ£€æŸ¥: X={len(X)}, y={len(y)}, dates={len(dates)}")
        
        if len(X) != len(y) or len(X) != len(dates):
            logger.warning(f"LTRæ•°æ®é•¿åº¦ä¸ä¸€è‡´: X={len(X)}, y={len(y)}, dates={len(dates)}")
            
            # è‡ªåŠ¨å¯¹é½åˆ°æœ€å°é•¿åº¦
            min_len = min(len(X), len(y), len(dates))
            if min_len == 0:
                raise ValueError("æ‰€æœ‰æ•°æ®é•¿åº¦ä¸º0ï¼Œæ— æ³•è®­ç»ƒLTRæ¨¡å‹")
            
            logger.info(f"è‡ªåŠ¨å¯¹é½åˆ°æœ€å°é•¿åº¦: {min_len}")
            
            # å¯¹é½æ•°æ®
            if isinstance(X, pd.DataFrame):
                X = X.iloc[:min_len].copy()
            else:
                X = X[:min_len]
                
            if isinstance(y, pd.Series):
                y = y.iloc[:min_len].copy()  
            else:
                y = y[:min_len]
                
            if isinstance(dates, pd.Series):
                dates = dates.iloc[:min_len].copy()
            else:
                dates = dates[:min_len]
                
            logger.info(f"LTRæ•°æ®å¯¹é½å®Œæˆ: X={len(X)}, y={len(y)}, dates={len(dates)}")
        
        # ç»„è£…å¹¶æ¸…æ´—
        df_temp = pd.DataFrame({'y': y, 'date': dates})
        df_temp = pd.concat([df_temp, X], axis=1)
        df_temp = df_temp.dropna()
        # æŒ‰æ—¥æœŸæ’åºï¼Œç¡®ä¿ç»„å†…æ ·æœ¬è¿ç»­ï¼Œä¾¿äºCatBoost/LightGBMåˆ†ç»„
        df_temp = df_temp.sort_values('date').reset_index(drop=True)
        
        if len(df_temp) == 0:
            raise ValueError("æ•°æ®æ¸…æ´—åä¸ºç©º")
        
        # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨åŒæ—¥æ¨ªæˆªé¢åˆ†ç»„ (æŒ‰æ—¥æœŸåˆ†ç»„ï¼Œé¿å…è·¨æœŸé…å¯¹)
        logger.info("ğŸ”¥ ä½¿ç”¨åŒæ—¥æ¨ªæˆªé¢åˆ†ç»„é¿å…è·¨æœŸé…å¯¹")
        unique_dates = sorted(df_temp['date'].unique())
        date_to_group = {date: i for i, date in enumerate(unique_dates)}
        group_ids = np.array([date_to_group[date] for date in df_temp['date']])
        
        # éªŒè¯åˆ†ç»„ï¼šç¡®ä¿åŒä¸€ç»„å†…çš„æ ·æœ¬éƒ½æ˜¯åŒä¸€å¤©
        sample_groups = {}
        for i, (date, group_id) in enumerate(zip(df_temp['date'], group_ids)):
            if group_id not in sample_groups:
                sample_groups[group_id] = {'dates': set(), 'count': 0}
            sample_groups[group_id]['dates'].add(date)
            sample_groups[group_id]['count'] += 1
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è·¨æ—¥åˆ†ç»„
        cross_day_groups = []
        for group_id, info in sample_groups.items():
            if len(info['dates']) > 1:
                cross_day_groups.append(group_id)
        
        if cross_day_groups:
            logger.error(f"ğŸš¨ å‘ç°è·¨æ—¥åˆ†ç»„: {len(cross_day_groups)}ä¸ªç»„ï¼Œè¿™ä¼šå¯¼è‡´æœªæ¥ä¿¡æ¯æ³„éœ²ï¼")
            for group_id in cross_day_groups[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                logger.error(f"  ç»„{group_id}: åŒ…å«æ—¥æœŸ{sample_groups[group_id]['dates']}")
        else:
            logger.info("âœ… LTRåˆ†ç»„éªŒè¯é€šè¿‡ï¼šæ‰€æœ‰ç»„éƒ½æ˜¯åŒæ—¥æ¨ªæˆªé¢")
        
        # æŠ½æ ·æ‰“å°å½“å¤©group/pairä¿¡æ¯
        logger.info("ğŸ” LTRåˆ†ç»„è¯¦æƒ…æŠ½æ ·:")
        sample_dates = unique_dates[:3] + unique_dates[-3:]  # å‰3ä¸ªå’Œå3ä¸ªæ—¥æœŸ
        for date in sample_dates:
            if date in date_to_group:
                group_id = date_to_group[date]
                count = sample_groups[group_id]['count']
                logger.info(f"  æ—¥æœŸ{date}: ç»„ID={group_id}, æ¨ªæˆªé¢è‚¡ç¥¨æ•°={count}")
        
        logger.info(f"LTRæ•°æ®é›†åˆ›å»ºå®Œæˆ: {len(unique_dates)}ä¸ªæ¨ªæˆªé¢ç»„, {len(df_temp)}ä¸ªæ ·æœ¬")
        
        # æå–ç‰¹å¾å’Œç›®æ ‡
        feature_cols = [col for col in df_temp.columns if col not in ['y', 'date']]
        X_clean = df_temp[feature_cols].values
        y_clean = df_temp['y'].values
        
        logger.info(f"æ’åºæ•°æ®é›†åˆ›å»ºå®Œæˆ: {X_clean.shape[0]}æ ·æœ¬, {len(np.unique(group_ids))}ç»„")
        
        return X_clean, y_clean, group_ids

    def _create_smart_labels(self, y: np.ndarray, group_ids: np.ndarray, 
                            mode: str = 'soft', n_bins: int = 5, temperature: float = 1.0) -> Dict[str, np.ndarray]:
        """
        æ™ºèƒ½æ ‡ç­¾å¤„ç†ï¼šæ”¯æŒè¿ç»­ã€ç¦»æ•£å’Œè½¯æ ‡ç­¾
        
        Args:
            y: åŸå§‹è¿ç»­æ ‡ç­¾
            group_ids: åˆ†ç»„ID
            mode: 'continuous' | 'discrete' | 'soft' | 'multi'
            n_bins: ç¦»æ•£åŒ–åˆ†ç®±æ•°ï¼ˆä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨ï¼‰
            temperature: è½¯æ ‡ç­¾æ¸©åº¦å‚æ•°
            
        Returns:
            åŒ…å«ä¸åŒç±»å‹æ ‡ç­¾çš„å­—å…¸
        """
        results = {
            'continuous': y.copy(),  # å§‹ç»ˆä¿ç•™åŸå§‹è¿ç»­æ ‡ç­¾
            'group_ids': group_ids
        }
        
        y_series = pd.Series(y)
        g_series = pd.Series(group_ids)
        
        # 1. è¿ç»­æ ‡ç­¾æ ‡å‡†åŒ–ï¼ˆç»„å†…ï¼‰
        standardized_labels = np.zeros_like(y)
        for g in np.unique(group_ids):
            mask = (g_series == g)
            if mask.sum() <= 1:
                continue
            y_group = y_series[mask]
            # ä½¿ç”¨Z-scoreæ ‡å‡†åŒ–ï¼Œä¿ç•™ç›¸å¯¹æ’åº
            standardized = (y_group - y_group.mean()) / (y_group.std() + 1e-8)
            standardized_labels[mask] = standardized.values
        
        results['standardized'] = standardized_labels
        
        # 2. ç¦»æ•£æ ‡ç­¾ï¼ˆä»…åœ¨éœ€è¦æ—¶åˆ›å»ºï¼Œå‡å°‘åˆ†ç®±æ•°ï¼‰
        if mode in ['discrete', 'multi']:
            discrete_labels = np.zeros_like(y, dtype=int)
            for g in np.unique(group_ids):
                mask = (g_series == g)
                if mask.sum() <= 1:
                    continue
                
                y_group = y_series[mask]
                if y_group.nunique() <= 1:
                    discrete_labels[mask] = 0
                else:
                    try:
                        # ä½¿ç”¨æ›´å°‘çš„åˆ†ç®±æ•°å‡å°‘ä¿¡æ¯æŸå¤±
                        effective_bins = min(n_bins, max(3, y_group.nunique() // 2))
                        discrete_labels[mask] = pd.qcut(
                            y_group, q=effective_bins, labels=False, duplicates='drop'
                        ).fillna(0).astype(int)
                    except Exception:
                        # å›é€€åˆ°ç®€å•äºŒåˆ†ç±»
                        median_val = y_group.median()
                        discrete_labels[mask] = (y_group > median_val).astype(int)
            
            results['discrete'] = discrete_labels
        
        # 3. è½¯æ ‡ç­¾ï¼ˆæ¦‚ç‡åˆ†å¸ƒï¼‰
        if mode in ['soft', 'multi']:
            soft_labels = np.zeros((len(y), n_bins))
            for g in np.unique(group_ids):
                mask = (g_series == g)
                if mask.sum() <= 1:
                    continue
                
                y_group = y_series[mask]
                # ä½¿ç”¨æ¸©åº¦å‚æ•°æ§åˆ¶è½¯åŒ–ç¨‹åº¦
                if y_group.std() > 0:
                    y_scaled = (y_group - y_group.min()) / (y_group.max() - y_group.min() + 1e-8)
                    
                    # åˆ›å»ºè½¯æ ‡ç­¾åˆ†å¸ƒ
                    for i, val in enumerate(y_scaled):
                        # åŸºäºå€¼è®¡ç®—åœ¨å„åˆ†ç®±çš„æ¦‚ç‡
                        bin_centers = np.linspace(0, 1, n_bins)
                        distances = np.abs(bin_centers - val)
                        probabilities = np.exp(-distances / temperature)
                        probabilities = probabilities / probabilities.sum()
                        
                        mask_indices = np.where(mask)[0]
                        if i < len(mask_indices):
                            soft_labels[mask_indices[i]] = probabilities
            
            results['soft'] = soft_labels
        
        logger.info(f"æ™ºèƒ½æ ‡ç­¾åˆ›å»ºå®Œæˆï¼Œæ¨¡å¼: {mode}, åŒ…å«: {list(results.keys())}")
        return results

    def _discretize_labels_by_group(self, y: np.ndarray, group_ids: np.ndarray, n_bins: int = 10) -> np.ndarray:
        """å‘åå…¼å®¹çš„ç¦»æ•£åŒ–æ–¹æ³•ï¼ˆå»ºè®®ä½¿ç”¨_create_smart_labelsï¼‰"""
        labels_dict = self._create_smart_labels(y, group_ids, mode='discrete', n_bins=n_bins)
        return labels_dict.get('discrete', np.zeros_like(y, dtype=int))
    

    def train_ranking_models(self, X: pd.DataFrame, y: pd.Series, dates: pd.Series,
                           cv_folds: int = 5, optimize_hyperparams: bool = True, 
                           validation_config=None, sample_weights=None) -> Dict[str, Any]:
        """
        è®­ç»ƒå¤šä¸ªæ’åºæ¨¡å‹
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: ç›®æ ‡å˜é‡
            dates: æ—¥æœŸåºåˆ—
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            optimize_hyperparams: æ˜¯å¦ä¼˜åŒ–è¶…å‚æ•°
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        logger.info("å¼€å§‹è®­ç»ƒæ’åºæ¨¡å‹")
        
        # åˆ›å»ºæ’åºæ•°æ®é›†
        X_rank, y_rank, group_ids = self.create_ranking_dataset(X, y, dates)
        # ä¸ºLightGBMå‡†å¤‡ç¦»æ•£æ ‡ç­¾
        y_rank_discrete = self._discretize_labels_by_group(y_rank, group_ids, n_bins=5)
        
        # ä½¿ç”¨V6æä¾›çš„æ ·æœ¬æƒé‡ï¼Œé¿å…é‡å¤æƒé‡è®¡ç®—
        if sample_weights is not None:
            logger.info("ä½¿ç”¨V6æä¾›çš„ç»Ÿä¸€æ ·æœ¬æƒé‡ï¼Œé¿å…é‡å¤æƒé‡è®¡ç®—")
            # å¯¹é½æƒé‡åˆ°æ’åºæ•°æ®é›†
            if hasattr(sample_weights, 'loc'):
                aligned_weights = sample_weights.loc[X_rank.index] if hasattr(X_rank, 'index') else None
            else:
                aligned_weights = sample_weights
        else:
            logger.warning("æœªæä¾›V6æ ·æœ¬æƒé‡ï¼Œæ’åºæ¨¡å‹å°†ä½¿ç”¨å‡ç­‰æƒé‡")
            aligned_weights = None
        # ä½¿ç”¨ç»Ÿä¸€çš„V6 validation_configï¼Œé¿å…CVé…ç½®æ¼‚ç§»
        if validation_config is not None:
            try:
                # ä½¿ç”¨V6æä¾›çš„éªŒè¯é…ç½®
                from .enhanced_temporal_validation import EnhancedPurgedTimeSeriesSplit
                
                cv = EnhancedPurgedTimeSeriesSplit(validation_config)
                unique_groups = np.unique(group_ids)
                
                # ç¡®ä¿group_idsæ˜¯pandas Series
                if isinstance(group_ids, np.ndarray):
                    group_ids_series = pd.Series(group_ids)
                else:
                    group_ids_series = group_ids
                cv_splits = list(cv.split(X_rank, y_rank, group_ids_series))
                
                logger.info(f"ä½¿ç”¨V6ç»Ÿä¸€éªŒè¯é…ç½®ï¼Œ{len(cv_splits)}ä¸ªfoldï¼Œgap={validation_config.gap}ï¼Œembargo={validation_config.embargo}")
                
            except Exception as e:
                logger.warning(f"V6éªŒè¯é…ç½®ä½¿ç”¨å¤±è´¥: {e}, ä½¿ç”¨å›é€€ç­–ç•¥")
                validation_config = None
        
        # å›é€€ç­–ç•¥ï¼šä½¿ç”¨ä¼ ç»ŸCVï¼ˆä»…å½“V6é…ç½®ä¸å¯ç”¨æ—¶ï¼‰
        if validation_config is None:
            logger.warning("ä½¿ç”¨ä¼ ç»ŸCVå›é€€ç­–ç•¥ï¼Œå¯èƒ½å­˜åœ¨é…ç½®æ¼‚ç§»é£é™©")
            tscv = TimeSeriesSplit(n_splits=cv_folds)
            unique_groups = np.unique(group_ids)
            cv_splits = list(tscv.split(unique_groups))
            
        # ç¡®ä¿cv_splitså¯ç”¨
        if not cv_splits:
            logger.error("CV splitsä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åˆ†å‰²")
            tscv = TimeSeriesSplit(n_splits=cv_folds)
            unique_groups = np.unique(group_ids)
            cv_splits = list(tscv.split(unique_groups))
        
        models_results = {}
        
        # 1. XGBoostæ’åºæ¨¡å‹
        if XGBOOST_AVAILABLE:
            logger.info("è®­ç»ƒXGBoostæ’åºæ¨¡å‹")
            xgb_results = self._train_xgboost_ranker(
                X_rank, y_rank, group_ids, cv_splits, unique_groups, optimize_hyperparams
            )
            models_results['xgboost_ranker'] = xgb_results
        
        # 2. LightGBMæ’åºæ¨¡å‹
        if LIGHTGBM_AVAILABLE:
            logger.info("è®­ç»ƒLightGBMæ’åºæ¨¡å‹")
            lgb_results = self._train_lightgbm_ranker(
                X_rank, y_rank_discrete, group_ids, cv_splits, unique_groups, optimize_hyperparams
            )
            models_results['lightgbm_ranker'] = lgb_results
        
        # CatBoost removed due to compatibility issues
        
        # 4. åˆ†ä½æ•°å›å½’æ¨¡å‹
        logger.info("è®­ç»ƒåˆ†ä½æ•°å›å½’æ¨¡å‹")
        quantile_results = self._train_quantile_models(
            X_rank, y_rank, group_ids, cv_splits, unique_groups
        )
        models_results['quantile_models'] = quantile_results
        
        # 5. ä¼ ç»Ÿå›å½’æ¨¡å‹ï¼ˆä½œä¸ºåŸºå‡†ï¼‰
        logger.info("è®­ç»ƒä¼ ç»Ÿå›å½’æ¨¡å‹")
        baseline_results = self._train_baseline_models(
            X_rank, y_rank, group_ids, cv_splits, unique_groups
        )
        models_results['baseline_models'] = baseline_results
        
        self.models = models_results
        
        # è®¡ç®—æ¨¡å‹æ€§èƒ½å’Œä¸ç¡®å®šæ€§
        self._evaluate_model_performance(X_rank, y_rank, group_ids)
        
        logger.info(f"æ’åºæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå…±{len(models_results)}ç±»æ¨¡å‹")
        
        return models_results
    
    def _train_xgboost_ranker(self, X: np.ndarray, y: np.ndarray, group_ids: np.ndarray,
                             cv_splits, unique_groups: np.ndarray,
                             optimize_hyperparams: bool) -> Dict[str, Any]:
        """è®­ç»ƒXGBoostæ’åºæ¨¡å‹"""
        if not XGBOOST_AVAILABLE:
            return {}
        
        # è®¡ç®—æ¯ç»„å¤§å°
        group_sizes = [np.sum(group_ids == g) for g in unique_groups]
        
        models = []
        oof_predictions = np.full(len(X), np.nan)
        oof_uncertainties = np.full(len(X), np.nan)
        
        # å…¼å®¹ä¸¤ç§splitå½¢å¼ï¼šåŸºäºç»„ç´¢å¼•æˆ–ç›´æ¥æ ·æœ¬ç´¢å¼•
        for split in cv_splits:
            # ä¿®å¤list objecté”™è¯¯ï¼šå®‰å…¨æ£€æŸ¥splitæ ¼å¼
            try:
                split_0_is_array = hasattr(split[0], 'shape') and len(split[0].shape) == 1
                split_0_has_int_elements = len(split[0]) > 0 and isinstance(split[0][0], (np.integer, int))
                
                if split_0_has_int_elements and split_0_is_array:
                    # æ ·æœ¬ç´¢å¼•ï¼ˆnumpyæ•°ç»„ï¼‰
                    train_mask = np.zeros(len(X), dtype=bool)
                    train_mask[split[0]] = True
                    test_mask = np.zeros(len(X), dtype=bool)
                    test_mask[split[1]] = True
                    train_groups = np.unique(group_ids[train_mask])
                    test_groups = np.unique(group_ids[test_mask])
                elif split_0_has_int_elements and isinstance(split[0], (list, tuple)):
                    # æ ·æœ¬ç´¢å¼•ï¼ˆåˆ—è¡¨æ ¼å¼ï¼‰
                    train_indices = np.array(split[0])
                    test_indices = np.array(split[1])
                    train_mask = np.zeros(len(X), dtype=bool)
                    train_mask[train_indices] = True
                    test_mask = np.zeros(len(X), dtype=bool)
                    test_mask[test_indices] = True
                    train_groups = np.unique(group_ids[train_mask])
                    test_groups = np.unique(group_ids[test_mask])
                else:
                    # ç»„ç´¢å¼• - æ·»åŠ è¾¹ç•Œæ£€æŸ¥é˜²æ­¢è¶Šç•Œ
                    train_groups_idx, test_groups_idx = split
                    # ä¿®å¤ç´¢å¼•è¶Šç•Œé—®é¢˜ï¼šç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    train_groups_idx = np.array(train_groups_idx)
                    test_groups_idx = np.array(test_groups_idx)
                    
                    # è¿‡æ»¤è¶…å‡ºèŒƒå›´çš„ç´¢å¼•
                    valid_train_idx = train_groups_idx[train_groups_idx < len(unique_groups)]
                    valid_test_idx = test_groups_idx[test_groups_idx < len(unique_groups)]
                    
                    train_groups = unique_groups[valid_train_idx]
                    test_groups = unique_groups[valid_test_idx]
                    
                    if len(valid_train_idx) < len(train_groups_idx) or len(valid_test_idx) < len(test_groups_idx):
                        logger.warning(f"è¿‡æ»¤äº†è¶…å‡ºèŒƒå›´çš„ç´¢å¼•: è®­ç»ƒ{len(train_groups_idx)-len(valid_train_idx)}, æµ‹è¯•{len(test_groups_idx)-len(valid_test_idx)}")
                        
            except Exception as e:
                logger.warning(f"splitæ ¼å¼è§£æå¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤ç»„ç´¢å¼•å¤„ç†")
                # é»˜è®¤æŒ‰ç»„ç´¢å¼•å¤„ç† - æ·»åŠ è¾¹ç•Œæ£€æŸ¥
                try:
                    train_groups_idx, test_groups_idx = split
                    train_groups_idx = np.array(train_groups_idx)
                    test_groups_idx = np.array(test_groups_idx)
                    
                    # è¿‡æ»¤è¶…å‡ºèŒƒå›´çš„ç´¢å¼•
                    valid_train_idx = train_groups_idx[train_groups_idx < len(unique_groups)]
                    valid_test_idx = test_groups_idx[test_groups_idx < len(unique_groups)]
                    
                    train_groups = unique_groups[valid_train_idx]
                    test_groups = unique_groups[valid_test_idx]
                except Exception as e2:
                    logger.error(f"ç»„ç´¢å¼•å¤„ç†å¤±è´¥: {e2}, è·³è¿‡æ­¤split")
                    continue
            
            # è·å–è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
            train_mask = np.isin(group_ids, train_groups)
            test_mask = np.isin(group_ids, test_groups)
            
            if train_mask.sum() == 0 or test_mask.sum() == 0:
                continue
            
            X_train, y_train = X[train_mask], y[train_mask]
            X_test = X[test_mask]
            
            # è®¡ç®—è®­ç»ƒé›†çš„ç»„å¤§å°
            train_group_ids = group_ids[train_mask]
            train_group_sizes = [np.sum(train_group_ids == g) for g in train_groups]
            
            # XGBoostæ’åºå‚æ•°
            if optimize_hyperparams:
                params = {
                    'objective': self.ranking_objective,
                    'eval_metric': 'ndcg@10',
                    'eta': 0.1,
                    'max_depth': 6,
                    'min_child_weight': 1,
                    'subsample': 0.8,
                    'colsample_bytree': 0.8,
                    'lambda': 1.0,
                    'alpha': 0.0,
                    'silent': 1,
                    'nthread': -1
                }
            else:
                params = {
                    'objective': self.ranking_objective,
                    'eval_metric': 'ndcg@10',
                    'eta': 0.1,
                    'max_depth': 6,
                    'silent': 1
                }
            
            # åˆ›å»ºDMatrix
            dtrain = xgb.DMatrix(X_train, label=y_train)
            dtrain.set_group(train_group_sizes)
            
            dtest = xgb.DMatrix(X_test)
            
            # è®­ç»ƒæ¨¡å‹
            try:
                model = xgb.train(
                    params=params,
                    dtrain=dtrain,
                    num_boost_round=100,
                    verbose_eval=False
                )
                
                # é¢„æµ‹
                test_pred = model.predict(dtest)
                
                # Bootstrapä¸ç¡®å®šæ€§ä¼°è®¡
                n_bootstrap = 10
                bootstrap_preds = []
                
                for _ in range(n_bootstrap):
                    # é‡é‡‡æ ·è®­ç»ƒæ•°æ®
                    n_samples = len(X_train)
                    bootstrap_idx = np.random.choice(n_samples, n_samples, replace=True)
                    X_bootstrap = X_train[bootstrap_idx]
                    y_bootstrap = y_train[bootstrap_idx]
                    
                    dtrain_bootstrap = xgb.DMatrix(X_bootstrap, label=y_bootstrap)
                    # è¿‘ä¼¼ç»„å¤§å°ï¼ˆç®€åŒ–ï¼‰
                    approx_group_sizes = [len(X_bootstrap) // len(train_groups)] * len(train_groups)
                    dtrain_bootstrap.set_group(approx_group_sizes)
                    
                    try:
                        bootstrap_model = xgb.train(
                            params=params,
                            dtrain=dtrain_bootstrap,
                            num_boost_round=50,
                            verbose_eval=False
                        )
                        bootstrap_pred = bootstrap_model.predict(dtest)
                        bootstrap_preds.append(bootstrap_pred)
                    except:
                        continue
                
                if bootstrap_preds:
                    test_uncertainty = np.std(bootstrap_preds, axis=0)
                else:
                    test_uncertainty = np.ones(len(test_pred)) * 0.1
                
                # ä¿å­˜OOFé¢„æµ‹
                oof_predictions[test_mask] = test_pred
                oof_uncertainties[test_mask] = test_uncertainty
                
                models.append(model)
                
            except Exception as e:
                logger.warning(f"XGBoostè®­ç»ƒå¤±è´¥: {e}")
                continue
        
        return {
            'models': models,
            'oof_predictions': oof_predictions,
            'oof_uncertainties': oof_uncertainties,
            'model_type': 'xgboost_ranker'
        }
    
    def _train_lightgbm_ranker(self, X: np.ndarray, y: np.ndarray, group_ids: np.ndarray,
                              cv_splits, unique_groups: np.ndarray,
                              optimize_hyperparams: bool) -> Dict[str, Any]:
        """è®­ç»ƒLightGBMæ’åºæ¨¡å‹"""
        if not LIGHTGBM_AVAILABLE:
            return {}
        
        models = []
        oof_predictions = np.full(len(X), np.nan)
        oof_uncertainties = np.full(len(X), np.nan)
        
        for split in cv_splits:
            # ä¿®å¤list objecté”™è¯¯ï¼šå®‰å…¨æ£€æŸ¥splitæ ¼å¼ï¼ˆLightGBMéƒ¨åˆ†ï¼‰
            try:
                split_0_is_array = hasattr(split[0], 'shape') and len(split[0].shape) == 1
                split_0_has_int_elements = len(split[0]) > 0 and isinstance(split[0][0], (np.integer, int))
                
                if split_0_has_int_elements and split_0_is_array:
                    # æ ·æœ¬ç´¢å¼•ï¼ˆnumpyæ•°ç»„ï¼‰
                    train_mask = np.zeros(len(X), dtype=bool)
                    train_mask[split[0]] = True
                    test_mask = np.zeros(len(X), dtype=bool)
                    test_mask[split[1]] = True
                    train_groups = np.unique(group_ids[train_mask])
                    test_groups = np.unique(group_ids[test_mask])
                elif split_0_has_int_elements and isinstance(split[0], (list, tuple)):
                    # æ ·æœ¬ç´¢å¼•ï¼ˆåˆ—è¡¨æ ¼å¼ï¼‰
                    train_indices = np.array(split[0])
                    test_indices = np.array(split[1])
                    train_mask = np.zeros(len(X), dtype=bool)
                    train_mask[train_indices] = True
                    test_mask = np.zeros(len(X), dtype=bool)
                    test_mask[test_indices] = True
                    train_groups = np.unique(group_ids[train_mask])
                    test_groups = np.unique(group_ids[test_mask])
                else:
                    # ç»„ç´¢å¼• - æ·»åŠ è¾¹ç•Œæ£€æŸ¥é˜²æ­¢è¶Šç•Œ (LightGBM)
                    train_groups_idx, test_groups_idx = split
                    # ä¿®å¤ç´¢å¼•è¶Šç•Œé—®é¢˜ï¼šç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    train_groups_idx = np.array(train_groups_idx)
                    test_groups_idx = np.array(test_groups_idx)
                    
                    # è¿‡æ»¤è¶…å‡ºèŒƒå›´çš„ç´¢å¼•
                    valid_train_idx = train_groups_idx[train_groups_idx < len(unique_groups)]
                    valid_test_idx = test_groups_idx[test_groups_idx < len(unique_groups)]
                    
                    train_groups = unique_groups[valid_train_idx]
                    test_groups = unique_groups[valid_test_idx]
                    
                    if len(valid_train_idx) < len(train_groups_idx) or len(valid_test_idx) < len(test_groups_idx):
                        logger.warning(f"LightGBMè¿‡æ»¤äº†è¶…å‡ºèŒƒå›´çš„ç´¢å¼•: è®­ç»ƒ{len(train_groups_idx)-len(valid_train_idx)}, æµ‹è¯•{len(test_groups_idx)-len(valid_test_idx)}")
                        
            except Exception as e:
                logger.warning(f"LightGBM splitæ ¼å¼è§£æå¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤ç»„ç´¢å¼•å¤„ç†")
                # é»˜è®¤æŒ‰ç»„ç´¢å¼•å¤„ç† - æ·»åŠ è¾¹ç•Œæ£€æŸ¥ 
                try:
                    train_groups_idx, test_groups_idx = split
                    train_groups_idx = np.array(train_groups_idx)
                    test_groups_idx = np.array(test_groups_idx)
                    
                    # è¿‡æ»¤è¶…å‡ºèŒƒå›´çš„ç´¢å¼•
                    valid_train_idx = train_groups_idx[train_groups_idx < len(unique_groups)]
                    valid_test_idx = test_groups_idx[test_groups_idx < len(unique_groups)]
                    
                    train_groups = unique_groups[valid_train_idx]
                    test_groups = unique_groups[valid_test_idx]
                except Exception as e2:
                    logger.error(f"LightGBMç»„ç´¢å¼•å¤„ç†å¤±è´¥: {e2}, è·³è¿‡æ­¤split")
                    continue
            
            train_mask = np.isin(group_ids, train_groups)
            test_mask = np.isin(group_ids, test_groups)
            
            if train_mask.sum() == 0 or test_mask.sum() == 0:
                continue
            
            X_train, y_train = X[train_mask], y[train_mask]
            X_test = X[test_mask]
            
            # è®¡ç®—ç»„å¤§å°ï¼Œå¹¶å¤„ç†LightGBMçš„10000è¡Œé™åˆ¶
            train_group_ids = group_ids[train_mask]
            original_group_sizes = [np.sum(train_group_ids == g) for g in train_groups]
            
            # å¦‚æœæœ‰ç»„è¶…è¿‡10000è¡Œï¼Œéœ€è¦åˆ†å‰²
            MAX_GROUP_SIZE = 9999  # LightGBMçš„ä¸Šé™æ˜¯10000
            train_group_sizes = []
            
            for size in original_group_sizes:
                if size > MAX_GROUP_SIZE:
                    # å°†å¤§ç»„åˆ†å‰²æˆå¤šä¸ªå°ç»„
                    num_splits = (size + MAX_GROUP_SIZE - 1) // MAX_GROUP_SIZE
                    split_size = size // num_splits
                    remaining = size % num_splits
                    
                    for i in range(num_splits):
                        if i < remaining:
                            train_group_sizes.append(split_size + 1)
                        else:
                            train_group_sizes.append(split_size)
                else:
                    train_group_sizes.append(size)
            
            logger.info(f"LightGBMç»„å¤§å°è°ƒæ•´: {len(original_group_sizes)}ç»„ -> {len(train_group_sizes)}ç»„")
            logger.info(f"æœ€å¤§ç»„å¤§å°: {max(train_group_sizes)}")
            
            # LightGBMæ’åºå‚æ•°
            if optimize_hyperparams:
                params = {
                    'objective': 'lambdarank',
                    'metric': 'ndcg',
                    'boosting_type': 'gbdt',
                    'num_leaves': 31,
                    'learning_rate': 0.1,
                    'feature_fraction': 0.8,
                    'bagging_fraction': 0.8,
                    'bagging_freq': 5,
                    'verbose': -1
                }
            else:
                params = {
                    'objective': 'lambdarank',
                    'metric': 'ndcg',
                    'learning_rate': 0.1,
                    'verbose': -1
                }
            
            try:
                # ä¸ºLightGBMåˆ›å»ºç¦»æ•£åŒ–æ ‡ç­¾å’Œæƒé‡
                y_discrete = self._discretize_labels_by_group(y_train, train_group_ids, n_bins=15)  # å¢åŠ åˆ†ç®±ç²¾åº¦
                
                # ç”Ÿæˆlabel_gain: åŸºäºåŸå§‹è¿ç»­yçš„åˆ†ä½æ•°æƒé‡
                label_gains = []
                for g in np.unique(train_group_ids):
                    mask = train_group_ids == g
                    if mask.sum() > 0:
                        y_group = y_train[mask]
                        ranks = y_group.argsort().argsort() + 1  # 1-based ranking
                        gains = np.log2(ranks + 1)  # NDCG-style gains
                        label_gains.extend(gains)
                    
                label_gains = np.array(label_gains)
                
                # åˆ›å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨ç¦»æ•£æ ‡ç­¾å’Œè‡ªå®šä¹‰æƒé‡ï¼‰
                train_data = lgb.Dataset(
                    X_train, 
                    label=y_discrete,
                    group=train_group_sizes,
                    weight=label_gains  # æ·»åŠ label_gainæƒé‡
                )
                
                # è®­ç»ƒæ¨¡å‹
                model = lgb.train(
                    params=params,
                    train_set=train_data,
                    num_boost_round=150  # å¢åŠ è®­ç»ƒè½®æ•°
                )
                
                # é¢„æµ‹
                test_pred = model.predict(X_test)
                
                # ç®€åŒ–çš„ä¸ç¡®å®šæ€§ä¼°è®¡
                test_uncertainty = np.ones(len(test_pred)) * 0.1
                
                oof_predictions[test_mask] = test_pred
                oof_uncertainties[test_mask] = test_uncertainty
                
                models.append(model)
                
            except Exception as e:
                logger.warning(f"LightGBMè®­ç»ƒå¤±è´¥: {e}")
                continue
        
        return {
            'models': models,
            'oof_predictions': oof_predictions,
            'oof_uncertainties': oof_uncertainties,
            'model_type': 'lightgbm_ranker'
        }
    
    # CatBoost ranker method removed due to compatibility issues
    
    def _train_quantile_models(self, X: np.ndarray, y: np.ndarray, group_ids: np.ndarray,
                              cv_splits: List[Tuple], unique_groups: np.ndarray) -> Dict[str, Any]:
        """è®­ç»ƒåˆ†ä½æ•°å›å½’æ¨¡å‹"""
        models = {}
        oof_predictions = {}
        
        # è®­ç»ƒå¤šä¸ªåˆ†ä½æ•°
        quantiles = [0.1, 0.25, 0.5, 0.7, 0.9]
        
        for quantile in quantiles:
            models[f'q{int(quantile*100)}'] = []
            oof_predictions[f'q{int(quantile*100)}'] = np.full(len(X), np.nan)
            
            for train_groups_idx, test_groups_idx in cv_splits:
                # ä¿®å¤ç´¢å¼•è¶Šç•Œé—®é¢˜ï¼šç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                train_groups_idx = np.array(train_groups_idx)
                test_groups_idx = np.array(test_groups_idx)
                
                # è¿‡æ»¤è¶…å‡ºèŒƒå›´çš„ç´¢å¼•
                valid_train_idx = train_groups_idx[train_groups_idx < len(unique_groups)]
                valid_test_idx = test_groups_idx[test_groups_idx < len(unique_groups)]
                
                if len(valid_train_idx) == 0 or len(valid_test_idx) == 0:
                    continue
                    
                train_groups = unique_groups[valid_train_idx]
                test_groups = unique_groups[valid_test_idx]
                
                train_mask = np.isin(group_ids, train_groups)
                test_mask = np.isin(group_ids, test_groups)
                
                if train_mask.sum() == 0 or test_mask.sum() == 0:
                    continue
                
                X_train, y_train = X[train_mask], y[train_mask]
                X_test = X[test_mask]
                
                try:
                    # é™åˆ¶è®­ç»ƒé›†è§„æ¨¡ï¼Œé¿å…LPæ±‚è§£è€—æ—¶è¿‡é•¿
                    max_train_samples = 5000
                    if len(X_train) > max_train_samples:
                        # é€‰æ‹©æœ€è¿‘çš„è®­ç»ƒæ ·æœ¬ï¼ˆæŒ‰ç»„æ—¶é—´é¡ºåºï¼‰
                        recent_idx = np.arange(len(X_train) - max_train_samples, len(X_train))
                        X_train_small = X_train[recent_idx]
                        y_train_small = y_train[recent_idx]
                    else:
                        X_train_small = X_train
                        y_train_small = y_train

                    # åˆ†ä½æ•°å›å½’ - ä½¿ç”¨æ›´ç¨³å¥çš„å‚æ•°
                    model = QuantileRegressor(
                        quantile=quantile, 
                        alpha=0.1, 
                        solver='highs',
                        solver_options={'presolve': 'off', 'time_limit': 2.0}
                    )
                    model.fit(X_train_small, y_train_small)
                    
                    test_pred = model.predict(X_test)
                    oof_predictions[f'q{int(quantile*100)}'][test_mask] = test_pred
                    
                    models[f'q{int(quantile*100)}'].append(model)
                    
                except Exception as e:
                    logger.warning(f"åˆ†ä½æ•°å›å½’è®­ç»ƒå¤±è´¥ (q={quantile}): {e}")
                    continue
        
        # è®¡ç®—ä¸ç¡®å®šæ€§ï¼ˆä½¿ç”¨åˆ†ä½æ•°èŒƒå›´ï¼‰
        oof_uncertainties = np.full(len(X), np.nan)
        if 'q90' in oof_predictions and 'q10' in oof_predictions:
            q90 = oof_predictions['q90']
            q10 = oof_predictions['q10']
            oof_uncertainties = (q90 - q10) / 2
        
        return {
            'models': models,
            'oof_predictions': oof_predictions['q50'] if 'q50' in oof_predictions else np.full(len(X), np.nan),
            'oof_uncertainties': oof_uncertainties,
            'quantile_predictions': oof_predictions,
            'model_type': 'quantile_regression'
        }
    
    def _train_baseline_models(self, X: np.ndarray, y: np.ndarray, group_ids: np.ndarray,
                              cv_splits: List[Tuple], unique_groups: np.ndarray) -> Dict[str, Any]:
        """è®­ç»ƒåŸºå‡†å›å½’æ¨¡å‹"""
        baseline_models = {
            'linear': LinearRegression(),
            'rf': RandomForestRegressor(n_estimators=30, max_depth=5, max_samples=0.5, random_state=42)  # å¤§å¹…å‡å°‘å†…å­˜ä½¿ç”¨
        }
        
        results = {}
        
        for model_name, base_model in baseline_models.items():
            models = []
            oof_predictions = np.full(len(X), np.nan)
            oof_uncertainties = np.full(len(X), np.nan)
            
            for train_groups_idx, test_groups_idx in cv_splits:
                # ä¿®å¤ç´¢å¼•è¶Šç•Œé—®é¢˜ï¼šç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                train_groups_idx = np.array(train_groups_idx)
                test_groups_idx = np.array(test_groups_idx)
                
                # è¿‡æ»¤è¶…å‡ºèŒƒå›´çš„ç´¢å¼•
                valid_train_idx = train_groups_idx[train_groups_idx < len(unique_groups)]
                valid_test_idx = test_groups_idx[test_groups_idx < len(unique_groups)]
                
                if len(valid_train_idx) == 0 or len(valid_test_idx) == 0:
                    continue
                    
                train_groups = unique_groups[valid_train_idx]
                test_groups = unique_groups[valid_test_idx]
                
                train_mask = np.isin(group_ids, train_groups)
                test_mask = np.isin(group_ids, test_groups)
                
                if train_mask.sum() == 0 or test_mask.sum() == 0:
                    continue
                
                X_train, y_train = X[train_mask], y[train_mask]
                X_test = X[test_mask]
                
                try:
                    model = clone(base_model)
                    model.fit(X_train, y_train)
                    
                    test_pred = model.predict(X_test)
                    
                    # ç®€åŒ–çš„ä¸ç¡®å®šæ€§ä¼°è®¡
                    if hasattr(model, 'predict_proba'):
                        # å¯¹äºèƒ½è¾“å‡ºæ¦‚ç‡çš„æ¨¡å‹ï¼Œä½¿ç”¨æ¦‚ç‡åˆ†å¸ƒçš„æ–¹å·®
                        test_uncertainty = np.ones(len(test_pred)) * 0.1
                    else:
                        test_uncertainty = np.ones(len(test_pred)) * 0.1
                    
                    oof_predictions[test_mask] = test_pred
                    oof_uncertainties[test_mask] = test_uncertainty
                    
                    models.append(model)
                    
                except Exception as e:
                    logger.warning(f"{model_name}æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
                    continue
            
            results[model_name] = {
                'models': models,
                'oof_predictions': oof_predictions,
                'oof_uncertainties': oof_uncertainties,
                'model_type': f'baseline_{model_name}'
            }
        
        return results
    
    def _evaluate_model_performance(self, X: np.ndarray, y: np.ndarray, group_ids: np.ndarray):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        logger.info("è¯„ä¼°æ¨¡å‹æ€§èƒ½")
        
        for model_category, model_results in self.models.items():
            if isinstance(model_results, dict) and 'oof_predictions' in model_results:
                oof_pred = model_results['oof_predictions']
                oof_unc = model_results.get('oof_uncertainties', np.ones(len(oof_pred)) * 0.1)
                
                # å»é™¤NaNå€¼
                valid_mask = ~(np.isnan(oof_pred) | np.isnan(y))
                if valid_mask.sum() < 10:
                    continue
                
                y_valid = y[valid_mask]
                pred_valid = oof_pred[valid_mask]
                unc_valid = oof_unc[valid_mask]
                group_valid = group_ids[valid_mask]
                
                # è®¡ç®—å„ç§æŒ‡æ ‡
                metrics = self._calculate_metrics(y_valid, pred_valid, group_valid)
                
                # ä¸ç¡®å®šæ€§æŒ‡æ ‡
                uncertainty_metrics = self._calculate_uncertainty_metrics(
                    y_valid, pred_valid, unc_valid
                )
                
                self.performance_stats['oof_metrics'][model_category] = metrics
                self.performance_stats['uncertainty_metrics'][model_category] = uncertainty_metrics
                
                logger.info(f"{model_category} - IC: {metrics.get('ic', 0):.4f}, "
                           f"RankIC: {metrics.get('rank_ic', 0):.4f}")
            
            # å¤„ç†åµŒå¥—æ¨¡å‹ç»“æœï¼ˆå¦‚baseline_modelsï¼‰
            elif isinstance(model_results, dict):
                for sub_model, sub_results in model_results.items():
                    if isinstance(sub_results, dict) and 'oof_predictions' in sub_results:
                        # ç±»ä¼¼å¤„ç†...
                        pass
    
    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, 
                          groups: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—é¢„æµ‹æŒ‡æ ‡"""
        metrics = {}
        
        try:
            # ä¿¡æ¯ç³»æ•°
            ic = np.corrcoef(y_true, y_pred)[0, 1]
            metrics['ic'] = ic if not np.isnan(ic) else 0.0
            
            # æ’åºä¿¡æ¯ç³»æ•°
            rank_ic = spearmanr(y_true, y_pred)[0]
            metrics['rank_ic'] = rank_ic if not np.isnan(rank_ic) else 0.0
            
            # NDCG (ç®€åŒ–ç‰ˆæœ¬)
            ndcg = self._calculate_ndcg(y_true, y_pred, groups)
            metrics['ndcg'] = ndcg
            
            # åˆ†ç»„ICå¹³å‡å€¼
            group_ics = []
            for group_id in np.unique(groups):
                group_mask = groups == group_id
                if group_mask.sum() > 5:  # è‡³å°‘5ä¸ªæ ·æœ¬
                    y_group = y_true[group_mask]
                    pred_group = y_pred[group_mask]
                    group_ic = np.corrcoef(y_group, pred_group)[0, 1]
                    if not np.isnan(group_ic):
                        group_ics.append(group_ic)
            
            metrics['mean_group_ic'] = np.mean(group_ics) if group_ics else 0.0
            metrics['ic_std'] = np.std(group_ics) if group_ics else 0.0
            metrics['ic_ir'] = metrics['mean_group_ic'] / (metrics['ic_std'] + 1e-12)
            
        except Exception as e:
            logger.warning(f"æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            metrics = {'ic': 0.0, 'rank_ic': 0.0, 'ndcg': 0.0, 'mean_group_ic': 0.0, 'ic_std': 1.0, 'ic_ir': 0.0}
        
        return metrics
    
    def _calculate_ndcg(self, y_true: np.ndarray, y_pred: np.ndarray, 
                       groups: np.ndarray, k: int = 10) -> float:
        """è®¡ç®—NDCG@k"""
        ndcg_scores = []
        
        for group_id in np.unique(groups):
            group_mask = groups == group_id
            if group_mask.sum() < k:
                continue
            
            y_group = y_true[group_mask]
            pred_group = y_pred[group_mask]
            
            # æŒ‰é¢„æµ‹å€¼æ’åº
            sorted_indices = np.argsort(pred_group)[::-1][:k]
            
            # DCG
            dcg = 0.0
            for i, idx in enumerate(sorted_indices):
                rel = y_group[idx]
                dcg += rel / np.log2(i + 2)
            
            # IDCG
            ideal_sorted = np.argsort(y_group)[::-1][:k]
            idcg = 0.0
            for i, idx in enumerate(ideal_sorted):
                rel = y_group[idx]
                idcg += rel / np.log2(i + 2)
            
            if idcg > 0:
                ndcg_scores.append(dcg / idcg)
        
        return np.mean(ndcg_scores) if ndcg_scores else 0.0
    
    def _calculate_uncertainty_metrics(self, y_true: np.ndarray, y_pred: np.ndarray,
                                     uncertainty: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—ä¸ç¡®å®šæ€§æŒ‡æ ‡"""
        metrics = {}
        
        try:
            # æ ¡å‡†æŒ‡æ ‡ï¼šä¸ç¡®å®šæ€§é«˜çš„æ ·æœ¬è¯¯å·®æ˜¯å¦æ›´å¤§
            errors = np.abs(y_true - y_pred)
            calibration_corr = np.corrcoef(uncertainty, errors)[0, 1]
            metrics['uncertainty_calibration'] = calibration_corr if not np.isnan(calibration_corr) else 0.0
            
            # åˆ†ä½æ•°æ ¡å‡†
            n_bins = 10
            bin_boundaries = np.linspace(0, 1, n_bins + 1)
            bin_lowers = bin_boundaries[:-1]
            bin_uppers = bin_boundaries[1:]
            
            calibration_errors = []
            for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
                # æ‰¾åˆ°è¿™ä¸ªä¸ç¡®å®šæ€§åˆ†ä½æ•°èŒƒå›´å†…çš„æ ·æœ¬
                bin_mask = (uncertainty >= np.quantile(uncertainty, bin_lower)) & \
                          (uncertainty <= np.quantile(uncertainty, bin_upper))
                
                if bin_mask.sum() > 0:
                    bin_errors = errors[bin_mask]
                    expected_error = np.mean(bin_errors)
                    predicted_uncertainty = np.mean(uncertainty[bin_mask])
                    calibration_errors.append(abs(expected_error - predicted_uncertainty))
            
            metrics['calibration_error'] = np.mean(calibration_errors) if calibration_errors else 0.0
            
            # è¦†ç›–æ¦‚ç‡
            confidence_levels = [0.68, 0.95]
            for confidence in confidence_levels:
                threshold = np.quantile(uncertainty, confidence)
                high_conf_mask = uncertainty <= threshold
                
                if high_conf_mask.sum() > 0:
                    high_conf_errors = errors[high_conf_mask]
                    coverage = np.mean(high_conf_errors <= threshold)
                    metrics[f'coverage_{int(confidence*100)}'] = coverage
            
        except Exception as e:
            logger.warning(f"ä¸ç¡®å®šæ€§æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            metrics = {'uncertainty_calibration': 0.0, 'calibration_error': 1.0}
        
        return metrics
    
    def compute_uncertainty_aware_bma_weights(self, alpha_predictions: Dict[str, np.ndarray],
                                             alpha_uncertainties: Dict[str, np.ndarray],
                                             performance_scores: Dict[str, float]) -> Dict[str, float]:
        """
        è®¡ç®—ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„BMAæƒé‡
        
        Args:
            alpha_predictions: Alphaé¢„æµ‹å­—å…¸
            alpha_uncertainties: Alphaä¸ç¡®å®šæ€§å­—å…¸
            performance_scores: æ€§èƒ½è¯„åˆ†å­—å…¸
            
        Returns:
            BMAæƒé‡å­—å…¸
        """
        logger.info("è®¡ç®—ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„BMAæƒé‡")
        
        weights = {}
        
        for alpha_name in alpha_predictions.keys():
            if alpha_name not in performance_scores:
                weights[alpha_name] = 0.0
                continue
            
            # åŸºç¡€æ€§èƒ½åˆ†æ•°
            base_score = performance_scores[alpha_name]
            
            # ä¸ç¡®å®šæ€§è°ƒæ•´
            if alpha_name in alpha_uncertainties:
                uncertainty = alpha_uncertainties[alpha_name]
                # å¹³å‡ä¸ç¡®å®šæ€§è¶Šä½ï¼Œæƒé‡åŠ æˆè¶Šå¤§
                avg_uncertainty = np.nanmean(uncertainty)
                uncertainty_factor = 1.0 / (1.0 + avg_uncertainty)
            else:
                uncertainty_factor = 1.0
            
            # è°ƒæ•´åçš„åˆ†æ•°
            adjusted_score = base_score * uncertainty_factor
            weights[alpha_name] = adjusted_score
        
        # æ ‡å‡†åŒ–æƒé‡ï¼ˆsoftmax with temperatureï¼‰
        if weights:
            scores_array = np.array(list(weights.values()))
            
            # æ ‡å‡†åŒ–
            scores_std = (scores_array - scores_array.mean()) / (scores_array.std() + 1e-12)
            scores_scaled = scores_std / self.temperature
            
            # Softmax
            exp_scores = np.exp(scores_scaled - scores_scaled.max())
            weights_normalized = exp_scores / exp_scores.sum()
            
            # æ›´æ–°æƒé‡å­—å…¸
            for i, alpha_name in enumerate(weights.keys()):
                weights[alpha_name] = weights_normalized[i]
        
        logger.info(f"BMAæƒé‡è®¡ç®—å®Œæˆ: {weights}")
        return weights
    
    def predict_with_uncertainty(self, X: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼ŒåŒæ—¶è¾“å‡ºä¸ç¡®å®šæ€§
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            
        Returns:
            (é¢„æµ‹å€¼, ä¸ç¡®å®šæ€§)
        """
        if not self.models:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        # ğŸ”§ Fix LTRç»´åº¦åŒ¹é…: ç¡®ä¿ç‰¹å¾ç»´åº¦ä¸è®­ç»ƒæ—¶ä¸€è‡´
        X_aligned = X.copy()
        if hasattr(self, 'training_feature_columns'):
            training_cols = self.training_feature_columns
            current_cols = X.columns.tolist()
            
            if len(current_cols) != len(training_cols):
                logger.warning(f"LTRé¢„æµ‹ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: å½“å‰{len(current_cols)} vs è®­ç»ƒ{len(training_cols)}")
                
                # å¯¹é½ç‰¹å¾åˆ—ï¼šåªä¿ç•™è®­ç»ƒæ—¶çš„ç‰¹å¾
                common_cols = [col for col in training_cols if col in current_cols]
                missing_cols = [col for col in training_cols if col not in current_cols]
                
                if missing_cols:
                    logger.warning(f"LTRé¢„æµ‹ç¼ºå¤±ç‰¹å¾åˆ—: {missing_cols}ï¼Œå°†ç”¨0å¡«å……")
                    for col in missing_cols:
                        X_aligned[col] = 0.0
                
                # é‡æ–°æ’åºå¹¶é€‰æ‹©è®­ç»ƒæ—¶çš„ç‰¹å¾
                X_aligned = X_aligned[training_cols]
                logger.info(f"LTRç‰¹å¾å¯¹é½å®Œæˆ: {X_aligned.shape}")
        
        all_predictions = []
        all_uncertainties = []
        
        for model_category, model_results in self.models.items():
            if isinstance(model_results, dict) and 'models' in model_results:
                models = model_results['models']
                if not models:
                    continue
                
                # é›†æˆé¢„æµ‹
                category_predictions = []
                for model in models:
                    try:
                        if hasattr(model, 'predict'):
                            # ç‰¹æ®Šå¤„ç†xgboost Booster
                            try:
                                import xgboost as xgb
                                if isinstance(model, xgb.Booster):
                                    dmat = xgb.DMatrix(X_aligned.values)
                                    pred = model.predict(dmat)
                                else:
                                    pred = model.predict(X_aligned.values)
                            except Exception:
                                pred = model.predict(X_aligned.values)
                            category_predictions.append(pred)
                    except Exception as e:
                        logger.warning(f"é¢„æµ‹å¤±è´¥: {e}")
                        continue
                
                if category_predictions:
                    # å¹³å‡é¢„æµ‹
                    mean_pred = np.mean(category_predictions, axis=0)
                    # é¢„æµ‹æ–¹å·®ä½œä¸ºä¸ç¡®å®šæ€§
                    pred_uncertainty = np.std(category_predictions, axis=0)
                    
                    all_predictions.append(mean_pred)
                    all_uncertainties.append(pred_uncertainty)
        
        if not all_predictions:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
        
        # ç®€å•å¹³å‡æ‰€æœ‰æ¨¡å‹ç±»åˆ«çš„é¢„æµ‹
        final_prediction = np.mean(all_predictions, axis=0)
        final_uncertainty = np.sqrt(np.mean(np.array(all_uncertainties)**2, axis=0))
        
        return final_prediction, final_uncertainty
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ€»ç»“"""
        return {
            'performance_stats': self.performance_stats,
            'model_types': list(self.models.keys()),
            'total_models': sum(len(v.get('models', [])) if isinstance(v, dict) else 1 
                               for v in self.models.values())
        }
    
    def calibrate_predictions_to_returns(self, scores: np.ndarray, returns: np.ndarray, 
                                       method: str = 'isotonic') -> Tuple[np.ndarray, Any]:
        """
        å°†æ¨¡å‹åˆ†æ•°æ ¡å‡†ä¸ºé¢„æœŸæ”¶ç›Šç‡ - Enhancedé£æ ¼æ ¸å¿ƒåŠŸèƒ½
        
        Args:
            scores: æ¨¡å‹é¢„æµ‹åˆ†æ•°
            returns: çœŸå®æ”¶ç›Šç‡
            method: æ ¡å‡†æ–¹æ³• ('isotonic', 'quantile_bins', 'linear')
            
        Returns:
            Tuple[æ ¡å‡†åçš„æ”¶ç›Šç‡é¢„æµ‹, æ ¡å‡†å™¨å¯¹è±¡]
        """
        try:
            if method == 'isotonic':
                from sklearn.isotonic import IsotonicRegression
                calibrator = IsotonicRegression(out_of_bounds='clip')
                calibrator.fit(scores, returns)
                calibrated_returns = calibrator.predict(scores)
                
            elif method == 'quantile_bins':
                # åˆ†ä½æ•°æ¡¶æ ¡å‡† - æ›´ç¨³å¥çš„åˆ†æ®µçº¿æ€§æ˜ å°„
                n_bins = 20
                score_quantiles = np.linspace(0, 1, n_bins + 1)
                bin_edges = np.quantile(scores, score_quantiles)
                
                # åˆ›å»ºåˆ†ä½æ•°æ˜ å°„
                bin_means = {}
                for i in range(n_bins):
                    mask = (scores >= bin_edges[i]) & (scores <= bin_edges[i + 1])
                    if mask.sum() > 0:
                        bin_means[i] = returns[mask].mean()
                    else:
                        bin_means[i] = 0.0
                
                # åº”ç”¨æ ¡å‡†
                calibrated_returns = np.zeros_like(scores)
                for i in range(n_bins):
                    mask = (scores >= bin_edges[i]) & (scores <= bin_edges[i + 1])
                    calibrated_returns[mask] = bin_means[i]
                
                calibrator = {'bin_edges': bin_edges, 'bin_means': bin_means}
            
            else:
                # çº¿æ€§æ ¡å‡† - ç®€å•ä½†ç¨³å®š
                from sklearn.linear_model import LinearRegression
                calibrator = LinearRegression()
                calibrator.fit(scores.reshape(-1, 1), returns)
                calibrated_returns = calibrator.predict(scores.reshape(-1, 1))
            
            # è®¡ç®—æ ¡å‡†è´¨é‡
            correlation = np.corrcoef(calibrated_returns, returns)[0,1]
            logger.info(f"ä½¿ç”¨{method}æ–¹æ³•å®Œæˆåˆ†æ•°æ ¡å‡†ï¼Œæ ¡å‡†åç›¸å…³æ€§: {correlation:.3f}")
            
            return calibrated_returns, calibrator
            
        except Exception as e:
            logger.warning(f"åˆ†æ•°æ ¡å‡†å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•çº¿æ€§æ˜ å°„
            returns_std = np.std(returns) if np.std(returns) > 1e-8 else 0.02
            scores_std = np.std(scores) if np.std(scores) > 1e-8 else 1.0
            slope = returns_std / scores_std
            calibrated_returns = scores * slope
            return calibrated_returns, {'slope': slope, 'method': 'linear_fallback'}
    
    def apply_calibration(self, scores: np.ndarray, calibrator: Any, method: str = 'isotonic') -> np.ndarray:
        """åº”ç”¨å·²è®­ç»ƒçš„æ ¡å‡†å™¨åˆ°æ–°åˆ†æ•°"""
        try:
            if method == 'isotonic' and hasattr(calibrator, 'predict'):
                return calibrator.predict(scores)
            elif method == 'quantile_bins' and isinstance(calibrator, dict):
                bin_edges = calibrator['bin_edges']
                bin_means = calibrator['bin_means']
                
                calibrated = np.zeros_like(scores)
                for i in range(len(bin_edges) - 1):
                    mask = (scores >= bin_edges[i]) & (scores <= bin_edges[i + 1])
                    calibrated[mask] = bin_means.get(i, 0.0)
                return calibrated
            elif hasattr(calibrator, 'predict'):
                return calibrator.predict(scores.reshape(-1, 1))
            else:
                # çº¿æ€§æ˜ å°„å›é€€
                slope = calibrator.get('slope', 1.0)
                return scores * slope
                
        except Exception as e:
            logger.warning(f"æ ¡å‡†åº”ç”¨å¤±è´¥: {e}")
            return scores


def clone(estimator):
    """ç®€å•çš„æ¨¡å‹å…‹éš†å‡½æ•°"""
    from copy import deepcopy
    return deepcopy(estimator)


# æµ‹è¯•ä»£ç å·²ç§»é™¤ï¼Œé¿å…ç”Ÿäº§ä»£ç åŒ…å«æ¼”ç¤ºé€»è¾‘
# å¦‚éœ€æµ‹è¯•ï¼Œè¯·å‚è€ƒ tests/ æˆ– examples/ ç›®å½•
