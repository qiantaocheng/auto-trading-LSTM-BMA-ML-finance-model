#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªé€‚åº”å› å­æƒé‡å­¦ä¹ ç³»ç»Ÿ
åŸºäºBMA (Bayesian Model Averaging) å­¦ä¹ æœ€ä¼˜å› å­æƒé‡
æ›¿ä»£ç¡¬ç¼–ç æƒé‡ï¼Œæä¾›åŠ¨æ€æƒé‡è°ƒæ•´
"""

import os
import sys
import json
import pickle
import logging
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
import scipy.stats as stats
from pathlib import Path

# BMAè®­ç»ƒç³»ç»Ÿå»¶è¿Ÿå¯¼å…¥ï¼ˆé¿å…å¾ªç¯ä¾èµ–ï¼‰
BMA_AVAILABLE = False

logger = logging.getLogger(__name__)

@dataclass
class FactorWeightResult:
    """å› å­æƒé‡å­¦ä¹ ç»“æœ"""
    weights: Dict[str, float]
    confidence: float
    performance_score: float
    learning_date: datetime
    validation_sharpe: float
    factor_contributions: Dict[str, float]
    metadata: Dict[str, Any]

@dataclass
class WeightLearningConfig:
    """æƒé‡å­¦ä¹ é…ç½®"""
    lookback_days: int = 252  # 1å¹´å†å²æ•°æ®
    validation_days: int = 63  # éªŒè¯æœŸ
    min_confidence: float = 0.6  # æœ€å°ç½®ä¿¡åº¦
    max_weight: float = 0.5  # å•å› å­æœ€å¤§æƒé‡
    min_weight: float = 0.05  # å•å› å­æœ€å°æƒé‡
    rebalance_frequency: int = 21  # æƒé‡æ›´æ–°é¢‘ç‡(å¤©)
    performance_threshold: float = 0.1  # æ€§èƒ½é˜ˆå€¼
    enable_regime_detection: bool = True  # å¯ç”¨å¸‚åœºçŠ¶æ€æ£€æµ‹

class AdaptiveFactorWeights:
    """
    è‡ªé€‚åº”å› å­æƒé‡å­¦ä¹ ç³»ç»Ÿ
    ä½¿ç”¨BMAå’Œå†å²å›æµ‹å­¦ä¹ æœ€ä¼˜å› å­æƒé‡
    """
    
    def __init__(self, config: WeightLearningConfig = None):
        self.config = config or WeightLearningConfig()
        self.cache_dir = Path("cache/factor_weights")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # é»˜è®¤å› å­åˆ—è¡¨
        self.factor_names = [
            'mean_reversion', 'momentum', 'trend', 
            'volume', 'volatility'
        ]
        
        # ç¡¬ç¼–ç æƒé‡ï¼ˆä½œä¸ºå›é€€ï¼‰
        self.fallback_weights = {
            'mean_reversion': 0.30,
            'trend': 0.30,
            'momentum': 0.25,
            'volume': 0.20,
            'volatility': 0.15
        }
        
        # å­¦ä¹ å†å²
        self.weight_history = []
        self.performance_history = []
        
        # å½“å‰å­¦ä¹ åˆ°çš„æƒé‡
        self.current_weights = None
        self.last_update = None
        
        # å¸‚åœºçŠ¶æ€æ£€æµ‹
        self.market_regimes = {
            'bull': {'volatility_threshold': 0.15, 'trend_threshold': 0.05},
            'bear': {'volatility_threshold': 0.25, 'trend_threshold': -0.05},
            'sideways': {'volatility_threshold': 0.20, 'trend_threshold': 0.02}
        }
        
        logger.info(f"AdaptiveFactorWeights initialized with {len(self.factor_names)} factors")
    
    def need_update(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æƒé‡"""
        if self.current_weights is None or self.last_update is None:
            return True
        
        days_since_update = (datetime.now() - self.last_update).days
        return days_since_update >= self.config.rebalance_frequency
    
    def learn_weights_from_bma(self, symbols: List[str] = None) -> FactorWeightResult:
        """
        ä½¿ç”¨BMAå­¦ä¹ å› å­æƒé‡
        
        Args:
            symbols: è‚¡ç¥¨åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤åˆ—è¡¨
            
        Returns:
            FactorWeightResult: å­¦ä¹ ç»“æœ
        """
        try:
            logger.info("å¼€å§‹BMAæƒé‡å­¦ä¹ ...")
            
            # å°è¯•å»¶è¿Ÿå¯¼å…¥BMAæ¨¡å‹
            try:
                from bma_models.é‡åŒ–æ¨¡å‹_bma_ultra_enhanced import UltraEnhancedQuantitativeModel
                bma_available = True
            except ImportError as e:
                logger.warning(f"BMAæ¨¡å‹å¯¼å…¥å¤±è´¥: {e}")
                bma_available = False
            
            if not bma_available:
                logger.warning("BMAä¸å¯ç”¨ï¼Œä½¿ç”¨å†å²å›æµ‹æ–¹æ³•")
                return self._learn_weights_from_backtest(symbols)
            
            # ä½¿ç”¨BMAè®­ç»ƒç³»ç»Ÿ
            bma_model = UltraEnhancedQuantitativeModel()
            
            # è·å–é»˜è®¤è‚¡ç¥¨åˆ—è¡¨
            if symbols is None:
                symbols = self._get_default_symbols()
            
            # å‡†å¤‡è®­ç»ƒæ•°æ® - ä¿®å¤æ—¶é—´è®¡ç®—ï¼ˆä½¿ç”¨æ—¥å†å¤©æ•°è€Œéäº¤æ˜“æ—¥ï¼‰
            end_date = datetime.now()
            # 252ä¸ªäº¤æ˜“æ—¥ â‰ˆ 365ä¸ªæ—¥å†å¤©ï¼Œè€ƒè™‘å‘¨æœ«å’ŒèŠ‚å‡æ—¥
            calendar_days = int(self.config.lookback_days * 1.45)  # 252 * 1.45 â‰ˆ 365å¤©
            start_date = end_date - timedelta(days=calendar_days)
            
            logger.info(f"å¼€å§‹å®Œæ•´BMAè®­ç»ƒï¼Œè‚¡ç¥¨æ•°é‡: {len(symbols)}")
            logger.info(f"è®­ç»ƒæœŸé—´: {start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')}")
            
            # ğŸ”¥ ä½¿ç”¨å®Œæ•´BMAè®­ç»ƒ
            try:
                # æ‰§è¡Œå®Œæ•´çš„BMAè®­ç»ƒ
                training_results = bma_model.run_complete_analysis(
                    tickers=symbols[:20],  # é™åˆ¶æ•°é‡é¿å…å†…å­˜é—®é¢˜ï¼Œä½†ä½¿ç”¨çœŸå®è®­ç»ƒ
                    start_date=start_date.strftime('%Y-%m-%d'),
                    end_date=end_date.strftime('%Y-%m-%d'),
                    top_n=len(symbols[:20])  # è·å–æ‰€æœ‰è‚¡ç¥¨çš„ç»“æœ
                )
                
                logger.info("å®Œæ•´BMAè®­ç»ƒå®Œæˆ")
                
                # ä»çœŸå®BMAç»“æœæå–å› å­æƒé‡
                factor_weights = self._extract_weights_from_bma_full(training_results)
                
                # æ·±åº¦éªŒè¯æƒé‡
                validation_result = self._deep_validate_weights(factor_weights, training_results, symbols[:10])
                
                # åˆ›å»ºç»“æœ
                result = FactorWeightResult(
                    weights=factor_weights,
                    confidence=validation_result['confidence'],
                    performance_score=validation_result['sharpe_ratio'],
                    learning_date=datetime.now(),
                    validation_sharpe=validation_result['sharpe_ratio'],
                    factor_contributions=validation_result['contributions'],
                    metadata={
                        'method': 'BMA_full',
                        'symbols_count': len(symbols),
                        'lookback_days': self.config.lookback_days,
                        'bma_results_available': training_results is not None,
                        'training_summary': validation_result.get('training_summary', {}),
                        'model_performances': validation_result.get('model_performances', {})
                    }
                )
                
            except Exception as training_error:
                logger.error(f"å®Œæ•´BMAè®­ç»ƒå¤±è´¥: {training_error}")
                logger.info("å›é€€åˆ°å†å²å›æµ‹æ–¹æ³•")
                return self._learn_weights_from_backtest(symbols)
            
            # ä¿å­˜å­¦ä¹ ç»“æœ
            self._save_weight_result(result)
            
            logger.info(f"BMAæƒé‡å­¦ä¹ å®Œæˆï¼ŒSharpe: {validation_result['sharpe_ratio']:.3f}")
            return result
            
        except Exception as e:
            logger.error(f"BMAæƒé‡å­¦ä¹ å¤±è´¥: {e}")
            return self._learn_weights_from_backtest(symbols)
    
    def _extract_weights_from_bma_full(self, bma_results: Any) -> Dict[str, float]:
        """ä»å®Œæ•´BMAç»“æœä¸­æå–å› å­æƒé‡"""
        try:
            logger.info("ä»å®Œæ•´BMAç»“æœæå–å› å­æƒé‡")
            
            if bma_results is None:
                logger.warning("BMAç»“æœä¸ºç©ºï¼Œä½¿ç”¨å›é€€æƒé‡")
                return self.fallback_weights.copy()
            
            # åˆ†æä¼ ç»Ÿæ¨¡å‹æ€§èƒ½
            model_weights = {}
            total_model_score = 0
            
            # æå–ä¼ ç»Ÿæ¨¡å‹æ€§èƒ½
            if 'traditional_models' in bma_results:
                model_perfs = bma_results['traditional_models'].get('model_performance', {})
                logger.info(f"ä¼ ç»Ÿæ¨¡å‹æ€§èƒ½: {model_perfs}")
                
                for model_name, perf in model_perfs.items():
                    ic = perf.get('oof_ic', 0.0)
                    rank_ic = perf.get('oof_rank_ic', 0.0)
                    
                    # è®¡ç®—æ¨¡å‹ç»¼åˆå¾—åˆ†
                    model_score = max(0, ic * 0.7 + rank_ic * 0.3)
                    model_weights[model_name] = model_score
                    total_model_score += model_score
                    
                    logger.info(f"æ¨¡å‹ {model_name}: IC={ic:.4f}, RankIC={rank_ic:.4f}, Score={model_score:.4f}")
            
            # åˆ†æAlphaç­–ç•¥æ€§èƒ½
            alpha_performance = 0
            if 'alpha_strategy' in bma_results:
                alpha_scores = bma_results['alpha_strategy'].get('alpha_scores', pd.Series())
                if len(alpha_scores) > 0:
                    alpha_performance = max(0, alpha_scores.mean())
                    logger.info(f"Alphaç­–ç•¥å¹³å‡å¾—åˆ†: {alpha_performance:.4f}")
            
            # åˆ†æRidgeæ€§èƒ½
            ridge_performance = 0
            # å‘åå…¼å®¹ï¼šæ£€æŸ¥æ—§çš„learning_to_ranké”®
            if 'learning_to_rank' in bma_results and 'ridge_stacker' not in bma_results:
                ridge_results = bma_results.get('ridge_stacker', bma_results.get('learning_to_rank', {}))
                if isinstance(ridge_results, dict):
                    ridge_perf = ridge_results.get('performance_summary', {})
                    if ridge_perf:
                        avg_ic = np.mean([p.get('ic', 0.0) for p in ridge_perf.values() if isinstance(p, dict)])
                        ridge_performance = max(0, avg_ic)
                        logger.info(f"Ridgeå¹³å‡IC: {ridge_performance:.4f}")
            
            # åŸºäºæ¨¡å‹æ€§èƒ½æ˜ å°„åˆ°å› å­æƒé‡
            factor_weights = {}
            
            # æ¨¡å‹åˆ°å› å­çš„æ˜ å°„ï¼ˆåŸºäºæ¨¡å‹ç‰¹æ€§ï¼‰
            model_factor_mapping = {
                'ridge': ['mean_reversion', 'trend'],
                'elastic': ['momentum', 'volatility'], 
                'rf': ['trend', 'volume'],
                'xgboost': ['momentum', 'mean_reversion', 'trend'],
                'lightgbm': ['momentum', 'volatility', 'volume']
            }
            
            # åˆå§‹åŒ–å› å­æƒé‡
            for factor in self.factor_names:
                factor_weights[factor] = 0.0
            
            # åŸºäºæ¨¡å‹æ€§èƒ½åˆ†é…å› å­æƒé‡
            if total_model_score > 0:
                for model_name, model_score in model_weights.items():
                    if model_name in model_factor_mapping:
                        weight_per_factor = (model_score / total_model_score) / len(model_factor_mapping[model_name])
                        for factor in model_factor_mapping[model_name]:
                            if factor in factor_weights:
                                factor_weights[factor] += weight_per_factor
            
            # åŠ å…¥Alphaç­–ç•¥çš„è´¡çŒ®
            if alpha_performance > 0:
                # Alphaç­–ç•¥ä¸»è¦è´¡çŒ®åŠ¨é‡å’Œå‡å€¼å›å½’
                alpha_factors = ['momentum', 'mean_reversion', 'trend']
                alpha_weight_per_factor = alpha_performance * 0.3 / len(alpha_factors)
                for factor in alpha_factors:
                    if factor in factor_weights:
                        factor_weights[factor] += alpha_weight_per_factor
            
            # åŠ å…¥Ridgeçš„è´¡çŒ®
            if ridge_performance > 0:
                # Ridgeä¸»è¦è´¡çŒ®è¶‹åŠ¿å’ŒåŠ¨é‡
                ridge_factors = ['trend', 'momentum']
                ridge_weight_per_factor = ridge_performance * 0.2 / len(ridge_factors)
                for factor in ridge_factors:
                    if factor in factor_weights:
                        factor_weights[factor] += ridge_weight_per_factor
            
            # ç¡®ä¿æ‰€æœ‰å› å­éƒ½æœ‰æœ€å°æƒé‡
            for factor in self.factor_names:
                if factor not in factor_weights:
                    factor_weights[factor] = self.config.min_weight
                else:
                    factor_weights[factor] = max(factor_weights[factor], self.config.min_weight)
            
            # åº”ç”¨æƒé‡çº¦æŸ
            factor_weights = self._apply_weight_constraints(factor_weights)
            
            logger.info(f"ä»BMAæå–çš„å› å­æƒé‡: {factor_weights}")
            return factor_weights
            
        except Exception as e:
            logger.error(f"ä»BMAæå–æƒé‡å¤±è´¥: {e}")
            return self.fallback_weights.copy()
    
    def _deep_validate_weights(self, weights: Dict[str, float], 
                              bma_results: Any, symbols: List[str]) -> Dict[str, Any]:
        """æ·±åº¦éªŒè¯æƒé‡çš„æœ‰æ•ˆæ€§"""
        try:
            logger.info("å¼€å§‹æ·±åº¦æƒé‡éªŒè¯")
            
            # åŸºç¡€æƒé‡éªŒè¯
            total_weight = sum(weights.values())
            weight_variance = np.var(list(weights.values()))
            
            # æƒé‡åˆ†å¸ƒè¯„åˆ†
            distribution_score = 1.0 - min(weight_variance, 0.5) / 0.5
            
            # BMAè®­ç»ƒè´¨é‡è¯„åˆ†
            training_quality_score = 0.5
            model_performances = {}
            
            if bma_results and 'traditional_models' in bma_results:
                model_perfs = bma_results['traditional_models'].get('model_performance', {})
                if model_perfs:
                    ic_values = [perf.get('oof_ic', 0.0) for perf in model_perfs.values()]
                    avg_ic = np.mean([ic for ic in ic_values if not np.isnan(ic)])
                    training_quality_score = min(max(avg_ic * 5, 0.0), 1.0)  # æ˜ å°„åˆ°0-1
                    
                    model_performances = {
                        'average_ic': avg_ic,
                        'ic_std': np.std(ic_values),
                        'positive_ic_ratio': np.mean([ic > 0 for ic in ic_values]),
                        'model_count': len(model_perfs)
                    }
                    
                    logger.info(f"è®­ç»ƒè´¨é‡è¯„åˆ†: {training_quality_score:.3f} (å¹³å‡IC: {avg_ic:.4f})")
            
            # ç»¼åˆç½®ä¿¡åº¦
            confidence = (distribution_score * 0.4 + training_quality_score * 0.6)
            
            # ä¼°ç®—æ€§èƒ½å¾—åˆ†ï¼ˆåŸºäºè®­ç»ƒè´¨é‡ï¼‰
            performance_score = training_quality_score * 2.0  # è½¬æ¢ä¸ºSharpe-likeæŒ‡æ ‡
            
            # è®­ç»ƒæ€»ç»“
            training_summary = {
                'weights_distribution_score': distribution_score,
                'training_quality_score': training_quality_score,
                'total_weight': total_weight,
                'weight_variance': weight_variance,
                'symbols_trained': len(symbols)
            }
            
            validation_result = {
                'confidence': confidence,
                'sharpe_ratio': performance_score,
                'contributions': weights.copy(),
                'total_weight': total_weight,
                'distribution_score': distribution_score,
                'training_summary': training_summary,
                'model_performances': model_performances
            }
            
            logger.info(f"æ·±åº¦éªŒè¯å®Œæˆ - ç½®ä¿¡åº¦: {confidence:.3f}, æ€§èƒ½: {performance_score:.3f}")
            return validation_result
            
        except Exception as e:
            logger.error(f"æ·±åº¦æƒé‡éªŒè¯å¤±è´¥: {e}")
            return {
                'confidence': 0.5,
                'sharpe_ratio': 0.1,
                'contributions': weights.copy(),
                'total_weight': sum(weights.values()),
                'distribution_score': 0.5,
                'training_summary': {'error': str(e)},
                'model_performances': {}
            }
    
    def _extract_weights_from_bma(self, bma_results: Any) -> Dict[str, float]:
        """ä»BMAç»“æœä¸­æå–å› å­æƒé‡"""
        try:
            if bma_results is None:
                return self.fallback_weights.copy()
            
            # å¦‚æœBMAç»“æœåŒ…å«å› å­é‡è¦æ€§
            if hasattr(bma_results, 'feature_importance'):
                importance = bma_results.feature_importance
                
                # æ˜ å°„å› å­é‡è¦æ€§åˆ°æƒé‡
                weights = {}
                total_importance = 0
                
                for factor in self.factor_names:
                    # æŸ¥æ‰¾åŒ¹é…çš„ç‰¹å¾
                    factor_importance = 0
                    for feature, imp in importance.items():
                        if factor.lower() in feature.lower():
                            factor_importance += imp
                    
                    weights[factor] = max(factor_importance, self.config.min_weight)
                    total_importance += weights[factor]
                
                # å½’ä¸€åŒ–æƒé‡
                if total_importance > 0:
                    weights = {k: v/total_importance for k, v in weights.items()}
                else:
                    weights = self.fallback_weights.copy()
                
                # åº”ç”¨æƒé‡çº¦æŸ
                weights = self._apply_weight_constraints(weights)
                
                return weights
            
            else:
                logger.warning("BMAç»“æœä¸åŒ…å«å› å­é‡è¦æ€§ï¼Œä½¿ç”¨ç­‰æƒé‡")
                equal_weight = 1.0 / len(self.factor_names)
                return {factor: equal_weight for factor in self.factor_names}
                
        except Exception as e:
            logger.error(f"æå–BMAæƒé‡å¤±è´¥: {e}")
            return self.fallback_weights.copy()
    
    def _learn_weights_from_backtest(self, symbols: List[str] = None) -> FactorWeightResult:
        """
        ä½¿ç”¨å†å²å›æµ‹å­¦ä¹ æƒé‡ï¼ˆBMAä¸å¯ç”¨æ—¶çš„åå¤‡æ–¹æ¡ˆï¼‰
        """
        try:
            logger.info("ä½¿ç”¨å†å²å›æµ‹æ–¹æ³•å­¦ä¹ æƒé‡...")
            
            if symbols is None:
                symbols = self._get_default_symbols()
            
            # æµ‹è¯•ä¸åŒæƒé‡ç»„åˆçš„æ•ˆæœ
            weight_combinations = self._generate_weight_combinations()
            
            best_weights = None
            best_sharpe = -999
            
            for weights in weight_combinations:
                try:
                    # æ¨¡æ‹Ÿå›æµ‹
                    sharpe = self._backtest_weights(weights, symbols[:20])  # é™åˆ¶æµ‹è¯•è§„æ¨¡
                    
                    if sharpe > best_sharpe:
                        best_sharpe = sharpe
                        best_weights = weights
                        
                except Exception as e:
                    logger.warning(f"æƒé‡ç»„åˆæµ‹è¯•å¤±è´¥: {e}")
                    continue
            
            # å¦‚æœæ‰¾ä¸åˆ°å¥½çš„æƒé‡ï¼Œä½¿ç”¨å›é€€æƒé‡
            if best_weights is None or best_sharpe < 0:
                best_weights = self.fallback_weights.copy()
                best_sharpe = 0.1  # å‡è®¾å›é€€æƒé‡æœ‰åŸºæœ¬è¡¨ç°
            
            # åˆ›å»ºç»“æœ
            result = FactorWeightResult(
                weights=best_weights,
                confidence=min(max(best_sharpe, 0.3), 1.0),
                performance_score=best_sharpe,
                learning_date=datetime.now(),
                validation_sharpe=best_sharpe,
                factor_contributions=best_weights,
                metadata={
                    'method': 'backtest',
                    'tested_combinations': len(weight_combinations),
                    'symbols_count': len(symbols)
                }
            )
            
            self._save_weight_result(result)
            
            logger.info(f"å›æµ‹æƒé‡å­¦ä¹ å®Œæˆï¼Œæœ€ä½³Sharpe: {best_sharpe:.3f}")
            return result
            
        except Exception as e:
            logger.error(f"å›æµ‹æƒé‡å­¦ä¹ å¤±è´¥: {e}")
            # è¿”å›å›é€€ç»“æœ
            return FactorWeightResult(
                weights=self.fallback_weights.copy(),
                confidence=0.5,
                performance_score=0.1,
                learning_date=datetime.now(),
                validation_sharpe=0.1,
                factor_contributions=self.fallback_weights.copy(),
                metadata={'method': 'fallback', 'error': str(e)}
            )
    
    def _generate_weight_combinations(self) -> List[Dict[str, float]]:
        """ç”Ÿæˆæƒé‡ç»„åˆè¿›è¡Œæµ‹è¯•"""
        combinations = []
        
        # åŸºç¡€ç»„åˆ
        combinations.append(self.fallback_weights.copy())
        
        # ç­‰æƒé‡
        equal_weight = 1.0 / len(self.factor_names)
        combinations.append({factor: equal_weight for factor in self.factor_names})
        
        # é‡ç‚¹å› å­ç»„åˆ
        focus_factors = ['mean_reversion', 'trend', 'momentum']
        for focus_factor in focus_factors:
            weights = {factor: 0.1 for factor in self.factor_names}
            weights[focus_factor] = 0.6
            remaining = 0.4 / (len(self.factor_names) - 1)
            for factor in self.factor_names:
                if factor != focus_factor:
                    weights[factor] = remaining
            combinations.append(weights)
        
        # éšæœºæƒé‡ç»„åˆ
        # np.random.seed removed
        for _ in range(10):
            random_weights = np.random.dirichlet(np.ones(len(self.factor_names)))
            weights_dict = {factor: float(weight) for factor, weight in 
                          zip(self.factor_names, random_weights)}
            combinations.append(weights_dict)
        
        return combinations
    
    def _backtest_weights(self, weights: Dict[str, float], symbols: List[str]) -> float:
        """
        ç®€åŒ–çš„æƒé‡å›æµ‹
        è¿”å›Sharpeæ¯”ç‡
        """
        try:
            # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„å›æµ‹é€»è¾‘
            # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡æ‹Ÿè®¡ç®—
            
            # æ¨¡æ‹Ÿå› å­è¡¨ç°
            factor_returns = {
                'mean_reversion': np.zeros(0.08),
                'trend': np.zeros(0.12),
                'momentum': np.zeros(0.10),
                'volume': np.zeros(0.06),
                'volatility': np.zeros(0.04)
            }
            
            # è®¡ç®—åŠ æƒç»„åˆæ”¶ç›Š
            portfolio_return = sum(weights[factor] * factor_returns[factor] 
                                 for factor in self.factor_names)
            
            # ä¼°ç®—é£é™©
            portfolio_risk = 0.15  # å‡è®¾ç»„åˆæ³¢åŠ¨ç‡
            
            # è®¡ç®—Sharpeæ¯”ç‡
            sharpe = portfolio_return / portfolio_risk if portfolio_risk > 0 else 0
            
            return sharpe
            
        except Exception as e:
            logger.error(f"æƒé‡å›æµ‹å¤±è´¥: {e}")
            return -1.0
    
    def _validate_weights(self, weights: Dict[str, float], symbols: List[str]) -> Dict[str, Any]:
        """éªŒè¯æƒé‡çš„æœ‰æ•ˆæ€§"""
        try:
            # è®¡ç®—æƒé‡çº¦æŸåˆè§„æ€§
            total_weight = sum(weights.values())
            weight_variance = np.var(list(weights.values()))
            
            # åŸºç¡€éªŒè¯åˆ†æ•°
            base_score = 0.8 if abs(total_weight - 1.0) < 0.01 else 0.5
            
            # å¤šæ ·æ€§è¯„åˆ†
            diversity_score = 1.0 - weight_variance if weight_variance < 0.5 else 0.3
            
            # ç»¼åˆç½®ä¿¡åº¦
            confidence = (base_score + diversity_score) / 2
            
            # æ¨¡æ‹ŸSharpeæ¯”ç‡
            estimated_sharpe = self._backtest_weights(weights, symbols)
            
            return {
                'confidence': confidence,
                'sharpe_ratio': estimated_sharpe,
                'contributions': weights.copy(),
                'total_weight': total_weight,
                'diversity_score': diversity_score
            }
            
        except Exception as e:
            logger.error(f"æƒé‡éªŒè¯å¤±è´¥: {e}")
            return {
                'confidence': 0.5,
                'sharpe_ratio': 0.1,
                'contributions': weights.copy(),
                'total_weight': sum(weights.values()),
                'diversity_score': 0.5
            }
    
    def _apply_weight_constraints(self, weights: Dict[str, float]) -> Dict[str, float]:
        """åº”ç”¨æƒé‡çº¦æŸ"""
        try:
            # ç¡®ä¿æœ€å°æƒé‡
            for factor in weights:
                weights[factor] = max(weights[factor], self.config.min_weight)
            
            # ç¡®ä¿æœ€å¤§æƒé‡
            for factor in weights:
                weights[factor] = min(weights[factor], self.config.max_weight)
            
            # é‡æ–°å½’ä¸€åŒ–
            total_weight = sum(weights.values())
            if total_weight > 0:
                weights = {k: v/total_weight for k, v in weights.items()}
            
            return weights
            
        except Exception as e:
            logger.error(f"åº”ç”¨æƒé‡çº¦æŸå¤±è´¥: {e}")
            return self.fallback_weights.copy()
    
    def _get_default_symbols(self) -> List[str]:
        """è·å–é»˜è®¤è‚¡ç¥¨åˆ—è¡¨"""
        # è¿”å›ä¸€äº›æµåŠ¨æ€§å¥½çš„å¤§ç›˜è‚¡
        return [
            'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA',
            'META', 'NVDA', 'JPM', 'JNJ', 'V',
            'PG', 'UNH', 'HD', 'DIS', 'MA',
            'PFE', 'BAC', 'KO', 'PEP', 'MRK'
        ]
    
    def _save_weight_result(self, result: FactorWeightResult):
        """ä¿å­˜æƒé‡å­¦ä¹ ç»“æœ"""
        try:
            # ä¿å­˜åˆ°ç¼“å­˜
            timestamp = result.learning_date.strftime('%Y%m%d_%H%M%S')
            cache_file = self.cache_dir / f"weights_{timestamp}.pkl"
            
            with open(cache_file, 'wb') as f:
                pickle.dump(result, f)
            
            # æ›´æ–°å½“å‰æƒé‡
            self.current_weights = result.weights
            self.last_update = result.learning_date
            
            # ä¿å­˜åˆ°JSONï¼ˆå¯è¯»æ ¼å¼ï¼‰
            json_file = self.cache_dir / f"weights_{timestamp}.json"
            with open(json_file, 'w') as f:
                json.dump(asdict(result), f, indent=2, default=str)
            
            # æ›´æ–°å†å²è®°å½•
            self.weight_history.append(result)
            
            # ä¿æŒå†å²è®°å½•å¤§å°
            if len(self.weight_history) > 100:
                self.weight_history = self.weight_history[-50:]
            
            logger.info(f"æƒé‡ç»“æœå·²ä¿å­˜: {cache_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æƒé‡ç»“æœå¤±è´¥: {e}")
    
    def load_latest_weights(self) -> Optional[FactorWeightResult]:
        """åŠ è½½æœ€æ–°çš„æƒé‡ç»“æœ"""
        try:
            # æŸ¥æ‰¾æœ€æ–°çš„æƒé‡æ–‡ä»¶
            weight_files = list(self.cache_dir.glob("weights_*.pkl"))
            if not weight_files:
                return None
            
            latest_file = max(weight_files, key=lambda x: x.stat().st_mtime)
            
            with open(latest_file, 'rb') as f:
                result = pickle.load(f)
            
            # æ›´æ–°å½“å‰çŠ¶æ€
            self.current_weights = result.weights
            self.last_update = result.learning_date
            
            logger.info(f"åŠ è½½æœ€æ–°æƒé‡: {latest_file}")
            return result
            
        except Exception as e:
            logger.error(f"åŠ è½½æƒé‡å¤±è´¥: {e}")
            return None
    
    def get_current_weights(self, force_update: bool = False) -> Dict[str, float]:
        """
        è·å–å½“å‰æƒé‡
        
        Args:
            force_update: å¼ºåˆ¶æ›´æ–°æƒé‡
            
        Returns:
            Dict[str, float]: å½“å‰å› å­æƒé‡
        """
        try:
            # ğŸ”¥ æ™ºèƒ½æƒé‡æ›´æ–°ç­–ç•¥ - ä¼˜å…ˆå°è¯•åŠ è½½æœ€æ–°çš„MLæƒé‡
            should_trigger_training = force_update and self.need_update()
            
            if should_trigger_training:
                logger.info("ğŸ’¡ å¼ºåˆ¶æ›´æ–°æ¨¡å¼ï¼Œå¼€å§‹å­¦ä¹ æ–°æƒé‡...")
                
                # å°è¯•å­¦ä¹ æ–°æƒé‡
                try:
                    result = self.learn_weights_from_bma()
                    if result.confidence >= self.config.min_confidence:
                        logger.info(f"âœ… MLæƒé‡æ›´æ–°æˆåŠŸï¼Œç½®ä¿¡åº¦: {result.confidence:.3f}")
                        self.current_weights = result.weights
                        self.last_update = datetime.now()
                        return result.weights
                    else:
                        logger.warning(f"âš ï¸ æ–°æƒé‡ç½®ä¿¡åº¦è¿‡ä½: {result.confidence:.3f}ï¼Œå°è¯•å†å²æƒé‡")
                except Exception as e:
                    logger.error(f"âŒ æƒé‡å­¦ä¹ å¤±è´¥: {e}")
            elif self.need_update():
                logger.info("ğŸ“Š æ£€æµ‹åˆ°æƒé‡éœ€è¦æ›´æ–°ï¼Œä¼˜å…ˆå°è¯•åŠ è½½ç°æœ‰MLæƒé‡")
            
            # ä¼˜å…ˆä½¿ç”¨å½“å‰å†…å­˜ä¸­çš„æƒé‡
            if self.current_weights is not None:
                logger.info("ğŸ¯ ä½¿ç”¨å½“å‰å†…å­˜ä¸­çš„MLæƒé‡")
                return self.current_weights
            
            # å°è¯•åŠ è½½æœ€æ–°çš„å†å²æƒé‡ï¼ˆMLå­¦ä¹ çš„ç»“æœï¼‰
            latest_result = self.load_latest_weights()
            if latest_result is not None:
                logger.info(f"ğŸ“‚ åŠ è½½å†å²MLæƒé‡ï¼Œå­¦ä¹ æ—¥æœŸ: {latest_result.learning_date}, ç½®ä¿¡åº¦: {latest_result.confidence:.3f}")
                self.current_weights = latest_result.weights
                return latest_result.weights
            
            # æœ€åå›é€€åˆ°ç¡¬ç¼–ç æƒé‡
            logger.warning("âš ï¸ æœªæ‰¾åˆ°MLæƒé‡ï¼Œä½¿ç”¨ç¡¬ç¼–ç å›é€€æƒé‡")
            return self.fallback_weights.copy()
            
        except Exception as e:
            logger.error(f"è·å–æƒé‡å¤±è´¥: {e}")
            return self.fallback_weights.copy()
    
    def get_or_learn_weights(self) -> Dict[str, float]:
        """
        è·å–æƒé‡æˆ–ä¸»åŠ¨å­¦ä¹ æ–°æƒé‡
        ä¸“ä¸ºBMA Enhancedç³»ç»Ÿè®¾è®¡ï¼Œç¡®ä¿ä½¿ç”¨MLæƒé‡è€Œéç¡¬ç¼–ç æƒé‡
        """
        try:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„MLæƒé‡
            latest_result = self.load_latest_weights()
            
            # å¦‚æœæœ‰æœ€è¿‘çš„MLæƒé‡ï¼ˆ30å¤©å†…ï¼‰ï¼Œä½¿ç”¨å®ƒ
            if latest_result is not None:
                days_old = (datetime.now() - latest_result.learning_date).days
                if days_old <= 30 and latest_result.confidence >= 0.6:
                    logger.info(f"ğŸ¯ ä½¿ç”¨æœ€è¿‘MLæƒé‡ ({days_old}å¤©å‰), ç½®ä¿¡åº¦: {latest_result.confidence:.3f}")
                    self.current_weights = latest_result.weights
                    return latest_result.weights
            
            # å¦‚æœæ²¡æœ‰åˆé€‚çš„MLæƒé‡ï¼Œä¸»åŠ¨è§¦å‘å­¦ä¹ 
            logger.info("ğŸš€ ä¸»åŠ¨è§¦å‘MLæƒé‡å­¦ä¹ ï¼Œé¿å…ä½¿ç”¨ç¡¬ç¼–ç æƒé‡")
            try:
                result = self.learn_weights_from_bma()
                if result.confidence >= self.config.min_confidence:
                    logger.info(f"âœ… ä¸»åŠ¨MLæƒé‡å­¦ä¹ æˆåŠŸï¼Œç½®ä¿¡åº¦: {result.confidence:.3f}")
                    self.current_weights = result.weights
                    self.last_update = datetime.now()
                    return result.weights
                else:
                    logger.warning(f"âš ï¸ MLæƒé‡ç½®ä¿¡åº¦ä¸è¶³: {result.confidence:.3f}")
            except Exception as e:
                logger.error(f"âŒ ä¸»åŠ¨MLæƒé‡å­¦ä¹ å¤±è´¥: {e}")
            
            # å¦‚æœMLå­¦ä¹ å¤±è´¥ï¼Œä½¿ç”¨æœ€æ–°å¯ç”¨æƒé‡
            if latest_result is not None:
                logger.info(f"ğŸ“‚ ä½¿ç”¨å¯ç”¨çš„å†å²æƒé‡ï¼Œç½®ä¿¡åº¦: {latest_result.confidence:.3f}")
                self.current_weights = latest_result.weights
                return latest_result.weights
            
            # æœ€åçš„å›é€€
            logger.warning("âš ï¸ MLæƒé‡ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼˜åŒ–çš„å›é€€æƒé‡")
            return self.fallback_weights.copy()
            
        except Exception as e:
            logger.error(f"è·å–æˆ–å­¦ä¹ æƒé‡å¤±è´¥: {e}")
            return self.fallback_weights.copy()
    
    def get_weight_statistics(self) -> Dict[str, Any]:
        """è·å–æƒé‡ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if not self.weight_history:
                return {'status': 'no_history'}
            
            # è®¡ç®—æƒé‡è¶‹åŠ¿
            recent_weights = [result.weights for result in self.weight_history[-10:]]
            weight_trends = {}
            
            for factor in self.factor_names:
                values = [w.get(factor, 0) for w in recent_weights]
                weight_trends[factor] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'trend': 'up' if len(values) > 1 and values[-1] > values[0] else 'down'
                }
            
            # æ€§èƒ½ç»Ÿè®¡
            performance_scores = [result.performance_score for result in self.weight_history[-10:]]
            
            return {
                'total_updates': len(self.weight_history),
                'last_update': self.last_update.isoformat() if self.last_update else None,
                'current_weights': self.current_weights,
                'weight_trends': weight_trends,
                'performance_stats': {
                    'mean_performance': np.mean(performance_scores),
                    'best_performance': np.max(performance_scores),
                    'recent_performance': performance_scores[-1] if performance_scores else 0
                }
            }
            
        except Exception as e:
            logger.error(f"è·å–æƒé‡ç»Ÿè®¡å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}


# å…¨å±€å®ä¾‹
_adaptive_weights_instance = None

def get_adaptive_factor_weights(config: WeightLearningConfig = None) -> AdaptiveFactorWeights:
    """è·å–å…¨å±€è‡ªé€‚åº”æƒé‡å®ä¾‹"""
    global _adaptive_weights_instance
    if _adaptive_weights_instance is None:
        _adaptive_weights_instance = AdaptiveFactorWeights(config)
    return _adaptive_weights_instance

def get_current_factor_weights(force_update: bool = False) -> Dict[str, float]:
    """ä¾¿æ·å‡½æ•°ï¼šè·å–å½“å‰å› å­æƒé‡"""
    weights_manager = get_adaptive_factor_weights()
    return weights_manager.get_current_weights(force_update)

def update_factor_weights_from_bma(symbols: List[str] = None) -> FactorWeightResult:
    """ä¾¿æ·å‡½æ•°ï¼šä½¿ç”¨BMAæ›´æ–°æƒé‡"""
    weights_manager = get_adaptive_factor_weights()
    return weights_manager.learn_weights_from_bma(symbols)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO)
    
    print("æµ‹è¯•è‡ªé€‚åº”å› å­æƒé‡ç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆ›å»ºæƒé‡å­¦ä¹ å™¨
    weights_manager = AdaptiveFactorWeights()
    
    # è·å–å½“å‰æƒé‡
    current_weights = weights_manager.get_current_weights(force_update=True)
    print(f"å½“å‰æƒé‡: {current_weights}")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = weights_manager.get_weight_statistics()
    print(f"æƒé‡ç»Ÿè®¡: {stats}")
    
    print("æµ‹è¯•å®Œæˆ")