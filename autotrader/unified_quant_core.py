#!/usr/bin/env python3
"""
ç»Ÿä¸€é‡åŒ–æ ¸å¿ƒ - æ•´åˆæ‰€æœ‰é‡åŒ–æ¨¡å—åŠŸèƒ½
æ›¿ä»£å¤šä¸ªé‡å¤çš„é£é™©ç®¡ç†ã€æ•°æ®å¤„ç†ã€ä¼˜åŒ–å™¨æ¨¡å—
"""

import numpy as np
import pandas as pd
import logging
import warnings
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
import json
from bma_models.unified_config_loader import get_time_config
import pickle
from threading import RLock
import threading
from enum import Enum

# ç§‘å­¦è®¡ç®—åº“
from scipy import optimize
from scipy.stats import norm, spearmanr
from scipy.optimize import minimize, LinearConstraint, Bounds
from scipy.linalg import inv, cholesky
from sklearn.linear_model import HuberRegressor
from sklearn.covariance import LedoitWolf, OAS, EmpiricalCovariance
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor
# ğŸš« å·²åˆ é™¤TimeSeriesSplitå¯¼å…¥ - ä½¿ç”¨ç»Ÿä¸€CVå·¥å‚

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


# =============================================================================
# æ ¸å¿ƒæ•°æ®ç»“æ„
# =============================================================================

class RiskLevel(Enum):
    """é£é™©ç­‰çº§"""
    LOW = "LOW"
    MEDIUM = "MEDIUM"
    HIGH = "HIGH"
    CRITICAL = "CRITICAL"


@dataclass
class FactorData:
    """å› å­æ•°æ®"""
    timestamp: datetime
    ticker: str
    factors: Dict[str, float] = field(default_factory=dict)
    returns: Optional[float] = None
    market_cap: Optional[float] = None
    sector: Optional[str] = None
    is_valid: bool = True


@dataclass
class RiskMetrics:
    """é£é™©æŒ‡æ ‡"""
    volatility: float = 0.0
    var_95: float = 0.0
    expected_shortfall: float = 0.0
    max_drawdown: float = 0.0
    sharpe_ratio: float = 0.0
    beta: float = 1.0
    tracking_error: float = 0.0
    information_ratio: float = 0.0


@dataclass
class PortfolioOptimizationResult:
    """æŠ•èµ„ç»„åˆä¼˜åŒ–ç»“æœ"""
    weights: Dict[str, float]
    expected_return: float
    expected_risk: float
    sharpe_ratio: float
    optimization_status: str
    risk_metrics: RiskMetrics
    constraints_satisfied: bool = True
    optimization_time: float = 0.0


@dataclass
class MarketDataCache:
    """å¸‚åœºæ•°æ®ç¼“å­˜"""
    prices: Dict[str, pd.DataFrame] = field(default_factory=dict)
    factors: Dict[str, pd.DataFrame] = field(default_factory=dict)
    last_update: Dict[str, datetime] = field(default_factory=dict)
    cache_ttl: int = 300  # 5åˆ†é’ŸTTL


# =============================================================================
# ç»Ÿä¸€é‡åŒ–æ ¸å¿ƒç±»
# =============================================================================

class UnifiedQuantCore:
    """ç»Ÿä¸€é‡åŒ–æ ¸å¿ƒ - é›†æˆæ‰€æœ‰é‡åŒ–åŠŸèƒ½"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.logger = logging.getLogger("UnifiedQuantCore")
        
        # çº¿ç¨‹å®‰å…¨ - ä½¿ç”¨å•ä¸€é”é¿å…æ­»é”
        self._master_lock = RLock()
        
        # ä¸ºäº†å‘åå…¼å®¹ä¿ç•™åŸæœ‰å±æ€§åï¼Œä½†éƒ½æŒ‡å‘åŒä¸€ä¸ªé”
        self.data_lock = self._master_lock
        self.optimization_lock = self._master_lock
        
        # é”é¡ºåºè®°å½•ï¼ˆè°ƒè¯•ç”¨ï¼‰
        self._lock_debug = False
        self._lock_owner = None
        
        # æ•°æ®å­˜å‚¨
        self.market_data_cache = MarketDataCache()
        self.factor_data: Dict[str, FactorData] = {}
        self.risk_models: Dict[str, Any] = {}
        
        # é…ç½®å‚æ•°
        self.risk_aversion = self.config.get('risk_aversion', 5.0)
        self.max_position = self.config.get('max_position', 0.03)
        self.max_turnover = self.config.get('max_turnover', 0.10)
        self.turnover_penalty = self.config.get('turnover_penalty', 1.0)
        
        # é£é™©æ¨¡å‹é…ç½®
        self.factor_groups = {
            'size': ['log_market_cap', 'market_cap_rank'],
            'value': ['pe_ratio', 'pb_ratio', 'ev_ebitda'],
            'momentum': ['return_1m', 'return_3m', 'return_6m'],
            'quality': ['roe', 'roa', 'debt_to_equity'],
            'volatility': ['volatility_20d', 'volatility_60d'],
            'liquidity': ['volume_20d', 'dollar_volume']
        }
        
        # åŸºå‡†å’Œçº¦æŸ
        self.benchmark_weights: Optional[Dict[str, float]] = None
        self.sector_constraints: Dict[str, float] = {}
        self.position_constraints: Dict[str, Tuple[float, float]] = {}
        
        self.logger.info("ç»Ÿä¸€é‡åŒ–æ ¸å¿ƒåˆå§‹åŒ–å®Œæˆ")
    
    def _acquire_lock_debug(self, operation: str):
        """è°ƒè¯•é”è·å–"""
        if self._lock_debug:
            thread_id = threading.current_thread().ident
            self.logger.debug(f"Thread {thread_id} acquiring lock for: {operation}")
            if self._lock_owner and self._lock_owner != thread_id:
                self.logger.warning(f"Potential lock contention: {thread_id} waiting for {self._lock_owner}")
    
    def _release_lock_debug(self, operation: str):
        """è°ƒè¯•é”é‡Šæ”¾"""
        if self._lock_debug:
            thread_id = threading.current_thread().ident
            self.logger.debug(f"Thread {thread_id} released lock for: {operation}")
    
    def enable_lock_debug(self):
        """å¯ç”¨é”è°ƒè¯•"""
        self._lock_debug = True
        self.logger.info("Lock debugging enabled")
    
    def disable_lock_debug(self):
        """ç¦ç”¨é”è°ƒè¯•"""
        self._lock_debug = False
        self.logger.info("Lock debugging disabled")
    
    def safe_lock(self, operation: str):
        """å®‰å…¨é”ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        class SafeLockContext:
            def __init__(self, core, op):
                self.core = core
                self.operation = op
                
            def __enter__(self):
                self.core._acquire_lock_debug(self.operation)
                self.core._master_lock.acquire()
                self.core._lock_owner = threading.current_thread().ident
                return self
                
            def __exit__(self, exc_type, exc_val, exc_tb):
                self.core._lock_owner = None
                self.core._master_lock.release()
                self.core._release_lock_debug(self.operation)
                
        return SafeLockContext(self, operation)
    
    # =============================================================================
    # æ•°æ®ç®¡ç†åŠŸèƒ½ (æ›¿ä»£ unified_market_data_manager.py + enhanced_data_processing.py)
    # =============================================================================
    
    def load_market_data(self, tickers: List[str], start_date: str, end_date: str,
                        data_source: str = "polygon") -> Dict[str, pd.DataFrame]:
        """åŠ è½½å¸‚åœºæ•°æ®"""
        try:
            with self.safe_lock("load_market_data"):
                # æ£€æŸ¥ç¼“å­˜
                cached_data = {}
                missing_tickers = []
                
                for ticker in tickers:
                    if (ticker in self.market_data_cache.prices and
                        ticker in self.market_data_cache.last_update):
                        
                        last_update = self.market_data_cache.last_update[ticker]
                        if (datetime.now() - last_update).seconds < self.market_data_cache.cache_ttl:
                            cached_data[ticker] = self.market_data_cache.prices[ticker]
                        else:
                            missing_tickers.append(ticker)
                    else:
                        missing_tickers.append(ticker)
                
                # åŠ è½½ç¼ºå¤±æ•°æ®
                if missing_tickers:
                    self.logger.info(f"åŠ è½½ {len(missing_tickers)} åªè‚¡ç¥¨çš„å¸‚åœºæ•°æ®")
                    
                    if data_source == "polygon":
                        new_data = self._load_polygon_data(missing_tickers, start_date, end_date)
                    else:
                        raise RuntimeError(f"å®æ—¶äº¤æ˜“ç³»ç»Ÿæ— æ³•è·å– {missing_tickers} çš„å¸‚åœºæ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®æºè¿æ¥")
                    
                    # éªŒè¯æ•°æ®çœŸå®æ€§
                    simulated_count = 0
                    for ticker, df in new_data.items():
                        if not df.empty and 'is_real_data' in df.columns and not df['is_real_data'].iloc[0]:
                            simulated_count += 1
                    
                    if simulated_count > 0:
                        warning_msg = f"WARNING: {simulated_count}/{len(new_data)} åªè‚¡ç¥¨ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®!"
                        self.logger.warning(warning_msg)
                        
                        # æ£€æŸ¥æ˜¯å¦ä¸ºç”Ÿäº§ç¯å¢ƒ
                        if self.config.get('environment') == 'production':
                            self.logger.critical(f"ç”Ÿäº§ç¯å¢ƒæ£€æµ‹åˆ°æ¨¡æ‹Ÿæ•°æ®ä½¿ç”¨: {list(new_data.keys())}")
                    
                    # æ›´æ–°ç¼“å­˜
                    for ticker, df in new_data.items():
                        self.market_data_cache.prices[ticker] = df
                        self.market_data_cache.last_update[ticker] = datetime.now()
                    
                    cached_data.update(new_data)
                
                self.logger.info(f"æˆåŠŸåŠ è½½ {len(cached_data)} åªè‚¡ç¥¨çš„å¸‚åœºæ•°æ®")
                return cached_data
                
        except Exception as e:
            self.logger.error(f"åŠ è½½å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            return {}
    
    def _load_polygon_data(self, tickers: List[str], start_date: str, end_date: str) -> Dict[str, pd.DataFrame]:
        """ä»PolygonåŠ è½½æ•°æ®"""
        try:
            from polygon_client import polygon_client, download
            
            data = {}
            for ticker in tickers:
                try:
                    df = download(ticker, start_date, end_date)
                    if not df.empty:
                        data[ticker] = df
                except Exception as e:
                    self.logger.warning(f"åŠ è½½{ticker}æ•°æ®å¤±è´¥: {e}")
                    continue
            
            return data
            
        except ImportError:
            self.logger.error("Polygonå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œå®æ—¶äº¤æ˜“ç³»ç»Ÿæ— æ³•ç»§ç»­")
            raise RuntimeError("å®æ—¶äº¤æ˜“ç³»ç»Ÿéœ€è¦Polygonæ•°æ®æºï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿æ¥")
    
    # _load_fallback_data å‡½æ•°å·²ç§»é™¤ - å®æ—¶äº¤æ˜“ç³»ç»Ÿä¸åº”ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
    
    def create_factors(self, market_data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """åˆ›å»ºå› å­æ•°æ®"""
        try:
            all_factors = []
            
            for ticker, df in market_data.items():
                if df.empty:
                    continue
                
                # è®¡ç®—å› å­
                factor_df = self._calculate_factors(df.copy(), ticker)
                all_factors.append(factor_df)
            
            if all_factors:
                combined_factors = pd.concat(all_factors, ignore_index=True)
                
                # æŒ‰tickeråˆ†ç»„è¿”å›
                factor_dict = {}
                for ticker in combined_factors['ticker'].unique():
                    factor_dict[ticker] = combined_factors[combined_factors['ticker'] == ticker].copy()
                
                self.logger.info(f"æˆåŠŸåˆ›å»º {len(factor_dict)} åªè‚¡ç¥¨çš„å› å­æ•°æ®")
                return factor_dict
            else:
                return {}
                
        except Exception as e:
            self.logger.error(f"åˆ›å»ºå› å­æ•°æ®å¤±è´¥: {e}")
            return {}
    
    def _calculate_factors(self, df: pd.DataFrame, ticker: str) -> pd.DataFrame:
        """è®¡ç®—å•åªè‚¡ç¥¨çš„å› å­"""
        try:
            # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸæ’åº
            df = df.sort_index()
            
            # åŸºç¡€ä»·æ ¼å› å­
            df['returns'] = df['close'].pct_change()
            df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
            
            # åŠ¨é‡å› å­
            df['return_1m'] = df['close'] / df['close'].shift(20) - 1
            df['return_3m'] = df['close'] / df['close'].shift(60) - 1
            df['return_6m'] = df['close'] / df['close'].shift(120) - 1
            
            # æ³¢åŠ¨ç‡å› å­
            df['volatility_20d'] = df['log_returns'].rolling(20).std() * np.sqrt(252)
            df['volatility_60d'] = df['log_returns'].rolling(60).std() * np.sqrt(252)
            
            # æŠ€æœ¯æŒ‡æ ‡
            df['ma_20'] = df['close'].rolling(20).mean()
            df['ma_50'] = df['close'].rolling(50).mean()
            df['price_to_ma20'] = df['close'] / df['ma_20']
            df['price_to_ma50'] = df['close'] / df['ma_50']
            
            # RSI
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
            rs = gain / loss
            df['rsi_14'] = 100 - (100 / (1 + rs))
            
            # æˆäº¤é‡å› å­
            if 'volume' in df.columns:
                df['volume_ma_20'] = df['volume'].rolling(20).mean()
                df['volume_ratio'] = df['volume'] / df['volume_ma_20']
                df['dollar_volume'] = df['close'] * df['volume']
                df['volume_20d'] = df['volume'].rolling(20).mean()
            
            # æ·»åŠ å…ƒæ•°æ®
            df['ticker'] = ticker
            
            # é‡ç½®ç´¢å¼•ï¼Œä¿ç•™æ—¥æœŸåˆ—
            df = df.reset_index()
            if 'index' in df.columns:
                df = df.rename(columns={'index': 'date'})
            
            # æ¸…ç†æ— æ•ˆæ•°æ®
            df = df.dropna(subset=['returns', 'volatility_20d'])
            
            return df
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—{ticker}å› å­å¤±è´¥: {e}")
            return pd.DataFrame()
    
    # =============================================================================
    # é£é™©ç®¡ç†åŠŸèƒ½ (æ›¿ä»£æ‰€æœ‰é£é™©æ¨¡å—)
    # =============================================================================
    
    def estimate_risk_model(self, factor_data: Dict[str, pd.DataFrame],
                          method: str = "barra") -> Dict[str, Any]:
        """ä¼°è®¡é£é™©æ¨¡å‹"""
        try:
            if method == "barra":
                return self._estimate_barra_model(factor_data)
            elif method == "statistical":
                return self._estimate_statistical_model(factor_data)
            else:
                return self._estimate_simple_model(factor_data)
                
        except Exception as e:
            self.logger.error(f"ä¼°è®¡é£é™©æ¨¡å‹å¤±è´¥: {e}")
            return {}
    
    def _estimate_barra_model(self, factor_data: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
        """Barraé£æ ¼é£é™©æ¨¡å‹"""
        try:
            # åˆå¹¶æ‰€æœ‰å› å­æ•°æ®
            all_data = []
            for ticker, df in factor_data.items():
                if not df.empty:
                    all_data.append(df)
            
            if not all_data:
                return {}
            
            combined_data = pd.concat(all_data, ignore_index=True)
            
            # é€‰æ‹©é£é™©å› å­
            risk_factors = [
                'return_1m', 'return_3m', 'volatility_20d', 'volatility_60d',
                'price_to_ma20', 'rsi_14', 'volume_ratio'
            ]
            
            # ç¡®ä¿å› å­å­˜åœ¨
            available_factors = [f for f in risk_factors if f in combined_data.columns]
            
            if len(available_factors) < 3:
                self.logger.warning("å¯ç”¨å› å­æ•°é‡ä¸è¶³ï¼Œä½¿ç”¨ç®€åŒ–æ¨¡å‹")
                return self._estimate_simple_model(factor_data)
            
            # å‡†å¤‡å›å½’æ•°æ®
            valid_data = combined_data.dropna(subset=['returns'] + available_factors)
            
            if len(valid_data) < 100:
                self.logger.warning("æœ‰æ•ˆæ•°æ®ç‚¹ä¸è¶³ï¼Œä½¿ç”¨ç®€åŒ–æ¨¡å‹")
                return self._estimate_simple_model(factor_data)
            
            X = valid_data[available_factors].values
            y = valid_data['returns'].values
            
            # æ ‡å‡†åŒ–å› å­
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # Huberå›å½’ä¼°è®¡å› å­è½½è·
            huber = HuberRegressor(epsilon=1.35, max_iter=1000)
            huber.fit(X_scaled, y)
            
            factor_loadings = pd.DataFrame({
                'factor': available_factors,
                'loading': huber.coef_
            })
            
            # ä¼°è®¡å› å­åæ–¹å·®çŸ©é˜µ
            factor_returns = pd.DataFrame(X_scaled, columns=available_factors)
            
            # Ledoit-Wolfæ”¶ç¼©ä¼°è®¡
            lw = LedoitWolf()
            factor_cov = lw.fit(factor_returns).covariance_
            
            # è®¡ç®—ç‰¹å¼‚é£é™©
            predicted_returns = huber.predict(X_scaled)
            residuals = y - predicted_returns
            
            # æŒ‰è‚¡ç¥¨åˆ†ç»„è®¡ç®—ç‰¹å¼‚é£é™©
            valid_data = valid_data.copy()
            valid_data['residuals'] = residuals
            specific_risk = valid_data.groupby('ticker')['residuals'].var()
            
            risk_model = {
                'type': 'barra',
                'factor_loadings': factor_loadings,
                'factor_covariance': factor_cov,
                'specific_risk': specific_risk.to_dict(),
                'available_factors': available_factors,
                'scaler': scaler,
                'r_squared': huber.score(X_scaled, y),
                'estimation_date': datetime.now()
            }
            
            self.logger.info(f"Barraé£é™©æ¨¡å‹ä¼°è®¡å®Œæˆï¼ŒRÂ²={risk_model['r_squared']:.3f}")
            return risk_model
            
        except Exception as e:
            self.logger.error(f"Barraæ¨¡å‹ä¼°è®¡å¤±è´¥: {e}")
            return self._estimate_simple_model(factor_data)
    
    def _estimate_statistical_model(self, factor_data: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
        """ç»Ÿè®¡é£é™©æ¨¡å‹"""
        try:
            # æ„å»ºæ”¶ç›Šç‡çŸ©é˜µ
            returns_data = {}
            
            for ticker, df in factor_data.items():
                if not df.empty and 'returns' in df.columns:
                    clean_returns = df['returns'].dropna()
                    if len(clean_returns) >= 20:  # è‡³å°‘20ä¸ªè§‚æµ‹
                        returns_data[ticker] = clean_returns
            
            if len(returns_data) < 2:
                return {}
            
            # åˆ›å»ºæ”¶ç›Šç‡çŸ©é˜µ
            returns_df = pd.DataFrame(returns_data)
            returns_df = returns_df.dropna()
            
            if len(returns_df) < 20:
                return {}
            
            # ä½¿ç”¨Ledoit-Wolfæ”¶ç¼©ä¼°è®¡åæ–¹å·®çŸ©é˜µ
            lw = LedoitWolf()
            cov_matrix = lw.fit(returns_df).covariance_
            
            # è®¡ç®—ç›¸å…³çŸ©é˜µ
            corr_matrix = pd.DataFrame(cov_matrix, 
                                     index=returns_df.columns, 
                                     columns=returns_df.columns).corr()
            
            # è®¡ç®—ä¸ªè‚¡æ³¢åŠ¨ç‡
            volatilities = returns_df.std() * np.sqrt(252)
            
            risk_model = {
                'type': 'statistical',
                'covariance_matrix': cov_matrix,
                'correlation_matrix': corr_matrix.values,
                'volatilities': volatilities.to_dict(),
                'tickers': list(returns_df.columns),
                'estimation_window': len(returns_df),
                'estimation_date': datetime.now()
            }
            
            self.logger.info(f"ç»Ÿè®¡é£é™©æ¨¡å‹ä¼°è®¡å®Œæˆï¼Œè¦†ç›–{len(returns_data)}åªè‚¡ç¥¨")
            return risk_model
            
        except Exception as e:
            self.logger.error(f"ç»Ÿè®¡æ¨¡å‹ä¼°è®¡å¤±è´¥: {e}")
            return {}
    
    def _estimate_simple_model(self, factor_data: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
        """ç®€åŒ–é£é™©æ¨¡å‹"""
        try:
            volatilities = {}
            correlations = {}
            
            for ticker, df in factor_data.items():
                if not df.empty and 'returns' in df.columns:
                    returns = df['returns'].dropna()
                    if len(returns) >= 10:
                        vol = returns.std() * np.sqrt(252)
                        volatilities[ticker] = vol
            
            # å‡è®¾è‚¡ç¥¨é—´ç›¸å…³æ€§
            tickers = list(volatilities.keys())
            n = len(tickers)
            
            if n >= 2:
                # ç®€å•ç›¸å…³çŸ©é˜µï¼ˆå‡è®¾0.3çš„å¹³å‡ç›¸å…³æ€§ï¼‰
                corr_matrix = np.full((n, n), 0.3)
                np.fill_diagonal(corr_matrix, 1.0)
                
                correlations = pd.DataFrame(corr_matrix, index=tickers, columns=tickers)
            
            risk_model = {
                'type': 'simple',
                'volatilities': volatilities,
                'correlation_matrix': correlations.values if not correlations.empty else None,
                'tickers': tickers,
                'estimation_date': datetime.now()
            }
            
            self.logger.info(f"ç®€åŒ–é£é™©æ¨¡å‹åˆ›å»ºå®Œæˆï¼Œè¦†ç›–{len(volatilities)}åªè‚¡ç¥¨")
            return risk_model
            
        except Exception as e:
            self.logger.error(f"ç®€åŒ–æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            return {}
    
    def calculate_portfolio_risk(self, weights: Dict[str, float],
                               risk_model: Dict[str, Any]) -> RiskMetrics:
        """è®¡ç®—æŠ•èµ„ç»„åˆé£é™©"""
        try:
            if risk_model.get('type') == 'barra':
                return self._calculate_barra_risk(weights, risk_model)
            elif risk_model.get('type') == 'statistical':
                return self._calculate_statistical_risk(weights, risk_model)
            else:
                return self._calculate_simple_risk(weights, risk_model)
                
        except Exception as e:
            self.logger.error(f"è®¡ç®—æŠ•èµ„ç»„åˆé£é™©å¤±è´¥: {e}")
            return RiskMetrics()
    
    def _calculate_barra_risk(self, weights: Dict[str, float], risk_model: Dict[str, Any]) -> RiskMetrics:
        """ä½¿ç”¨Barraæ¨¡å‹è®¡ç®—é£é™©"""
        try:
            # æå–æ¨¡å‹ç»„ä»¶
            factor_cov = risk_model.get('factor_covariance')
            specific_risk = risk_model.get('specific_risk', {})
            
            if factor_cov is None:
                return RiskMetrics()
            
            # è®¡ç®—æŠ•èµ„ç»„åˆå±‚é¢çš„å› å­æš´éœ²
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¹æ®å› å­è½½è·è®¡ç®—
            
            # ä¼°ç®—æŠ•èµ„ç»„åˆæ³¢åŠ¨ç‡
            portfolio_vol = 0.0
            total_weight = sum(abs(w) for w in weights.values())
            
            if total_weight > 0:
                # ç®€åŒ–çš„é£é™©è®¡ç®—
                weighted_var = 0.0
                for ticker, weight in weights.items():
                    specific_var = specific_risk.get(ticker, 0.01) ** 2
                    weighted_var += (weight ** 2) * specific_var
                
                portfolio_vol = np.sqrt(weighted_var) * np.sqrt(252)
            
            return RiskMetrics(
                volatility=portfolio_vol,
                var_95=portfolio_vol * 1.645,  # æ­£æ€åˆ†å¸ƒ95%åˆ†ä½æ•°
                expected_shortfall=portfolio_vol * 2.06,  # è¿‘ä¼¼ES
                beta=1.0,  # éœ€è¦åŸºå‡†æ•°æ®è®¡ç®—
                tracking_error=portfolio_vol * 0.5  # ä¼°ç®—
            )
            
        except Exception as e:
            self.logger.error(f"Barraé£é™©è®¡ç®—å¤±è´¥: {e}")
            return RiskMetrics()
    
    def _calculate_statistical_risk(self, weights: Dict[str, float], risk_model: Dict[str, Any]) -> RiskMetrics:
        """ä½¿ç”¨ç»Ÿè®¡æ¨¡å‹è®¡ç®—é£é™©"""
        try:
            cov_matrix = risk_model.get('covariance_matrix')
            tickers = risk_model.get('tickers', [])
            
            if cov_matrix is None or not tickers:
                return RiskMetrics()
            
            # æ„å»ºæƒé‡å‘é‡
            weight_vector = np.zeros(len(tickers))
            for i, ticker in enumerate(tickers):
                weight_vector[i] = weights.get(ticker, 0.0)
            
            # è®¡ç®—æŠ•èµ„ç»„åˆæ–¹å·®
            portfolio_variance = np.dot(weight_vector, np.dot(cov_matrix, weight_vector))
            portfolio_vol = np.sqrt(portfolio_variance) * np.sqrt(252)
            
            return RiskMetrics(
                volatility=portfolio_vol,
                var_95=portfolio_vol * 1.645,
                expected_shortfall=portfolio_vol * 2.06,
                beta=1.0,
                tracking_error=portfolio_vol * 0.3
            )
            
        except Exception as e:
            self.logger.error(f"ç»Ÿè®¡é£é™©è®¡ç®—å¤±è´¥: {e}")
            return RiskMetrics()
    
    def _calculate_simple_risk(self, weights: Dict[str, float], risk_model: Dict[str, Any]) -> RiskMetrics:
        """ä½¿ç”¨ç®€åŒ–æ¨¡å‹è®¡ç®—é£é™©"""
        try:
            volatilities = risk_model.get('volatilities', {})
            
            # ç®€åŒ–çš„ç»„åˆæ³¢åŠ¨ç‡è®¡ç®—
            weighted_vol = 0.0
            total_weight = 0.0
            
            for ticker, weight in weights.items():
                if ticker in volatilities:
                    weighted_vol += abs(weight) * volatilities[ticker]
                    total_weight += abs(weight)
            
            if total_weight > 0:
                avg_vol = weighted_vol / total_weight
                # è€ƒè™‘åˆ†æ•£åŒ–æ•ˆåº”
                portfolio_vol = avg_vol * np.sqrt(len(weights)) * 0.7  # åˆ†æ•£åŒ–æŠ˜æ‰£
            else:
                portfolio_vol = 0.0
            
            return RiskMetrics(
                volatility=portfolio_vol,
                var_95=portfolio_vol * 1.645,
                expected_shortfall=portfolio_vol * 2.06,
                beta=1.0,
                tracking_error=portfolio_vol * 0.4
            )
            
        except Exception as e:
            self.logger.error(f"ç®€åŒ–é£é™©è®¡ç®—å¤±è´¥: {e}")
            return RiskMetrics()
    
    # =============================================================================
    # æŠ•èµ„ç»„åˆä¼˜åŒ–åŠŸèƒ½ (æ›¿ä»£æ‰€æœ‰ä¼˜åŒ–å™¨)
    # =============================================================================
    
    def optimize_portfolio(self, expected_returns: Dict[str, float],
                         risk_model: Dict[str, Any],
                         current_weights: Optional[Dict[str, float]] = None,
                         constraints: Optional[Dict[str, Any]] = None) -> PortfolioOptimizationResult:
        """æŠ•èµ„ç»„åˆä¼˜åŒ–"""
        
        start_time = datetime.now()
        
        try:
            with self.safe_lock("optimize_portfolio"):
                # å‡†å¤‡æ•°æ®
                tickers = list(expected_returns.keys())
                n_assets = len(tickers)
                
                if n_assets == 0:
                    return PortfolioOptimizationResult(
                        weights={},
                        expected_return=0.0,
                        expected_risk=0.0,
                        sharpe_ratio=0.0,
                        optimization_status="ERROR: No assets",
                        risk_metrics=RiskMetrics()
                    )
                
                # æ„å»ºæœŸæœ›æ”¶ç›Šå‘é‡
                mu = np.array([expected_returns[ticker] for ticker in tickers])
                
                # æ„å»ºåæ–¹å·®çŸ©é˜µ
                if risk_model.get('type') == 'statistical':
                    cov_matrix = risk_model.get('covariance_matrix')
                    if cov_matrix is None:
                        cov_matrix = np.eye(n_assets) * 0.01  # é»˜è®¤åæ–¹å·®
                else:
                    # ç®€åŒ–åæ–¹å·®çŸ©é˜µæ„å»º
                    volatilities = risk_model.get('volatilities', {})
                    cov_matrix = np.eye(n_assets)
                    
                    for i, ticker in enumerate(tickers):
                        vol = volatilities.get(ticker, 0.2)
                        cov_matrix[i, i] = vol ** 2
                    
                    # æ·»åŠ ç›¸å…³æ€§
                    corr = 0.3  # å‡è®¾å¹³å‡ç›¸å…³æ€§
                    for i in range(n_assets):
                        for j in range(i + 1, n_assets):
                            cov_val = corr * np.sqrt(cov_matrix[i, i] * cov_matrix[j, j])
                            cov_matrix[i, j] = cov_val
                            cov_matrix[j, i] = cov_val
                
                # å¤„ç†å½“å‰æƒé‡
                current_w = np.zeros(n_assets)
                if current_weights:
                    for i, ticker in enumerate(tickers):
                        current_w[i] = current_weights.get(ticker, 0.0)
                
                # è®¾ç½®çº¦æŸ
                constraints_list = []
                bounds = []
                
                # æƒé‡å’Œçº¦æŸ
                constraints_list.append({
                    'type': 'eq',
                    'fun': lambda x: np.sum(x) - 1.0
                })
                
                # ä½ç½®çº¦æŸ
                max_pos = self.max_position
                if constraints and 'max_position' in constraints:
                    max_pos = constraints['max_position']
                
                for i in range(n_assets):
                    bounds.append((-max_pos, max_pos))  # å…è®¸åšç©º
                
                # æ¢æ‰‹ç‡çº¦æŸ
                if current_weights and self.max_turnover > 0:
                    def turnover_constraint(x):
                        turnover = np.sum(np.abs(x - current_w))
                        return self.max_turnover - turnover
                    
                    constraints_list.append({
                        'type': 'ineq',
                        'fun': turnover_constraint
                    })
                
                # ç›®æ ‡å‡½æ•°ï¼šæœ€å¤§åŒ–æ•ˆç”¨ï¼ˆæ”¶ç›Š - é£é™©æƒ©ç½š - æ¢æ‰‹ç‡æƒ©ç½šï¼‰
                def objective(x):
                    portfolio_return = np.dot(x, mu)
                    portfolio_risk = np.sqrt(np.dot(x, np.dot(cov_matrix, x)))
                    
                    utility = portfolio_return - 0.5 * self.risk_aversion * (portfolio_risk ** 2)
                    
                    # æ¢æ‰‹ç‡æƒ©ç½š
                    if current_weights and self.turnover_penalty > 0:
                        turnover = np.sum(np.abs(x - current_w))
                        utility -= self.turnover_penalty * turnover
                    
                    return -utility  # æœ€å°åŒ–è´Ÿæ•ˆç”¨
                
                # åˆå§‹è§£
                x0 = np.ones(n_assets) / n_assets
                if current_weights:
                    x0 = current_w.copy()
                
                # ä¼˜åŒ–
                result = minimize(
                    objective,
                    x0,
                    method='SLSQP',
                    bounds=bounds,
                    constraints=constraints_list,
                    options={'maxiter': 1000, 'ftol': 1e-9}
                )
                
                # å¤„ç†ç»“æœ
                if result.success:
                    optimal_weights = result.x
                    
                    # è®¡ç®—ç»„åˆæŒ‡æ ‡
                    portfolio_return = np.dot(optimal_weights, mu)
                    portfolio_risk = np.sqrt(np.dot(optimal_weights, np.dot(cov_matrix, optimal_weights)))
                    sharpe_ratio = portfolio_return / max(portfolio_risk, 1e-8)
                    
                    # æ„å»ºæƒé‡å­—å…¸
                    weights_dict = {ticker: float(optimal_weights[i]) for i, ticker in enumerate(tickers)}
                    
                    # è®¡ç®—é£é™©æŒ‡æ ‡
                    risk_metrics = self.calculate_portfolio_risk(weights_dict, risk_model)
                    
                    optimization_time = (datetime.now() - start_time).total_seconds()
                    
                    result_obj = PortfolioOptimizationResult(
                        weights=weights_dict,
                        expected_return=float(portfolio_return),
                        expected_risk=float(portfolio_risk),
                        sharpe_ratio=float(sharpe_ratio),
                        optimization_status="SUCCESS",
                        risk_metrics=risk_metrics,
                        constraints_satisfied=True,
                        optimization_time=optimization_time
                    )
                    
                    self.logger.info(f"æŠ•èµ„ç»„åˆä¼˜åŒ–æˆåŠŸï¼Œå¤æ™®æ¯”ç‡: {sharpe_ratio:.3f}")
                    return result_obj
                
                else:
                    self.logger.error(f"ä¼˜åŒ–å¤±è´¥: {result.message}")
                    
                    # è¿”å›ç­‰æƒé‡ç»„åˆ
                    equal_weights = {ticker: 1.0 / n_assets for ticker in tickers}
                    portfolio_return = np.mean(list(expected_returns.values()))
                    portfolio_risk = 0.2  # ä¼°ç®—
                    
                    return PortfolioOptimizationResult(
                        weights=equal_weights,
                        expected_return=portfolio_return,
                        expected_risk=portfolio_risk,
                        sharpe_ratio=portfolio_return / portfolio_risk,
                        optimization_status=f"FALLBACK: {result.message}",
                        risk_metrics=RiskMetrics(volatility=portfolio_risk),
                        constraints_satisfied=False,
                        optimization_time=(datetime.now() - start_time).total_seconds()
                    )
                
        except Exception as e:
            self.logger.error(f"æŠ•èµ„ç»„åˆä¼˜åŒ–å¤±è´¥: {e}")
            
            # è¿”å›ç©ºç»“æœ
            return PortfolioOptimizationResult(
                weights={},
                expected_return=0.0,
                expected_risk=0.0,
                sharpe_ratio=0.0,
                optimization_status=f"ERROR: {str(e)}",
                risk_metrics=RiskMetrics(),
                constraints_satisfied=False,
                optimization_time=(datetime.now() - start_time).total_seconds()
            )
    
    # =============================================================================
    # é›†æˆæ¥å£
    # =============================================================================
    
    def run_full_analysis(self, tickers: List[str], start_date: str, end_date: str,
                         current_weights: Optional[Dict[str, float]] = None) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        try:
            self.logger.info(f"å¼€å§‹å®Œæ•´åˆ†æï¼Œè‚¡ç¥¨æ•°é‡: {len(tickers)}")
            
            # 1. åŠ è½½å¸‚åœºæ•°æ®
            market_data = self.load_market_data(tickers, start_date, end_date)
            if not market_data:
                return {'error': 'æ— æ³•åŠ è½½å¸‚åœºæ•°æ®'}
            
            # 2. åˆ›å»ºå› å­
            factor_data = self.create_factors(market_data)
            if not factor_data:
                return {'error': 'æ— æ³•åˆ›å»ºå› å­æ•°æ®'}
            
            # 3. ä¼°è®¡é£é™©æ¨¡å‹
            risk_model = self.estimate_risk_model(factor_data, method="barra")
            if not risk_model:
                return {'error': 'æ— æ³•ä¼°è®¡é£é™©æ¨¡å‹'}
            
            # 4. è®¡ç®—æœŸæœ›æ”¶ç›Šï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            expected_returns = {}
            for ticker, df in factor_data.items():
                if not df.empty and 'returns' in df.columns:
                    recent_returns = df['returns'].dropna().tail(20)
                    if len(recent_returns) > 0:
                        expected_returns[ticker] = float(recent_returns.mean() * 252)  # å¹´åŒ–
            
            if not expected_returns:
                return {'error': 'æ— æ³•è®¡ç®—æœŸæœ›æ”¶ç›Š'}
            
            # 5. ä¼˜åŒ–æŠ•èµ„ç»„åˆ
            optimization_result = self.optimize_portfolio(
                expected_returns=expected_returns,
                risk_model=risk_model,
                current_weights=current_weights
            )
            
            # 6. è¿”å›ç»¼åˆç»“æœ
            result = {
                'analysis_date': datetime.now().isoformat(),
                'universe_size': len(tickers),
                'data_coverage': len(market_data),
                'factor_coverage': len(factor_data),
                'risk_model': {
                    'type': risk_model.get('type'),
                    'estimation_date': risk_model.get('estimation_date', datetime.now()).isoformat()
                },
                'expected_returns': expected_returns,
                'optimization_result': {
                    'weights': optimization_result.weights,
                    'expected_return': optimization_result.expected_return,
                    'expected_risk': optimization_result.expected_risk,
                    'sharpe_ratio': optimization_result.sharpe_ratio,
                    'status': optimization_result.optimization_status,
                    'optimization_time': optimization_result.optimization_time
                },
                'risk_metrics': {
                    'volatility': optimization_result.risk_metrics.volatility,
                    'var_95': optimization_result.risk_metrics.var_95,
                    'sharpe_ratio': optimization_result.risk_metrics.sharpe_ratio
                }
            }
            
            self.logger.info(f"å®Œæ•´åˆ†æå®Œæˆï¼Œå¤æ™®æ¯”ç‡: {optimization_result.sharpe_ratio:.3f}")
            return result
            
        except Exception as e:
            self.logger.error(f"å®Œæ•´åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            'core_status': 'RUNNING',
            'cached_data_count': len(self.market_data_cache.prices),
            'factor_data_count': len(self.factor_data),
            'risk_models_count': len(self.risk_models),
            'config': {
                'risk_aversion': self.risk_aversion,
                'max_position': self.max_position,
                'max_turnover': self.max_turnover
            }
        }


# =============================================================================
# å·¥å‚å‡½æ•°
# =============================================================================

def create_unified_quant_core(config: Dict[str, Any] = None) -> UnifiedQuantCore:
    """åˆ›å»ºç»Ÿä¸€é‡åŒ–æ ¸å¿ƒå®ä¾‹"""
    return UnifiedQuantCore(config)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºé‡åŒ–æ ¸å¿ƒ
    core = create_unified_quant_core({
        'risk_aversion': 3.0,
        'max_position': 0.05,
        'max_turnover': 0.15
    })
    
    print("=== ç»Ÿä¸€é‡åŒ–æ ¸å¿ƒæµ‹è¯• ===")
    
    # æµ‹è¯•è‚¡ç¥¨æ± 
    tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    result = core.run_full_analysis(
        tickers=tickers,
        start_date='2024-01-01',
        end_date='2024-12-31'
    )
    
    if 'error' in result:
        print(f"åˆ†æå¤±è´¥: {result['error']}")
    else:
        print(f"åˆ†ææˆåŠŸï¼Œè¦†ç›– {result['data_coverage']} åªè‚¡ç¥¨")
        print(f"ä¼˜åŒ–çŠ¶æ€: {result['optimization_result']['status']}")
        print(f"ç»„åˆå¤æ™®æ¯”ç‡: {result['optimization_result']['sharpe_ratio']:.3f}")
        print(f"é¢„æœŸå¹´åŒ–æ”¶ç›Š: {result['optimization_result']['expected_return']:.1%}")
        print(f"é¢„æœŸå¹´åŒ–é£é™©: {result['optimization_result']['expected_risk']:.1%}")
        
        # æ˜¾ç¤ºå‰5å¤§æŒä»“
        weights = result['optimization_result']['weights']
        top_positions = sorted(weights.items(), key=lambda x: abs(x[1]), reverse=True)[:5]
        print("å‰5å¤§æŒä»“:")
        for ticker, weight in top_positions:
            print(f"  {ticker}: {weight:.1%}")
    
    # ç³»ç»ŸçŠ¶æ€
    status = core.get_status()
    print(f"ç³»ç»ŸçŠ¶æ€: {status}")
    
    print("ç»Ÿä¸€é‡åŒ–æ ¸å¿ƒæµ‹è¯•å®Œæˆ")