# åˆ†æï¼šä¸ºä»€ä¹ˆä¼šæœ‰é‡å¤æ•°æ®ï¼Ÿä¸æ˜¯åº”è¯¥æ¯æ¬¡åªè·å–ä¸€ä¸ªæ•°æ®å—ï¼Ÿ

## ğŸ” ç”¨æˆ·çš„é—®é¢˜

**ç”¨æˆ·è´¨ç–‘**: 
- ä¸æ˜¯åº”è¯¥æ¯æ¬¡åªè·å–ä¸€ä¸ªæ•°æ®å—ï¼Ÿ
- æœ‰`shift(1)`é€»è¾‘ï¼Œä¸ºä»€ä¹ˆè¿˜ä¼šæœ‰é‡å¤æ•°æ®ï¼Ÿ

---

## ğŸ“Š å…³é”®ç†è§£

### `shift(1)`çš„ä½œç”¨

**`shift(1)`åªæ˜¯ç”¨äºæ—¶é—´åºåˆ—è®¡ç®—**ï¼Œä¸ä¼šäº§ç”Ÿæˆ–æ¶ˆé™¤é‡å¤æ•°æ®ï¼š

```python
# shift(1)çš„ä½œç”¨ï¼šå°†æ•°æ®å‘åç§»åŠ¨1ä¸ªæ—¶é—´ç‚¹
raw_price_chg = grouped['Close'].transform(
    lambda x: x.pct_change(periods=30).shift(1)  # ä½¿ç”¨å‰ä¸€å¤©çš„æ•°æ®
)
```

**`shift(1)`ä¸ä¼š**:
- âŒ æ¶ˆé™¤é‡å¤çš„(date, ticker)ç»„åˆ
- âŒ æ”¹å˜æ•°æ®çš„è¡Œæ•°
- âŒ å½±å“ç´¢å¼•çš„å”¯ä¸€æ€§

**`shift(1)`åªä¼š**:
- âœ… å°†å€¼å‘åç§»åŠ¨ï¼ˆé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²ï¼‰
- âœ… ä¿æŒç›¸åŒçš„ç´¢å¼•ç»“æ„

---

## ğŸ¯ çœŸæ­£çš„é—®é¢˜ï¼šæ•°æ®æºæœ¬èº«æœ‰é‡å¤

### é—®é¢˜1: APIå¯èƒ½è¿”å›é‡å¤æ•°æ®

**ä½ç½®**: `bma_models/simple_25_factor_engine.py` line ~234-246

**ä»£ç **:
```python
df = polygon_client.get_historical_bars(
    symbol=symbol,
    start_date=start_date,
    end_date=end_date,
    timespan='day',
    multiplier=1
)
```

**å¯èƒ½çš„åŸå› **:
1. **APIè¿”å›äº†åŒä¸€tickeråœ¨åŒä¸€å¤©çš„å¤šæ¡è®°å½•**
   - ä¾‹å¦‚ï¼šä¸åŒæ—¶é—´ç‚¹çš„æ•°æ®ï¼ˆå¼€ç›˜ã€æ”¶ç›˜ã€ç›˜ä¸­ï¼‰
   - ä¾‹å¦‚ï¼šæ•°æ®æ›´æ–°å¯¼è‡´é‡å¤
   - ä¾‹å¦‚ï¼šAPI bugè¿”å›é‡å¤æ•°æ®

2. **æ—¥æœŸæ ‡å‡†åŒ–é—®é¢˜**
   - å¦‚æœæ—¥æœŸæœ‰æ—¶é—´æˆ³éƒ¨åˆ†ï¼ˆä¾‹å¦‚ï¼š`2024-01-15 09:30:00` vs `2024-01-15 16:00:00`ï¼‰
   - `dt.normalize()`ä¼šå°†å®ƒä»¬éƒ½æ ‡å‡†åŒ–ä¸º`2024-01-15`
   - ä½†å¦‚æœåŸå§‹æ•°æ®æœ‰ä¸¤æ¡è®°å½•ï¼Œæ ‡å‡†åŒ–åä»ç„¶æ˜¯ä¸¤æ¡

### é—®é¢˜2: æ•°æ®åˆå¹¶æ—¶äº§ç”Ÿé‡å¤

**ä½ç½®**: `bma_models/simple_25_factor_engine.py` line ~253

**ä»£ç **:
```python
combined = pd.concat(all_data, ignore_index=False)
combined = combined.reset_index()  # This creates 'Date' column from DatetimeIndex
```

**å¯èƒ½çš„é—®é¢˜**:
- å¦‚æœ`all_data`ä¸­çš„å¤šä¸ªDataFrameæœ‰é‡å çš„ç´¢å¼•
- `pd.concat(ignore_index=False)`ä¼šä¿ç•™æ‰€æœ‰ç´¢å¼•
- å¦‚æœåŒä¸€ä¸ªtickeråœ¨åŒä¸€å¤©æœ‰å¤šæ¡è®°å½•ï¼Œåˆå¹¶åä»ç„¶æœ‰å¤šæ¡

### é—®é¢˜3: `compute_data`æ²¡æœ‰å»é‡

**ä½ç½®**: `bma_models/simple_25_factor_engine.py` line ~345-350

**ä»£ç **:
```python
compute_data = market_data_clean.copy()
compute_data['date'] = pd.to_datetime(compute_data['date']).dt.normalize()
compute_data = compute_data.sort_values(['ticker', 'date']).reset_index(drop=True)
```

**é—®é¢˜**:
- `dt.normalize()`åªæ˜¯æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ï¼Œ**ä¸ä¼šæ¶ˆé™¤é‡å¤**
- å¦‚æœ`market_data_clean`ä¸­åŒä¸€tickeråœ¨åŒä¸€å¤©æœ‰å¤šæ¡è®°å½•
- `normalize()`åä»ç„¶æ˜¯å¤šæ¡è®°å½•
- `reset_index(drop=True)`ä¼šä¿ç•™æ‰€æœ‰è¿™äº›è®°å½•

---

## ğŸ” éªŒè¯ï¼šæ£€æŸ¥æ•°æ®æµ

### æ­¥éª¤1: æ£€æŸ¥`fetch_market_data`è¿”å›çš„æ•°æ®

**é—®é¢˜**: APIæ˜¯å¦è¿”å›äº†é‡å¤çš„(date, ticker)ç»„åˆï¼Ÿ

**æ£€æŸ¥æ–¹æ³•**:
```python
# åœ¨fetch_market_dataè¿”å›å‰
if 'date' in combined.columns and 'ticker' in combined.columns:
    duplicates = combined.duplicated(subset=['date', 'ticker'])
    if duplicates.any():
        logger.warning(f"âš ï¸ fetch_market_data returned {duplicates.sum()} duplicate (date, ticker) combinations!")
        # æ£€æŸ¥å…·ä½“å“ªäº›tickerå’Œæ—¥æœŸé‡å¤
        dup_data = combined[combined.duplicated(subset=['date', 'ticker'], keep=False)]
        logger.warning(f"âš ï¸ Duplicate examples:")
        for (date, ticker), group in dup_data.groupby(['date', 'ticker']):
            logger.warning(f"  ({date}, {ticker}): {len(group)} rows")
```

### æ­¥éª¤2: æ£€æŸ¥`compute_data`çš„é‡å¤

**é—®é¢˜**: `compute_data`æ˜¯å¦æœ‰é‡å¤ï¼Ÿ

**æ£€æŸ¥æ–¹æ³•**:
```python
# åœ¨compute_dataåˆ›å»ºå
if 'date' in compute_data.columns and 'ticker' in compute_data.columns:
    duplicates = compute_data.duplicated(subset=['date', 'ticker'])
    if duplicates.any():
        logger.warning(f"âš ï¸ compute_data has {duplicates.sum()} duplicate (date, ticker) combinations!")
        # æ£€æŸ¥å…·ä½“å“ªäº›é‡å¤
        dup_data = compute_data[compute_data.duplicated(subset=['date', 'ticker'], keep=False)]
        logger.warning(f"âš ï¸ Duplicate examples:")
        for (date, ticker), group in dup_data.groupby(['date', 'ticker']):
            logger.warning(f"  ({date}, {ticker}): {len(group)} rows")
            logger.warning(f"    Values: {group[['Close', 'Volume']].to_dict('records')}")
```

### æ­¥éª¤3: æ£€æŸ¥`factors_df`çš„é‡å¤

**é—®é¢˜**: `factors_df`æ˜¯å¦æœ‰é‡å¤ç´¢å¼•ï¼Ÿ

**æ£€æŸ¥æ–¹æ³•**:
```python
# åœ¨factors_dfè®¾ç½®MultiIndexå
duplicates = factors_df.index.duplicated()
if duplicates.any():
    logger.warning(f"âš ï¸ factors_df has {duplicates.sum()} duplicate indices!")
    # æ£€æŸ¥å…·ä½“å“ªäº›é‡å¤
    dup_indices = factors_df.index[duplicates]
    logger.warning(f"âš ï¸ Duplicate index examples:")
    for idx in dup_indices[:10]:
        logger.warning(f"  {idx}")
```

---

## ğŸ¯ æ ¹æœ¬åŸå› åˆ†æ

### ä¸ºä»€ä¹ˆä¹‹å‰æ²¡æœ‰è¿™ä¸ªé—®é¢˜ï¼Ÿ

**ä¹‹å‰ï¼ˆè®­ç»ƒ/è¯„ä¼°ï¼‰**:
- ä½¿ç”¨parquetæ–‡ä»¶ï¼ˆ`polygon_factors_all_filtered_clean_final_v2.parquet`ï¼‰
- Parquetæ–‡ä»¶åœ¨åˆ›å»ºæ—¶å·²ç»å»é‡
- æ¯ä¸ª(date, ticker)ç»„åˆåªå‡ºç°ä¸€æ¬¡
- å³ä½¿`compute_all_17_factors`æ²¡æœ‰å»é‡ï¼Œè¾“å…¥æ•°æ®æœ¬èº«æ²¡æœ‰é‡å¤

**ç°åœ¨ï¼ˆDirect Predictï¼‰**:
- ä½¿ç”¨Polygon APIå®æ—¶æ•°æ®
- APIå¯èƒ½è¿”å›é‡å¤æ•°æ®ï¼ˆåŒä¸€tickeråœ¨åŒä¸€å¤©æœ‰å¤šæ¡è®°å½•ï¼‰
- `fetch_market_data`æ²¡æœ‰å»é‡
- `compute_all_17_factors`æ²¡æœ‰å»é‡
- å¯¼è‡´æœ€ç»ˆæ•°æ®æœ‰é‡å¤

### ä¸ºä»€ä¹ˆ`shift(1)`ä¸èƒ½è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ

**`shift(1)`çš„ä½œç”¨**:
- å°†æ—¶é—´åºåˆ—æ•°æ®å‘åç§»åŠ¨1ä¸ªæ—¶é—´ç‚¹
- ç”¨äºé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²

**`shift(1)`ä¸èƒ½è§£å†³çš„é—®é¢˜**:
- âŒ ä¸èƒ½æ¶ˆé™¤é‡å¤çš„(date, ticker)ç»„åˆ
- âŒ ä¸èƒ½æ”¹å˜æ•°æ®çš„è¡Œæ•°
- âŒ ä¸èƒ½å½±å“ç´¢å¼•çš„å”¯ä¸€æ€§

**ç¤ºä¾‹**:
```python
# å‡è®¾æœ‰é‡å¤æ•°æ®ï¼š
#   (2024-01-15, AAPL): Close=150.0
#   (2024-01-15, AAPL): Close=150.5  # é‡å¤ï¼

# shift(1)åï¼š
#   (2024-01-15, AAPL): Close=149.0  # ä½¿ç”¨å‰ä¸€å¤©çš„å€¼
#   (2024-01-15, AAPL): Close=149.5  # ä»ç„¶æ˜¯é‡å¤ï¼
```

---

## âœ… è§£å†³æ–¹æ¡ˆ

### ä¿®å¤1: åœ¨`fetch_market_data`è¿”å›å‰å»é‡

**ä½ç½®**: `bma_models/simple_25_factor_engine.py` line ~210, ~260

**ä¿®æ”¹**:
```python
# åœ¨optimized_downloaderè¿”å›å
if not optimized_data.empty:
    data_with_cols = optimized_data.reset_index()
    
    # ğŸ”§ FIX: Remove duplicate (date, ticker) combinations
    if 'date' in data_with_cols.columns and 'ticker' in data_with_cols.columns:
        # Normalize dates first
        data_with_cols['date'] = pd.to_datetime(data_with_cols['date']).dt.normalize()
        
        # Remove duplicates, keep the last one (most recent data)
        duplicates = data_with_cols.duplicated(subset=['date', 'ticker'], keep='last')
        if duplicates.any():
            logger.warning(f"âš ï¸ fetch_market_data: Removing {duplicates.sum()} duplicate (date, ticker) combinations")
            data_with_cols = data_with_cols[~duplicates].reset_index(drop=True)
    
    return data_with_cols

# åœ¨legacy methodè¿”å›å‰
if all_data:
    combined = pd.concat(all_data, ignore_index=False)
    combined = combined.reset_index()
    
    if 'Date' in combined.columns:
        combined = combined.rename(columns={'Date': 'date'})
    
    # ğŸ”§ FIX: Remove duplicate (date, ticker) combinations
    if 'date' in combined.columns and 'ticker' in combined.columns:
        # Normalize dates first
        combined['date'] = pd.to_datetime(combined['date']).dt.normalize()
        
        # Remove duplicates, keep the last one (most recent data)
        duplicates = combined.duplicated(subset=['date', 'ticker'], keep='last')
        if duplicates.any():
            logger.warning(f"âš ï¸ fetch_market_data (legacy): Removing {duplicates.sum()} duplicate (date, ticker) combinations")
            combined = combined[~duplicates].reset_index(drop=True)
    
    return combined
```

### ä¿®å¤2: åœ¨`compute_data`åˆ›å»ºåç«‹å³å»é‡

**ä½ç½®**: `bma_models/simple_25_factor_engine.py` line ~350

**ä¿®æ”¹**:
```python
compute_data = market_data_clean.copy()
compute_data['date'] = pd.to_datetime(compute_data['date']).dt.normalize()
compute_data = compute_data.sort_values(['ticker', 'date']).reset_index(drop=True)

# ğŸ”§ FIX: Remove duplicate (date, ticker) combinations immediately
if 'date' in compute_data.columns and 'ticker' in compute_data.columns:
    duplicates = compute_data.duplicated(subset=['date', 'ticker'], keep='last')
    if duplicates.any():
        logger.warning(f"âš ï¸ compute_data: Removing {duplicates.sum()} duplicate (date, ticker) combinations")
        compute_data = compute_data[~duplicates].reset_index(drop=True)
    logger.info(f"âœ… compute_data after deduplication: {len(compute_data)} rows, {compute_data.groupby(['date', 'ticker']).size().shape[0]} unique (date, ticker) pairs")
```

### ä¿®å¤3: åœ¨`compute_all_17_factors`è¿”å›å‰å»é‡

**ä½ç½®**: `bma_models/simple_25_factor_engine.py` line ~572

**ä¿®æ”¹**:
```python
factors_df.index = pd.MultiIndex.from_arrays(
    [compute_data['date'], compute_data['ticker']], 
    names=['date', 'ticker']
)

# ğŸ”§ FIX: Remove duplicate indices immediately after setting MultiIndex
duplicates = factors_df.index.duplicated()
if duplicates.any():
    logger.warning(f"âš ï¸ compute_all_17_factors: Removing {duplicates.sum()} duplicate indices")
    factors_df = factors_df[~duplicates]

# Ensure each (date, ticker) combination appears only once
factors_df = factors_df.groupby(level=['date', 'ticker']).first()
logger.info(f"âœ… compute_all_17_factors: Final shape {factors_df.shape}, unique (date, ticker) pairs: {len(factors_df)}")
```

---

## ğŸ¯ æ€»ç»“

**ä¸ºä»€ä¹ˆä¼šæœ‰é‡å¤æ•°æ®ï¼Ÿ**

1. **APIå¯èƒ½è¿”å›é‡å¤æ•°æ®** - åŒä¸€tickeråœ¨åŒä¸€å¤©æœ‰å¤šæ¡è®°å½•
2. **æ•°æ®åˆå¹¶æ—¶æ²¡æœ‰å»é‡** - `pd.concat`ä¿ç•™äº†æ‰€æœ‰è®°å½•
3. **æ—¥æœŸæ ‡å‡†åŒ–ä¸ä¼šæ¶ˆé™¤é‡å¤** - `dt.normalize()`åªæ˜¯æ ¼å¼åŒ–ï¼Œä¸ä¼šå»é‡
4. **`shift(1)`ä¸èƒ½è§£å†³é‡å¤** - å®ƒåªç”¨äºæ—¶é—´åºåˆ—è®¡ç®—ï¼Œä¸å½±å“æ•°æ®è¡Œæ•°

**è§£å†³æ–¹æ¡ˆ**:
- åœ¨æ•°æ®æµçš„**æ¯ä¸ªå…³é”®èŠ‚ç‚¹**æ·»åŠ å»é‡é€»è¾‘
- ç¡®ä¿æ¯ä¸ª(date, ticker)ç»„åˆåªå‡ºç°ä¸€æ¬¡
- ç‰¹åˆ«æ˜¯åœ¨`fetch_market_data`å’Œ`compute_all_17_factors`è¿”å›å‰

---

**çŠ¶æ€**: âš ï¸ **éœ€è¦ä¿®å¤æ•°æ®è·å–å’Œå› å­è®¡ç®—çš„å»é‡é€»è¾‘**

**ä¸‹ä¸€æ­¥**: å®æ–½ä¿®å¤ï¼Œç¡®ä¿æ¯ä¸ª(date, ticker)ç»„åˆåªå‡ºç°ä¸€æ¬¡
