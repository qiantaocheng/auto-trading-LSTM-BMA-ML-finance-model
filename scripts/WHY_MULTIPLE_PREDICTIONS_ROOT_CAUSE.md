# ä¸ºä»€ä¹ˆä¼šæœ‰å¤šä¸ªé¢„æµ‹ - æ ¹æœ¬åŸå› åˆ†æ

## ğŸ” é—®é¢˜æœ¬è´¨

**ç°è±¡**: åŒä¸€ä¸ªtickeråœ¨åŒä¸€ä¸ªæ—¥æœŸå‡ºç°äº†å¤šæ¬¡é¢„æµ‹ï¼Œå¯¼è‡´Top20è¡¨æ ¼æ˜¾ç¤ºç›¸åŒè‚¡ç¥¨é‡å¤20æ¬¡ã€‚

**æ ¹æœ¬é—®é¢˜**: **æ•°æ®æºæˆ–æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­äº§ç”Ÿäº†é‡å¤çš„(date, ticker)ç»„åˆ**

---

## ğŸ“Š æ•°æ®æµåˆ†æ

### æ•°æ®æµè·¯å¾„

```
1. fetch_market_data() 
   â†“
2. compute_all_17_factors()
   â†“
3. all_feature_data (MultiIndex: date, ticker)
   â†“
4. date_feature_data = all_feature_data[date_mask]
   â†“
5. predict_with_snapshot(feature_data=date_feature_data)
   â†“
6. _prepare_standard_data_format(feature_data)
   â†“
7. X_df (MultiIndex: date, ticker)
   â†“
8. ç¬¬ä¸€å±‚æ¨¡å‹é¢„æµ‹ â†’ first_layer_preds
   â†“
9. è¿”å› predictions_raw å’Œ base_predictions
```

---

## ğŸ” å¯èƒ½çš„åŸå› 

### åŸå› 1: `compute_all_17_factors`è¿”å›äº†é‡å¤æ•°æ®

**ä½ç½®**: `bma_models/simple_25_factor_engine.py` line ~270-2000

**å¯èƒ½é—®é¢˜**:
- åœ¨è®¡ç®—å› å­æ—¶ï¼Œå¯èƒ½å¯¹åŒä¸€ä¸ªtickeråœ¨åŒä¸€ä¸ªæ—¥æœŸäº§ç”Ÿäº†å¤šæ¡è®°å½•
- ä¾‹å¦‚ï¼šå¤šä¸ªå› å­è®¡ç®—å‡½æ•°éƒ½æ·»åŠ äº†ç›¸åŒæ—¥æœŸçš„æ•°æ®
- æˆ–è€…åœ¨åˆå¹¶å› å­ç»“æœæ—¶äº§ç”Ÿäº†é‡å¤

**æ£€æŸ¥æ–¹æ³•**:
```python
# åœ¨compute_all_17_factorsè¿”å›å‰
if isinstance(all_factors, pd.DataFrame):
    duplicates = all_factors.index.duplicated()
    if duplicates.any():
        logger.warning(f"âš ï¸ compute_all_17_factors returned {duplicates.sum()} duplicate indices!")
        all_factors = all_factors[~duplicates]
```

### åŸå› 2: `_prepare_standard_data_format`å»é‡ä¸å½»åº•

**ä½ç½®**: `bma_models/é‡åŒ–æ¨¡å‹_bma_ultra_enhanced.py` line ~6624

**ä»£ç **:
```python
feature_data = feature_data[~feature_data.index.duplicated(keep='last')]
```

**é—®é¢˜**:
- è¿™ä¸ªå»é‡é€»è¾‘åº”è¯¥èƒ½ç§»é™¤é‡å¤ï¼Œä½†å¦‚æœ`feature_data`åœ¨ä¼ å…¥å‰å°±å·²ç»æœ‰é‡å¤
- æˆ–è€…å»é‡ååˆè¢«å…¶ä»–æ“ä½œé‡æ–°å¼•å…¥é‡å¤

**æ£€æŸ¥æ–¹æ³•**:
```python
# åœ¨_prepare_standard_data_formatå¼€å§‹å¤„
if isinstance(feature_data.index, pd.MultiIndex):
    duplicates_before = feature_data.index.duplicated().sum()
    if duplicates_before > 0:
        logger.warning(f"âš ï¸ feature_data has {duplicates_before} duplicate indices before _prepare_standard_data_format")
```

### åŸå› 3: `date_feature_data`æå–æ—¶äº§ç”Ÿé‡å¤

**ä½ç½®**: `autotrader/app.py` line ~1795

**ä»£ç **:
```python
date_mask = all_feature_data.index.get_level_values('date') <= pred_date
date_feature_data = all_feature_data[date_mask].copy()
```

**é—®é¢˜**:
- å¦‚æœ`all_feature_data`ä¸­åŒä¸€ä¸ªtickeråœ¨åŒä¸€ä¸ªæ—¥æœŸæœ‰å¤šæ¡è®°å½•
- `date_mask`ä¼šä¿ç•™æ‰€æœ‰è¿™äº›è®°å½•
- å¯¼è‡´`date_feature_data`ä¸­åŒä¸€ä¸ªtickeråœ¨åŒä¸€ä¸ªæ—¥æœŸå‡ºç°å¤šæ¬¡

**æ£€æŸ¥æ–¹æ³•**:
```python
# åœ¨æå–date_feature_dataå
if isinstance(date_feature_data.index, pd.MultiIndex):
    duplicates = date_feature_data.index.duplicated()
    if duplicates.any():
        self.log(f"[DirectPredict] âš ï¸ date_feature_data has {duplicates.sum()} duplicate indices!")
        ticker_level = date_feature_data.index.get_level_values('ticker')
        date_level = date_feature_data.index.get_level_values('date')
        # æ£€æŸ¥æ¯ä¸ªæ—¥æœŸçš„é‡å¤ticker
        for date in date_level.unique():
            date_mask = date_level == date
            date_tickers = ticker_level[date_mask]
            if date_tickers.duplicated().any():
                self.log(f"[DirectPredict] âš ï¸ Date {date} has {date_tickers.duplicated().sum()} duplicate tickers!")
```

### åŸå› 4: `predict_with_snapshot`å†…éƒ¨äº§ç”Ÿé‡å¤

**ä½ç½®**: `bma_models/é‡åŒ–æ¨¡å‹_bma_ultra_enhanced.py` line ~9693-9723

**é—®é¢˜**:
- `_prepare_standard_data_format`å¯èƒ½æ²¡æœ‰æ­£ç¡®å¤„ç†æ‰€æœ‰æƒ…å†µ
- Fallbacké€»è¾‘å¯èƒ½äº§ç”Ÿé‡å¤ç´¢å¼•

**æ£€æŸ¥æ–¹æ³•**:
```python
# åœ¨X_dfåˆ›å»ºå
if isinstance(X_df.index, pd.MultiIndex):
    duplicates = X_df.index.duplicated()
    if duplicates.any():
        logger.warning(f"[SNAPSHOT] âš ï¸ X_df has {duplicates.sum()} duplicate indices!")
        # æŒ‰(date, ticker)åˆ†ç»„ï¼Œå–ç¬¬ä¸€ä¸ª
        X_df = X_df.groupby(level=['date', 'ticker']).first()
```

### åŸå› 5: `first_layer_preds`æ„å»ºæ—¶äº§ç”Ÿé‡å¤

**ä½ç½®**: `bma_models/é‡åŒ–æ¨¡å‹_bma_ultra_enhanced.py` line ~9736

**é—®é¢˜**:
- `first_layer_preds = pd.DataFrame(index=X_df.index)`
- å¦‚æœ`X_df.index`æœ‰é‡å¤ï¼Œ`first_layer_preds`ä¹Ÿä¼šæœ‰é‡å¤
- æ¨¡å‹é¢„æµ‹æ—¶ï¼Œå¯¹æ¯ä¸ªé‡å¤çš„ç´¢å¼•éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªé¢„æµ‹

**æ£€æŸ¥æ–¹æ³•**:
```python
# åœ¨åˆ›å»ºfirst_layer_predså‰
if isinstance(X_df.index, pd.MultiIndex):
    duplicates = X_df.index.duplicated()
    if duplicates.any():
        logger.error(f"[SNAPSHOT] âŒ X_df has {duplicates.sum()} duplicate indices before first_layer_preds creation!")
        logger.error(f"[SNAPSHOT] âŒ This will cause duplicate predictions!")
        # å»é‡
        X_df = X_df[~duplicates]
```

---

## ğŸ¯ æœ€å¯èƒ½çš„åŸå› 

**æœ€å¯èƒ½çš„åŸå› **: **`compute_all_17_factors`è¿”å›çš„æ•°æ®ä¸­ï¼ŒåŒä¸€ä¸ªtickeråœ¨åŒä¸€ä¸ªæ—¥æœŸå‡ºç°äº†å¤šæ¬¡**

**æ ¹æœ¬åŸå› åˆ†æ**:

### åŸå› A: `compute_data`æœ¬èº«æœ‰é‡å¤çš„(date, ticker)ç»„åˆ

**ä½ç½®**: `bma_models/simple_25_factor_engine.py` line ~345-350

**ä»£ç **:
```python
compute_data = market_data_clean.copy()
compute_data['date'] = pd.to_datetime(compute_data['date']).dt.normalize()
compute_data = compute_data.sort_values(['ticker', 'date']).reset_index(drop=True)
```

**é—®é¢˜**: 
- å¦‚æœ`market_data_clean`ä¸­åŒä¸€ä¸ªtickeråœ¨åŒä¸€ä¸ªæ—¥æœŸæœ‰å¤šæ¡è®°å½•ï¼ˆä¾‹å¦‚ï¼šä¸åŒæ—¶é—´ç‚¹çš„æ•°æ®ï¼‰
- `reset_index(drop=True)`ä¼šä¿ç•™æ‰€æœ‰è¿™äº›è®°å½•
- å¯¼è‡´`compute_data`ä¸­åŒä¸€ä¸ª(date, ticker)ç»„åˆå‡ºç°å¤šæ¬¡

**æ£€æŸ¥æ–¹æ³•**:
```python
# åœ¨compute_dataåˆ›å»ºå
date_ticker_combos = compute_data.groupby(['date', 'ticker']).size()
dup_combos = date_ticker_combos[date_ticker_combos > 1]
if len(dup_combos) > 0:
    logger.warning(f"âš ï¸ compute_data has {len(dup_combos)} duplicate (date, ticker) combinations!")
```

### åŸå› B: `pd.concat`æ—¶ç´¢å¼•ä¸ä¸€è‡´

**ä½ç½®**: `bma_models/simple_25_factor_engine.py` line ~563

**ä»£ç **:
```python
factors_df = pd.concat(all_factors, axis=1)
factors_df.index = pd.MultiIndex.from_arrays(
    [compute_data['date'], compute_data['ticker']], 
    names=['date', 'ticker']
)
```

**é—®é¢˜**:
- å¦‚æœ`all_factors`ä¸­çš„å„ä¸ªDataFrameç´¢å¼•ä¸ä¸€è‡´
- `pd.concat(axis=1)`å¯èƒ½ä¼šäº§ç”Ÿé‡å¤çš„ç´¢å¼•
- ç„¶å`factors_df.index = ...`ä¼šåŸºäº`compute_data`é‡æ–°è®¾ç½®ç´¢å¼•
- å¦‚æœ`compute_data`æœ‰é‡å¤ï¼Œ`factors_df`ä¹Ÿä¼šæœ‰é‡å¤

**æ£€æŸ¥æ–¹æ³•**:
```python
# åœ¨pd.concatå‰
for i, factor_df in enumerate(all_factors):
    if isinstance(factor_df.index, pd.Index):
        duplicates = factor_df.index.duplicated()
        if duplicates.any():
            logger.warning(f"âš ï¸ Factor DataFrame {i} has {duplicates.sum()} duplicate indices!")
```

### åŸå› C: `fetch_market_data`è¿”å›äº†é‡å¤æ•°æ®

**ä½ç½®**: `bma_models/simple_25_factor_engine.py` line ~178-210

**é—®é¢˜**:
- Polygon APIå¯èƒ½è¿”å›åŒä¸€ä¸ªtickeråœ¨åŒä¸€å¤©çš„å¤šæ¡è®°å½•ï¼ˆä¾‹å¦‚ï¼šä¸åŒæ—¶é—´ç‚¹çš„æ•°æ®ï¼‰
- å¦‚æœæ•°æ®æ²¡æœ‰å»é‡ï¼Œä¼šå¯¼è‡´åç»­æ‰€æœ‰ç¯èŠ‚éƒ½æœ‰é‡å¤

**æ£€æŸ¥æ–¹æ³•**:
```python
# åœ¨fetch_market_dataè¿”å›å
if 'date' in market_data.columns and 'ticker' in market_data.columns:
    date_ticker_combos = market_data.groupby(['date', 'ticker']).size()
    dup_combos = date_ticker_combos[date_ticker_combos > 1]
    if len(dup_combos) > 0:
        logger.warning(f"âš ï¸ fetch_market_data returned {len(dup_combos)} duplicate (date, ticker) combinations!")
```

---

## âœ… ä¿®å¤å»ºè®®

### ä¿®å¤1: åœ¨`compute_data`åˆ›å»ºåç«‹å³å»é‡

**ä½ç½®**: `bma_models/simple_25_factor_engine.py` line ~350

**ä¿®æ”¹**:
```python
compute_data = market_data_clean.copy()
compute_data['date'] = pd.to_datetime(compute_data['date']).dt.normalize()
compute_data = compute_data.sort_values(['ticker', 'date']).reset_index(drop=True)

# ğŸ”§ FIX: Remove duplicate (date, ticker) combinations immediately
if 'date' in compute_data.columns and 'ticker' in compute_data.columns:
    duplicates = compute_data.duplicated(subset=['date', 'ticker'], keep='last')
    if duplicates.any():
        logger.warning(f"âš ï¸ compute_data: Removing {duplicates.sum()} duplicate (date, ticker) combinations")
        compute_data = compute_data[~duplicates].reset_index(drop=True)
    logger.info(f"âœ… compute_data after deduplication: {len(compute_data)} rows, {compute_data.groupby(['date', 'ticker']).size().shape[0]} unique (date, ticker) pairs")
```

### ä¿®å¤2: åœ¨`compute_all_17_factors`è¿”å›å‰å»é‡

**ä½ç½®**: `bma_models/simple_25_factor_engine.py` line ~563-572

**ä¿®æ”¹**:
```python
# Combine all factor DataFrames
factors_df = pd.concat(all_factors, axis=1)

# Add Close prices BEFORE setting MultiIndex to preserve alignment
factors_df['Close'] = compute_data['Close']

# Set MultiIndex using the prepared date and ticker columns
factors_df.index = pd.MultiIndex.from_arrays(
    [compute_data['date'], compute_data['ticker']], 
    names=['date', 'ticker']
)

# ğŸ”§ FIX: Remove duplicate indices immediately after setting MultiIndex
duplicates = factors_df.index.duplicated()
if duplicates.any():
    logger.warning(f"âš ï¸ compute_all_17_factors: Removing {duplicates.sum()} duplicate indices")
    factors_df = factors_df[~duplicates]

# Ensure each (date, ticker) combination appears only once
factors_df = factors_df.groupby(level=['date', 'ticker']).first()
logger.info(f"âœ… compute_all_17_factors: Final shape {factors_df.shape}, unique (date, ticker) pairs: {len(factors_df)}")
```

### ä¿®å¤2: åœ¨`date_feature_data`æå–åç«‹å³å»é‡

**ä½ç½®**: `autotrader/app.py` line ~1796

**ä¿®æ”¹**:
```python
date_feature_data = all_feature_data[date_mask].copy()

# ğŸ”§ FIX: Remove duplicate indices immediately
if isinstance(date_feature_data.index, pd.MultiIndex):
    duplicates = date_feature_data.index.duplicated()
    if duplicates.any():
        self.log(f"[DirectPredict] âš ï¸ date_feature_data has {duplicates.sum()} duplicate indices, removing...")
        date_feature_data = date_feature_data[~duplicates]
    
    # Ensure each (date, ticker) combination appears only once
    date_feature_data = date_feature_data.groupby(level=['date', 'ticker']).first()
    self.log(f"[DirectPredict] âœ… date_feature_data after deduplication: {len(date_feature_data)} rows")
```

### ä¿®å¤3: åœ¨`X_df`åˆ›å»ºåç«‹å³å»é‡

**ä½ç½®**: `bma_models/é‡åŒ–æ¨¡å‹_bma_ultra_enhanced.py` line ~9723

**ä¿®æ”¹**:
```python
X_df = X.copy()

# ğŸ”§ FIX: Remove duplicate indices immediately
if isinstance(X_df.index, pd.MultiIndex):
    duplicates = X_df.index.duplicated()
    if duplicates.any():
        logger.warning(f"[SNAPSHOT] âš ï¸ X_df has {duplicates.sum()} duplicate indices, removing...")
        X_df = X_df[~duplicates]
    
    # Ensure each (date, ticker) combination appears only once
    X_df = X_df.groupby(level=['date', 'ticker']).first()
    logger.info(f"[SNAPSHOT] âœ… X_df after deduplication: {len(X_df)} rows, {X_df.index.get_level_values('ticker').nunique()} unique tickers")
```

---

## ğŸ” è¯Šæ–­æ­¥éª¤

### æ­¥éª¤1: æ£€æŸ¥`compute_all_17_factors`çš„è¾“å‡º

åœ¨`autotrader/app.py`ä¸­æ·»åŠ ï¼š

```python
all_feature_data = engine.compute_all_17_factors(market_data, mode='predict')

# ğŸ” DIAGNOSTIC: Check for duplicates
if isinstance(all_feature_data.index, pd.MultiIndex):
    duplicates = all_feature_data.index.duplicated()
    if duplicates.any():
        self.log(f"[DirectPredict] âš ï¸ all_feature_data has {duplicates.sum()} duplicate indices!")
        ticker_level = all_feature_data.index.get_level_values('ticker')
        date_level = all_feature_data.index.get_level_values('date')
        # æ£€æŸ¥æ¯ä¸ªæ—¥æœŸçš„é‡å¤ticker
        for date in sorted(date_level.unique())[-5:]:  # æ£€æŸ¥æœ€å5ä¸ªæ—¥æœŸ
            date_mask = date_level == date
            date_tickers = ticker_level[date_mask]
            if date_tickers.duplicated().any():
                dup_count = date_tickers.duplicated().sum()
                dup_tickers = date_tickers[date_tickers.duplicated()].unique()
                self.log(f"[DirectPredict] âš ï¸ Date {date}: {dup_count} duplicate tickers: {dup_tickers[:10].tolist()}")
```

### æ­¥éª¤2: æ£€æŸ¥`date_feature_data`çš„ç»“æ„

```python
date_feature_data = all_feature_data[date_mask].copy()

# ğŸ” DIAGNOSTIC
if isinstance(date_feature_data.index, pd.MultiIndex):
    self.log(f"[DirectPredict] ğŸ“Š date_feature_data shape: {date_feature_data.shape}")
    self.log(f"[DirectPredict] ğŸ“Š date_feature_data unique dates: {date_feature_data.index.get_level_values('date').nunique()}")
    self.log(f"[DirectPredict] ğŸ“Š date_feature_data unique tickers: {date_feature_data.index.get_level_values('ticker').nunique()}")
    duplicates = date_feature_data.index.duplicated()
    if duplicates.any():
        self.log(f"[DirectPredict] âš ï¸ date_feature_data has {duplicates.sum()} duplicate indices!")
```

### æ­¥éª¤3: æ£€æŸ¥`X_df`çš„ç»“æ„

åœ¨`predict_with_snapshot`ä¸­æ·»åŠ ï¼š

```python
X_df = X.copy()

# ğŸ” DIAGNOSTIC
if isinstance(X_df.index, pd.MultiIndex):
    logger.info(f"[SNAPSHOT] ğŸ“Š X_df shape: {X_df.shape}")
    logger.info(f"[SNAPSHOT] ğŸ“Š X_df unique dates: {X_df.index.get_level_values('date').nunique()}")
    logger.info(f"[SNAPSHOT] ğŸ“Š X_df unique tickers: {X_df.index.get_level_values('ticker').nunique()}")
    duplicates = X_df.index.duplicated()
    if duplicates.any():
        logger.error(f"[SNAPSHOT] âŒ X_df has {duplicates.sum()} duplicate indices!")
        # æŒ‰æ—¥æœŸæ£€æŸ¥
        for date in X_df.index.get_level_values('date').unique()[:5]:
            date_mask = X_df.index.get_level_values('date') == date
            date_tickers = X_df.index.get_level_values('ticker')[date_mask]
            if date_tickers.duplicated().any():
                logger.error(f"[SNAPSHOT] âŒ Date {date} has {date_tickers.duplicated().sum()} duplicate tickers!")
```

---

## ğŸ¯ æ ¹æœ¬åŸå› æ€»ç»“

**ä¸ºä»€ä¹ˆä¼šæœ‰å¤šä¸ªé¢„æµ‹ï¼Ÿ**

**ç­”æ¡ˆ**: å› ä¸º**æ•°æ®æºæˆ–æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­ï¼ŒåŒä¸€ä¸ªtickeråœ¨åŒä¸€ä¸ªæ—¥æœŸå‡ºç°äº†å¤šæ¬¡**

**å¯èƒ½çš„åŸå› **:
1. **`compute_all_17_factors`è¿”å›äº†é‡å¤æ•°æ®** - æœ€å¯èƒ½
2. **`fetch_market_data`è¿”å›äº†é‡å¤æ•°æ®** - å¯èƒ½
3. **å› å­è®¡ç®—æ—¶äº§ç”Ÿäº†é‡å¤** - å¯èƒ½
4. **æ•°æ®åˆå¹¶æ—¶äº§ç”Ÿäº†é‡å¤** - å¯èƒ½

**è§£å†³æ–¹æ¡ˆ**:
- åœ¨æ•°æ®æµçš„æ¯ä¸ªå…³é”®èŠ‚ç‚¹æ·»åŠ å»é‡é€»è¾‘
- ç¡®ä¿æ¯ä¸ª(date, ticker)ç»„åˆåªå‡ºç°ä¸€æ¬¡
- æ·»åŠ è¯Šæ–­æ—¥å¿—ï¼Œå®šä½é‡å¤å‘ç”Ÿçš„å…·ä½“ç¯èŠ‚

---

**çŠ¶æ€**: âš ï¸ **éœ€è¦æ·»åŠ è¯Šæ–­æ—¥å¿—ç¡®è®¤æ ¹æœ¬åŸå› **

**ä¸‹ä¸€æ­¥**: æ·»åŠ è¯Šæ–­æ—¥å¿—ï¼Œè¿è¡ŒDirect Predictï¼ŒæŸ¥çœ‹æ—¥å¿—ç¡®è®¤é‡å¤å‘ç”Ÿåœ¨å“ªä¸ªç¯èŠ‚
