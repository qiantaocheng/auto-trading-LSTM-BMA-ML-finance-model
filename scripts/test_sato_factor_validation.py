#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sato Square Root Factor Validation Framework
ä¸¥è°¨çš„å®è¯éªŒè¯æ¶æ„ - æµ‹è¯• Sato å¹³æ–¹æ ¹å› å­æ˜¯å¦æä¾›å¢é‡ä¿¡æ¯

åŸºäºç”¨æˆ·æä¾›çš„æ¡†æ¶ï¼Œä½¿ç”¨çœŸå® MultiIndex cleaned æ•°æ®è¿›è¡ŒéªŒè¯
"""

import sys
import os
from pathlib import Path
import pandas as pd
import numpy as np
import statsmodels.api as sm
from scipy.stats import spearmanr
import warnings
warnings.filterwarnings('ignore')

# Fix Windows console encoding
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
    except:
        pass

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class SatoValidator:
    """
    Sato Square Root Factor Validator
    
    æ ¸å¿ƒç§‘å­¦é—®é¢˜ï¼š
    "Satoå¹³æ–¹æ ¹å› å­æ˜¯å¦æä¾›äº†ç°æœ‰å› å­ï¼ˆå¦‚ä¼ ç»ŸåŠ¨é‡ã€æ³¢åŠ¨ç‡ï¼‰ä¹‹å¤–çš„å¢é‡ä¿¡æ¯ï¼Ÿ"
    """
    
    def __init__(self, data_path: str = None):
        """
        Initialize validator with MultiIndex cleaned data
        
        Args:
            data_path: Path to cleaned parquet file (MultiIndex format)
        """
        if data_path is None:
            data_path = r"D:\trade\data\factor_exports\polygon_factors_all_filtered_clean.parquet"
        
        print("=" * 80)
        print("Sato Square Root Factor Validation Framework")
        print("=" * 80)
        print(f"Loading data from: {data_path}")
        
        # Load MultiIndex data
        self.df = pd.read_parquet(data_path)
        
        # Validate MultiIndex format
        if not isinstance(self.df.index, pd.MultiIndex):
            raise ValueError("Data must have MultiIndex (date, ticker) format")
        
        if self.df.index.names[0].lower() != 'date' or self.df.index.names[1].lower() not in ['ticker', 'symbol']:
            raise ValueError(f"MultiIndex must be (date, ticker), got {self.df.index.names}")
        
        # Sort by date and ticker
        self.df = self.df.sort_index()
        
        print(f"[OK] Data loaded: {self.df.shape[0]:,} rows, {self.df.shape[1]} columns")
        print(f"   Date range: {self.df.index.get_level_values('date').min()} to {self.df.index.get_level_values('date').max()}")
        print(f"   Unique tickers: {self.df.index.get_level_values(self.df.index.names[1]).nunique():,}")
        
        # Check if we need to fetch market data
        # The cleaned parquet file may only have factors, not raw price/volume
        has_price_data = any(c.lower() in ['close', 'adj_close'] for c in self.df.columns)
        has_volume_data = any(c.lower() == 'volume' for c in self.df.columns)
        
        # Use Close as adj_close
        if 'Close' in self.df.columns:
            self.df['adj_close'] = self.df['Close']
            print("[OK] Found Close price data")
        else:
            raise ValueError("Close price data not found in data file")
        
        # Handle Volume: if not present, estimate from vol_ratio_20d
        if not has_volume_data:
            print("[WARNING] Volume data not found, estimating from vol_ratio_20d...")
            if 'vol_ratio_20d' in self.df.columns:
                # vol_ratio_20d = Volume / Volume_MA20
                # For Sato factor, we mainly need relative volume
                # Estimate: use vol_ratio_20d as a proxy for relative volume
                # Set a base volume and scale by vol_ratio
                base_volume = 1_000_000  # Base volume assumption (1M shares)
                self.df['Volume'] = base_volume * (self.df['vol_ratio_20d'].fillna(1.0).clip(lower=0.1))
                print("[OK] Estimated Volume from vol_ratio_20d")
            else:
                raise ValueError(
                    "Volume data required but not found.\n"
                    "Data file must include either 'Volume' column or 'vol_ratio_20d' factor."
                )
        else:
            print("[OK] Found Volume data")
        
        print("[OK] Required columns ready")
    
    def prepare_factors(self):
        """
        ç”Ÿæˆ Sato å› å­ä»¥åŠå¯¹ç…§ç»„å› å­
        
        æ ¸å¿ƒå…¬å¼ï¼š
        Sato Momentum = sum( sign(r) * sigma * sqrt(V_rel) )
        å…¶ä¸­ï¼š
        - r: å¯¹æ•°æ”¶ç›Š
        - sigma: æ³¢åŠ¨ç‡
        - V_rel: ç›¸å¯¹æˆäº¤é‡
        """
        print("\n" + "=" * 80)
        print(">>> Generating Factors...")
        print("=" * 80)
        
        # 1. åŸºç¡€ç‰¹å¾è®¡ç®—ï¼ˆæŒ‰tickeråˆ†ç»„ï¼Œç¡®ä¿æ—¶é—´åºåˆ—æ­£ç¡®ï¼‰
        print("   Computing log returns...")
        # Calculate log returns properly for MultiIndex
        adj_close_series = self.df['adj_close']
        log_ret = adj_close_series.groupby(level=1).apply(
            lambda x: np.log(x / x.shift(1))
        )
        # Ensure proper index alignment - flatten MultiIndex if needed
        if isinstance(log_ret.index, pd.MultiIndex) and len(log_ret.index.names) > 2:
            log_ret = log_ret.droplevel(0)
        log_ret = log_ret.reindex(self.df.index)
        
        print("   Computing volatility (20D rolling)...")
        # å…³é”®ä¿®æ­£ï¼šç¡®ä¿æŒ‰tickeråˆ†ç»„è®¡ç®—ï¼Œé¿å…è‚¡ç¥¨åˆ‡æ¢æ—¶çš„æ•°æ®æ±¡æŸ“
        vol = log_ret.groupby(level=1).rolling(20).std().droplevel(0)
        vol = vol.reindex(self.df.index)
        vol = vol.fillna(method='bfill') + 1e-6  # å¡«å……NaNå¹¶æ·»åŠ æå°å€¼é˜²æ­¢é™¤é›¶
        
        print("   Computing relative volume...")
        # å…³é”®ä¿®æ­£ï¼šä¼˜å…ˆç›´æ¥ä½¿ç”¨vol_ratio_20dï¼Œé¿å…é‡å¤è®¡ç®—
        if 'vol_ratio_20d' in self.df.columns:
            # ç›´æ¥ä½¿ç”¨vol_ratio_20dä½œä¸ºç›¸å¯¹æˆäº¤é‡ï¼ˆé¿å…å…ˆä¼°ç®—Volumeå†è®¡ç®—çš„ç²¾åº¦æŸå¤±ï¼‰
            rel_vol = self.df['vol_ratio_20d'].fillna(1.0).clip(lower=0.01)  # æœ€å°ç›¸å¯¹æˆäº¤é‡0.01
            print("   Using vol_ratio_20d directly as relative volume")
        else:
            # ä»Volumeè®¡ç®—ç›¸å¯¹æˆäº¤é‡ï¼ˆä»…åœ¨vol_ratioä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰
            adv20 = self.df.groupby(level=1)['Volume'].rolling(20).mean().droplevel(0)
            adv20 = adv20.reindex(self.df.index)
            rel_vol = self.df['Volume'] / (adv20 + 1e-6)  # Avoid division by zero
            rel_vol = rel_vol.fillna(1.0).clip(lower=0.01)  # ç¡®ä¿æœ€å°å€¼
            print("   Computing relative volume from Volume")
        
        # 2. æ„å»º Sato æ ¸å¿ƒå› å­ (T+10 é¢„æµ‹ç‰ˆ)
        print("   Building Sato factor...")
        # å…³é”®ä¿®æ­£ï¼š
        # 1. æ·»åŠ clipä¿æŠ¤ï¼šé˜²æ­¢volæ¥è¿‘0æ—¶äº§ç”Ÿæ— ç©·å¤§ï¼ˆæ­»è‚¡ã€åœç‰Œç­‰ï¼‰
        # 2. ç®€åŒ–å…¬å¼ï¼šsign(x) * |x| = xï¼Œç›´æ¥ä½¿ç”¨normalized_ret
        normalized_ret = log_ret / vol  # volå·²ç»åŠ äº†1e-6ï¼Œä¸éœ€è¦å†åŠ 
        normalized_ret = normalized_ret.clip(-5, 5)  # æˆªæ–­æå€¼ï¼šé˜²æ­¢æ­»è‚¡æˆ–åœç‰Œå¯¼è‡´çš„sigma=0äº§ç”Ÿæ— é™å¤§
        sato_impact = normalized_ret * np.sqrt(rel_vol)  # ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨normalized_retï¼Œrel_volå·²ç»clipè¿‡
        
        # 10æ—¥æ»šåŠ¨æ±‚å’Œ
        sato_factor_series = sato_impact.groupby(level=1).rolling(10).sum()
        if isinstance(sato_factor_series.index, pd.MultiIndex):
            if len(sato_factor_series.index.names) > 2:
                sato_factor_series = sato_factor_series.droplevel(0)
            sato_factor_series.index = self.df.index
        self.df['factor_sato'] = sato_factor_series
        
        # 3. æ„å»ºå¯¹ç…§ç»„å› å­ï¼ˆç”¨äºæ­£äº¤åŒ–æµ‹è¯•ï¼‰
        print("   Building benchmark factors...")
        # ä¼ ç»ŸåŠ¨é‡ï¼ˆ10æ—¥æ¶¨å¹…ï¼‰
        mom_raw_series = log_ret.groupby(level=1).rolling(10).sum()
        if isinstance(mom_raw_series.index, pd.MultiIndex):
            if len(mom_raw_series.index.names) > 2:
                mom_raw_series = mom_raw_series.droplevel(0)
            mom_raw_series.index = self.df.index
        self.df['factor_mom_raw'] = mom_raw_series
        
        # çº¯æ³¢åŠ¨ç‡
        self.df['factor_vol'] = vol
        
        # 4. ç”Ÿæˆæ ‡ç­¾ (T+10 Future Return)
        print("   Computing T+10 forward returns...")
        # shift(-10) å°†æœªæ¥çš„æ”¶ç›Šå¹³ç§»åˆ°å½“å‰è¡Œ
        def calc_fwd_ret(group):
            ret = group.pct_change(10).shift(-10)
            # Filter extreme returns (likely data errors, splits, etc.)
            # Clip to reasonable range: -90% to +1000% (10x)
            ret = ret.clip(lower=-0.9, upper=10.0)
            return ret
        
        fwd_ret_series = self.df.groupby(level=1)['adj_close'].apply(calc_fwd_ret)
        # Handle MultiIndex result - remove extra level if present
        if isinstance(fwd_ret_series.index, pd.MultiIndex):
            if len(fwd_ret_series.index.names) > 2:
                fwd_ret_series = fwd_ret_series.droplevel(0)
            # Ensure same index structure as self.df
            fwd_ret_series.index = self.df.index
        
        self.df['fwd_ret_10d'] = fwd_ret_series
        
        # æ¸…æ´—ï¼šå»é™¤NaNå€¼
        initial_len = len(self.df)
        self.df = self.df.dropna(subset=['factor_sato', 'factor_mom_raw', 'factor_vol', 'fwd_ret_10d'])
        final_len = len(self.df)
        
        print(f"[OK] Factors prepared: {final_len:,} rows (dropped {initial_len - final_len:,} NaN rows)")
        print(f"   Date range after cleaning: {self.df.index.get_level_values('date').min()} to {self.df.index.get_level_values('date').max()}")
    
    def run_ic_test(self):
        """
        æµ‹è¯• 1: Information Coefficient (IC) åˆ†æ
        
        è®¡ç®—æ¯æ—¥ Rank ICï¼Œè¯„ä¼° Sato å› å­çš„é¢„æµ‹èƒ½åŠ›
        """
        print("\n" + "=" * 80)
        print(">>> Running IC Analysis (T+10)...")
        print("=" * 80)
        
        # æ¯æ—¥è®¡ç®— Rank ICï¼ˆæ¨ªæˆªé¢æ¯”è¾ƒï¼‰
        def calc_ic(group):
            if len(group) < 10:  # è‡³å°‘éœ€è¦10åªè‚¡ç¥¨
                return np.nan
            try:
                ic, pval = spearmanr(group['factor_sato'], group['fwd_ret_10d'])
                return ic
            except:
                return np.nan
        
        ic_series = self.df.groupby(level=0).apply(calc_ic)
        ic_series = ic_series.dropna()
        
        if len(ic_series) == 0:
            print("[ERROR] No valid IC values computed")
            return None
        
        mean_ic = ic_series.mean()
        std_ic = ic_series.std()
        ic_ir = mean_ic / std_ic if std_ic > 0 else 0
        positive_ratio = (ic_series > 0).mean()
        
        print(f"\n[STATS] IC Statistics:")
        print(f"   Mean Rank IC:       {mean_ic:.4f}")
        print(f"   IC Std Dev:         {std_ic:.4f}")
        print(f"   IC IR (Sharpe):     {ic_ir:.4f}")
        print(f"   Positive IC Ratio:  {positive_ratio:.1%}")
        print(f"   Valid Days:         {len(ic_series):,}")
        
        # ICåˆ†å¸ƒç»Ÿè®¡
        print(f"\n[DIST] IC Distribution:")
        print(f"   Min IC:             {ic_series.min():.4f}")
        print(f"   25th Percentile:    {ic_series.quantile(0.25):.4f}")
        print(f"   Median IC:          {ic_series.median():.4f}")
        print(f"   75th Percentile:    {ic_series.quantile(0.75):.4f}")
        print(f"   Max IC:             {ic_series.max():.4f}")
        
        return ic_series
    
    def run_orthogonality_test(self):
        """
        æµ‹è¯• 2: æ­£äº¤åŒ–æµ‹è¯•ï¼ˆæœ€é‡è¦çš„ç¯èŠ‚ï¼‰
        
        Sato å› å­æ˜¯å¦åªæ˜¯ 'æ³¢åŠ¨ç‡' æˆ– 'æ™®é€šåŠ¨é‡' çš„é©¬ç”²ï¼Ÿ
        æˆ‘ä»¬æŠŠ Sato å› å­å¯¹è¿™ä¸¤ä¸ªæ—§å› å­åšå›å½’ï¼Œå–æ®‹å·® (Residual) å†æµ‹ ICã€‚
        """
        print("\n" + "=" * 80)
        print(">>> Running Orthogonality Check...")
        print("=" * 80)
        print("   Testing if Sato factor provides unique information beyond Mom & Vol")
        
        def get_residual(group):
            """å¯¹æ¯ä¸€å¤©åšæ¨ªæˆªé¢å›å½’ï¼Œæå–æ®‹å·®"""
            if len(group) < 20:  # è‡³å°‘éœ€è¦20åªè‚¡ç¥¨åšå›å½’
                return pd.Series(np.nan, index=group.index)
            
            try:
                # Y = Sato, X = [Momentum, Volatility]
                X = group[['factor_mom_raw', 'factor_vol']].values
                X = sm.add_constant(X)  # æ·»åŠ å¸¸æ•°é¡¹
                y = group['factor_sato'].values
                
                # OLSå›å½’
                model = sm.OLS(y, X).fit()
                return pd.Series(model.resid, index=group.index)
            except Exception as e:
                return pd.Series(np.nan, index=group.index)
        
        # å¯¹æ¯ä¸€å¤©åšæ¨ªæˆªé¢å›å½’
        print("   Computing residuals (removing Mom & Vol components)...")
        residuals = self.df.groupby(level=0).apply(get_residual)
        
        # å¤„ç†MultiIndexç»“æœ
        if isinstance(residuals.index, pd.MultiIndex):
            residuals = residuals.droplevel(0)
        
        self.df['sato_pure_residual'] = residuals.reindex(self.df.index)
        
        # æµ‹è¯•"çº¯å‡€ç‰ˆ"Satoå› å­çš„IC
        def calc_pure_ic(group):
            if len(group) < 10:
                return np.nan
            try:
                ic, _ = spearmanr(group['sato_pure_residual'], group['fwd_ret_10d'])
                return ic
            except:
                return np.nan
        
        pure_ic_series = self.df.groupby(level=0).apply(calc_pure_ic)
        pure_ic_series = pure_ic_series.dropna()
        
        if len(pure_ic_series) == 0:
            print("[ERROR] No valid pure IC values computed")
            return None
        
        pure_ic_mean = pure_ic_series.mean()
        pure_ic_std = pure_ic_series.std()
        pure_ic_ir = pure_ic_mean / pure_ic_std if pure_ic_std > 0 else 0
        
        print(f"\n[STATS] Pure Sato IC Statistics (after removing Mom & Vol):")
        print(f"   Mean Pure IC:       {pure_ic_mean:.4f}")
        print(f"   Pure IC Std Dev:    {pure_ic_std:.4f}")
        print(f"   Pure IC IR:         {pure_ic_ir:.4f}")
        print(f"   Valid Days:         {len(pure_ic_series):,}")
        
        # è¯„ä¼°ç»“æœ
        print(f"\n[EVAL] Evaluation:")
        if pure_ic_mean > 0.02:
            print("   [PASS] Pure IC > 0.02")
            print("   [PASS] ç»“è®º: æœ‰æ•ˆã€‚Sato å› å­æä¾›äº†ç‹¬ç‰¹çš„ç‰©ç†å­¦Alphaã€‚")
            print("   [PASS] å¹³æ–¹æ ¹å®šå¾‹ç¡®å®æ•æ‰åˆ°äº†æ™®é€šåŠ¨é‡æ•æ‰ä¸åˆ°çš„ä¿¡æ¯")
        elif pure_ic_mean > 0.01:
            print("   [MARGINAL] 0.01 < Pure IC < 0.02")
            print("   [MARGINAL] ç»“è®º: è¾¹é™…æœ‰æ•ˆï¼Œä½†ä¿¡å·è¾ƒå¼±")
        else:
            print("   [FAIL] Pure IC < 0.01")
            print("   [FAIL] ç»“è®º: æ— æ•ˆã€‚Sato å› å­åªæ˜¯ç°æœ‰å› å­çš„çº¿æ€§ç»„åˆã€‚")
            print("   [FAIL] å¼•å…¥å¹³æ–¹æ ¹å®šå¾‹ï¼ˆâˆšVï¼‰æ²¡æœ‰å¸¦æ¥é¢å¤–ä»·å€¼")
        
        return pure_ic_series
    
    def run_decay_analysis(self):
        """
        æµ‹è¯• 3: ä¿¡å·è¡°å‡æµ‹è¯• (Sato vs Traditional)
        
        éªŒè¯ 'å¹³æ–¹æ ¹å†²å‡»' æ˜¯å¦æ¯” 'çº¿æ€§å†²å‡»' æ›´æŒä¹…
        """
        print("\n" + "=" * 80)
        print(">>> Running Signal Decay Analysis...")
        print("=" * 80)
        print("   Testing signal persistence across different horizons")
        
        horizons = [1, 5, 10, 20]
        results = {}
        
        for h in horizons:
            print(f"   Computing T+{h} forward returns...")
            # ç”Ÿæˆ T+h æ ‡ç­¾
            def calc_fwd_ret_h(group):
                ret = group.pct_change(h).shift(-h)
                # Filter extreme returns
                ret = ret.clip(lower=-0.9, upper=10.0)
                return ret
            
            col_name = f'fwd_ret_{h}d'
            fwd_ret_h = self.df.groupby(level=1)['adj_close'].apply(calc_fwd_ret_h)
            if isinstance(fwd_ret_h.index, pd.MultiIndex) and len(fwd_ret_h.index.names) > 2:
                fwd_ret_h = fwd_ret_h.droplevel(0)
            fwd_ret_h.index = self.df.index
            self.df[col_name] = fwd_ret_h
            
            # è®¡ç®— IC
            valid_df = self.df.dropna(subset=['factor_sato', col_name])
            if len(valid_df) == 0:
                results[h] = np.nan
                continue
            
            def calc_ic_h(group):
                if len(group) < 10:
                    return np.nan
                try:
                    ic, _ = spearmanr(group['factor_sato'], group[col_name])
                    return ic
                except:
                    return np.nan
            
            ic_h = valid_df.groupby(level=0).apply(calc_ic_h)
            ic_h = ic_h.dropna()
            
            if len(ic_h) > 0:
                results[h] = ic_h.mean()
            else:
                results[h] = np.nan
        
        print(f"\n[STATS] IC Decay Profile:")
        for h in horizons:
            ic_val = results[h]
            if not np.isnan(ic_val):
                print(f"   T+{h:2d} IC:  {ic_val:7.4f}")
            else:
                print(f"   T+{h:2d} IC:  {'N/A':>7}")
        
        # è¯Šæ–­
        print(f"\n[ANALYSIS] Decay Analysis:")
        valid_results = {k: v for k, v in results.items() if not np.isnan(v)}
        if len(valid_results) >= 3:
            ic_1 = valid_results.get(1, np.nan)
            ic_5 = valid_results.get(5, np.nan)
            ic_10 = valid_results.get(10, np.nan)
            ic_20 = valid_results.get(20, np.nan)
            
            if not np.isnan(ic_1) and not np.isnan(ic_5) and not np.isnan(ic_10):
                decay_5_1 = ic_5 - ic_1
                decay_10_5 = ic_10 - ic_5
                
                if abs(decay_5_1) < 0.01 and abs(decay_10_5) < 0.01:
                    print("   [PASS] éªŒè¯æˆåŠŸï¼šICåœ¨T+5å’ŒT+10ä¿æŒç¨³å®š")
                    print("   [PASS] è¿™ç¬¦åˆ'Metaorderï¼ˆå¤§å•æ‹†åˆ†ï¼‰'ç†è®º")
                    print("   [PASS] æœºæ„çš„å¤§å•æŒç»­äº†æ•°å¤©ï¼Œå¯¼è‡´ä»·æ ¼å†²å‡»åœ¨T+10ä¾ç„¶æ˜¾è‘—")
                    print("   [PASS] è¿™æ˜¯å°†å…¶æ”¾å…¥æœºå™¨å­¦ä¹ æ¨¡å‹çš„æœ€ä½³ä¿¡å·")
                elif ic_1 > 0.05 and ic_10 < 0.01:
                    print("   [WARNING] è¯Šæ–­ï¼šè¿™æ˜¯ä¸€ä¸ªé«˜é¢‘å¾®è§‚ç»“æ„å› å­ï¼Œä¸é€‚åˆT+10é¢„æµ‹")
                    print("   [WARNING] å¹³æ–¹æ ¹å®šå¾‹åœ¨è¿™é‡Œåªåæ˜ äº†åšå¸‚å•†çš„çŸ­æœŸåº“å­˜å‹åŠ›")
                else:
                    print("   [WARNING] ä¿¡å·è¡°å‡æ¨¡å¼ï¼šéœ€è¦è¿›ä¸€æ­¥åˆ†æ")
        
        return results
    
    def run_quantile_analysis(self):
        """
        æµ‹è¯• 4: åˆ†ç»„æ”¶ç›Šçš„å•è°ƒæ€§ (Quantile Monotonicity)
        
        å°† sato_pure_residual åˆ†ä¸º 5 ç»„ï¼Œæ£€æŸ¥æ”¶ç›Šçš„å•è°ƒæ€§
        """
        print("\n" + "=" * 80)
        print(">>> Running Quantile Monotonicity Analysis...")
        print("=" * 80)
        
        # ç¡®ä¿æœ‰çº¯å‡€æ®‹å·®
        if 'sato_pure_residual' not in self.df.columns:
            print("   [WARNING] Pure residual not computed, skipping quantile analysis")
            return None
        
        # æŒ‰æ—¥æœŸåˆ†ç»„ï¼Œæ¯å¤©è®¡ç®—åˆ†ä½æ•°
        def assign_quantiles(group):
            if len(group) < 50:  # è‡³å°‘éœ€è¦50åªè‚¡ç¥¨
                return pd.Series([np.nan] * len(group), index=group.index)
            
            try:
                quantiles = pd.qcut(
                    group['sato_pure_residual'],
                    q=5,
                    labels=[1, 2, 3, 4, 5],
                    duplicates='drop'
                )
                return quantiles
            except:
                return pd.Series([np.nan] * len(group), index=group.index)
        
        print("   Assigning quantiles (5 groups)...")
        self.df['quantile'] = self.df.groupby(level=0).apply(assign_quantiles).droplevel(0).reindex(self.df.index)
        
        # è®¡ç®—æ¯ç»„çš„å¹³å‡æ”¶ç›Š
        valid_df = self.df.dropna(subset=['quantile', 'fwd_ret_10d'])
        
        if len(valid_df) == 0:
            print("   [ERROR] No valid data for quantile analysis")
            return None
        
        quantile_returns = valid_df.groupby(['quantile'])['fwd_ret_10d'].agg(['mean', 'std', 'count'])
        
        print(f"\n[STATS] Quantile Returns (T+10):")
        print(f"{'Group':<8} {'Mean Return':<15} {'Std Dev':<15} {'Count':<10}")
        print("-" * 50)
        for q in sorted(valid_df['quantile'].dropna().unique()):
            q_data = quantile_returns.loc[q]
            print(f"Group {int(q):<5} {q_data['mean']:>12.4%}  {q_data['std']:>12.4%}  {int(q_data['count']):>8,}")
        
        # æ£€æŸ¥å•è°ƒæ€§
        print(f"\n[ANALYSIS] Monotonicity Check:")
        q_means = quantile_returns['mean'].sort_index()
        
        if len(q_means) >= 5:
            # Group 5 (æœ€é«˜) vs Group 1 (æœ€ä½)
            group5_return = q_means.iloc[-1]
            group1_return = q_means.iloc[0]
            spread = group5_return - group1_return
            
            print(f"   Group 5 (Highest) Return:  {group5_return:.4%}")
            print(f"   Group 1 (Lowest) Return:   {group1_return:.4%}")
            print(f"   Spread (5-1):               {spread:.4%}")
            
            if spread > 0.01:  # 1% spread
                print("   [PASS] æ˜¾è‘—çš„å•è°ƒæ€§")
                print("   [PASS] Group 5 æ”¶ç›Šæ˜¾è‘— > Group 1")
                if group1_return < 0:
                    print("   [PASS] Group 1 æ˜¾è‘—è·‘è¾“ï¼ˆåšç©ºä¿¡å·æœ‰æ•ˆï¼‰")
                print("   [PASS] Sato ç†è®ºç‰¹åˆ«æ“…é•¿é¢„æµ‹ Group 1 (ä¸‹è·Œ)")
                print("   [PASS] 'ä»·æ ¼æ¶¨ä½†é‡ä¸å¤Ÿï¼ˆåç¦»å¹³æ–¹æ ¹ï¼‰'æ˜¯æä½³çš„åšç©ºä¿¡å·")
            else:
                print("   [MARGINAL] å•è°ƒæ€§è¾ƒå¼±")
        else:
            print("   [WARNING] Insufficient quantiles for analysis")
        
        return quantile_returns
    
    def run_full_validation(self):
        """
        è¿è¡Œå®Œæ•´çš„éªŒè¯æµç¨‹
        """
        print("\n" + "=" * 80)
        print("STARTING FULL VALIDATION PIPELINE")
        print("=" * 80)
        
        # Step 1: Prepare factors
        self.prepare_factors()
        
        # Step 2: IC Test
        ic_series = self.run_ic_test()
        
        # Step 3: Orthogonality Test (Most Important)
        pure_ic_series = self.run_orthogonality_test()
        
        # Step 4: Decay Analysis
        decay_results = self.run_decay_analysis()
        
        # Step 5: Quantile Analysis
        quantile_results = self.run_quantile_analysis()
        
        # Final Summary
        print("\n" + "=" * 80)
        print("FINAL VALIDATION SUMMARY")
        print("=" * 80)
        
        if pure_ic_series is not None and len(pure_ic_series) > 0:
            pure_ic_mean = pure_ic_series.mean()
            
            print(f"\nğŸ¯ Key Metrics:")
            print(f"   Pure IC (after orthogonalization): {pure_ic_mean:.4f}")
            
            if pure_ic_mean > 0.02:
                print(f"\n[PASS] OVERALL VERDICT: PASS")
                print(f"   Sato å¹³æ–¹æ ¹å› å­æä¾›äº†æ˜¾è‘—çš„å¢é‡ä¿¡æ¯")
                print(f"   å»ºè®®ï¼šå°†æ­¤å› å­çº³å…¥æœºå™¨å­¦ä¹ æ¨¡å‹")
            elif pure_ic_mean > 0.01:
                print(f"\n[MARGINAL] OVERALL VERDICT: MARGINAL")
                print(f"   Sato å› å­æœ‰ä¸€å®šä»·å€¼ï¼Œä½†ä¿¡å·è¾ƒå¼±")
            else:
                print(f"\n[FAIL] OVERALL VERDICT: FAIL")
                print(f"   Sato å› å­æœªèƒ½æä¾›å¢é‡ä¿¡æ¯")
        
        print("\n" + "=" * 80)
        
        return {
            'ic_series': ic_series,
            'pure_ic_series': pure_ic_series,
            'decay_results': decay_results,
            'quantile_results': quantile_results
        }


def main():
    """Main execution"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Sato Square Root Factor Validation")
    parser.add_argument(
        "--data-file",
        type=str,
        default=r"D:\trade\data\factor_exports\polygon_factors_all_filtered_clean.parquet",
        help="Path to cleaned MultiIndex parquet file"
    )
    
    args = parser.parse_args()
    
    # Initialize validator
    validator = SatoValidator(data_path=args.data_file)
    
    # Run full validation
    results = validator.run_full_validation()
    
    print("\n[OK] Validation complete!")
    print(f"   Results saved in memory (can be exported if needed)")


if __name__ == "__main__":
    main()
