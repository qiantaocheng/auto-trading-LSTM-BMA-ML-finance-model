# EMA运算量减少分析：Top300 Filter vs 全量EMA

## 当前实现（全量EMA）

### 运算量分析

假设每天有 **N** 只股票（例如 N = 3000-5000）

**每只股票需要的操作：**
1. 检查/创建历史记录：`if ticker not in ema_history` - O(1) 字典查找
2. 获取历史列表：`history = ema_history[model_name][ticker]` - O(1)
3. 计算EMA：
   - 判断历史长度：O(1)
   - 3次乘法 + 2次加法：O(1)
   - 赋值：O(1)
4. 更新历史记录：
   - `history.insert(0, score_today)` - O(k)，k是历史长度（最多3）
   - `history.pop()` - O(1)

**总运算量（每天）：**
- 股票数量：N
- 每只股票：O(1) 到 O(k)，k≤3
- **总复杂度：O(N)**

**内存占用：**
- 历史记录字典：N个ticker × 每个最多3个分数 = **3N个浮点数**
- 例如：5000只股票 × 3 = **15,000个浮点数**

## Top300 Filter版本

### 运算量分析

**额外操作（排名计算）- 优化版本：**
1. 部分排序：`np.argpartition` 找到top300 - **O(N)** ✅
2. 对top300排序：O(300 log 300) ≈ O(2,400)
3. 计算排名：O(N)
4. 排名历史跟踪：最多300个ticker × 3个排名 = **900个整数**

**EMA计算：**
- 只对**连续3天都在top300**的股票计算EMA
- 假设每天有 **M** 只股票满足条件（M ≤ 300，实际可能更少）

**每只满足条件的股票：**
1. 检查历史排名：O(1)
2. 判断是否连续3天在top300：O(1)
3. 计算EMA：O(1)
4. 更新历史和排名历史：O(1)

**每只不满足条件的股票：**
1. 直接赋值原始分数：O(1)
2. **不需要EMA计算**

**总运算量（每天）- 优化版本：**
- 部分排序：O(N) ✅
- Top300排序：O(300 log 300) ≈ O(2,400)
- 排名计算：O(N)
- EMA计算：O(M)，M ≤ 300
- 其他股票赋值：O(N - M)
- **总复杂度：O(N)** ✅（优化后）

**内存占用：**
- EMA历史记录：最多300个ticker × 3个分数 = **900个浮点数**
- 排名历史记录：最多300个ticker × 3个排名 = **900个整数**
- **总内存：约1,800个数值**（vs 15,000个）

## 运算量对比

### 假设场景

假设：
- 每天股票数：**N = 4,000**
- 每天在top300的股票：**300**
- 连续3天都在top300的股票：**M = 150**（估计值）

### 当前实现（全量EMA）

| 操作 | 次数 | 复杂度 |
|------|------|--------|
| 字典查找 | 4,000 | O(1) × 4,000 |
| EMA计算 | 4,000 | O(1) × 4,000 |
| 历史更新 | 4,000 | O(k) × 4,000, k≤3 |
| **总计** | **~12,000次操作** | **O(N)** |

**内存：** 4,000 × 3 = **12,000个浮点数**

### Top300 Filter版本

| 操作 | 次数 | 复杂度 |
|------|------|--------|
| 部分排序（argpartition） | 1 | O(N) = **4,000** ✅ |
| Top300排序 | 1 | O(300 log 300) ≈ **2,400** |
| 排名计算 | 4,000 | O(1) × 4,000 |
| 排名检查 | 4,000 | O(1) × 4,000 |
| EMA计算 | 150 | O(1) × 150 |
| 历史更新 | 150 | O(1) × 150 |
| 原始分数赋值 | 3,850 | O(1) × 3,850 |
| **总计** | **~14,550次操作** | **O(N)** ✅ |

**内存：** 300 × 3 + 300 × 3 = **1,800个数值**

## 关键发现

### ❌ 运算量可能不会减少

**原因：**
1. **排序是瓶颈**：O(N log N) 比 O(N) 大
2. 对于N=4,000，排序需要约48,000次操作
3. 全量EMA只需要约12,000次操作

**但是：**

### ✅ 内存占用大幅减少

- **减少约 85%**：从12,000个浮点数减少到1,800个数值
- 对于多个模型，内存节省更明显

### ✅ 实际运行时间可能更快

**原因：**
1. **缓存友好**：只维护300个ticker的历史，内存访问更集中
2. **分支预测**：大部分股票走"原始分数"分支，CPU分支预测更准确
3. **减少字典操作**：字典更小，查找更快
4. **减少内存分配**：不需要为所有股票分配历史记录

## 优化建议

### 方案1：优化排序（如果N很大）

如果N > 10,000，可以考虑：
- 使用部分排序（只排序top300）
- 使用numpy的`argpartition`：O(N)而不是O(N log N)

```python
# 优化：只排序top300
top300_indices = group['prediction'].nlargest(300).index
# 或者使用numpy
top300_indices = np.argpartition(group['prediction'].values, -300)[-300:]
```

### 方案2：混合策略

- 如果N < 2,000：使用全量EMA（排序开销大）
- 如果N > 2,000：使用Top300 filter（内存和实际运行时间优势）

### 方案3：增量排名计算

- 维护一个排序的分数列表
- 每天只更新变化的部分
- 复杂度可以降到接近O(N)

## 实际测试建议

### 测试指标

1. **运行时间**：
   - 全量EMA：记录总时间
   - Top300 filter：记录总时间
   - 对比

2. **内存占用**：
   - 全量EMA：记录峰值内存
   - Top300 filter：记录峰值内存
   - 对比

3. **EMA覆盖率**：
   - 每天有多少股票应用了EMA
   - 连续3天都在top300的股票比例

### 预期结果

**对于N = 4,000的场景：**

| 指标 | 全量EMA | Top300 Filter | 改善 |
|------|---------|---------------|------|
| 运算次数 | ~12,000 | ~60,000 | ❌ 增加 |
| 内存占用 | 12,000 | 1,800 | ✅ **-85%** |
| 实际运行时间 | T | ~0.8T - 1.2T | ⚠️ 可能稍快或稍慢 |
| EMA质量 | 中等 | 高 | ✅ **提升** |

## 结论（优化后）

### 运算量（操作次数）
- ⚠️ **略增约21%**（从12,000到14,550）
- 但复杂度相同：O(N)
- 实际运行时间可能更快（缓存友好）

### 内存占用
- ✅ **大幅减少**（约85%）
- 对于多个模型，节省更明显

### 实际运行时间
- ⚠️ **可能稍快或稍慢**，取决于：
  - 股票数量N
  - CPU缓存性能
  - 内存访问模式

### EMA质量
- ✅ **显著提升**
- 只对稳定高质量股票应用EMA

## 推荐

**建议使用Top300 Filter（优化版本），原因：**
1. ✅ 内存占用大幅减少（85%）
2. ✅ EMA质量显著提升
3. ✅ 运算量略增但复杂度相同（O(N)）
4. ✅ 实际运行时间可能更快（缓存友好、分支预测更好）
5. ✅ 已优化排序部分（使用numpy argpartition）

**如果N很大（>10,000），建议：**
- 使用部分排序优化
- 或者使用增量排名计算
