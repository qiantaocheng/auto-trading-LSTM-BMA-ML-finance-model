# Direct Predictæ•°æ®æµç¨‹æ½œåœ¨é—®é¢˜åˆ†æ

## ğŸ” æ½œåœ¨é—®é¢˜æ¸…å•

### âš ï¸ é—®é¢˜1: æ—¥æœŸè¿‡æ»¤é€»è¾‘å¯èƒ½ä¸å‡†ç¡®

**ä½ç½®**: `autotrader/app.py` line ~1873

**é—®é¢˜æè¿°**:
```python
date_mask = all_feature_data.index.get_level_values('date') <= pred_date
date_feature_data = all_feature_data[date_mask].copy()
```

**æ½œåœ¨é—®é¢˜**:
- `pred_date`æ˜¯"æœ€åæœ‰æ”¶ç›˜æ•°æ®çš„äº¤æ˜“æ—¥"
- å¦‚æœ`pred_date`æ˜¯T-0ï¼ˆä»Šå¤©ï¼‰ï¼Œé‚£ä¹ˆåº”è¯¥é¢„æµ‹T+10
- å¦‚æœ`pred_date`æ˜¯T-1ï¼ˆæ˜¨å¤©ï¼‰ï¼Œé‚£ä¹ˆåº”è¯¥é¢„æµ‹T+9
- ä½†ä»£ç ä¸­ä½¿ç”¨`<= pred_date`ï¼Œè¿™æ„å‘³ç€åŒ…å«`pred_date`å½“å¤©çš„æ•°æ®
- **é—®é¢˜**: å¦‚æœ`pred_date`æ˜¯T-0ï¼Œä½†T-0çš„æ•°æ®å¯èƒ½ä¸å®Œæ•´ï¼ˆæ”¶ç›˜ä»·å¯èƒ½è¿˜æœªç¡®å®šï¼‰ï¼Œè¿™å¯èƒ½å¯¼è‡´ä½¿ç”¨ä¸å®Œæ•´çš„æ•°æ®è¿›è¡Œé¢„æµ‹

**å½±å“**: 
- å¯èƒ½ä½¿ç”¨ä¸å®Œæ•´çš„æ•°æ®è¿›è¡Œé¢„æµ‹
- é¢„æµ‹æ—¥æœŸå¯èƒ½ä¸å‡†ç¡®

**å»ºè®®ä¿®å¤**:
```python
# æ˜ç¡®ä½¿ç”¨T-1çš„æ•°æ®ï¼ˆæœ€åå®Œæ•´äº¤æ˜“æ—¥ï¼‰
# å¦‚æœpred_dateæ˜¯T-0ï¼Œåº”è¯¥ä½¿ç”¨T-1çš„æ•°æ®
if pred_date == pd.Timestamp.now().normalize():
    # å¦‚æœpred_dateæ˜¯ä»Šå¤©ï¼Œä½¿ç”¨æ˜¨å¤©çš„æ•°æ®
    pred_date = pred_date - pd.Timedelta(days=1)
    # æ‰¾åˆ°æœ€è¿‘çš„äº¤æ˜“æ—¥
    while pred_date not in all_feature_data.index.get_level_values('date'):
        pred_date = pred_date - pd.Timedelta(days=1)

date_mask = all_feature_data.index.get_level_values('date') <= pred_date
date_feature_data = all_feature_data[date_mask].copy()
```

---

### âš ï¸ é—®é¢˜2: Satoå› å­reindexå¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±

**ä½ç½®**: `autotrader/app.py` line ~1783-1784

**é—®é¢˜æè¿°**:
```python
all_feature_data['feat_sato_momentum_10d'] = sato_factors_df['feat_sato_momentum_10d'].reindex(all_feature_data.index).fillna(0.0)
all_feature_data['feat_sato_divergence_10d'] = sato_factors_df['feat_sato_divergence_10d'].reindex(all_feature_data.index).fillna(0.0)
```

**æ½œåœ¨é—®é¢˜**:
- `reindex`å¯èƒ½å¯¼è‡´æŸäº›tickerçš„Satoå› å­ç¼ºå¤±
- ä½¿ç”¨`fillna(0.0)`å¡«å……ç¼ºå¤±å€¼å¯èƒ½ä¸æ­£ç¡®
- å¦‚æœæŸä¸ªtickeråœ¨`sato_factors_df`ä¸­ä¸å­˜åœ¨ï¼Œä¼šè¢«å¡«å……ä¸º0.0ï¼Œè¿™å¯èƒ½ä¸æ˜¯æ­£ç¡®çš„å€¼

**å½±å“**:
- Satoå› å­å¯èƒ½ä¸å‡†ç¡®
- æŸäº›tickerçš„Satoå› å­å¯èƒ½è¢«é”™è¯¯åœ°è®¾ç½®ä¸º0.0

**å»ºè®®ä¿®å¤**:
```python
# ç¡®ä¿sato_factors_dfå’Œall_feature_dataçš„ç´¢å¼•å¯¹é½
# åªå¡«å……çœŸæ­£ç¼ºå¤±çš„å€¼ï¼Œè€Œä¸æ˜¯æ‰€æœ‰reindexåçš„NaN
if isinstance(sato_factors_df.index, pd.MultiIndex):
    # ä½¿ç”¨mergeè€Œä¸æ˜¯reindexï¼Œä¿ç•™åŸå§‹å€¼
    sato_momentum = sato_factors_df['feat_sato_momentum_10d'].reindex(all_feature_data.index)
    sato_divergence = sato_factors_df['feat_sato_divergence_10d'].reindex(all_feature_data.index)
    
    # åªå¡«å……çœŸæ­£ç¼ºå¤±çš„å€¼ï¼ˆåœ¨sato_factors_dfä¸­ä¸å­˜åœ¨çš„tickerï¼‰
    # å¯¹äºå­˜åœ¨çš„tickerï¼Œå¦‚æœå€¼ä¸ºNaNï¼Œå¯èƒ½æ˜¯è®¡ç®—é”™è¯¯ï¼Œåº”è¯¥è­¦å‘Š
    missing_mask = ~sato_factors_df.index.isin(all_feature_data.index)
    if missing_mask.any():
        logger.warning(f"âš ï¸ {missing_mask.sum()} tickers in sato_factors_df not in all_feature_data")
    
    all_feature_data['feat_sato_momentum_10d'] = sato_momentum.fillna(0.0)
    all_feature_data['feat_sato_divergence_10d'] = sato_divergence.fillna(0.0)
```

---

### âš ï¸ é—®é¢˜3: base_predictionså¯¹é½é€»è¾‘å¤æ‚ä¸”å¯èƒ½å¤±è´¥

**ä½ç½®**: `autotrader/app.py` line ~2012-2015

**é—®é¢˜æè¿°**:
```python
base_predictions_aligned = base_predictions.reindex(pred_df.index)
if base_predictions_aligned.isna().any().any():
    # Try to align by ticker
    base_predictions_aligned = base_predictions.reindex(pred_df.index.get_level_values('ticker'))
    base_predictions_aligned.index = pred_df.index
```

**æ½œåœ¨é—®é¢˜**:
- å¯¹é½é€»è¾‘å¤æ‚ï¼Œæœ‰å¤šä¸ªfallback
- å¦‚æœç¬¬ä¸€æ¬¡`reindex`å¤±è´¥ï¼Œå°è¯•æŒ‰tickerå¯¹é½ï¼Œä½†å¯èƒ½ä»ç„¶å¤±è´¥
- æ²¡æœ‰æ˜ç¡®çš„é”™è¯¯å¤„ç†ï¼Œå¯èƒ½å¯¼è‡´é™é»˜å¤±è´¥

**å½±å“**:
- base_predictionså¯èƒ½æ— æ³•æ­£ç¡®å¯¹é½
- Top20è¡¨æ ¼å¯èƒ½æ˜¾ç¤ºé”™è¯¯çš„æ•°æ®

**å»ºè®®ä¿®å¤**:
```python
# æ˜ç¡®çš„å¯¹é½é€»è¾‘
if isinstance(base_predictions.index, pd.MultiIndex):
    # å¦‚æœbase_predictionså·²ç»æ˜¯MultiIndexï¼Œç›´æ¥å¯¹é½
    base_predictions_aligned = base_predictions.reindex(pred_df.index)
    
    # æ£€æŸ¥å¯¹é½ç»“æœ
    missing_count = base_predictions_aligned.isna().sum().sum()
    if missing_count > 0:
        logger.warning(f"âš ï¸ {missing_count} values missing after alignment")
        # å°è¯•æŒ‰tickerå¯¹é½ï¼ˆå¦‚æœæ—¥æœŸä¸åŒ¹é…ï¼‰
        if base_predictions.index.nlevels == 2:
            # å‡è®¾base_predictionsæ˜¯(date, ticker)æ ¼å¼
            base_predictions_aligned = base_predictions.reindex(pred_df.index, method='nearest')
        else:
            # å¦‚æœæ ¼å¼ä¸åŒ¹é…ï¼Œå°è¯•æŒ‰tickerå¯¹é½
            base_predictions_by_ticker = base_predictions.groupby(level='ticker').last()
            base_predictions_aligned = base_predictions_by_ticker.reindex(pred_df.index.get_level_values('ticker'))
            base_predictions_aligned.index = pred_df.index
else:
    # å¦‚æœbase_predictionsä¸æ˜¯MultiIndexï¼Œå°è¯•è½¬æ¢
    if 'ticker' in base_predictions.index.names if isinstance(base_predictions.index, pd.MultiIndex) else False:
        base_predictions_aligned = base_predictions.reindex(pred_df.index)
    else:
        raise ValueError(f"Cannot align base_predictions: unexpected index format {type(base_predictions.index)}")
```

---

### âš ï¸ é—®é¢˜4: ç‰¹å¾ç¼ºå¤±å¡«å……å¯èƒ½ä¸æ­£ç¡®

**ä½ç½®**: `bma_models/é‡åŒ–æ¨¡å‹_bma_ultra_enhanced.py` line ~5506-5510

**é—®é¢˜æè¿°**:
```python
missing = [c for c in cols if c not in X_use.columns]
for c in missing:
    X_use[c] = 0.0
```

**æ½œåœ¨é—®é¢˜**:
- ç¼ºå¤±ç‰¹å¾è¢«å¡«å……ä¸º0.0ï¼Œä½†ä¹‹å‰æˆ‘ä»¬ä¿®å¤è¿‡åº”è¯¥ä½¿ç”¨ä¸­ä½æ•°å¡«å……
- è¿™ä¸ªä¿®å¤å¯èƒ½åªåº”ç”¨äºæŸäº›æ¨¡å‹ï¼Œè€Œä¸æ˜¯æ‰€æœ‰æ¨¡å‹
- å¯¹äºæŸäº›ç‰¹å¾ï¼Œ0.0å¯èƒ½ä¸æ˜¯åˆç†çš„é»˜è®¤å€¼

**å½±å“**:
- é¢„æµ‹å¯èƒ½ä¸å‡†ç¡®
- æŸäº›æ¨¡å‹çš„é¢„æµ‹å¯èƒ½ä¸å…¶ä»–æ¨¡å‹ä¸ä¸€è‡´

**å»ºè®®ä¿®å¤**:
```python
# ä½¿ç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±ç‰¹å¾ï¼ˆä¸ä¹‹å‰çš„ä¿®å¤ä¸€è‡´ï¼‰
missing = [c for c in cols if c not in X_use.columns]
if missing:
    logger.warning(f"âš ï¸ Missing features for {model_name}: {missing}")
    # ä½¿ç”¨cross-sectional medianå¡«å……ï¼ˆå¦‚æœå¯èƒ½ï¼‰
    if len(X_use) > 0:
        for c in missing:
            # å°è¯•ä½¿ç”¨cross-sectional median
            if c in X.columns:
                median_val = X[c].median()
                X_use[c] = median_val if not pd.isna(median_val) else 0.0
            else:
                X_use[c] = 0.0
    else:
        for c in missing:
            X_use[c] = 0.0
```

---

### âš ï¸ é—®é¢˜5: æ—¥æœŸæ ‡å‡†åŒ–å¯èƒ½æœ‰æ—¶åŒºé—®é¢˜

**ä½ç½®**: å¤šä¸ªä½ç½®ï¼ˆ`autotrader/app.py` line ~1818, `bma_models/é‡åŒ–æ¨¡å‹_bma_ultra_enhanced.py` line ~6634ï¼‰

**é—®é¢˜æè¿°**:
```python
date_normalized = pd.to_datetime(date_level).dt.tz_localize(None).dt.normalize()
```

**æ½œåœ¨é—®é¢˜**:
- `tz_localize(None)`ç§»é™¤æ—¶åŒºä¿¡æ¯ï¼Œä½†å¦‚æœåŸå§‹æ•°æ®æœ‰æ—¶åŒºï¼Œå¯èƒ½å¯¼è‡´æ—¥æœŸåç§»
- å¦‚æœåŸå§‹æ•°æ®æ˜¯UTCï¼Œè½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´å¯èƒ½å¯¼è‡´æ—¥æœŸå˜åŒ–

**å½±å“**:
- æ—¥æœŸå¯èƒ½ä¸å‡†ç¡®
- å¯èƒ½å¯¼è‡´æ•°æ®å¯¹é½é—®é¢˜

**å»ºè®®ä¿®å¤**:
```python
# æ˜ç¡®å¤„ç†æ—¶åŒº
if date_level.dt.tz is not None:
    # å¦‚æœæœ‰æ—¶åŒºï¼Œå…ˆè½¬æ¢ä¸ºUTCï¼Œå†ç§»é™¤æ—¶åŒº
    date_normalized = pd.to_datetime(date_level).dt.tz_convert('UTC').dt.tz_localize(None).dt.normalize()
else:
    # å¦‚æœæ²¡æœ‰æ—¶åŒºï¼Œç›´æ¥æ ‡å‡†åŒ–
    date_normalized = pd.to_datetime(date_level).dt.normalize()
```

---

### âš ï¸ é—®é¢˜6: é‡å¤æ•°æ®ç§»é™¤å¯èƒ½ä¸å½»åº•

**ä½ç½®**: å¤šä¸ªä½ç½®ï¼ˆ`bma_models/simple_25_factor_engine.py` line ~636-646, `autotrader/app.py` line ~1880-1888ï¼‰

**é—®é¢˜æè¿°**:
- è™½ç„¶æœ‰å¤šå¤„ç§»é™¤é‡å¤æ•°æ®çš„é€»è¾‘ï¼Œä½†å¯èƒ½åœ¨æŸä¸ªæ­¥éª¤åé‡æ–°å¼•å…¥é‡å¤
- ä¾‹å¦‚ï¼Œåœ¨æ·»åŠ Satoå› å­åï¼Œå¯èƒ½å¼•å…¥é‡å¤ç´¢å¼•

**æ½œåœ¨é—®é¢˜**:
- é‡å¤æ•°æ®å¯èƒ½åœ¨æŸä¸ªæ­¥éª¤åé‡æ–°å‡ºç°
- æ²¡æœ‰åœ¨æ‰€æœ‰å…³é”®æ­¥éª¤åéƒ½æ£€æŸ¥é‡å¤

**å½±å“**:
- å¯èƒ½å¯¼è‡´é¢„æµ‹ä¸å‡†ç¡®
- Top20è¡¨æ ¼å¯èƒ½æ˜¾ç¤ºé‡å¤çš„ticker

**å»ºè®®ä¿®å¤**:
```python
# åœ¨æ‰€æœ‰å…³é”®æ­¥éª¤åéƒ½æ£€æŸ¥å¹¶ç§»é™¤é‡å¤
def ensure_no_duplicates(df, stage_name):
    """ç¡®ä¿DataFrameæ²¡æœ‰é‡å¤ç´¢å¼•"""
    duplicates = df.index.duplicated()
    if duplicates.any():
        dup_count = duplicates.sum()
        logger.warning(f"âš ï¸ {stage_name}: Removing {dup_count} duplicate indices")
        df = df[~duplicates]
        df = df.groupby(level=['date', 'ticker']).first()
    return df

# åœ¨æ¯ä¸ªå…³é”®æ­¥éª¤åè°ƒç”¨
all_feature_data = ensure_no_duplicates(all_feature_data, "after compute_all_17_factors")
all_feature_data = ensure_no_duplicates(all_feature_data, "after adding Sato factors")
date_feature_data = ensure_no_duplicates(date_feature_data, "after date filtering")
```

---

### âš ï¸ é—®é¢˜7: ç‰¹å¾åˆ—é¡ºåºå¯èƒ½ä¸åŒ¹é…

**ä½ç½®**: `bma_models/é‡åŒ–æ¨¡å‹_bma_ultra_enhanced.py` line ~5511-5520

**é—®é¢˜æè¿°**:
```python
X_use = X_use[list(cols)]
# å°è¯•å¯¹é½åˆ°è®­ç»ƒæ—¶æ¨¡å‹æ¥æ”¶çš„ç‰¹å¾é¡ºåº
try:
    expected_names = getattr(model, 'feature_names_in_', None)
    if expected_names is not None and len(expected_names) > 0:
        available_expected = [name for name in expected_names if name in X_use.columns]
        if len(available_expected) == len(expected_names):
            # æ‰€æœ‰æœŸæœ›ç‰¹å¾éƒ½å­˜åœ¨ï¼Œé‡æ’åº
```

**æ½œåœ¨é—®é¢˜**:
- ç‰¹å¾åˆ—é¡ºåºå¯èƒ½ä¸åŒ¹é…è®­ç»ƒæ—¶çš„é¡ºåº
- è™½ç„¶ä»£ç å°è¯•é‡æ’åºï¼Œä½†é€»è¾‘å¯èƒ½ä¸å®Œæ•´
- å¦‚æœæŸäº›ç‰¹å¾åœ¨è®­ç»ƒæ—¶è¢«åˆ é™¤ï¼ˆå¦‚å…±çº¿æ€§ç‰¹å¾ï¼‰ï¼Œå¯èƒ½å¯¼è‡´ä¸åŒ¹é…

**å½±å“**:
- é¢„æµ‹å¯èƒ½ä¸å‡†ç¡®
- æŸäº›æ¨¡å‹å¯èƒ½æ— æ³•æ­£ç¡®é¢„æµ‹

**å»ºè®®ä¿®å¤**:
```python
# æ˜ç¡®å¤„ç†ç‰¹å¾é¡ºåºå’Œç¼ºå¤±ç‰¹å¾
expected_names = getattr(model, 'feature_names_in_', None)
if expected_names is not None and len(expected_names) > 0:
    # æ£€æŸ¥å“ªäº›ç‰¹å¾åœ¨è®­ç»ƒæ—¶å­˜åœ¨ä½†ç°åœ¨ç¼ºå¤±
    missing_training_features = [name for name in expected_names if name not in X_use.columns]
    if missing_training_features:
        logger.warning(f"âš ï¸ Missing training features: {missing_training_features}")
        # ä½¿ç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±ç‰¹å¾
        for name in missing_training_features:
            if name in X.columns:
                median_val = X[name].median()
                X_use[name] = median_val if not pd.isna(median_val) else 0.0
            else:
                X_use[name] = 0.0
    
    # é‡æ’åºåˆ°è®­ç»ƒæ—¶çš„é¡ºåº
    available_expected = [name for name in expected_names if name in X_use.columns]
    if len(available_expected) == len(expected_names):
        X_use = X_use[available_expected]
    else:
        logger.warning(f"âš ï¸ Feature count mismatch: expected {len(expected_names)}, got {len(available_expected)}")
        # ä½¿ç”¨å¯ç”¨ç‰¹å¾ï¼Œä½†ä¿æŒé¡ºåº
        X_use = X_use[[name for name in expected_names if name in X_use.columns]]
```

---

## ğŸ“Š é—®é¢˜ä¼˜å…ˆçº§

### ğŸ”´ é«˜ä¼˜å…ˆçº§ï¼ˆå¯èƒ½ä¸¥é‡å½±å“é¢„æµ‹å‡†ç¡®æ€§ï¼‰

1. **é—®é¢˜1: æ—¥æœŸè¿‡æ»¤é€»è¾‘** - å¯èƒ½å¯¼è‡´ä½¿ç”¨ä¸å®Œæ•´æ•°æ®
2. **é—®é¢˜4: ç‰¹å¾ç¼ºå¤±å¡«å……** - å¯èƒ½å¯¼è‡´é¢„æµ‹ä¸å‡†ç¡®
3. **é—®é¢˜7: ç‰¹å¾åˆ—é¡ºåº** - å¯èƒ½å¯¼è‡´æ¨¡å‹é¢„æµ‹é”™è¯¯

### ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼ˆå¯èƒ½å½±å“æ•°æ®è´¨é‡ï¼‰

4. **é—®é¢˜2: Satoå› å­reindex** - å¯èƒ½å½±å“Satoå› å­å‡†ç¡®æ€§
5. **é—®é¢˜6: é‡å¤æ•°æ®ç§»é™¤** - å¯èƒ½å½±å“Top20è¡¨æ ¼

### ğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼ˆå¯èƒ½å½±å“ä»£ç å¥å£®æ€§ï¼‰

6. **é—®é¢˜3: base_predictionså¯¹é½** - å¯èƒ½å½±å“Top20è¡¨æ ¼æ˜¾ç¤º
7. **é—®é¢˜5: æ—¥æœŸæ ‡å‡†åŒ–æ—¶åŒº** - å¯èƒ½å½±å“æ—¥æœŸå‡†ç¡®æ€§ï¼ˆå¦‚æœæœ‰æ—¶åŒºé—®é¢˜ï¼‰

---

## ğŸ¯ å»ºè®®ä¿®å¤é¡ºåº

1. **é¦–å…ˆä¿®å¤**: é—®é¢˜1ï¼ˆæ—¥æœŸè¿‡æ»¤é€»è¾‘ï¼‰- ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æ•°æ®
2. **å…¶æ¬¡ä¿®å¤**: é—®é¢˜4ï¼ˆç‰¹å¾ç¼ºå¤±å¡«å……ï¼‰- ç¡®ä¿é¢„æµ‹å‡†ç¡®æ€§
3. **ç„¶åä¿®å¤**: é—®é¢˜7ï¼ˆç‰¹å¾åˆ—é¡ºåºï¼‰- ç¡®ä¿æ¨¡å‹æ­£ç¡®é¢„æµ‹
4. **æœ€åä¿®å¤**: å…¶ä»–é—®é¢˜ï¼ˆé—®é¢˜2, 3, 5, 6ï¼‰- æé«˜ä»£ç å¥å£®æ€§

---

## ğŸ“ æ€»ç»“

**å‘ç°çš„é—®é¢˜æ•°**: 7ä¸ª

**é«˜ä¼˜å…ˆçº§é—®é¢˜**: 3ä¸ª
**ä¸­ä¼˜å…ˆçº§é—®é¢˜**: 2ä¸ª
**ä½ä¼˜å…ˆçº§é—®é¢˜**: 2ä¸ª

**å»ºè®®**: ä¼˜å…ˆä¿®å¤é«˜ä¼˜å…ˆçº§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯æ—¥æœŸè¿‡æ»¤é€»è¾‘å’Œç‰¹å¾ç¼ºå¤±å¡«å……ï¼Œè¿™äº›å¯èƒ½ä¸¥é‡å½±å“é¢„æµ‹å‡†ç¡®æ€§ã€‚

---

**åˆ†ææ—¶é—´**: 2025-01-20
