# åˆ†æï¼šæ˜¯å¦å› ä¸ºè·å–äº†å‘¨æœ«æˆ–å‡æœŸæ•°æ®å¯¼è‡´é‡å¤ï¼Ÿ

## ğŸ” ç”¨æˆ·çš„é—®é¢˜

**ç”¨æˆ·é—®**: "would that be because fetch weekend day's data or holiday's data?"

**é—®é¢˜**: æ˜¯å¦å› ä¸ºè·å–äº†å‘¨æœ«æˆ–å‡æœŸçš„æ•°æ®å¯¼è‡´é‡å¤çš„(date, ticker)ç»„åˆï¼Ÿ

---

## ğŸ“Š Polygon APIè¡Œä¸ºåˆ†æ

### Polygon APIçš„`timespan='day'`è¡Œä¸º

**ä»£ç ä½ç½®**: `polygon_client.py` line ~433-506

```python
df = polygon_client.get_historical_bars(
    symbol=symbol,
    timespan='day',  # æ¯å¤©ä¸€æ¡æ•°æ®
    multiplier=1
)
```

**Polygon APIæ–‡æ¡£è¯´æ˜**:
- `timespan='day'`åº”è¯¥**åªè¿”å›äº¤æ˜“æ—¥çš„æ•°æ®**
- å‘¨æœ«å’Œå‡æœŸ**ä¸åº”è¯¥**æœ‰æ•°æ®
- æ¯å¤©æ¯ä¸ªtickeråº”è¯¥**åªæœ‰ä¸€æ¡è®°å½•**

**ä½†æ˜¯**: APIå¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹è¿”å›éäº¤æ˜“æ—¥æ•°æ®ï¼Œæˆ–è€…è¿”å›åŒä¸€å¤©çš„å¤šæ¡è®°å½•ã€‚

---

## ğŸ¯ å¯èƒ½çš„æƒ…å†µ

### æƒ…å†µ1: APIè¿”å›äº†å‘¨æœ«/å‡æœŸæ•°æ®

**å¦‚æœAPIè¿”å›äº†å‘¨æœ«æ•°æ®**:
```python
# APIè¿”å›:
#   2024-01-13 (Saturday), AAPL, Close=150.0
#   2024-01-15 (Monday), AAPL, Close=150.5

# normalize()å:
#   2024-01-13, AAPL, Close=150.0
#   2024-01-15, AAPL, Close=150.5

# ç»“æœ: ä¸ä¼šäº§ç”Ÿé‡å¤ï¼å› ä¸ºæ—¥æœŸä¸åŒ
```

**ç»“è®º**: å‘¨æœ«/å‡æœŸæ•°æ®**ä¸ä¼š**å¯¼è‡´é‡å¤ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ä¸åŒçš„æ—¥æœŸã€‚

### æƒ…å†µ2: APIè¿”å›äº†åŒä¸€å¤©çš„å¤šæ¡è®°å½•

**å¦‚æœAPIè¿”å›äº†åŒä¸€å¤©çš„å¤šæ¡è®°å½•**:
```python
# APIè¿”å›:
#   2024-01-15 09:30:00, AAPL, Close=150.0
#   2024-01-15 16:00:00, AAPL, Close=150.5  # åŒä¸€å¤©ï¼Œä¸åŒæ—¶é—´

# normalize()å:
#   2024-01-15, AAPL, Close=150.0
#   2024-01-15, AAPL, Close=150.5  # é‡å¤ï¼

# ç»“æœ: ä¼šäº§ç”Ÿé‡å¤çš„(date, ticker)ç»„åˆ
```

**ç»“è®º**: å¦‚æœAPIè¿”å›äº†**åŒä¸€å¤©çš„å¤šæ¡è®°å½•**ï¼ˆä¸åŒæ—¶é—´æˆ³ï¼‰ï¼Œæ ‡å‡†åŒ–åä¼šå˜æˆé‡å¤ã€‚

### æƒ…å†µ3: æ•°æ®åˆå¹¶æ—¶äº§ç”Ÿé‡å¤

**å¦‚æœå¤šä¸ªDataFrameæœ‰é‡å çš„ç´¢å¼•**:
```python
# å‡è®¾æœ‰ä¸¤ä¸ªDataFrameéƒ½åŒ…å«AAPLçš„æ•°æ®
df1 = pd.DataFrame({'Close': [150.0]}, index=[pd.Timestamp('2024-01-15')])
df2 = pd.DataFrame({'Close': [150.5]}, index=[pd.Timestamp('2024-01-15')])

# åˆå¹¶å
combined = pd.concat([df1, df2], ignore_index=False)
# combined.index = [2024-01-15, 2024-01-15]  # é‡å¤ï¼
```

**ç»“è®º**: å¦‚æœæ•°æ®åˆå¹¶æ—¶æ²¡æœ‰æ­£ç¡®å¤„ç†ï¼Œå¯èƒ½äº§ç”Ÿé‡å¤ã€‚

---

## ğŸ” éªŒè¯æ–¹æ³•

### æ£€æŸ¥1: æ£€æŸ¥APIè¿”å›çš„æ•°æ®æ˜¯å¦åŒ…å«å‘¨æœ«/å‡æœŸ

```python
# åœ¨fetch_market_dataè¿”å›å
if 'date' in combined.columns:
    combined['date'] = pd.to_datetime(combined['date'])
    combined['weekday'] = combined['date'].dt.dayofweek  # 0=Monday, 6=Sunday
    
    # æ£€æŸ¥å‘¨æœ«æ•°æ®
    weekend_data = combined[combined['weekday'].isin([5, 6])]  # Saturday, Sunday
    if len(weekend_data) > 0:
        logger.warning(f"âš ï¸ API returned {len(weekend_data)} weekend records!")
        logger.warning(f"âš ï¸ Weekend dates: {weekend_data['date'].unique()[:10]}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„(date, ticker)ç»„åˆ
    duplicates = combined.duplicated(subset=['date', 'ticker'])
    if duplicates.any():
        logger.error(f"âŒ API returned {duplicates.sum()} duplicate (date, ticker) combinations!")
        # æ£€æŸ¥è¿™äº›é‡å¤æ˜¯å¦åœ¨å‘¨æœ«
        dup_data = combined[combined.duplicated(subset=['date', 'ticker'], keep=False)]
        weekend_dups = dup_data[dup_data['weekday'].isin([5, 6])]
        if len(weekend_dups) > 0:
            logger.error(f"âŒ {len(weekend_dups)} duplicates are on weekends!")
```

### æ£€æŸ¥2: æ£€æŸ¥æ—¥æœŸæ ‡å‡†åŒ–åæ˜¯å¦äº§ç”Ÿé‡å¤

```python
# åœ¨compute_dataåˆ›å»ºå
compute_data['date'] = pd.to_datetime(compute_data['date']).dt.normalize()
compute_data['weekday'] = compute_data['date'].dt.dayofweek

# æ£€æŸ¥é‡å¤
duplicates = compute_data.duplicated(subset=['date', 'ticker'])
if duplicates.any():
    dup_data = compute_data[compute_data.duplicated(subset=['date', 'ticker'], keep=False)]
    
    # æ£€æŸ¥é‡å¤æ˜¯å¦åœ¨å‘¨æœ«
    weekend_dups = dup_data[dup_data['weekday'].isin([5, 6])]
    if len(weekend_dups) > 0:
        logger.warning(f"âš ï¸ {len(weekend_dups)} duplicates are on weekends!")
        logger.warning(f"âš ï¸ This suggests API returned weekend data!")
    
    # æ£€æŸ¥é‡å¤æ˜¯å¦åœ¨åŒä¸€å¤©ï¼ˆä¸åŒæ—¶é—´æˆ³ï¼‰
    for (date, ticker), group in dup_data.groupby(['date', 'ticker']):
        if len(group) > 1:
            logger.warning(f"âš ï¸ ({date}, {ticker}): {len(group)} rows")
            logger.warning(f"âš ï¸   Weekday: {group['weekday'].iloc[0]} ({'Weekend' if group['weekday'].iloc[0] in [5, 6] else 'Weekday'})")
```

---

## âœ… è§£å†³æ–¹æ¡ˆ

### ä¿®å¤1: è¿‡æ»¤æ‰å‘¨æœ«/å‡æœŸæ•°æ®

**ä½ç½®**: `bma_models/simple_25_factor_engine.py` line ~350

**ä¿®æ”¹**:
```python
compute_data = market_data_clean.copy()
compute_data['date'] = pd.to_datetime(compute_data['date']).dt.normalize()
compute_data = compute_data.sort_values(['ticker', 'date']).reset_index(drop=True)

# ğŸ”§ FIX: Filter out weekend data (Saturday=5, Sunday=6)
if 'date' in compute_data.columns:
    compute_data['weekday'] = compute_data['date'].dt.dayofweek
    weekend_count = (compute_data['weekday'].isin([5, 6])).sum()
    if weekend_count > 0:
        logger.warning(f"âš ï¸ Filtering out {weekend_count} weekend records (should not exist)")
        compute_data = compute_data[~compute_data['weekday'].isin([5, 6])].reset_index(drop=True)
    compute_data = compute_data.drop(columns=['weekday'])

# ğŸ”§ FIX: Remove duplicate (date, ticker) combinations immediately
if 'date' in compute_data.columns and 'ticker' in compute_data.columns:
    duplicates = compute_data.duplicated(subset=['date', 'ticker'], keep='last')
    if duplicates.any():
        dup_count = duplicates.sum()
        logger.warning(f"âš ï¸ compute_data: Removing {dup_count} duplicate (date, ticker) combinations")
        # Log examples for debugging
        dup_data = compute_data[compute_data.duplicated(subset=['date', 'ticker'], keep=False)]
        if len(dup_data) > 0:
            logger.warning(f"âš ï¸ Duplicate examples (first 3):")
            for (date, ticker), group in list(dup_data.groupby(['date', 'ticker']))[:3]:
                logger.warning(f"  ({date}, {ticker}): {len(group)} rows")
                if 'Close' in group.columns:
                    logger.warning(f"    Close values: {group['Close'].tolist()}")
        
        compute_data = compute_data[~duplicates].reset_index(drop=True)
    
    unique_pairs = compute_data.groupby(['date', 'ticker']).size().shape[0]
    logger.info(f"âœ… compute_data after deduplication: {len(compute_data)} rows, {unique_pairs} unique (date, ticker) pairs")
```

### ä¿®å¤2: åœ¨`fetch_market_data`è¿”å›å‰è¿‡æ»¤

**ä½ç½®**: `bma_models/simple_25_factor_engine.py` line ~260

**ä¿®æ”¹**:
```python
if all_data:
    combined = pd.concat(all_data, ignore_index=False)
    combined = combined.reset_index()
    
    if 'Date' in combined.columns:
        combined = combined.rename(columns={'Date': 'date'})
    
    # ğŸ”§ FIX: Filter out weekend data
    if 'date' in combined.columns:
        combined['date'] = pd.to_datetime(combined['date']).dt.normalize()
        combined['weekday'] = combined['date'].dt.dayofweek
        weekend_count = (combined['weekday'].isin([5, 6])).sum()
        if weekend_count > 0:
            logger.warning(f"âš ï¸ fetch_market_data: Filtering out {weekend_count} weekend records")
            combined = combined[~combined['weekday'].isin([5, 6])].reset_index(drop=True)
        combined = combined.drop(columns=['weekday'])
    
    # ğŸ”§ FIX: Remove duplicate (date, ticker) combinations
    if 'date' in combined.columns and 'ticker' in combined.columns:
        duplicates = combined.duplicated(subset=['date', 'ticker'], keep='last')
        if duplicates.any():
            logger.warning(f"âš ï¸ fetch_market_data: Removing {duplicates.sum()} duplicate (date, ticker) combinations")
            combined = combined[~duplicates].reset_index(drop=True)
    
    return combined
```

---

## ğŸ¯ æ€»ç»“

**æ˜¯å¦å› ä¸ºå‘¨æœ«/å‡æœŸæ•°æ®å¯¼è‡´é‡å¤ï¼Ÿ**

**ç­”æ¡ˆ**: **ä¸å¤ªå¯èƒ½**ï¼Œå› ä¸ºï¼š
1. å‘¨æœ«å’Œå‡æœŸæ˜¯**ä¸åŒçš„æ—¥æœŸ**ï¼Œä¸ä¼šäº§ç”Ÿé‡å¤çš„(date, ticker)ç»„åˆ
2. Polygon APIçš„`timespan='day'`åº”è¯¥**åªè¿”å›äº¤æ˜“æ—¥æ•°æ®**

**æ›´å¯èƒ½çš„åŸå› **:
1. **APIè¿”å›äº†åŒä¸€å¤©çš„å¤šæ¡è®°å½•**ï¼ˆä¸åŒæ—¶é—´æˆ³ï¼‰â†’ æ ‡å‡†åŒ–åå˜æˆé‡å¤
2. **æ•°æ®åˆå¹¶æ—¶äº§ç”Ÿäº†é‡å¤** â†’ `pd.concat`ä¿ç•™äº†é‡å çš„ç´¢å¼•
3. **API bugè¿”å›äº†é‡å¤æ•°æ®** â†’ è™½ç„¶ä¸åº”è¯¥ï¼Œä½†å¯èƒ½å‘ç”Ÿ

**è§£å†³æ–¹æ¡ˆ**:
- è¿‡æ»¤æ‰å‘¨æœ«æ•°æ®ï¼ˆè™½ç„¶ä¸åº”è¯¥å­˜åœ¨ï¼‰
- åœ¨æ•°æ®æµçš„æ¯ä¸ªå…³é”®èŠ‚ç‚¹å»é‡
- æ·»åŠ è¯Šæ–­æ—¥å¿—ï¼Œå®šä½é‡å¤æ¥æº

---

**çŠ¶æ€**: âš ï¸ **éœ€è¦æ·»åŠ å‘¨æœ«è¿‡æ»¤å’Œå»é‡é€»è¾‘**

**ä¸‹ä¸€æ­¥**: å®æ–½ä¿®å¤ï¼Œè¿‡æ»¤å‘¨æœ«æ•°æ®å¹¶ç¡®ä¿æ¯ä¸ª(date, ticker)ç»„åˆåªå‡ºç°ä¸€æ¬¡
