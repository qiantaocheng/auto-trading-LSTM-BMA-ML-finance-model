# ä¿®å¤å­é›†è®­ç»ƒæŒ‚èµ·é—®é¢˜

## æ ¹æœ¬åŸå› 

**é—®é¢˜**: å­é›†æ•°æ®ï¼ˆ1244ä¸ªäº¤æ˜“æ—¥ï¼‰åœ¨6æŠ˜CVä¸­ï¼Œå‰å‡ ä¸ªfoldçš„è®­ç»ƒçª—å¯èƒ½ < 252å¤©ï¼Œå¯¼è‡´æ‰€æœ‰foldè¢«è·³è¿‡ï¼Œ`oof_pred` å…¨0ï¼Œåç»­Ridge Stackerè®­ç»ƒæŒ‚èµ·ã€‚

## ä¿®å¤æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: åŠ¨æ€è°ƒæ•´æœ€å°è®­ç»ƒçª—ï¼ˆæ¨èï¼‰

åœ¨ `_unified_model_training` ä¸­ï¼Œæ ¹æ®æ•°æ®è§„æ¨¡åŠ¨æ€è°ƒæ•´ `min_train_window_days`ï¼š

```python
# Line 11426-11432 ä¿®æ”¹ä¸ºï¼š
# ğŸ”§ æœ€å°è®­ç»ƒçª—é™åˆ¶ï¼šæ ¹æ®æ•°æ®è§„æ¨¡åŠ¨æ€è°ƒæ•´
try:
    from bma_models.unified_config_loader import get_time_config
    time_config = get_time_config()
    base_min_train_window = getattr(time_config, 'min_train_window_days', 252)
    
    # åŠ¨æ€è°ƒæ•´ï¼šå­é›†æ•°æ®é™ä½è¦æ±‚
    unique_dates_count = len(pd.Series(groups_norm).unique()) if groups_norm is not None else sample_size // 500
    if unique_dates_count < 1500:  # å­é›†æ•°æ®ï¼ˆçº¦3å¹´ï¼‰
        min_train_window_days = max(126, base_min_train_window // 2)  # é™ä½åˆ°åŠå¹´
        logger.info(f"[FIRST_LAYER] å­é›†æ•°æ®æ£€æµ‹ï¼šå”¯ä¸€æ—¥æœŸ={unique_dates_count}ï¼Œé™ä½æœ€å°è®­ç»ƒçª—åˆ°{min_train_window_days}å¤©")
    else:  # å…¨é‡æ•°æ®
        min_train_window_days = base_min_train_window
        logger.info(f"[FIRST_LAYER] å…¨é‡æ•°æ®ï¼šå”¯ä¸€æ—¥æœŸ={unique_dates_count}ï¼Œä½¿ç”¨æ ‡å‡†æœ€å°è®­ç»ƒçª—{min_train_window_days}å¤©")
except:
    min_train_window_days = 252  # é»˜è®¤1å¹´äº¤æ˜“æ—¥
```

### æ–¹æ¡ˆ2: æ·»åŠ å®‰å…¨æ£€æŸ¥

åœ¨è®­ç»ƒå®Œæˆåæ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„foldï¼š

```python
# Line 11989-11992 åæ·»åŠ ï¼š
scores_clean = [s for s in scores if not np.isnan(s) and np.isfinite(s)]
if len(scores_clean) == 0:
    error_msg = (
        f"[FIRST_LAYER][{name}] æ‰€æœ‰CV foldéƒ½è¢«è·³è¿‡ï¼"
        f"è®­ç»ƒçª—ä¸è¶³{min_train_window_days}å¤©ã€‚"
        f"æ•°æ®å”¯ä¸€æ—¥æœŸæ•°: {len(pd.Series(groups_norm).unique()) if groups_norm is not None else 'unknown'}"
    )
    logger.error(error_msg)
    raise ValueError(error_msg)

cv_scores[name] = np.mean(scores_clean) if scores_clean else 0.0
```

### æ–¹æ¡ˆ3: å‡å°‘CVæŠ˜æ•°ï¼ˆå­é›†æ•°æ®ï¼‰

åœ¨åˆ›å»ºCVåˆ†å‰²å™¨æ—¶ï¼Œæ ¹æ®æ•°æ®è§„æ¨¡å‡å°‘æŠ˜æ•°ï¼š

```python
# Line 11108-11126 ä¿®æ”¹ä¸ºï¼š
adapted_splits = self._CV_SPLITS
adapted_test_size = self._TEST_SIZE

enforce_full_cv = getattr(self, 'enforce_full_cv', False)

# å­é›†æ•°æ®ä¼˜åŒ–ï¼šå‡å°‘CVæŠ˜æ•°
unique_dates_count = len(pd.Series(groups_norm).unique()) if groups_norm is not None else sample_size // 500
if unique_dates_count < 1500 and not enforce_full_cv:  # å­é›†æ•°æ®
    adapted_splits = min(3, self._CV_SPLITS)  # å‡å°‘åˆ°3æŠ˜
    adapted_test_size = min(42, self._TEST_SIZE)
    logger.info(f"å­é›†æ•°æ®ä¼˜åŒ–: CVæŠ˜æ•°={adapted_splits}, test_size={adapted_test_size}")
elif sample_size > 1000000 and not enforce_full_cv:  # è¶…å¤§æ•°æ®é›†
    adapted_splits = min(3, self._CV_SPLITS)
    adapted_test_size = min(42, self._TEST_SIZE)
    logger.info(f"è¶…å¤§æ•°æ®é›†ä¼˜åŒ–: CVæŠ˜æ•°={adapted_splits}, test_size={adapted_test_size}")
elif enforce_full_cv:
    logger.info(f"Full CV enforced: ä½¿ç”¨ splits={adapted_splits}, test_size={adapted_test_size}")
```

## ç«‹å³ä¿®å¤æ­¥éª¤

1. **ä¿®æ”¹ `_unified_model_training` æ–¹æ³•**:
   - æ·»åŠ åŠ¨æ€ `min_train_window_days` è°ƒæ•´
   - æ·»åŠ å®‰å…¨æ£€æŸ¥ï¼Œå¦‚æœæ‰€æœ‰foldè¢«è·³è¿‡åˆ™æŠ¥é”™

2. **æµ‹è¯•ä¿®å¤**:
   - ä½¿ç”¨å­é›†æ•°æ®é‡æ–°è¿è¡Œè®­ç»ƒ
   - ç¡®è®¤ä¸ä¼šæŒ‚èµ·ï¼Œè¦ä¹ˆæˆåŠŸè¦ä¹ˆæ˜ç¡®æŠ¥é”™

3. **éªŒè¯**:
   - æ£€æŸ¥æ—¥å¿—ä¸­æ˜¯å¦æœ‰ "æ‰€æœ‰CV foldéƒ½è¢«è·³è¿‡" çš„é”™è¯¯
   - å¦‚æœæœ‰ï¼Œè¯´æ˜æ•°æ®ä¸è¶³ï¼Œéœ€è¦è¿›ä¸€æ­¥é™ä½è¦æ±‚æˆ–å¢åŠ æ•°æ®
