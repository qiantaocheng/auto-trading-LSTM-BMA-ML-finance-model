#!/usr/bin/env python3
"""
é€æ—¥æ­£äº¤åŒ–ä¸ä¸­æ€§åŒ–ç®¡çº¿ - æœºæ„çº§ç‰¹å¾å·¥ç¨‹æ ‡å‡†
====================================================
ç»Ÿä¸€ç‰¹å¾å¤„ç†æµæ°´çº¿ï¼šå»æå€¼â†’æ ‡å‡†åŒ–â†’è¡Œä¸š/è§„æ¨¡/å›½å®¶ä¸­æ€§åŒ–â†’æ­£äº¤â†’æ’åº
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass
import logging
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.linear_model import LinearRegression
from scipy import stats
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)

@dataclass
class NeutralizationConfig:
    """ä¸­æ€§åŒ–é…ç½®"""
    # å»æå€¼é…ç½®
    winsorize_method: str = "mad"              # mad/quantile/zscore
    winsorize_factor: float = 3.0              # MADå€æ•°æˆ–Z-scoreå€æ•°
    quantile_limits: Tuple[float, float] = (0.01, 0.99)  # åˆ†ä½æ•°é™åˆ¶
    
    # æ ‡å‡†åŒ–é…ç½®  
    standardization_method: str = "robust"     # robust/standard/none
    center: bool = True                        # æ˜¯å¦å»ä¸­å¿ƒåŒ–
    scale: bool = True                         # æ˜¯å¦æ ‡å‡†åŒ–
    target_std: float = 1.0                    # ç›®æ ‡æ ‡å‡†å·®
    std_tolerance: float = 0.05                # æ ‡å‡†å·®å®¹å·®
    enforce_std_precision: bool = True         # å¼ºåˆ¶æ ‡å‡†å·®ç²¾åº¦
    
    # ä¸­æ€§åŒ–é…ç½®
    industry_neutralize: bool = True           # è¡Œä¸šä¸­æ€§åŒ–
    size_neutralize: bool = True               # è§„æ¨¡ä¸­æ€§åŒ–
    country_neutralize: bool = True            # å›½å®¶ä¸­æ€§åŒ–
    market_cap_log: bool = True                # å¯¹æ•°å¸‚å€¼
    
    # æ­£äº¤åŒ–é…ç½®
    orthogonalize: bool = True                 # å¯ç”¨æ­£äº¤åŒ–
    correlation_threshold: float = 0.85        # ç›¸å…³æ€§é˜ˆå€¼
    orthogonalize_method: str = "qr"           # qr/gram_schmidt/regression
    
    # æ’åºé…ç½®
    final_ranking: bool = True                 # æœ€ç»ˆæ’åº
    ranking_method: str = "normal"             # normal/uniform
    
    # è´¨é‡æ§åˆ¶
    min_samples_per_date: int = 20             # æ¯æ—¥æœ€å°‘æ ·æœ¬æ•°
    max_missing_rate: float = 0.5              # æœ€å¤§ç¼ºå¤±ç‡
    handle_outliers_after_neutralize: bool = True  # ä¸­æ€§åŒ–åå†æ¬¡å»æå€¼

@dataclass
class MarketData:
    """å¸‚åœºæ•°æ®å®¹å™¨"""
    market_cap: pd.DataFrame = None            # å¸‚å€¼æ•°æ®
    industry_codes: pd.DataFrame = None        # è¡Œä¸šä»£ç 
    country_codes: pd.DataFrame = None         # å›½å®¶ä»£ç 
    trading_volume: pd.DataFrame = None        # æˆäº¤é‡
    list_date: pd.DataFrame = None             # ä¸Šå¸‚æ—¥æœŸ

class DailyNeutralizationPipeline:
    """é€æ—¥ä¸­æ€§åŒ–ç®¡çº¿"""
    
    def __init__(self, config: NeutralizationConfig = None, 
                 market_data: MarketData = None):
        """åˆå§‹åŒ–ä¸­æ€§åŒ–ç®¡çº¿"""
        self.config = config or NeutralizationConfig()
        self.market_data = market_data or MarketData()
        
        # ç¼“å­˜æ ‡å‡†åŒ–å™¨ï¼ˆé€æ—¥é‡æ–°æ‹Ÿåˆï¼‰
        self.daily_scalers = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'dates_processed': 0,
            'factors_processed': 0,
            'outliers_winsorized': 0,
            'neutralization_applied': 0,
            'orthogonalization_applied': 0,
            'quality_issues': 0
        }
        
        logger.info("é€æ—¥ä¸­æ€§åŒ–ç®¡çº¿åˆå§‹åŒ–å®Œæˆ")
    
    def process_daily_factors(self, factor_data: pd.DataFrame,
                             date: pd.Timestamp) -> pd.DataFrame:
        """
        å¤„ç†å•æ—¥å› å­æ•°æ®
        
        Args:
            factor_data: å•æ—¥å› å­æ•°æ® (index=tickers, columns=factors)
            date: æ—¥æœŸ
            
        Returns:
            å¤„ç†åçš„å› å­æ•°æ®
        """
        if factor_data.empty:
            return factor_data
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        if len(factor_data) < self.config.min_samples_per_date:
            logger.warning(f"æ—¥æœŸ {date} æ ·æœ¬æ•°ä¸è¶³: {len(factor_data)}")
            self.stats['quality_issues'] += 1
            return factor_data
        
        processed_data = factor_data.copy()
        
        try:
            # æ­¥éª¤1: å»æå€¼
            processed_data = self._winsorize_factors(processed_data, date)
            
            # æ­¥éª¤2: ä¸­æ€§åŒ–ï¼ˆğŸ”§ ä¿®å¤ï¼šåœ¨æ ‡å‡†åŒ–ä¹‹å‰è¿›è¡Œï¼‰
            # æ­£ç¡®é¡ºåºï¼šä¸­æ€§åŒ–ä¼šæ”¹å˜åˆ†å¸ƒï¼Œåº”è¯¥åœ¨æ ‡å‡†åŒ–ä¹‹å‰å®Œæˆ
            processed_data = self._neutralize_factors(processed_data, date)
            
            # æ­¥éª¤3: æ ‡å‡†åŒ–ï¼ˆåœ¨ä¸­æ€§åŒ–ä¹‹åï¼‰
            processed_data = self._standardize_factors(processed_data, date)
            
            # æ­¥éª¤4: ä¸­æ€§åŒ–åå†æ¬¡å»æå€¼ï¼ˆå¯é€‰ï¼‰
            if self.config.handle_outliers_after_neutralize:
                processed_data = self._winsorize_factors(processed_data, date, suffix="_post")
            
            # æ­¥éª¤5: æ­£äº¤åŒ–
            processed_data = self._orthogonalize_factors(processed_data, date)
            
            # æ­¥éª¤6: æœ€ç»ˆæ’åº
            if self.config.final_ranking:
                processed_data = self._rank_factors(processed_data, date)
            
            self.stats['dates_processed'] += 1
            self.stats['factors_processed'] += processed_data.shape[1]
            
        except Exception as e:
            logger.error(f"æ—¥æœŸ {date} å› å­å¤„ç†å¤±è´¥: {e}")
            self.stats['quality_issues'] += 1
            return factor_data
        
        return processed_data
    
    def process_multi_date_factors(self, factor_data: pd.DataFrame) -> pd.DataFrame:
        """
        å¤„ç†å¤šæ—¥å› å­æ•°æ®ï¼ˆä¸»æ¥å£ï¼‰
        
        Args:
            factor_data: å¤šæ—¥å› å­æ•°æ® (index=date, columns=MultiIndex[ticker, factor])
                        æˆ– (index=[date, ticker], columns=factors)
        
        Returns:
            å¤„ç†åçš„å› å­æ•°æ®
        """
        if factor_data.empty:
            return factor_data
        
        # æ£€æµ‹æ•°æ®æ ¼å¼
        if isinstance(factor_data.index, pd.MultiIndex):
            # æ ¼å¼: MultiIndex[date, ticker] x factors
            return self._process_multiindex_format(factor_data)
        elif isinstance(factor_data.columns, pd.MultiIndex):
            # æ ¼å¼: date x MultiIndex[ticker, factor]
            return self._process_wide_format(factor_data)
        else:
            # å‡è®¾å•æ—¥æ•°æ®
            if len(factor_data.index.unique()) == 1:
                date = factor_data.index[0] if hasattr(factor_data.index[0], 'date') else pd.Timestamp.now()
                return self.process_daily_factors(factor_data, date)
            else:
                logger.error("æ— æ³•è¯†åˆ«å› å­æ•°æ®æ ¼å¼")
                return factor_data
    
    def _process_wide_format(self, factor_data: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†å®½æ ¼å¼æ•°æ® (date x MultiIndex[ticker, factor])"""
        processed_dates = []
        
        for date in factor_data.index:
            # æå–å½“æ—¥æ•°æ®å¹¶è½¬ç½®ä¸º tickers x factors
            daily_data = factor_data.loc[date].unstack().fillna(method='ffill')
            
            # å¤„ç†å½“æ—¥æ•°æ®
            processed_daily = self.process_daily_factors(daily_data, date)
            
            # è½¬æ¢å›åŸæ ¼å¼å¹¶æ·»åŠ åˆ°ç»“æœ
            processed_wide = processed_daily.stack().to_frame().T
            processed_wide.index = [date]
            processed_dates.append(processed_wide)
        
        if processed_dates:
            return pd.concat(processed_dates, axis=0)
        else:
            return pd.DataFrame()
    
    def _process_multiindex_format(self, factor_data: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†MultiIndexæ ¼å¼æ•°æ® (MultiIndex[date, ticker] x factors)"""
        processed_data = []
        
        for date in factor_data.index.get_level_values(0).unique():
            # æå–å½“æ—¥æ•°æ®
            daily_data = factor_data.xs(date, level=0)
            
            # å¤„ç†å½“æ—¥æ•°æ®
            processed_daily = self.process_daily_factors(daily_data, date)
            
            # æ·»åŠ æ—¥æœŸç´¢å¼•
            processed_daily.index = pd.MultiIndex.from_product(
                [[date], processed_daily.index], names=['date', 'ticker']
            )
            processed_data.append(processed_daily)
        
        if processed_data:
            return pd.concat(processed_data, axis=0)
        else:
            return pd.DataFrame()
    
    def _winsorize_factors(self, factor_data: pd.DataFrame, 
                          date: pd.Timestamp, suffix: str = "") -> pd.DataFrame:
        """å»æå€¼å¤„ç†"""
        winsorized_data = factor_data.copy()
        
        for factor_name in factor_data.columns:
            factor_values = factor_data[factor_name].dropna()
            
            if len(factor_values) < 10:  # æ ·æœ¬å¤ªå°‘è·³è¿‡
                continue
            
            if self.config.winsorize_method == "mad":
                # MADæ–¹æ³•
                median = factor_values.median()
                mad = np.median(np.abs(factor_values - median))
                lower_bound = median - self.config.winsorize_factor * mad
                upper_bound = median + self.config.winsorize_factor * mad
                
            elif self.config.winsorize_method == "quantile":
                # åˆ†ä½æ•°æ–¹æ³•
                lower_bound = factor_values.quantile(self.config.quantile_limits[0])
                upper_bound = factor_values.quantile(self.config.quantile_limits[1])
                
            elif self.config.winsorize_method == "zscore":
                # Z-scoreæ–¹æ³•
                mean = factor_values.mean()
                std = factor_values.std()
                lower_bound = mean - self.config.winsorize_factor * std
                upper_bound = mean + self.config.winsorize_factor * std
            
            else:
                continue
            
            # åº”ç”¨Winsorization
            outlier_mask = (factor_data[factor_name] < lower_bound) | (factor_data[factor_name] > upper_bound)
            outlier_count = outlier_mask.sum()
            
            winsorized_data.loc[factor_data[factor_name] < lower_bound, factor_name] = lower_bound
            winsorized_data.loc[factor_data[factor_name] > upper_bound, factor_name] = upper_bound
            
            if outlier_count > 0:
                self.stats['outliers_winsorized'] += outlier_count
        
        return winsorized_data
    
    def _standardize_factors(self, factor_data: pd.DataFrame, 
                           date: pd.Timestamp) -> pd.DataFrame:
        """æ ‡å‡†åŒ–å¤„ç†"""
        if self.config.standardization_method == "none":
            return factor_data
        
        standardized_data = factor_data.copy()
        
        for factor_name in factor_data.columns:
            factor_values = factor_data[factor_name].dropna()
            
            if len(factor_values) < 10:
                continue
            
            if self.config.standardization_method == "robust":
                # ä½¿ç”¨ä¸­ä½æ•°å’ŒMADæ ‡å‡†åŒ–
                median = factor_values.median()
                mad = np.median(np.abs(factor_values - median))
                if mad > 0:
                    standardized_values = (factor_data[factor_name] - median) / mad
                    
                    # ç²¾åº¦æ§åˆ¶ï¼šè°ƒæ•´åˆ°ç›®æ ‡æ ‡å‡†å·®
                    if self.config.enforce_std_precision:
                        current_std = standardized_values.dropna().std()
                        if current_std > 0:
                            adjustment_factor = self.config.target_std / current_std
                            standardized_values = standardized_values * adjustment_factor
                            
                            # éªŒè¯æ ‡å‡†å·®ç²¾åº¦
                            final_std = standardized_values.dropna().std()
                            if abs(final_std - self.config.target_std) > self.config.std_tolerance:
                                logger.warning(f"å› å­{factor_name}æ ‡å‡†å·®ç²¾åº¦è­¦å‘Š: {final_std:.4f} (ç›®æ ‡:{self.config.target_std})")
                    
                    standardized_data[factor_name] = standardized_values
                    
            elif self.config.standardization_method == "standard":
                # ä½¿ç”¨å‡å€¼å’Œæ ‡å‡†å·®
                mean = factor_values.mean()
                std = factor_values.std()
                if std > 0:
                    standardized_values = (factor_data[factor_name] - mean) / std
                    
                    # ç²¾åº¦æ§åˆ¶ï¼šç¡®ä¿æ ‡å‡†å·®ä¸º1
                    if self.config.enforce_std_precision:
                        current_std = standardized_values.dropna().std()
                        if abs(current_std - self.config.target_std) > self.config.std_tolerance:
                            # ç›´æ¥ä½¿ç”¨sklearnçš„StandardScalerç¡®ä¿ç²¾åº¦
                            scaler = StandardScaler(with_mean=self.config.center, with_std=self.config.scale)
                            valid_mask = ~factor_data[factor_name].isna()
                            if valid_mask.sum() > 0:
                                scaled_values = scaler.fit_transform(factor_data.loc[valid_mask, factor_name].values.reshape(-1, 1))
                                standardized_values = factor_data[factor_name].copy()
                                standardized_values.loc[valid_mask] = scaled_values.flatten()
                    
                    standardized_data[factor_name] = standardized_values
        
        # éªŒè¯æ ‡å‡†åŒ–è´¨é‡
        if self.config.enforce_std_precision:
            self._validate_standardization_quality(standardized_data, date)
        
        return standardized_data
    
    def _neutralize_factors(self, factor_data: pd.DataFrame, 
                          date: pd.Timestamp) -> pd.DataFrame:
        """ä¸­æ€§åŒ–å¤„ç†"""
        neutralized_data = factor_data.copy()
        
        # æ„å»ºä¸­æ€§åŒ–å˜é‡
        neutralization_vars = self._build_neutralization_variables(
            factor_data.index, date
        )
        
        if neutralization_vars.empty:
            logger.warning(f"æ—¥æœŸ {date} æ— ä¸­æ€§åŒ–å˜é‡ï¼Œè·³è¿‡ä¸­æ€§åŒ–")
            return factor_data
        
        # å¯¹æ¯ä¸ªå› å­è¿›è¡Œä¸­æ€§åŒ–
        for factor_name in factor_data.columns:
            try:
                factor_values = factor_data[factor_name].dropna()
                
                # å¯¹é½ä¸­æ€§åŒ–å˜é‡
                common_tickers = factor_values.index.intersection(neutralization_vars.index)
                
                if len(common_tickers) < 10:
                    continue
                
                y = factor_values.loc[common_tickers]
                X = neutralization_vars.loc[common_tickers]
                
                # ç§»é™¤å¸¸æ•°åˆ—
                X = X.loc[:, X.std() > 1e-8]
                
                if X.empty:
                    continue
                
                # å›å½’ä¸­æ€§åŒ–
                reg = LinearRegression(fit_intercept=True)
                reg.fit(X, y)
                
                # è·å–æ®‹å·®
                predicted = reg.predict(X)
                residuals = y - predicted
                
                # æ›´æ–°ä¸­æ€§åŒ–åçš„å€¼
                neutralized_data.loc[common_tickers, factor_name] = residuals
                
                self.stats['neutralization_applied'] += 1
                
            except Exception as e:
                logger.debug(f"å› å­ {factor_name} ä¸­æ€§åŒ–å¤±è´¥: {e}")
                continue
        
        return neutralized_data
    
    def _build_neutralization_variables(self, tickers: pd.Index, 
                                      date: pd.Timestamp) -> pd.DataFrame:
        """æ„å»ºä¸­æ€§åŒ–å˜é‡"""
        neutralization_data = []
        
        # è¡Œä¸šå“‘å˜é‡
        if self.config.industry_neutralize and self.market_data.industry_codes is not None:
            try:
                industry_data = self._get_market_data_for_date(
                    self.market_data.industry_codes, date, tickers
                )
                if not industry_data.empty:
                    # åˆ›å»ºè¡Œä¸šå“‘å˜é‡
                    industry_dummies = pd.get_dummies(
                        industry_data.iloc[:, 0], prefix='industry'
                    )
                    industry_dummies.index = industry_data.index
                    neutralization_data.append(industry_dummies)
            except Exception as e:
                logger.debug(f"è¡Œä¸šä¸­æ€§åŒ–å˜é‡æ„å»ºå¤±è´¥: {e}")
        
        # è§„æ¨¡å› å­ï¼ˆå¯¹æ•°å¸‚å€¼ï¼‰
        if self.config.size_neutralize and self.market_data.market_cap is not None:
            try:
                market_cap_data = self._get_market_data_for_date(
                    self.market_data.market_cap, date, tickers
                )
                if not market_cap_data.empty:
                    log_market_cap = np.log(market_cap_data.iloc[:, 0] + 1)
                    log_market_cap.name = 'log_market_cap'
                    neutralization_data.append(log_market_cap.to_frame())
            except Exception as e:
                logger.debug(f"è§„æ¨¡ä¸­æ€§åŒ–å˜é‡æ„å»ºå¤±è´¥: {e}")
        
        # å›½å®¶å“‘å˜é‡
        if self.config.country_neutralize and self.market_data.country_codes is not None:
            try:
                country_data = self._get_market_data_for_date(
                    self.market_data.country_codes, date, tickers
                )
                if not country_data.empty:
                    country_dummies = pd.get_dummies(
                        country_data.iloc[:, 0], prefix='country'
                    )
                    country_dummies.index = country_data.index
                    neutralization_data.append(country_dummies)
            except Exception as e:
                logger.debug(f"å›½å®¶ä¸­æ€§åŒ–å˜é‡æ„å»ºå¤±è´¥: {e}")
        
        # åˆå¹¶æ‰€æœ‰ä¸­æ€§åŒ–å˜é‡
        if neutralization_data:
            combined_neutralization = pd.concat(neutralization_data, axis=1)
            # ç§»é™¤å¸¸æ•°åˆ—å’Œé«˜åº¦ç›¸å…³åˆ—
            combined_neutralization = combined_neutralization.loc[:, combined_neutralization.std() > 1e-8]
            return combined_neutralization.fillna(0)
        
        return pd.DataFrame()
    
    def _get_market_data_for_date(self, market_data: pd.DataFrame, 
                                date: pd.Timestamp, 
                                tickers: pd.Index) -> pd.DataFrame:
        """è·å–ç‰¹å®šæ—¥æœŸçš„å¸‚åœºæ•°æ®"""
        if market_data is None or market_data.empty:
            return pd.DataFrame()
        
        # å¤„ç†æ—¥æœŸç´¢å¼•
        if isinstance(market_data.index, pd.DatetimeIndex):
            # é€‰æ‹©æœ€æ¥è¿‘çš„æ—¥æœŸ
            available_dates = market_data.index[market_data.index <= date]
            if available_dates.empty:
                return pd.DataFrame()
            closest_date = available_dates.max()
            date_data = market_data.loc[closest_date:closest_date]
        else:
            # å‡è®¾ä¸ºtickerç´¢å¼•
            date_data = market_data
        
        # ç­›é€‰ç›®æ ‡tickers
        common_tickers = date_data.columns.intersection(tickers) if hasattr(date_data, 'columns') else tickers
        if not common_tickers.empty:
            return date_data[common_tickers].T
        
        return pd.DataFrame()
    
    def _orthogonalize_factors(self, factor_data: pd.DataFrame, 
                             date: pd.Timestamp) -> pd.DataFrame:
        """å› å­æ­£äº¤åŒ–"""
        if not self.config.orthogonalize or factor_data.shape[1] <= 1:
            return factor_data
        
        # è®¡ç®—å› å­é—´ç›¸å…³æ€§
        correlation_matrix = factor_data.corr()
        
        # å¯»æ‰¾é«˜ç›¸å…³å› å­å¯¹
        high_corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_val = abs(correlation_matrix.iloc[i, j])
                if corr_val > self.config.correlation_threshold:
                    high_corr_pairs.append((
                        correlation_matrix.columns[i],
                        correlation_matrix.columns[j],
                        corr_val
                    ))
        
        if not high_corr_pairs:
            return factor_data
        
        orthogonalized_data = factor_data.copy()
        
        try:
            if self.config.orthogonalize_method == "qr":
                # QRåˆ†è§£æ­£äº¤åŒ–
                Q, R = np.linalg.qr(factor_data.fillna(0).values)
                orthogonal_factors = pd.DataFrame(
                    Q, index=factor_data.index, 
                    columns=[f"orth_{i}" for i in range(Q.shape[1])]
                )
                return orthogonal_factors
                
            elif self.config.orthogonalize_method == "regression":
                # é€æ­¥å›å½’æ­£äº¤åŒ–
                for factor1, factor2, corr_val in high_corr_pairs:
                    # ç”¨factor1å›å½’factor2ï¼Œä¿ç•™æ®‹å·®
                    valid_mask = ~(factor_data[factor1].isna() | factor_data[factor2].isna())
                    
                    if valid_mask.sum() < 10:
                        continue
                    
                    X = factor_data.loc[valid_mask, [factor1]]
                    y = factor_data.loc[valid_mask, factor2]
                    
                    reg = LinearRegression(fit_intercept=True)
                    reg.fit(X, y)
                    
                    predicted = reg.predict(X)
                    residuals = y - predicted
                    
                    # æ›¿æ¢factor2ä¸ºæ®‹å·®
                    orthogonalized_data.loc[valid_mask, factor2] = residuals
                
                self.stats['orthogonalization_applied'] += len(high_corr_pairs)
                
        except Exception as e:
            logger.debug(f"æ­£äº¤åŒ–å¤±è´¥: {e}")
            return factor_data
        
        return orthogonalized_data
    
    def _rank_factors(self, factor_data: pd.DataFrame, 
                     date: pd.Timestamp) -> pd.DataFrame:
        """å› å­æ’åº"""
        ranked_data = factor_data.copy()
        
        for factor_name in factor_data.columns:
            factor_values = factor_data[factor_name].dropna()
            
            if len(factor_values) < 10:
                continue
            
            if self.config.ranking_method == "normal":
                # æ­£æ€åŒ–æ’åº
                ranks = stats.rankdata(factor_values, method='average')
                normal_scores = stats.norm.ppf(ranks / (len(ranks) + 1))
                ranked_data.loc[factor_values.index, factor_name] = normal_scores
                
            elif self.config.ranking_method == "uniform":
                # å‡åŒ€åˆ†å¸ƒæ’åº
                ranks = factor_values.rank(method='average')
                uniform_scores = (ranks - 1) / (len(ranks) - 1)
                ranked_data.loc[factor_values.index, factor_name] = uniform_scores
        
        return ranked_data
    
    def _validate_standardization_quality(self, standardized_data: pd.DataFrame, 
                                        date: pd.Timestamp) -> None:
        """éªŒè¯æ ‡å‡†åŒ–è´¨é‡"""
        quality_issues = []
        
        for factor_name in standardized_data.columns:
            factor_values = standardized_data[factor_name].dropna()
            
            if len(factor_values) < 5:
                continue
            
            # æ£€æŸ¥æ ‡å‡†å·®
            current_std = factor_values.std()
            if abs(current_std - self.config.target_std) > self.config.std_tolerance:
                quality_issues.append({
                    'factor': factor_name,
                    'issue': 'std_deviation',
                    'current_std': current_std,
                    'target_std': self.config.target_std,
                    'tolerance': self.config.std_tolerance
                })
            
            # æ£€æŸ¥å‡å€¼ï¼ˆåº”è¯¥æ¥è¿‘0ï¼‰
            current_mean = abs(factor_values.mean())
            if current_mean > 0.1:  # å‡å€¼åç¦»0è¶…è¿‡0.1
                quality_issues.append({
                    'factor': factor_name,
                    'issue': 'mean_deviation', 
                    'current_mean': factor_values.mean(),
                    'abs_mean': current_mean
                })
                
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼ï¼ˆç»å¯¹å€¼è¶…è¿‡5ï¼‰
            extreme_values = (abs(factor_values) > 5).sum()
            if extreme_values > 0:
                quality_issues.append({
                    'factor': factor_name,
                    'issue': 'extreme_values',
                    'extreme_count': extreme_values,
                    'total_count': len(factor_values)
                })
        
        if quality_issues:
            logger.warning(f"æ—¥æœŸ {date} æ ‡å‡†åŒ–è´¨é‡é—®é¢˜: {len(quality_issues)}é¡¹")
            for issue in quality_issues[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé—®é¢˜
                if issue['issue'] == 'std_deviation':
                    logger.warning(f"  {issue['factor']}: æ ‡å‡†å·® {issue['current_std']:.4f} (ç›®æ ‡: {issue['target_std']})")
                elif issue['issue'] == 'mean_deviation':
                    logger.warning(f"  {issue['factor']}: å‡å€¼åç¦» {issue['current_mean']:.4f}")
                elif issue['issue'] == 'extreme_values':
                    logger.warning(f"  {issue['factor']}: æå€¼ {issue['extreme_count']}/{issue['total_count']}")
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['standardization_quality_issues'] = self.stats.get('standardization_quality_issues', 0) + len(quality_issues)
        else:
            self.stats['standardization_quality_passed'] = self.stats.get('standardization_quality_passed', 0) + len(standardized_data.columns)
    
    def get_pipeline_stats(self) -> Dict[str, Any]:
        """è·å–ç®¡çº¿ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'processing_stats': self.stats,
            'config': self.config.__dict__,
            'market_data_availability': {
                'market_cap': self.market_data.market_cap is not None,
                'industry_codes': self.market_data.industry_codes is not None,
                'country_codes': self.market_data.country_codes is not None,
                'trading_volume': self.market_data.trading_volume is not None
            }
        }

# å…¨å±€ä¸­æ€§åŒ–ç®¡çº¿å®ä¾‹
def create_neutralization_pipeline(config: NeutralizationConfig = None,
                                 market_data: MarketData = None) -> DailyNeutralizationPipeline:
    """åˆ›å»ºä¸­æ€§åŒ–ç®¡çº¿å®ä¾‹"""
    return DailyNeutralizationPipeline(config, market_data)

if __name__ == "__main__":
    # æµ‹è¯•ä¸­æ€§åŒ–ç®¡çº¿
    pipeline = create_neutralization_pipeline()
    
    # æ¨¡æ‹Ÿæ•°æ®
    dates = pd.date_range('2023-01-01', periods=5, freq='D')
    tickers = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA']
    factors = ['momentum', 'value', 'quality']
    
    # åˆ›å»ºæ¨¡æ‹Ÿå› å­æ•°æ®
    # np.random.seed removed
    factor_data_list = []
    
    for date in dates:
        daily_factor_data = pd.DataFrame(
            np.zeros(len(tickers), len(factors)),
            index=tickers,
            columns=factors
        )
        processed_daily = pipeline.process_daily_factors(daily_factor_data, date)
        print(f"æ—¥æœŸ {date.date()} - å¤„ç†å‰å½¢çŠ¶: {daily_factor_data.shape}, å¤„ç†åå½¢çŠ¶: {processed_daily.shape}")
    
    print("\n=== ä¸­æ€§åŒ–ç®¡çº¿ç»Ÿè®¡ ===")
    stats = pipeline.get_pipeline_stats()
    for key, value in stats['processing_stats'].items():
        print(f"{key}: {value}")