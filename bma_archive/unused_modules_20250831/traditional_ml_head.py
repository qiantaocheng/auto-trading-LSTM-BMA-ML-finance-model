#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼ ç»ŸMLè®­ç»ƒå¤´ - æœºæ„çº§å¯æ’æ‹”è®­ç»ƒæ¨¡å—
å°†EnhancedMLTraineræ”¹é€ ä¸ºä¸»ç³»ç»Ÿçš„ä¼ ç»ŸMLè®­ç»ƒå¤´
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime
import logging
from sklearn.metrics import mean_squared_error, mean_absolute_error

logger = logging.getLogger(__name__)


class TraditionalMLHead:
    """
    ä¼ ç»ŸMLè®­ç»ƒå¤´ - æœºæ„çº§å¯æ’æ‹”è®­ç»ƒæ¨¡å—
    
    èŒè´£ï¼š
    1. æ¥å—ä¸»å¹²ä¼ å…¥çš„å·²é€‰ç‰¹å¾ã€ç»Ÿä¸€CVã€ç»Ÿä¸€æƒé‡
    2. ä¸“å¿ƒè®­ç»ƒLightGBM/GBDTæ¨¡å‹
    3. è¿”å›è§„èŒƒåŒ–ç»“æœç»™ä¸»å¹²çš„training_results['traditional_models']
    4. ä¸å†è‡ªå»ºç‰¹å¾é€‰æ‹©ã€CVã€ç‹¬ç«‹è¯„ä¼°
    """
    
    def __init__(self, enable_hyperparam_opt: bool = True):
        """
        Args:
            enable_hyperparam_opt: æ˜¯å¦å¯ç”¨è¶…å‚æ•°ä¼˜åŒ–
        """
        self.enable_hyperparam_opt = enable_hyperparam_opt
        self.best_params = None
        self.oof_predictions = None
        self.cv_summary = None
        self.trained_models = {}
        
        logger.info("ä¼ ç»ŸMLè®­ç»ƒå¤´åˆå§‹åŒ–å®Œæˆ - å¯æ’æ‹”æ¨¡å¼")
    
    def fit(self, X: pd.DataFrame, y: pd.Series, dates: pd.Series, tickers: pd.Series, 
            cv_factory: callable, sample_weights=None, params=None) -> dict:
        """
        ä¼ ç»ŸMLè®­ç»ƒå¤´ä¸»æ¥å£ - å¯¹é½ä¸»å¹²ç¼–æ’
        
        Args:
            X: å·²é€‰ç‰¹å¾çŸ©é˜µï¼ˆæ¥è‡ªä¸»å¹²RobustFeatureSelectorï¼‰
            y: ç›®æ ‡å˜é‡
            dates: æ—¥æœŸåºåˆ—
            tickers: è‚¡ç¥¨ä»£ç 
            cv_factory: ç»Ÿä¸€CVå·¥å‚ï¼ˆæ¥è‡ªä¸»å¹²ï¼‰
            sample_weights: æ ·æœ¬æƒé‡ï¼ˆå¯é€‰ï¼‰
            params: é¢å¤–å‚æ•°ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è§„èŒƒåŒ–è®­ç»ƒç»“æœ: {"models": {}, "oof": pd.Series, "cv": {}}
        """
        logger.info("=" * 60)
        logger.info("ä¼ ç»ŸMLè®­ç»ƒå¤´ - å¼€å§‹è®­ç»ƒ")
        logger.info("=" * 60)
        logger.info(f"è¾“å…¥æ•°æ®: {len(X)}æ ·æœ¬ Ã— {len(X.columns)}ç‰¹å¾")
        
        # ğŸš¨ ç‰¹å¾SSOTéªŒè¯ï¼šè®­ç»ƒå¤´ä¸å…è®¸æ”¹å˜ç‰¹å¾åˆ—
        input_feature_names = set(X.columns)
        
        try:
            # 1. æ•°æ®éªŒè¯
            if len(X) < 100:
                raise ValueError("æ ·æœ¬æ•°é‡ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘100ä¸ªï¼‰")
            
            # ğŸš€ å¼ºåˆ¶å¯ç”¨å®Œæ•´MLå¢å¼ºç³»ç»Ÿï¼ˆ35+ç®—æ³•ï¼‰
            logger.info("ğŸ”¥ å¼ºåˆ¶å¯ç”¨å®Œæ•´MLå¢å¼ºç³»ç»Ÿï¼šä¸‰ä»¶å¥—+é›†æˆ+BMA+è¶…å‚ä¼˜åŒ–")
            logger.info("   - ElasticNetï¼ˆçº¿æ€§æ”¶ç¼©é”šï¼‰")
            logger.info("   - LightGBMï¼ˆæµ…+å¼ºæ­£åˆ™+å­é‡‡æ ·ï¼‰") 
            logger.info("   - ExtraTreesï¼ˆæ·±+é«˜éšæœºè¢‹è£…æ ‘ï¼‰")
            logger.info("   - VotingRegressor + StackingRegressor + DynamicBMA")
            logger.info("   - OOFæ ‡å‡†åŒ– + ç›¸å…³æƒ©ç½šBMA")
            
            # ğŸ”¥ ä½¿ç”¨ç»Ÿä¸€OOFç”Ÿæˆå™¨æ›¿ä»£ç‹¬ç«‹é¢„æµ‹ç”Ÿæˆ
            final_models, unified_oof_result = self._run_unified_training_pipeline(
                X, y, dates, tickers, cv_factory, sample_weights, params
            )
            
            # 7. ä¿å­˜ç»Ÿä¸€ç»“æœ
            self.oof_predictions = unified_oof_result['primary_oof']
            self.unified_oof_result = unified_oof_result
            self.cv_summary = cv_summary
            self.trained_models = final_models
            self.best_params = best_params
            
            # ğŸš¨ è®­ç»ƒç»“æŸéªŒè¯ï¼šç¡®ä¿ç‰¹å¾åˆ—æœªè¢«æ”¹å˜
            output_feature_names = set(X.columns)
            if input_feature_names != output_feature_names:
                raise ValueError(
                    f"è¿åSSOTåŸåˆ™ï¼šè®­ç»ƒå¤´ä¸å…è®¸æ”¹å˜ç‰¹å¾åˆ—ï¼\n"
                    f"è¾“å…¥ç‰¹å¾: {sorted(input_feature_names)}\n"
                    f"è¾“å‡ºç‰¹å¾: {sorted(output_feature_names)}\n"
                    f"ä¿®å¤æŒ‡å—: ä»…å…è®¸æ¨¡å‹å†…æ”¶ç¼©ï¼ˆL1/L2ã€feature_fractionç­‰ï¼‰ï¼Œä¸å¯åˆ é™¤/æ–°å¢åˆ—"
                )
            
            logger.info("âœ… ä¼ ç»ŸMLè®­ç»ƒå¤´è®­ç»ƒå®Œæˆ")
            
            return {
                "models": final_models,
                "oof": oof_predictions,
                "cv": cv_summary,
                "metadata": {
                    "training_head": "TraditionalML",
                    "samples": len(X),
                    "features": len(X.columns),
                    "cv_folds": len(cv_splits),
                    "hyperopt_enabled": self.enable_hyperparam_opt,
                    "best_params": best_params
                }
            }
            
        except Exception as e:
            logger.error(f"ä¼ ç»ŸMLè®­ç»ƒå¤´è®­ç»ƒå¤±è´¥: {e}")
            return {
                "models": {},
                "oof": pd.Series(dtype=float),
                "cv": {"error": str(e)},
                "success": False
            }
    
    def _optimize_hyperparameters(self, X: pd.DataFrame, y: pd.Series, cv_splits: list) -> dict:
        """
        è¶…å‚æ•°ä¼˜åŒ–ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸“æ³¨æ ¸å¿ƒå‚æ•°ï¼‰
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: ç›®æ ‡å˜é‡
            cv_splits: CVåˆ†å‰²
            
        Returns:
            æœ€ä¼˜å‚æ•°
        """
        if not self.enable_hyperparam_opt:
            return None
        
        logger.info("å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–...")
        
        try:
            import lightgbm as lgb
            
            # ç²¾ç®€çš„å‚æ•°ç½‘æ ¼ï¼ˆæœºæ„çº§å®ç”¨é…ç½®ï¼‰
            param_grid = {
                'n_estimators': [100, 200],
                'num_leaves': [20, 31, 50],
                'learning_rate': [0.03, 0.05, 0.1],
                'feature_fraction': [0.7, 0.8, 0.9],
                'bagging_fraction': [0.8, 0.9],
                'lambda_l1': [0.0, 0.1],
                'lambda_l2': [0.0, 0.1],
                'min_child_samples': [20, 30]
            }
            
            best_score = -np.inf
            best_params = None
            
            # ç®€åŒ–çš„ç½‘æ ¼æœç´¢ï¼ˆåªæµ‹è¯•å‰3ä¸ªCV foldåŠ é€Ÿï¼‰
            from itertools import product
            param_combinations = [dict(zip(param_grid.keys(), v)) for v in product(*param_grid.values())]
            
            # éšæœºé‡‡æ ·å‡å°‘è®¡ç®—é‡
            import random
            if len(param_combinations) > 20:
                param_combinations = random.sample(param_combinations, 20)
            
            for params in param_combinations:
                scores = []
                
                for train_idx, val_idx in cv_splits[:3]:  # åªç”¨å‰3æŠ˜åŠ é€Ÿ
                    X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]
                    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]
                    
                    model = lgb.LGBMRegressor(**params, random_state=42, verbose=-1)
                    model.fit(X_tr.fillna(0), y_tr.fillna(0))
                    
                    y_pred = model.predict(X_val.fillna(0))
                    # ä½¿ç”¨ICä½œä¸ºè¯„åˆ†æ ‡å‡†
                    ic = np.corrcoef(y_val.fillna(0), y_pred)[0, 1] if len(y_val) > 1 else 0
                    scores.append(ic)
                
                avg_score = np.nanmean(scores)
                if avg_score > best_score:
                    best_score = avg_score
                    best_params = params
                    logger.debug(f"æ–°æœ€ä¼˜å‚æ•°: IC={avg_score:.4f}")
            
            logger.info(f"âœ… è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ: IC={best_score:.4f}")
            return best_params
            
        except ImportError:
            logger.warning("LightGBMä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
            return None
        except Exception as e:
            logger.error(f"è¶…å‚æ•°ä¼˜åŒ–å¤±è´¥: {e}")
            return None
    
    def _run_cv_training(self, X: pd.DataFrame, y: pd.Series, cv_splits: list, 
                        sample_weights=None, best_params=None) -> tuple:
        """
        CVè®­ç»ƒå¾ªç¯
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: ç›®æ ‡å˜é‡
            cv_splits: CVåˆ†å‰²
            sample_weights: æ ·æœ¬æƒé‡
            best_params: æœ€ä¼˜å‚æ•°
            
        Returns:
            (cv_results, oof_predictions)
        """
        logger.info("å¼€å§‹CVè®­ç»ƒå¾ªç¯...")
        
        oof_predictions = pd.Series(index=X.index, dtype=float)
        cv_results = []
        
        for fold_idx, (train_idx, val_idx) in enumerate(cv_splits):
            fold_start = datetime.now()
            
            X_train = X.iloc[train_idx].fillna(0)
            y_train = y.iloc[train_idx].fillna(0)
            X_val = X.iloc[val_idx].fillna(0)
            y_val = y.iloc[val_idx].fillna(0)
            
            # è®­ç»ƒLightGBMå’ŒGBDT
            fold_models = {}
            fold_predictions = {}
            
            # LightGBM
            try:
                import lightgbm as lgb
                lgb_params = best_params if best_params else {
                    'n_estimators': 100, 'num_leaves': 31, 'learning_rate': 0.05,
                    'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'random_state': 42
                }
                
                lgb_model = lgb.LGBMRegressor(**lgb_params, verbose=-1)
                lgb_model.fit(X_train, y_train)
                lgb_pred = lgb_model.predict(X_val)
                
                fold_models['lightgbm'] = lgb_model
                fold_predictions['lightgbm'] = lgb_pred
                
            except ImportError:
                logger.warning("LightGBMä¸å¯ç”¨")
            
            # GradientBoosting (å¤‡é€‰)
            try:
                from sklearn.ensemble import GradientBoostingRegressor
                gb_params = {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.05, 'random_state': 42}
                
                gb_model = GradientBoostingRegressor(**gb_params)
                gb_model.fit(X_train, y_train)
                gb_pred = gb_model.predict(X_val)
                
                fold_models['gradient_boosting'] = gb_model
                fold_predictions['gradient_boosting'] = gb_pred
                
            except Exception as e:
                logger.warning(f"GradientBoostingè®­ç»ƒå¤±è´¥: {e}")
            
            # é€‰æ‹©æœ€ä½³é¢„æµ‹ï¼ˆä»¥LightGBMä¸ºä¸»ï¼‰
            if 'lightgbm' in fold_predictions:
                fold_pred = fold_predictions['lightgbm']
            elif 'gradient_boosting' in fold_predictions:
                fold_pred = fold_predictions['gradient_boosting']
            else:
                logger.error(f"ç¬¬{fold_idx+1}æŠ˜æ²¡æœ‰å¯ç”¨æ¨¡å‹")
                continue
            
            # ä¿å­˜OOFé¢„æµ‹
            oof_predictions.iloc[val_idx] = fold_pred
            
            # è®¡ç®—foldæŒ‡æ ‡
            ic = np.corrcoef(y_val, fold_pred)[0, 1] if len(y_val) > 1 else 0
            mse = np.mean((y_val - fold_pred) ** 2)
            
            fold_time = (datetime.now() - fold_start).total_seconds()
            
            cv_results.append({
                'fold': fold_idx,
                'models': fold_models,
                'ic': ic,
                'mse': mse,
                'train_size': len(train_idx),
                'val_size': len(val_idx),
                'time_seconds': fold_time
            })
            
            logger.info(f"ç¬¬{fold_idx+1}æŠ˜å®Œæˆ: IC={ic:.4f}, MSE={mse:.4f}, ç”¨æ—¶={fold_time:.1f}s")
        
        logger.info("âœ… CVè®­ç»ƒå¾ªç¯å®Œæˆ")
        return cv_results, oof_predictions
    
    def _train_final_models(self, X: pd.DataFrame, y: pd.Series, sample_weights=None, best_params=None) -> dict:
        """
        åœ¨å…¨éƒ¨æ•°æ®ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: ç›®æ ‡å˜é‡
            sample_weights: æ ·æœ¬æƒé‡
            best_params: æœ€ä¼˜å‚æ•°
            
        Returns:
            æœ€ç»ˆæ¨¡å‹å­—å…¸
        """
        logger.info("è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        
        X_clean = X.fillna(0)
        y_clean = y.fillna(0)
        
        final_models = {}
        
        # LightGBM
        try:
            import lightgbm as lgb
            lgb_params = best_params if best_params else {
                'n_estimators': 100, 'num_leaves': 31, 'learning_rate': 0.05,
                'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'random_state': 42
            }
            
            lgb_model = lgb.LGBMRegressor(**lgb_params, verbose=-1)
            lgb_model.fit(X_clean, y_clean)
            
            final_models['lightgbm'] = lgb_model
            logger.info("âœ… LightGBMæœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
            
        except ImportError:
            logger.warning("LightGBMä¸å¯ç”¨ï¼Œè·³è¿‡æœ€ç»ˆæ¨¡å‹")
        
        # GradientBoosting
        try:
            from sklearn.ensemble import GradientBoostingRegressor
            gb_params = {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.05, 'random_state': 42}
            
            gb_model = GradientBoostingRegressor(**gb_params)
            gb_model.fit(X_clean, y_clean)
            
            final_models['gradient_boosting'] = gb_model
            logger.info("âœ… GradientBoostingæœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
            
        except Exception as e:
            logger.warning(f"GradientBoostingæœ€ç»ˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        
        return final_models
    
    def _generate_cv_summary(self, cv_results: list) -> dict:
        """
        ç”ŸæˆCVè®­ç»ƒæ‘˜è¦
        
        Args:
            cv_results: CVè®­ç»ƒç»“æœ
            
        Returns:
            CVæ‘˜è¦
        """
        if not cv_results:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆçš„CVç»“æœ"}
        
        avg_ic = np.mean([r['ic'] for r in cv_results])
        avg_mse = np.mean([r['mse'] for r in cv_results])
        total_time = sum([r['time_seconds'] for r in cv_results])
        
        summary = {
            'n_folds': len(cv_results),
            'avg_ic': avg_ic,
            'std_ic': np.std([r['ic'] for r in cv_results]),
            'avg_mse': avg_mse,
            'total_time': total_time,
            'ic_by_fold': [r['ic'] for r in cv_results],
            'models_used': list(cv_results[0]['models'].keys()) if cv_results else []
        }
        
        return summary
    
    def _run_unified_training_pipeline(self, X: pd.DataFrame, y: pd.Series, 
                                      dates: pd.Series, tickers: pd.Series,
                                      cv_factory: callable, sample_weights=None, 
                                      params=None) -> tuple:
        """ğŸ”¥ å¼ºåˆ¶è¿è¡Œå®Œæ•´MLå¢å¼ºç³»ç»Ÿç®¡é“ï¼ˆ35+ç®—æ³•ï¼‰"""
        logger.info("ğŸš€ å¼ºåˆ¶å¯åŠ¨é«˜çº§MLç®—æ³•æ ˆ...")
        
        # ğŸš¨ å¼ºåˆ¶å¯¼å…¥æ£€æŸ¥ï¼šMLå¢å¼ºç³»ç»Ÿå¿…é¡»å¯ç”¨
        try:
            from ml_enhancement_integration import MLEnhancementSystem, MLEnhancementConfig
            from ml_ensemble_enhanced import EnsembleConfig
        except ImportError as e:
            raise RuntimeError(
                f"ğŸš¨ å¼ºåˆ¶æ¨¡å¼å¤±è´¥ï¼šMLå¢å¼ºç³»ç»Ÿå¯¼å…¥å¤±è´¥ï¼\n"
                f"é”™è¯¯: {e}\n"
                f"ä¿®å¤æŒ‡å—: ç¡®ä¿ml_enhancement_integration.pyå’Œml_ensemble_enhanced.pyå¯ç”¨\n"
                f"å½“å‰ä¸ºå¼ºåˆ¶é«˜çº§ç®—æ³•æ¨¡å¼ï¼Œä¸å…è®¸å›é€€åˆ°åŸºç¡€æ¨¡å¼ï¼"
            )
        
        # ğŸš¨ å¼ºåˆ¶ä¾èµ–æ£€æŸ¥ï¼šå…³é”®ç®—æ³•åº“å¿…é¡»å¯ç”¨
        missing_deps = []
        try:
            import lightgbm
            logger.info("âœ… LightGBMå¯ç”¨")
        except ImportError:
            missing_deps.append("lightgbm")
            
        try:
            import sklearn.ensemble
            logger.info("âœ… sklearn.ensembleå¯ç”¨")
        except ImportError:
            missing_deps.append("sklearn.ensemble")
            
        if missing_deps:
            raise RuntimeError(
                f"ğŸš¨ å¼ºåˆ¶æ¨¡å¼å¤±è´¥ï¼šå…³é”®ä¾èµ–ç¼ºå¤±ï¼\n"
                f"ç¼ºå¤±ä¾èµ–: {missing_deps}\n"
                f"ä¿®å¤æŒ‡å—: pip install lightgbm scikit-learn\n"
                f"å½“å‰ä¸ºå¼ºåˆ¶é«˜çº§ç®—æ³•æ¨¡å¼ï¼Œä¸å…è®¸å›é€€ï¼"
            )
        
        # ğŸ”¥ å¼ºåˆ¶å¯ç”¨æ‰€æœ‰é«˜çº§åŠŸèƒ½çš„é…ç½®
        logger.info("ğŸ”§ é…ç½®å¼ºåˆ¶é«˜çº§ç®—æ³•å‚æ•°...")
        
        # åˆ›å»ºä¸‰ä»¶å¥—åŸºåº§é…ç½®
        ensemble_config = EnsembleConfig(
            base_models=['ElasticNet', 'LightGBM', 'ExtraTrees'],  # å¼ºåˆ¶ä¸‰ä»¶å¥—
            ensemble_methods=['voting', 'stacking', 'dynamic_bma'],  # å¼ºåˆ¶æ‰€æœ‰é›†æˆæ–¹æ³•
            diversity_threshold=0.85,  # å¼ºåˆ¶ç›¸å…³æ€§é—¨æ§›
            bma_learning_rate=0.01,
            bma_momentum=0.9, 
            bma_weight_decay=0.001
        )
        
        ml_config = MLEnhancementConfig(
            enable_feature_selection=False,  # å¼ºåˆ¶ç¦ç”¨ï¼šä»…RobustFeatureSelectorå¯æ”¹åˆ—
            enable_hyperparameter_optimization=True,  # ğŸ”¥ å¼ºåˆ¶å¯ç”¨è¶…å‚ä¼˜åŒ–
            enable_ensemble_learning=True,             # ğŸ”¥ å¼ºåˆ¶å¯ç”¨é›†æˆå­¦ä¹ 
            enable_dynamic_bma_weights=True,           # ğŸ”¥ å¼ºåˆ¶å¯ç”¨åŠ¨æ€BMA
            ensemble_config=ensemble_config,
            n_jobs=-1,
            random_state=42
        )
        
        logger.info("ğŸš€ å¼ºåˆ¶å¯åŠ¨MLå¢å¼ºç³»ç»Ÿ...")
        logger.info(f"   åŸºç¡€æ¨¡å‹: {ensemble_config.base_models}")
        logger.info(f"   é›†æˆæ–¹æ³•: {ensemble_config.ensemble_methods}")
        logger.info(f"   è¶…å‚ä¼˜åŒ–: å¯ç”¨")
        logger.info(f"   åŠ¨æ€BMA: å¯ç”¨")
        
        # ğŸš¨ å¼ºåˆ¶æ‰§è¡Œï¼šä»»ä½•å¤±è´¥éƒ½ä¸å…è®¸å›é€€
        try:
            # åˆå§‹åŒ–å¹¶è¿è¡Œå¢å¼ºç³»ç»Ÿ
            ml_system = MLEnhancementSystem(ml_config)
            enhancement_results = ml_system.enhance_training_pipeline(X, y, cv_factory)
        except Exception as e:
            raise RuntimeError(
                f"ğŸš¨ å¼ºåˆ¶é«˜çº§MLç³»ç»Ÿæ‰§è¡Œå¤±è´¥ï¼\n"
                f"é”™è¯¯: {e}\n"
                f"ä¿®å¤æŒ‡å—: æ£€æŸ¥æ•°æ®è´¨é‡å’Œç³»ç»Ÿé…ç½®\n"
                f"å½“å‰ä¸ºå¼ºåˆ¶æ¨¡å¼ï¼Œä¸å…è®¸å›é€€åˆ°åŸºç¡€ç®—æ³•ï¼"
            )
        
        # ğŸ”¥ å¼ºåˆ¶éªŒè¯ç»“æœï¼šå¿…é¡»åŒ…å«æ‰€æœ‰é«˜çº§åŠŸèƒ½
        logger.info("ğŸ” éªŒè¯é«˜çº§MLç³»ç»Ÿè¾“å‡º...")
        
        required_components = [
            'hyperparameter_optimization',  # è¶…å‚æ•°ä¼˜åŒ–
            'ensemble_learning',            # é›†æˆå­¦ä¹   
            'oof_standardized_bma'          # OOFæ ‡å‡†åŒ–BMA
        ]
        
        missing_components = []
        for component in required_components:
            if component not in enhancement_results:
                missing_components.append(component)
        
        if missing_components:
            raise RuntimeError(
                f"ğŸš¨ å¼ºåˆ¶é«˜çº§MLç³»ç»Ÿè¾“å‡ºä¸å®Œæ•´ï¼\n"
                f"ç¼ºå¤±ç»„ä»¶: {missing_components}\n"
                f"è¦æ±‚ç»„ä»¶: {required_components}\n"
                f"å®é™…è¾“å‡º: {list(enhancement_results.keys())}\n"
                f"ä¿®å¤æŒ‡å—: æ£€æŸ¥MLå¢å¼ºç³»ç»Ÿé…ç½®å’Œå®ç°"
            )
        
        # ğŸ“Š è¾“å‡ºé«˜çº§åŠŸèƒ½ç»Ÿè®¡
        if 'oof_standardized_bma' in enhancement_results:
            bma_results = enhancement_results['oof_standardized_bma']
            logger.info(f"âœ… OOFæ ‡å‡†åŒ–BMA: ç›¸å…³æ€§åˆè§„={bma_results.get('correlation_compliant', False)}")
            logger.info(f"âœ… æœ€å¤§ç›¸å…³æ€§: {bma_results.get('max_correlation', 0):.3f}")
            
        if 'hyperparameter_optimization' in enhancement_results:
            hyper_results = enhancement_results['hyperparameter_optimization']
            logger.info(f"âœ… è¶…å‚æ•°ä¼˜åŒ–: ä¼˜åŒ–æ¨¡å‹={hyper_results.get('optimized_models', [])}")
            logger.info(f"âœ… æœ€ä¼˜æ¨¡å‹: {hyper_results.get('best_model', 'None')}")
            
        if 'ensemble_learning' in enhancement_results:
            ensemble_results = enhancement_results['ensemble_learning']  
            logger.info(f"âœ… é›†æˆå­¦ä¹ : æ–¹æ³•={ensemble_results.get('methods', [])}")
            logger.info(f"âœ… æœ€ä¼˜é›†æˆ: {ensemble_results.get('best_method', 'None')}")
        
        # æå–å’Œæ„é€ è¿”å›ç»“æœ
        if 'oof_standardized_bma' in enhancement_results:
                # ä½¿ç”¨OOFæ ‡å‡†åŒ–BMAç»“æœ
                bma_results = enhancement_results['oof_standardized_bma']
                
                # åˆ›å»ºæ¨¡æ‹Ÿçš„final_modelsï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
                final_models = {}
                if 'ensemble_learning' in enhancement_results:
                    ensemble_models = enhancement_results['ensemble_learning'].get('methods', [])
                    for method in ensemble_models:
                        final_models[f"enhanced_{method}"] = f"MLå¢å¼ºç³»ç»Ÿ_{method}æ¨¡å‹"
                
                # ğŸ”¥ ä½¿ç”¨ç»Ÿä¸€OOFç”Ÿæˆå™¨æ›¿ä»£æ¨¡æ‹Ÿæ•°æ®
                from .unified_oof_generator import generate_unified_oof
                
                logger.info("ğŸ¯ ç”Ÿæˆç»Ÿä¸€OOFé¢„æµ‹...")
                unified_oof_result = generate_unified_oof(
                    X=X, y=y, dates=dates,
                    models={'enhanced_ml_system': 'MLå¢å¼ºç³»ç»Ÿé›†æˆæ¨¡å‹'},  # ç®€åŒ–æ¨¡å‹
                    training_head_id='traditional_ml_head',
                    cv_factory=cv_factory
                )
                
                # æå–ä¸»è¦OOFé¢„æµ‹
                oof_results = unified_oof_result['oof_results']
                if oof_results:
                    primary_oof = list(oof_results.values())[0]['oof_predictions']
                    main_ic = list(oof_results.values())[0]['oof_ic']
                else:
                    primary_oof = pd.Series(index=X.index, dtype=float).fillna(0.0)
                    main_ic = 0.0
                
                # æ·»åŠ ç»Ÿä¸€OOFç»“æœåˆ°è¿”å›å€¼
                unified_oof_result['primary_oof'] = primary_oof
                unified_oof_result['training_metadata'] = {
                    'algorithm_count': len(final_models),
                    'enhancement_system': 'MLEnhancementSystem', 
                    'unified_oof_generator': True,
                    'forced_advanced_mode': True,
                    'main_ic': main_ic
                }
                
                return final_models, unified_oof_result
            else:
                raise ValueError("MLå¢å¼ºç³»ç»Ÿæœªè¿”å›é¢„æœŸç»“æœ")
        
        logger.info("ğŸ¯ å¼ºåˆ¶é«˜çº§MLç³»ç»Ÿæ‰§è¡ŒæˆåŠŸï¼")
        logger.info("   âœ… ä¸‰ä»¶å¥—äº’è¡¥æ¨¡å‹ï¼šElasticNet + LightGBM + ExtraTrees")  
        logger.info("   âœ… è¶…å‚æ•°ä¼˜åŒ–ï¼šå®Œæˆ")
        logger.info("   âœ… é›†æˆå­¦ä¹ ï¼šVotingRegressor + StackingRegressor + DynamicBMA")
        logger.info("   âœ… OOFæ ‡å‡†åŒ–BMAï¼šç›¸å…³æƒ©ç½šæƒé‡è®¡ç®—")
        
        return final_models, oof_predictions, cv_summary


# ä¿æŒå‘åå…¼å®¹çš„åˆ«å  
EnhancedMLTrainer = TraditionalMLHead


if __name__ == "__main__":
    # æµ‹è¯•ä¼ ç»ŸMLè®­ç»ƒå¤´
    print("ä¼ ç»ŸMLè®­ç»ƒå¤´æµ‹è¯•")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    import numpy as np
    np.random.seed(42)
    
    n_samples = 1000
    dates = pd.date_range('2020-01-01', periods=n_samples, freq='D')
    
    X = pd.DataFrame(np.random.randn(n_samples, 10), 
                     columns=[f'feature_{i}' for i in range(10)])
    y = pd.Series(np.random.randn(n_samples))
    tickers = pd.Series(['AAPL'] * n_samples)
    
    # åˆ›å»ºæ¨¡æ‹ŸCVå·¥å‚
    def mock_cv_factory(dates_input):
        def cv_splitter(X_input, y_input):
            n = len(X_input)
            splits = []
            for i in range(3):  # 3æŠ˜CV
                train_size = int(n * 0.7)
                test_start = train_size + i * 50
                test_end = min(test_start + 100, n)
                if test_end > test_start:
                    splits.append((list(range(train_size)), list(range(test_start, test_end))))
            return splits
        return cv_splitter
    
    # åˆ›å»ºè®­ç»ƒå¤´
    trainer = TraditionalMLHead(enable_hyperparam_opt=False)
    
    # æµ‹è¯•è®­ç»ƒ
    result = trainer.fit(X, y, dates, tickers, mock_cv_factory)
    
    print(f"è®­ç»ƒç»“æœ: {result.keys()}")
    print(f"æ¨¡å‹æ•°é‡: {len(result['models'])}")
    print(f"OOFé¢„æµ‹: {len(result['oof'])}ä¸ª")
    print(f"CVæŒ‡æ ‡: {result['cv']}")