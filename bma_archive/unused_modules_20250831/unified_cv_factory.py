#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€CVå·¥å‚ - æœºæ„çº§å•ä¸€çœŸç›¸æº
æä¾›å”¯ä¸€çš„Purged CVå®ç°ï¼Œæ¶ˆé™¤å„æ¨¡å—é‡å¤CVå®šä¹‰
"""

import pandas as pd
import numpy as np
from typing import Iterator, Tuple, Callable, Optional
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class UnifiedCVFactory:
    """
    ç»Ÿä¸€CVå·¥å‚ - å•ä¸€çœŸç›¸æº
    
    èŒè´£ï¼š
    1. æä¾›å”¯ä¸€çš„PurgedGroupTimeSeriesSplitå®ç°
    2. å¼ºåˆ¶ä½¿ç”¨T10_CONFIGçš„ç»Ÿä¸€å‚æ•°
    3. ç¡®ä¿æ‰€æœ‰è®­ç»ƒå¤´ä½¿ç”¨ç›¸åŒçš„CVç­–ç•¥
    4. é˜²æ­¢CVå‚æ•°è¢«å„æ¨¡å—ç§è‡ªä¿®æ”¹
    """
    
    def __init__(self, config_source='t10_config'):
        """
        åˆå§‹åŒ–CVå·¥å‚
        
        Args:
            config_source: é…ç½®æ¥æº ('t10_config' | 'manual')
        """
        self.config_source = config_source
        self._load_unified_config()
        self._cv_cache = {}  # CVå®ä¾‹ç¼“å­˜ï¼Œé¿å…é‡å¤åˆ›å»º
        
        logger.info(f"ç»Ÿä¸€CVå·¥å‚åˆå§‹åŒ– - é…ç½®æº: {config_source}")
        logger.info(f"ç»Ÿä¸€å‚æ•°: isolation={self.isolation_days}, embargo={self.embargo_days}, splits={self.cv_n_splits}")
    
    def _load_unified_config(self):
        """ä»ç»Ÿä¸€é…ç½®æºåŠ è½½CVå‚æ•°"""
        try:
            if self.config_source == 't10_config':
                from .t10_config import T10_CONFIG
                
                # ğŸ”¥ å•ä¸€çœŸç›¸æºï¼šåªä»T10_CONFIGè¯»å–ï¼Œç¦æ­¢å„æ¨¡å—è¦†ç›–
                self.prediction_horizon = T10_CONFIG.PREDICTION_HORIZON
                self.isolation_days = T10_CONFIG.ISOLATION_DAYS  
                self.embargo_days = T10_CONFIG.EMBARGO_DAYS
                self.cv_n_splits = T10_CONFIG.CV_N_SPLITS
                
                # éªŒè¯å‚æ•°åˆç†æ€§
                self._validate_config()
                
            else:
                # æ‰‹åŠ¨é…ç½®æ¨¡å¼ï¼ˆç”¨äºæµ‹è¯•ï¼‰
                self.prediction_horizon = 10
                self.isolation_days = 21
                self.embargo_days = 15
                self.cv_n_splits = 5
                
        except ImportError:
            logger.warning("T10_CONFIGä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
            self.prediction_horizon = 10
            self.isolation_days = 21
            self.embargo_days = 15
            self.cv_n_splits = 5
    
    def _validate_config(self):
        """éªŒè¯CVé…ç½®å‚æ•°"""
        if self.isolation_days < 1:
            raise ValueError(f"ISOLATION_DAYSå¿…é¡»>=1ï¼Œå½“å‰å€¼: {self.isolation_days}")
        
        if self.embargo_days < 1:
            raise ValueError(f"EMBARGO_DAYSå¿…é¡»>=1ï¼Œå½“å‰å€¼: {self.embargo_days}")
        
        if self.cv_n_splits < 2:
            raise ValueError(f"CV_N_SPLITSå¿…é¡»>=2ï¼Œå½“å‰å€¼: {self.cv_n_splits}")
        
        # æ£€æŸ¥embargo >= æŒæœ‰æœŸ+1çš„è¦æ±‚
        holding_period = 1  # T+1æŒæœ‰
        required_embargo = holding_period + 1
        if self.embargo_days < required_embargo:
            logger.warning(f"EMBARGO_DAYS({self.embargo_days}) < æ¨èå€¼({required_embargo})ï¼Œå¯èƒ½å­˜åœ¨å¾®å°æ³„éœ²é£é™©")
    
    def create_cv_splitter(self, dates: pd.Series, strict_validation: bool = True) -> Callable:
        """
        åˆ›å»ºç»Ÿä¸€çš„CVåˆ†å‰²å™¨
        
        Args:
            dates: æ—¥æœŸåºåˆ—
            strict_validation: æ˜¯å¦å¯ç”¨ä¸¥æ ¼éªŒè¯
            
        Returns:
            CVåˆ†å‰²å‡½æ•°
        """
        # ç”Ÿæˆç¼“å­˜é”®
        if hasattr(dates, 'iloc'):
            date_hash = hash(tuple(dates.iloc[::100]))  # é‡‡æ ·hashé¿å…è¿‡é•¿
        else:
            date_hash = hash(tuple(dates[::100]))  # DatetimeIndexä½¿ç”¨ç›´æ¥ç´¢å¼•
        cache_key = f"{date_hash}_{strict_validation}_{self.cv_n_splits}"
        
        if cache_key in self._cv_cache:
            logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„CVåˆ†å‰²å™¨: {cache_key}")
            return self._cv_cache[cache_key]
        
        # åˆ›å»ºgroupsï¼ˆä»datesæ´¾ç”Ÿï¼‰
        groups = self._create_groups_from_dates(dates)
        
        # åˆ›å»ºé…ç½®
        from .fixed_purged_time_series_cv import ValidationConfig
        
        config = ValidationConfig(
            n_splits=self.cv_n_splits,  # ğŸ”¥ ç»Ÿä¸€æŠ˜æ•°
            test_size=63,  # çº¦3ä¸ªæœˆæµ‹è¯•é›†
            gap=self.isolation_days,   # ğŸ”¥ ç»Ÿä¸€éš”ç¦»æœŸ
            embargo=self.embargo_days, # ğŸ”¥ ç»Ÿä¸€embargo
            min_train_size=252,  # æœ€å°1å¹´è®­ç»ƒé›†
            group_freq='D',
            strict_validation=strict_validation
        )
        
        # åˆ›å»ºCVåˆ†å‰²å™¨
        from .fixed_purged_time_series_cv import FixedPurgedGroupTimeSeriesSplit
        
        cv_splitter = FixedPurgedGroupTimeSeriesSplit(config)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåˆ›å»ºç»Ÿä¸€çš„åˆ†å‰²å‡½æ•°ï¼Œç¡®ä¿groupså¿…ä¼ 
        def unified_split_function(X, y=None, **kwargs):
            """
            ç»Ÿä¸€çš„CVåˆ†å‰²å‡½æ•°
            å¼ºåˆ¶ä¼ å…¥groupsï¼Œé˜²æ­¢å„æ¨¡å—ç»•è¿‡groupçº¦æŸ
            """
            # éªŒè¯è¾“å…¥
            if len(X) != len(groups):
                raise ValueError(f"ç‰¹å¾é•¿åº¦({len(X)})ä¸æ—¥æœŸé•¿åº¦({len(groups)})ä¸åŒ¹é…")
            
            # ğŸ”¥ CRITICAL: groupså¿…ä¼ ï¼Œç¦æ­¢é€€åŒ–åˆ°TimeSeriesSplit
            if 'groups' in kwargs and kwargs['groups'] is not None:
                logger.warning("æ£€æµ‹åˆ°å¤–éƒ¨ä¼ å…¥groupsï¼Œå°†è¢«ç»Ÿä¸€groupsè¦†ç›–")
            
            logger.info(f"æ‰§è¡Œç»Ÿä¸€CVåˆ†å‰²: {len(groups)}ä¸ªæ ·æœ¬, {self.cv_n_splits}æŠ˜")
            logger.info(f"CVå‚æ•°: isolation={self.isolation_days}, embargo={self.embargo_days}")
            
            # æ‰§è¡Œåˆ†å‰²
            splits = list(cv_splitter.split(X, y, groups=groups))
            
            if len(splits) == 0:
                raise ValueError("CVåˆ†å‰²å¤±è´¥ï¼Œæœªç”Ÿæˆä»»ä½•æœ‰æ•ˆåˆ†å‰²")
            
            logger.info(f"âœ… CVåˆ†å‰²æˆåŠŸ: {len(splits)}ä¸ªæœ‰æ•ˆåˆ†å‰²")
            
            return splits
        
        # ç¼“å­˜åˆ†å‰²å™¨
        self._cv_cache[cache_key] = unified_split_function
        
        return unified_split_function
    
    def _create_groups_from_dates(self, dates: pd.Series) -> np.ndarray:
        """ä»æ—¥æœŸåºåˆ—åˆ›å»ºgroups"""
        try:
            # ç¡®ä¿æ˜¯datetimeç±»å‹
            dt_dates = pd.to_datetime(dates)
            
            # ç”ŸæˆYYYYMMDDæ ¼å¼çš„groups
            groups = dt_dates.dt.strftime("%Y%m%d").values
            
            logger.debug(f"ç”Ÿæˆgroups: {len(groups)}ä¸ª, èŒƒå›´: {groups[0]} - {groups[-1]}")
            
            return groups
            
        except Exception as e:
            logger.error(f"groupsç”Ÿæˆå¤±è´¥: {e}")
            # å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨ç´¢å¼•ç”Ÿæˆä¼ªgroups
            logger.warning("ä½¿ç”¨ç´¢å¼•å›é€€æ–¹æ¡ˆç”Ÿæˆgroups")
            return np.arange(len(dates)).astype(str)
    
    def create_cv_factory(self) -> Callable:
        """
        åˆ›å»ºCVå·¥å‚å‡½æ•°ï¼ˆä¸»è¦æ¥å£ï¼‰
        
        Returns:
            cv_factoryå‡½æ•°ï¼Œæ¥å—dateså‚æ•°å¹¶è¿”å›CVåˆ†å‰²å™¨
        """
        def cv_factory(dates: pd.Series, strict_validation: bool = True) -> Callable:
            """
            CVå·¥å‚å‡½æ•°
            
            Args:
                dates: æ—¥æœŸåºåˆ—
                strict_validation: æ˜¯å¦å¯ç”¨ä¸¥æ ¼éªŒè¯
                
            Returns:
                CVåˆ†å‰²å‡½æ•°
            """
            return self.create_cv_splitter(dates, strict_validation)
        
        return cv_factory
    
    def validate_cv_integrity(self, dates: pd.Series, X: pd.DataFrame = None) -> dict:
        """
        éªŒè¯CVå®Œæ•´æ€§ï¼ˆç±»ä¼¼åŸç³»ç»Ÿçš„validate_timesplit_integrityï¼‰
        
        Args:
            dates: æ—¥æœŸåºåˆ—
            X: ç‰¹å¾æ•°æ®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            éªŒè¯ç»“æœ
        """
        validation_result = {
            'valid': True,
            'warnings': [],
            'errors': [],
            'stats': {}
        }
        
        try:
            # åŸºæœ¬ç»Ÿè®¡
            validation_result['stats'] = {
                'total_samples': len(dates),
                'date_range_days': (dates.max() - dates.min()).days,
                'unique_dates': dates.nunique(),
                'cv_splits': self.cv_n_splits,
                'isolation_days': self.isolation_days,
                'embargo_days': self.embargo_days
            }
            
            # æ•°æ®å……è¶³æ€§æ£€æŸ¥
            min_required_days = (self.cv_n_splits + 2) * 63 + self.isolation_days + self.embargo_days
            if validation_result['stats']['date_range_days'] < min_required_days:
                validation_result['warnings'].append(
                    f"æ•°æ®æ—¶é—´è·¨åº¦({validation_result['stats']['date_range_days']}å¤©) < æ¨èå€¼({min_required_days}å¤©)"
                )
            
            # CVåˆ†å‰²æµ‹è¯•
            cv_splitter = self.create_cv_splitter(dates, strict_validation=False)
            
            if X is not None:
                test_splits = cv_splitter(X)
                validation_result['stats']['actual_splits'] = len(test_splits)
                
                if len(test_splits) != self.cv_n_splits:
                    validation_result['warnings'].append(
                        f"å®é™…åˆ†å‰²æ•°({len(test_splits)}) != æœŸæœ›åˆ†å‰²æ•°({self.cv_n_splits})"
                    )
                
                # æ£€æŸ¥åˆ†å‰²å¤§å°
                for i, (train_idx, test_idx) in enumerate(test_splits):
                    if len(train_idx) < 252:  # æœ€å°è®­ç»ƒé›†è¦æ±‚
                        validation_result['warnings'].append(
                            f"ç¬¬{i+1}æŠ˜è®­ç»ƒé›†è¿‡å°: {len(train_idx)}æ ·æœ¬"
                        )
                    if len(test_idx) < 20:  # æœ€å°æµ‹è¯•é›†è¦æ±‚
                        validation_result['warnings'].append(
                            f"ç¬¬{i+1}æŠ˜æµ‹è¯•é›†è¿‡å°: {len(test_idx)}æ ·æœ¬"
                        )
            
        except Exception as e:
            validation_result['valid'] = False
            validation_result['errors'].append(f"CVéªŒè¯å¤±è´¥: {str(e)}")
        
        # åˆ¤æ–­æ€»ä½“æœ‰æ•ˆæ€§
        if validation_result['errors']:
            validation_result['valid'] = False
        
        return validation_result
    
    def get_config_summary(self) -> dict:
        """è·å–é…ç½®æ‘˜è¦"""
        return {
            'config_source': self.config_source,
            'prediction_horizon': self.prediction_horizon,
            'isolation_days': self.isolation_days,
            'embargo_days': self.embargo_days,
            'cv_n_splits': self.cv_n_splits,
            'cache_size': len(self._cv_cache),
            'is_production_config': self.config_source == 't10_config'
        }
    
    def get_fingerprint(self, dates: pd.Series, X: pd.DataFrame = None) -> dict:
        """
        ç”ŸæˆCVåˆ†å‰²æŒ‡çº¹ï¼ˆç”¨äºsplit_fingerprint.jsonï¼‰
        
        Args:
            dates: æ—¥æœŸåºåˆ—
            X: ç‰¹å¾æ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºæ•°æ®hashï¼‰
            
        Returns:
            CVåˆ†å‰²æŒ‡çº¹å­—å…¸
        """
        import hashlib
        from datetime import datetime
        
        try:
            # åŸºæœ¬é…ç½®æŒ‡çº¹
            fingerprint = {
                'cv_config': {
                    'n_splits': self.cv_n_splits,
                    'isolation_days': self.isolation_days,
                    'embargo_days': self.embargo_days,
                    'prediction_horizon': self.prediction_horizon,
                    'cv_type': 'FixedPurgedGroupTimeSeriesSplit'
                },
                'data_info': {
                    'total_samples': len(dates),
                    'date_range': {
                        'start': str(dates.min()),
                        'end': str(dates.max()),
                        'days': (dates.max() - dates.min()).days
                    },
                    'unique_dates': dates.nunique()
                },
                'generation_timestamp': datetime.now().isoformat(),
                'config_source': self.config_source
            }
            
            # Groupsç”Ÿæˆ
            groups = self._create_groups_from_dates(dates)
            fingerprint['groups_info'] = {
                'total_groups': len(groups),
                'unique_groups': len(set(groups)),
                'first_group': str(groups[0]),
                'last_group': str(groups[-1])
            }
            
            # æ•°æ®hashï¼ˆå¦‚æœæä¾›äº†Xï¼‰
            if X is not None:
                data_str = f"{X.shape}_{X.columns.tolist()}_{X.fillna(0).sum().sum()}"
                data_hash = hashlib.md5(data_str.encode()).hexdigest()[:16]
                fingerprint['data_hash'] = data_hash
            
            # Gitä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                import subprocess
                git_sha = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip()[:8]
                fingerprint['git_sha'] = git_sha
            except:
                fingerprint['git_sha'] = 'unknown'
            
            # ç§å­ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
            fingerprint['seed_info'] = {
                'numpy_seed_state': 'not_set',  # CVåˆ†å‰²é€šå¸¸ä¸ä½¿ç”¨éšæœºç§å­
                'deterministic': True  # Purged CVæ˜¯ç¡®å®šæ€§çš„
            }
            
            return fingerprint
            
        except Exception as e:
            logger.error(f"ç”ŸæˆCVæŒ‡çº¹å¤±è´¥: {e}")
            return {
                'error': str(e),
                'generation_timestamp': datetime.now().isoformat(),
                'cv_config': {
                    'n_splits': self.cv_n_splits,
                    'isolation_days': self.isolation_days,
                    'embargo_days': self.embargo_days
                }
            }
    
    def save_split_fingerprint(self, dates: pd.Series, X: pd.DataFrame = None, 
                             output_path: str = "split_fingerprint.json") -> bool:
        """
        ä¿å­˜CVåˆ†å‰²æŒ‡çº¹åˆ°æ–‡ä»¶
        
        Args:
            dates: æ—¥æœŸåºåˆ—
            X: ç‰¹å¾æ•°æ®ï¼ˆå¯é€‰ï¼‰
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            import json
            
            fingerprint = self.get_fingerprint(dates, X)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(fingerprint, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… CVåˆ†å‰²æŒ‡çº¹å·²ä¿å­˜: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜CVåˆ†å‰²æŒ‡çº¹å¤±è´¥: {e}")
            return False


# å…¨å±€CVå·¥å‚å®ä¾‹
_global_cv_factory = None

def get_unified_cv_factory() -> UnifiedCVFactory:
    """è·å–å…¨å±€ç»Ÿä¸€CVå·¥å‚å®ä¾‹"""
    global _global_cv_factory
    if _global_cv_factory is None:
        _global_cv_factory = UnifiedCVFactory('t10_config')
    return _global_cv_factory

def create_cv_for_training(dates: pd.Series) -> Callable:
    """
    ä¾¿æ·å‡½æ•°ï¼šä¸ºè®­ç»ƒåˆ›å»ºç»Ÿä¸€CV
    
    Args:
        dates: æ—¥æœŸåºåˆ—
        
    Returns:
        CVåˆ†å‰²å‡½æ•°
    """
    factory = get_unified_cv_factory()
    return factory.create_cv_splitter(dates, strict_validation=True)


if __name__ == "__main__":
    # æµ‹è¯•ç»Ÿä¸€CVå·¥å‚
    logger.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    dates = pd.date_range('2020-01-01', periods=1000, freq='D')
    X = pd.DataFrame(np.random.randn(1000, 10))
    
    print("æµ‹è¯•ç»Ÿä¸€CVå·¥å‚")
    
    # åˆ›å»ºå·¥å‚
    factory = UnifiedCVFactory('manual')
    
    # é…ç½®æ‘˜è¦
    config = factory.get_config_summary()
    print(f"é…ç½®æ‘˜è¦: {config}")
    
    # åˆ›å»ºCVåˆ†å‰²å™¨
    cv_splitter = factory.create_cv_splitter(dates)
    
    # æµ‹è¯•åˆ†å‰²
    splits = cv_splitter(X)
    print(f"ç”Ÿæˆåˆ†å‰²: {len(splits)}ä¸ª")
    
    for i, (train_idx, test_idx) in enumerate(splits):
        print(f"  åˆ†å‰²{i+1}: è®­ç»ƒ{len(train_idx)}, æµ‹è¯•{len(test_idx)}")
    
    # éªŒè¯å®Œæ•´æ€§
    validation = factory.validate_cv_integrity(dates, X)
    print(f"éªŒè¯ç»“æœ: {validation['valid']}")
    if validation['warnings']:
        print("è­¦å‘Š:")
        for warning in validation['warnings']:
            print(f"  - {warning}")