#!/usr/bin/env python3
"""
å¢å¼ºçš„æ•°æ®å¤„ç†å’Œç¼ºå¤±å€¼å¤„ç†ç³»ç»Ÿ
"""

import numpy as np
import pandas as pd
import warnings
from typing import Dict, List, Optional, Any, Tuple, Union
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import KNNImputer, IterativeImputer
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class DataQualityReport:
    """æ•°æ®è´¨é‡æŠ¥å‘Š"""
    total_samples: int
    total_features: int
    missing_ratio: float
    outlier_ratio: float
    duplicate_ratio: float
    feature_quality: Dict[str, Dict[str, Any]]
    processing_actions: List[str]
    data_issues: List[str]

class SmartImputer:
    """æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†å™¨"""
    
    def __init__(self, strategy: str = 'adaptive'):
        """
        åˆå§‹åŒ–ç¼ºå¤±å€¼å¤„ç†å™¨
        
        Args:
            strategy: 'adaptive', 'knn', 'iterative', 'simple'
        """
        self.strategy = strategy
        self.imputers = {}
        self.feature_strategies = {}
        
    def fit(self, X: pd.DataFrame, feature_types: Optional[Dict[str, str]] = None) -> 'SmartImputer':
        """
        è®­ç»ƒç¼ºå¤±å€¼å¤„ç†å™¨
        
        Args:
            X: è®­ç»ƒæ•°æ®
            feature_types: ç‰¹å¾ç±»å‹æ˜ å°„ {'feature_name': 'price'/'volume'/'technical'/'fundamental'}
        """
        if feature_types is None:
            feature_types = self._infer_feature_types(X)
        
        self.feature_types = feature_types
        
        for feature_type in set(feature_types.values()):
            features = [col for col, ftype in feature_types.items() if ftype == feature_type]
            
            if not features:
                continue
                
            X_subset = X[features]
            
            # æ ¹æ®ç‰¹å¾ç±»å‹é€‰æ‹©æœ€ä½³ç­–ç•¥
            if feature_type == 'price':
                # ä»·æ ¼ç‰¹å¾ï¼šå‰å‘å¡«å…… + çº¿æ€§æ’å€¼
                self.feature_strategies[feature_type] = 'forward_fill'
                
            elif feature_type == 'volume':
                # æˆäº¤é‡ç‰¹å¾ï¼šé›¶å¡«å……æˆ–ä¸­ä½æ•°
                self.feature_strategies[feature_type] = 'median_fill'
                
            elif feature_type == 'technical':
                # æŠ€æœ¯æŒ‡æ ‡ï¼šKNNæ’å€¼
                imputer = KNNImputer(n_neighbors=5, weights='distance')
                self.imputers[feature_type] = imputer.fit(X_subset)
                self.feature_strategies[feature_type] = 'knn'
                
            elif feature_type == 'fundamental':
                # åŸºæœ¬é¢æ•°æ®ï¼šè¿­ä»£æ’å€¼
                imputer = IterativeImputer(random_state=42, max_iter=10)
                self.imputers[feature_type] = imputer.fit(X_subset)
                self.feature_strategies[feature_type] = 'iterative'
                
            else:
                # é»˜è®¤ï¼šKNN
                imputer = KNNImputer(n_neighbors=5)
                self.imputers[feature_type] = imputer.fit(X_subset)
                self.feature_strategies[feature_type] = 'knn'
        
        logger.info(f"ç¼ºå¤±å€¼å¤„ç†å™¨è®­ç»ƒå®Œæˆï¼Œç­–ç•¥: {self.feature_strategies}")
        return self
    
    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        """åº”ç”¨ç¼ºå¤±å€¼å¤„ç†"""
        X_imputed = X.copy()
        
        for feature_type, strategy in self.feature_strategies.items():
            features = [col for col, ftype in self.feature_types.items() if ftype == feature_type]
            
            if not features or not any(col in X_imputed.columns for col in features):
                continue
            
            available_features = [col for col in features if col in X_imputed.columns]
            
            if strategy == 'forward_fill':
                # å‰å‘å¡«å……
                X_imputed[available_features] = X_imputed[available_features].fillna(method='ffill')
                # å›å¡«ç¬¬ä¸€ä¸ªå€¼
                X_imputed[available_features] = X_imputed[available_features].fillna(method='bfill')
                
            elif strategy == 'median_fill':
                # ä¸­ä½æ•°å¡«å……
                for col in available_features:
                    median_val = X_imputed[col].median()
                    X_imputed[col] = X_imputed[col].fillna(median_val)
                    
            elif strategy == 'knn':
                # KNNæ’å€¼
                if feature_type in self.imputers:
                    X_subset = X_imputed[available_features]
                    X_imputed[available_features] = self.imputers[feature_type].transform(X_subset)
                    
            elif strategy == 'iterative':
                # è¿­ä»£æ’å€¼
                if feature_type in self.imputers:
                    X_subset = X_imputed[available_features]
                    X_imputed[available_features] = self.imputers[feature_type].transform(X_subset)
        
        # æ ‡è®°åŸæœ¬ç¼ºå¤±çš„ä½ç½®
        for col in X.columns:
            if X[col].isna().any():
                X_imputed[f'{col}_was_missing'] = X[col].isna().astype(int)
        
        return X_imputed
    
    def _infer_feature_types(self, X: pd.DataFrame) -> Dict[str, str]:
        """æ¨æ–­ç‰¹å¾ç±»å‹"""
        feature_types = {}
        
        for col in X.columns:
            col_lower = col.lower()
            
            if any(keyword in col_lower for keyword in ['price', 'open', 'high', 'low', 'close', 'adj']):
                feature_types[col] = 'price'
            elif any(keyword in col_lower for keyword in ['volume', 'amount', 'turnover']):
                feature_types[col] = 'volume'
            elif any(keyword in col_lower for keyword in ['ma_', 'ema_', 'rsi', 'macd', 'bb_', 'momentum']):
                feature_types[col] = 'technical'
            elif any(keyword in col_lower for keyword in ['pe', 'pb', 'roe', 'roa', 'eps', 'revenue']):
                feature_types[col] = 'fundamental'
            else:
                feature_types[col] = 'other'
        
        return feature_types

class AnomalyDetector:
    """å¤šç­–ç•¥å¼‚å¸¸å€¼æ£€æµ‹å™¨"""
    
    def __init__(self, contamination: float = 0.01):
        """
        åˆå§‹åŒ–å¼‚å¸¸å€¼æ£€æµ‹å™¨
        
        Args:
            contamination: é¢„æœŸå¼‚å¸¸å€¼æ¯”ä¾‹
        """
        self.contamination = contamination
        self.detectors = {
            'isolation_forest': IsolationForest(
                contamination=contamination,
                random_state=42,
                n_estimators=100
            ),
            'local_outlier_factor': LocalOutlierFactor(
                contamination=contamination,
                n_neighbors=20
            )
        }
        
    def fit(self, X: pd.DataFrame) -> 'AnomalyDetector':
        """è®­ç»ƒå¼‚å¸¸æ£€æµ‹å™¨"""
        X_numeric = X.select_dtypes(include=[np.number])
        
        if len(X_numeric.columns) == 0:
            logger.warning("æ²¡æœ‰æ•°å€¼å‹ç‰¹å¾ç”¨äºå¼‚å¸¸æ£€æµ‹")
            return self
        
        # æ ‡å‡†åŒ–æ•°æ®
        self.scaler = RobustScaler()
        X_scaled = self.scaler.fit_transform(X_numeric.fillna(X_numeric.median()))
        
        # è®­ç»ƒæ£€æµ‹å™¨
        try:
            self.detectors['isolation_forest'].fit(X_scaled)
            logger.info("Isolation Forestå¼‚å¸¸æ£€æµ‹å™¨è®­ç»ƒå®Œæˆ")
        except Exception as e:
            logger.warning(f"Isolation Forestè®­ç»ƒå¤±è´¥: {e}")
        
        return self
    
    def detect_outliers(self, X: pd.DataFrame) -> Dict[str, np.ndarray]:
        """æ£€æµ‹å¼‚å¸¸å€¼"""
        X_numeric = X.select_dtypes(include=[np.number])
        
        if len(X_numeric.columns) == 0:
            return {'combined': np.zeros(len(X), dtype=bool)}
        
        # æ ‡å‡†åŒ–æ•°æ®
        X_scaled = self.scaler.transform(X_numeric.fillna(X_numeric.median()))
        
        outlier_masks = {}
        
        # Isolation Forest
        try:
            outlier_masks['isolation_forest'] = self.detectors['isolation_forest'].predict(X_scaled) == -1
        except Exception as e:
            logger.warning(f"Isolation Forestæ£€æµ‹å¤±è´¥: {e}")
            outlier_masks['isolation_forest'] = np.zeros(len(X), dtype=bool)
        
        # Local Outlier Factor
        try:
            outlier_masks['local_outlier_factor'] = self.detectors['local_outlier_factor'].fit_predict(X_scaled) == -1
        except Exception as e:
            logger.warning(f"LOFæ£€æµ‹å¤±è´¥: {e}")
            outlier_masks['local_outlier_factor'] = np.zeros(len(X), dtype=bool)
        
        # ç»Ÿè®¡å­¦æ–¹æ³•ï¼šIQR
        outlier_masks['statistical'] = self._statistical_outliers(X_numeric)
        
        # ç»„åˆç»“æœï¼šè‡³å°‘ä¸¤ç§æ–¹æ³•è®¤ä¸ºæ˜¯å¼‚å¸¸å€¼
        combined_mask = np.zeros(len(X), dtype=bool)
        for i in range(len(X)):
            outlier_count = sum(mask[i] for mask in outlier_masks.values())
            combined_mask[i] = outlier_count >= 2
        
        outlier_masks['combined'] = combined_mask
        
        return outlier_masks
    
    def _statistical_outliers(self, X: pd.DataFrame) -> np.ndarray:
        """åŸºäºIQRçš„ç»Ÿè®¡å­¦å¼‚å¸¸æ£€æµ‹"""
        outlier_mask = np.zeros(len(X), dtype=bool)
        
        for col in X.columns:
            if X[col].dtype in [np.number]:
                Q1 = X[col].quantile(0.25)
                Q3 = X[col].quantile(0.75)
                IQR = Q3 - Q1
                
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                col_outliers = (X[col] < lower_bound) | (X[col] > upper_bound)
                outlier_mask = outlier_mask | col_outliers.fillna(False)
        
        return outlier_mask

class EnhancedDataProcessor:
    """å¢å¼ºçš„æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, handle_outliers: bool = True, 
                 handle_missing: bool = True,
                 optimize_dtypes: bool = True):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        
        Args:
            handle_outliers: æ˜¯å¦å¤„ç†å¼‚å¸¸å€¼
            handle_missing: æ˜¯å¦å¤„ç†ç¼ºå¤±å€¼
            optimize_dtypes: æ˜¯å¦ä¼˜åŒ–æ•°æ®ç±»å‹
        """
        self.handle_outliers = handle_outliers
        self.handle_missing = handle_missing
        self.optimize_dtypes = optimize_dtypes
        
        self.imputer = SmartImputer()
        self.anomaly_detector = AnomalyDetector()
        self.fitted = False
        
    def fit(self, X: pd.DataFrame, feature_types: Optional[Dict[str, str]] = None) -> 'EnhancedDataProcessor':
        """è®­ç»ƒæ•°æ®å¤„ç†å™¨"""
        logger.info("å¼€å§‹è®­ç»ƒæ•°æ®å¤„ç†å™¨...")
        
        if self.handle_missing:
            self.imputer.fit(X, feature_types)
        
        if self.handle_outliers:
            self.anomaly_detector.fit(X)
        
        self.fitted = True
        logger.info("æ•°æ®å¤„ç†å™¨è®­ç»ƒå®Œæˆ")
        
        return self
    
    def transform(self, X: pd.DataFrame, remove_outliers: bool = False) -> Tuple[pd.DataFrame, DataQualityReport]:
        """å¤„ç†æ•°æ®å¹¶ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        if not self.fitted:
            raise ValueError("æ•°æ®å¤„ç†å™¨å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fit()æ–¹æ³•")
        
        logger.info("å¼€å§‹æ•°æ®å¤„ç†...")
        X_processed = X.copy()
        processing_actions = []
        data_issues = []
        
        # 1. æ•°æ®ç±»å‹ä¼˜åŒ–
        if self.optimize_dtypes:
            X_processed, dtype_actions = self._optimize_dtypes(X_processed)
            processing_actions.extend(dtype_actions)
        
        # 2. é‡å¤å€¼å¤„ç†
        initial_len = len(X_processed)
        X_processed = X_processed.drop_duplicates()
        duplicate_count = initial_len - len(X_processed)
        
        if duplicate_count > 0:
            processing_actions.append(f"ç§»é™¤{duplicate_count}ä¸ªé‡å¤è¡Œ")
            data_issues.append(f"å‘ç°{duplicate_count}ä¸ªé‡å¤è¡Œ")
        
        # 3. å¼‚å¸¸å€¼æ£€æµ‹
        outlier_masks = {}
        if self.handle_outliers:
            outlier_masks = self.anomaly_detector.detect_outliers(X_processed)
            outlier_count = outlier_masks['combined'].sum()
            
            if outlier_count > 0:
                data_issues.append(f"æ£€æµ‹åˆ°{outlier_count}ä¸ªå¼‚å¸¸å€¼")
                
                if remove_outliers:
                    X_processed = X_processed[~outlier_masks['combined']]
                    processing_actions.append(f"ç§»é™¤{outlier_count}ä¸ªå¼‚å¸¸å€¼")
                else:
                    # æ ‡è®°å¼‚å¸¸å€¼
                    X_processed['is_outlier'] = outlier_masks['combined']
                    processing_actions.append(f"æ ‡è®°{outlier_count}ä¸ªå¼‚å¸¸å€¼")
        
        # 4. ç¼ºå¤±å€¼å¤„ç†
        missing_before = X_processed.isna().sum().sum()
        if self.handle_missing and missing_before > 0:
            X_processed = self.imputer.transform(X_processed)
            missing_after = X_processed.isna().sum().sum()
            
            processing_actions.append(f"å¤„ç†ç¼ºå¤±å€¼: {missing_before} -> {missing_after}")
            
            if missing_before > 0:
                data_issues.append(f"åŸå§‹æ•°æ®åŒ…å«{missing_before}ä¸ªç¼ºå¤±å€¼")
        
        # 5. ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        quality_report = self._generate_quality_report(
            X_original=X,
            X_processed=X_processed,
            outlier_masks=outlier_masks,
            processing_actions=processing_actions,
            data_issues=data_issues,
            duplicate_count=duplicate_count
        )
        
        logger.info(f"æ•°æ®å¤„ç†å®Œæˆï¼Œè´¨é‡å¾—åˆ†: {quality_report.missing_ratio:.2%} ç¼ºå¤±, {quality_report.outlier_ratio:.2%} å¼‚å¸¸")
        
        return X_processed, quality_report
    
    def _optimize_dtypes(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
        """ä¼˜åŒ–æ•°æ®ç±»å‹ä»¥èŠ‚çœå†…å­˜"""
        actions = []
        X_optimized = X.copy()
        
        for col in X_optimized.columns:
            col_type = X_optimized[col].dtype
            
            if str(col_type)[:3] == 'int':
                c_min = X_optimized[col].min()
                c_max = X_optimized[col].max()
                
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    X_optimized[col] = X_optimized[col].astype(np.int8)
                    actions.append(f"{col}: int64 -> int8")
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    X_optimized[col] = X_optimized[col].astype(np.int16)
                    actions.append(f"{col}: int64 -> int16")
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    X_optimized[col] = X_optimized[col].astype(np.int32)
                    actions.append(f"{col}: int64 -> int32")
                    
            elif str(col_type)[:5] == 'float':
                c_min = X_optimized[col].min()
                c_max = X_optimized[col].max()
                
                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    X_optimized[col] = X_optimized[col].astype(np.float32)
                    actions.append(f"{col}: float64 -> float32")
        
        return X_optimized, actions
    
    def _generate_quality_report(self, X_original: pd.DataFrame, X_processed: pd.DataFrame,
                                outlier_masks: Dict, processing_actions: List[str],
                                data_issues: List[str], duplicate_count: int) -> DataQualityReport:
        """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
        
        # åŸºæœ¬ç»Ÿè®¡
        total_samples = len(X_original)
        total_features = len(X_original.columns)
        missing_ratio = X_original.isna().sum().sum() / (total_samples * total_features)
        outlier_ratio = outlier_masks.get('combined', np.array([])).sum() / total_samples if outlier_masks else 0.0
        duplicate_ratio = duplicate_count / total_samples
        
        # ç‰¹å¾è´¨é‡åˆ†æ
        feature_quality = {}
        for col in X_original.columns:
            feature_quality[col] = {
                'missing_ratio': X_original[col].isna().mean(),
                'unique_ratio': X_original[col].nunique() / len(X_original),
                'dtype': str(X_original[col].dtype),
                'zero_ratio': (X_original[col] == 0).mean() if X_original[col].dtype in [np.number] else 0,
                'infinite_values': np.isinf(X_original[col]).sum() if X_original[col].dtype in [np.number] else 0
            }
        
        return DataQualityReport(
            total_samples=total_samples,
            total_features=total_features,
            missing_ratio=missing_ratio,
            outlier_ratio=outlier_ratio,
            duplicate_ratio=duplicate_ratio,
            feature_quality=feature_quality,
            processing_actions=processing_actions,
            data_issues=data_issues
        )


def example_usage():
    """ç¤ºä¾‹ç”¨æ³•"""
    print("ğŸ”§ å¢å¼ºæ•°æ®å¤„ç†ç³»ç»Ÿç¤ºä¾‹")
    
    # ç”Ÿæˆå¸¦é—®é¢˜çš„æµ‹è¯•æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    
    data = {
        'close_price': np.random.normal(100, 10, n_samples),
        'volume': np.random.exponential(1000000, n_samples),
        'ma_20': np.random.normal(100, 8, n_samples),
        'rsi': np.random.uniform(0, 100, n_samples),
        'pe_ratio': np.random.normal(15, 5, n_samples)
    }
    
    df = pd.DataFrame(data)
    
    # å¼•å…¥æ•°æ®é—®é¢˜
    # 1. ç¼ºå¤±å€¼
    missing_indices = np.random.choice(n_samples, size=int(n_samples * 0.1), replace=False)
    df.loc[missing_indices, 'close_price'] = np.nan
    
    missing_indices = np.random.choice(n_samples, size=int(n_samples * 0.05), replace=False)
    df.loc[missing_indices, 'pe_ratio'] = np.nan
    
    # 2. å¼‚å¸¸å€¼
    outlier_indices = np.random.choice(n_samples, size=int(n_samples * 0.02), replace=False)
    df.loc[outlier_indices, 'close_price'] = np.random.normal(200, 50, len(outlier_indices))
    
    # 3. é‡å¤å€¼
    duplicate_indices = np.random.choice(n_samples, size=20, replace=False)
    df = pd.concat([df, df.iloc[duplicate_indices]], ignore_index=True)
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)}è¡Œ, {len(df.columns)}åˆ—")
    print(f"   ç¼ºå¤±å€¼: {df.isna().sum().sum()}")
    print(f"   é‡å¤è¡Œ: {df.duplicated().sum()}")
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = EnhancedDataProcessor(
        handle_outliers=True,
        handle_missing=True,
        optimize_dtypes=True
    )
    
    # è®­ç»ƒå’Œå¤„ç†
    processor.fit(df)
    df_processed, quality_report = processor.transform(df, remove_outliers=False)
    
    print(f"\nâœ… å¤„ç†åæ•°æ®: {len(df_processed)}è¡Œ, {len(df_processed.columns)}åˆ—")
    print(f"   ç¼ºå¤±å€¼: {df_processed.isna().sum().sum()}")
    print(f"   å¼‚å¸¸å€¼æ¯”ä¾‹: {quality_report.outlier_ratio:.2%}")
    print(f"   æ•°æ®è´¨é‡: {(1 - quality_report.missing_ratio) * 100:.1f}%")
    
    print(f"\nğŸ”§ å¤„ç†åŠ¨ä½œ:")
    for action in quality_report.processing_actions:
        print(f"   - {action}")
    
    print(f"\nâš ï¸  æ•°æ®é—®é¢˜:")
    for issue in quality_report.data_issues:
        print(f"   - {issue}")
    
    return processor, quality_report


if __name__ == "__main__":
    example_usage()
