#!/usr/bin/env python3
"""
æ•°æ®ç»“æ„éªŒè¯è„šæœ¬ - æ£€æµ‹BMAæ¨¡å‹ä¸­çš„æ‰€æœ‰æ•°æ®ç»“æ„é—®é¢˜
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime, timedelta
import traceback
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataStructureValidator:
    """æ•°æ®ç»“æ„éªŒè¯å™¨ - æ£€æµ‹æ‰€æœ‰æ½œåœ¨çš„æ•°æ®ç»“æ„é—®é¢˜"""
    
    def __init__(self):
        self.issues = {
            'critical': [],
            'high': [],
            'medium': [],
            'low': []
        }
        self.validation_results = {}
        
    def validate_multiindex(self, df: pd.DataFrame, name: str = "DataFrame") -> bool:
        """éªŒè¯MultiIndexç»“æ„"""
        logger.info(f"\n{'='*60}")
        logger.info(f"éªŒè¯ {name} çš„MultiIndexç»“æ„")
        
        passed = True
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºMultiIndex
        if not isinstance(df.index, pd.MultiIndex):
            self.issues['critical'].append(f"{name}: ä¸æ˜¯MultiIndexæ ¼å¼")
            logger.error(f"âŒ {name} ä¸æ˜¯MultiIndexæ ¼å¼")
            passed = False
        else:
            logger.info(f"âœ… {name} æ˜¯MultiIndexæ ¼å¼")
            
            # æ£€æŸ¥MultiIndexçš„levels
            if df.index.nlevels != 2:
                self.issues['high'].append(f"{name}: MultiIndexåº”è¯¥æœ‰2ä¸ªlevelsï¼Œå®é™…æœ‰{df.index.nlevels}ä¸ª")
                passed = False
            
            # æ£€æŸ¥level names
            expected_names = ['date', 'ticker']
            actual_names = list(df.index.names)
            if actual_names != expected_names:
                self.issues['medium'].append(f"{name}: MultiIndex namesåº”è¯¥æ˜¯{expected_names}ï¼Œå®é™…æ˜¯{actual_names}")
                logger.warning(f"âš ï¸ MultiIndex namesä¸æ ‡å‡†: {actual_names}")
        
        # æ£€æŸ¥é‡å¤ç´¢å¼•
        if df.index.duplicated().any():
            dup_count = df.index.duplicated().sum()
            self.issues['critical'].append(f"{name}: å­˜åœ¨{dup_count}ä¸ªé‡å¤ç´¢å¼•")
            logger.error(f"âŒ å‘ç°{dup_count}ä¸ªé‡å¤ç´¢å¼•")
            passed = False
        
        # æ£€æŸ¥ç´¢å¼•æ’åº
        if isinstance(df.index, pd.MultiIndex):
            if not df.index.is_monotonic_increasing:
                self.issues['medium'].append(f"{name}: ç´¢å¼•æœªæ’åº")
                logger.warning(f"âš ï¸ ç´¢å¼•æœªæŒ‰å‡åºæ’åº")
        
        return passed
    
    def validate_data_types(self, df: pd.DataFrame, name: str = "DataFrame") -> bool:
        """éªŒè¯æ•°æ®ç±»å‹ä¸€è‡´æ€§"""
        logger.info(f"\néªŒè¯ {name} çš„æ•°æ®ç±»å‹")
        
        passed = True
        
        # æ£€æŸ¥objectç±»å‹åˆ—
        object_cols = df.select_dtypes(include=['object']).columns.tolist()
        if object_cols:
            self.issues['high'].append(f"{name}: åŒ…å«objectç±»å‹åˆ—: {object_cols}")
            logger.warning(f"âš ï¸ å‘ç°objectç±»å‹åˆ—: {object_cols}")
            passed = False
        
        # æ£€æŸ¥æ··åˆç±»å‹
        for col in df.columns:
            if df[col].dtype == 'object':
                continue
            # æ£€æŸ¥æ˜¯å¦æœ‰æ··åˆç±»å‹
            try:
                pd.to_numeric(df[col], errors='coerce')
            except:
                self.issues['medium'].append(f"{name}.{col}: åŒ…å«æ··åˆæ•°æ®ç±»å‹")
                passed = False
        
        # æ£€æŸ¥æ•°å€¼åˆ—çš„dtypeä¸€è‡´æ€§
        numeric_dtypes = df.select_dtypes(include=[np.number]).dtypes
        if len(numeric_dtypes.unique()) > 1:
            self.issues['low'].append(f"{name}: æ•°å€¼åˆ—ä½¿ç”¨äº†å¤šç§dtype: {numeric_dtypes.unique()}")
            logger.info(f"â„¹ï¸ æ•°å€¼åˆ—dtypeä¸ç»Ÿä¸€: {numeric_dtypes.unique()}")
        
        return passed
    
    def validate_nan_handling(self, df: pd.DataFrame, name: str = "DataFrame") -> bool:
        """éªŒè¯NaNå¤„ç†"""
        logger.info(f"\néªŒè¯ {name} çš„NaNå¤„ç†")
        
        passed = True
        
        # æ£€æŸ¥NaNæ•°é‡
        nan_count = df.isnull().sum().sum()
        if nan_count > 0:
            nan_percentage = (nan_count / (df.shape[0] * df.shape[1])) * 100
            
            if nan_percentage > 50:
                self.issues['critical'].append(f"{name}: NaNæ¯”ä¾‹è¿‡é«˜ ({nan_percentage:.2f}%)")
                logger.error(f"âŒ NaNæ¯”ä¾‹è¿‡é«˜: {nan_percentage:.2f}%")
                passed = False
            elif nan_percentage > 10:
                self.issues['high'].append(f"{name}: åŒ…å«è¾ƒå¤šNaN ({nan_percentage:.2f}%)")
                logger.warning(f"âš ï¸ åŒ…å«{nan_count}ä¸ªNaN ({nan_percentage:.2f}%)")
            else:
                self.issues['low'].append(f"{name}: åŒ…å«{nan_count}ä¸ªNaN ({nan_percentage:.2f}%)")
                logger.info(f"â„¹ï¸ åŒ…å«{nan_count}ä¸ªNaN ({nan_percentage:.2f}%)")
        
        # æ£€æŸ¥infå€¼
        inf_count = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()
        if inf_count > 0:
            self.issues['high'].append(f"{name}: åŒ…å«{inf_count}ä¸ªinfå€¼")
            logger.error(f"âŒ å‘ç°{inf_count}ä¸ªinfå€¼")
            passed = False
        
        return passed
    
    def validate_temporal_consistency(self, df: pd.DataFrame, name: str = "DataFrame") -> bool:
        """éªŒè¯æ—¶é—´åºåˆ—ä¸€è‡´æ€§"""
        logger.info(f"\néªŒè¯ {name} çš„æ—¶é—´åºåˆ—ä¸€è‡´æ€§")
        
        passed = True
        
        if isinstance(df.index, pd.MultiIndex) and 'date' in df.index.names:
            dates = df.index.get_level_values('date')
            
            # æ£€æŸ¥æ—¥æœŸç±»å‹
            if not isinstance(dates[0], (pd.Timestamp, datetime)):
                self.issues['high'].append(f"{name}: æ—¥æœŸä¸æ˜¯datetimeç±»å‹")
                logger.error(f"âŒ æ—¥æœŸä¸æ˜¯datetimeç±»å‹: {type(dates[0])}")
                passed = False
            
            # æ£€æŸ¥æ—¥æœŸèŒƒå›´
            date_range = dates.max() - dates.min()
            if date_range.days < 30:
                self.issues['medium'].append(f"{name}: æ—¥æœŸèŒƒå›´è¿‡çŸ­ ({date_range.days}å¤©)")
                logger.warning(f"âš ï¸ æ—¥æœŸèŒƒå›´ä»…{date_range.days}å¤©")
            
            # æ£€æŸ¥æ—¥æœŸè¿ç»­æ€§
            unique_dates = sorted(dates.unique())
            expected_dates = pd.date_range(unique_dates[0], unique_dates[-1], freq='D')
            missing_dates = set(expected_dates) - set(unique_dates)
            if len(missing_dates) > len(unique_dates) * 0.3:  # ç¼ºå¤±è¶…è¿‡30%
                self.issues['medium'].append(f"{name}: ç¼ºå¤±{len(missing_dates)}ä¸ªæ—¥æœŸ")
                logger.warning(f"âš ï¸ æ—¶é—´åºåˆ—ä¸è¿ç»­ï¼Œç¼ºå¤±{len(missing_dates)}ä¸ªæ—¥æœŸ")
        
        return passed
    
    def validate_cross_sectional_consistency(self, df: pd.DataFrame, name: str = "DataFrame") -> bool:
        """éªŒè¯æ¨ªæˆªé¢ä¸€è‡´æ€§"""
        logger.info(f"\néªŒè¯ {name} çš„æ¨ªæˆªé¢ä¸€è‡´æ€§")
        
        passed = True
        
        if isinstance(df.index, pd.MultiIndex) and 'date' in df.index.names and 'ticker' in df.index.names:
            # æ£€æŸ¥æ¯ä¸ªæ—¥æœŸçš„è‚¡ç¥¨æ•°é‡
            ticker_counts = df.groupby(level='date').size()
            
            min_tickers = ticker_counts.min()
            max_tickers = ticker_counts.max()
            
            if min_tickers < 2:
                self.issues['high'].append(f"{name}: æŸäº›æ—¥æœŸè‚¡ç¥¨æ•°é‡è¿‡å°‘ (æœ€å°‘{min_tickers}åª)")
                logger.error(f"âŒ æŸäº›æ—¥æœŸä»…æœ‰{min_tickers}åªè‚¡ç¥¨")
                passed = False
            
            # æ£€æŸ¥è‚¡ç¥¨æ•°é‡å˜åŒ–
            ticker_std = ticker_counts.std()
            if ticker_std > ticker_counts.mean() * 0.5:  # æ ‡å‡†å·®è¶…è¿‡å‡å€¼çš„50%
                self.issues['medium'].append(f"{name}: æ¨ªæˆªé¢è‚¡ç¥¨æ•°é‡å˜åŒ–è¿‡å¤§ (std={ticker_std:.2f})")
                logger.warning(f"âš ï¸ æ¨ªæˆªé¢è‚¡ç¥¨æ•°é‡ä¸ç¨³å®š: {min_tickers} ~ {max_tickers}")
        
        return passed
    
    def validate_dimension_consistency(self, train_df: pd.DataFrame, test_df: pd.DataFrame) -> bool:
        """éªŒè¯è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç»´åº¦ä¸€è‡´æ€§"""
        logger.info(f"\n{'='*60}")
        logger.info("éªŒè¯è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç»´åº¦ä¸€è‡´æ€§")
        
        passed = True
        
        # æ£€æŸ¥åˆ—æ•°
        if train_df.shape[1] != test_df.shape[1]:
            self.issues['critical'].append(f"ç»´åº¦ä¸åŒ¹é…: è®­ç»ƒé›†{train_df.shape[1]}åˆ—, æµ‹è¯•é›†{test_df.shape[1]}åˆ—")
            logger.error(f"âŒ åˆ—æ•°ä¸åŒ¹é…: {train_df.shape[1]} vs {test_df.shape[1]}")
            passed = False
        
        # æ£€æŸ¥åˆ—å
        train_cols = set(train_df.columns)
        test_cols = set(test_df.columns)
        
        missing_in_test = train_cols - test_cols
        extra_in_test = test_cols - train_cols
        
        if missing_in_test:
            self.issues['critical'].append(f"æµ‹è¯•é›†ç¼ºå¤±åˆ—: {missing_in_test}")
            logger.error(f"âŒ æµ‹è¯•é›†ç¼ºå¤±åˆ—: {missing_in_test}")
            passed = False
        
        if extra_in_test:
            self.issues['high'].append(f"æµ‹è¯•é›†å¤šä½™åˆ—: {extra_in_test}")
            logger.warning(f"âš ï¸ æµ‹è¯•é›†å¤šä½™åˆ—: {extra_in_test}")
        
        # æ£€æŸ¥åˆ—é¡ºåº
        if list(train_df.columns) != list(test_df.columns):
            self.issues['medium'].append("åˆ—é¡ºåºä¸ä¸€è‡´")
            logger.warning("âš ï¸ åˆ—é¡ºåºä¸ä¸€è‡´")
        
        return passed
    
    def validate_cv_temporal_safety(self, cv_gap: int, embargo: int, horizon: int) -> bool:
        """éªŒè¯CVæ—¶é—´å®‰å…¨æ€§"""
        logger.info(f"\n{'='*60}")
        logger.info("éªŒè¯CVæ—¶é—´å®‰å…¨å‚æ•°")
        
        passed = True
        
        # æ£€æŸ¥gapæ˜¯å¦è¶³å¤Ÿ
        if cv_gap < horizon - 1:
            self.issues['critical'].append(f"CV gapä¸è¶³: {cv_gap} < {horizon-1}")
            logger.error(f"âŒ CV gapä¸è¶³ä»¥é˜²æ­¢æ•°æ®æ³„æ¼: gap={cv_gap}, éœ€è¦>={horizon-1}")
            passed = False
        
        # æ£€æŸ¥embargoæ˜¯å¦åˆç†
        if embargo < horizon:
            self.issues['high'].append(f"CV embargoä¸è¶³: {embargo} < {horizon}")
            logger.warning(f"âš ï¸ CV embargoå¯èƒ½ä¸è¶³: embargo={embargo}, horizon={horizon}")
        
        # æ£€æŸ¥æ€»éš”ç¦»æœŸ
        total_isolation = cv_gap + embargo
        if total_isolation < horizon * 1.5:
            self.issues['medium'].append(f"æ€»éš”ç¦»æœŸåçŸ­: {total_isolation} < {horizon * 1.5}")
            logger.info(f"â„¹ï¸ æ€»éš”ç¦»æœŸ={total_isolation}å¤©")
        
        logger.info(f"æ—¶é—´é…ç½®: gap={cv_gap}, embargo={embargo}, horizon={horizon}")
        
        return passed
    
    def run_comprehensive_validation(self, feature_data: Optional[pd.DataFrame] = None) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆéªŒè¯"""
        logger.info("="*80)
        logger.info("å¼€å§‹æ•°æ®ç»“æ„ç»¼åˆéªŒè¯")
        logger.info("="*80)
        
        # å¦‚æœæ²¡æœ‰æä¾›æ•°æ®ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        if feature_data is None:
            feature_data = self.create_test_data()
        
        # 1. éªŒè¯MultiIndex
        self.validate_multiindex(feature_data, "feature_data")
        
        # 2. éªŒè¯æ•°æ®ç±»å‹
        self.validate_data_types(feature_data, "feature_data")
        
        # 3. éªŒè¯NaNå¤„ç†
        self.validate_nan_handling(feature_data, "feature_data")
        
        # 4. éªŒè¯æ—¶é—´åºåˆ—ä¸€è‡´æ€§
        self.validate_temporal_consistency(feature_data, "feature_data")
        
        # 5. éªŒè¯æ¨ªæˆªé¢ä¸€è‡´æ€§
        self.validate_cross_sectional_consistency(feature_data, "feature_data")
        
        # 6. æ¨¡æ‹Ÿè®­ç»ƒ/æµ‹è¯•åˆ†å‰²å¹¶éªŒè¯
        if len(feature_data) > 100:
            split_point = int(len(feature_data) * 0.8)
            train_data = feature_data.iloc[:split_point]
            test_data = feature_data.iloc[split_point:]
            self.validate_dimension_consistency(train_data, test_data)
        
        # 7. éªŒè¯CVæ—¶é—´å®‰å…¨æ€§
        self.validate_cv_temporal_safety(cv_gap=9, embargo=10, horizon=10)
        
        # ç”ŸæˆæŠ¥å‘Š
        return self.generate_report()
    
    def create_test_data(self) -> pd.DataFrame:
        """åˆ›å»ºæµ‹è¯•æ•°æ®"""
        dates = pd.date_range('2024-01-01', periods=100, freq='D')
        tickers = ['AAPL', 'GOOGL', 'MSFT']
        
        index = pd.MultiIndex.from_product([dates, tickers], names=['date', 'ticker'])
        
        data = pd.DataFrame({
            'feature1': np.random.randn(len(index)),
            'feature2': np.random.randn(len(index)),
            'feature3': np.random.randn(len(index)),
            'target': np.random.randn(len(index))
        }, index=index)
        
        # æ·»åŠ ä¸€äº›é—®é¢˜
        data.iloc[10:20, 0] = np.nan  # æ·»åŠ NaN
        data.iloc[50, 1] = np.inf     # æ·»åŠ inf
        
        return data
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        logger.info("\n" + "="*80)
        logger.info("æ•°æ®ç»“æ„éªŒè¯æŠ¥å‘Š")
        logger.info("="*80)
        
        total_issues = sum(len(issues) for issues in self.issues.values())
        
        logger.info(f"\nå‘ç°é—®é¢˜æ€»æ•°: {total_issues}")
        logger.info(f"  - å…³é”®é—®é¢˜: {len(self.issues['critical'])}")
        logger.info(f"  - é«˜é£é™©é—®é¢˜: {len(self.issues['high'])}")
        logger.info(f"  - ä¸­é£é™©é—®é¢˜: {len(self.issues['medium'])}")
        logger.info(f"  - ä½é£é™©é—®é¢˜: {len(self.issues['low'])}")
        
        if self.issues['critical']:
            logger.error("\nğŸ”´ å…³é”®é—®é¢˜ï¼ˆå¿…é¡»ç«‹å³ä¿®å¤ï¼‰:")
            for issue in self.issues['critical']:
                logger.error(f"  â€¢ {issue}")
        
        if self.issues['high']:
            logger.warning("\nğŸŸ  é«˜é£é™©é—®é¢˜:")
            for issue in self.issues['high']:
                logger.warning(f"  â€¢ {issue}")
        
        if self.issues['medium']:
            logger.info("\nğŸŸ¡ ä¸­é£é™©é—®é¢˜:")
            for issue in self.issues['medium']:
                logger.info(f"  â€¢ {issue}")
        
        if self.issues['low']:
            logger.info("\nğŸŸ¢ ä½é£é™©é—®é¢˜:")
            for issue in self.issues['low']:
                logger.info(f"  â€¢ {issue}")
        
        # ç”Ÿæˆä¿®å¤å»ºè®®
        if self.issues['critical'] or self.issues['high']:
            logger.info("\n" + "="*60)
            logger.info("ä¿®å¤å»ºè®®:")
            logger.info("1. ä¼˜å…ˆä¿®å¤æ‰€æœ‰å…³é”®é—®é¢˜")
            logger.info("2. ç»Ÿä¸€ä½¿ç”¨MultiIndex(date, ticker)æ ¼å¼")
            logger.info("3. ç¡®ä¿CVå‚æ•°æ»¡è¶³: gap >= horizon-1")
            logger.info("4. å®æ–½ä¸¥æ ¼çš„NaNå¤„ç†ç­–ç•¥")
            logger.info("5. æ·»åŠ æ•°æ®éªŒè¯æ–­è¨€")
        
        return {
            'total_issues': total_issues,
            'issues_by_severity': self.issues,
            'validation_timestamp': datetime.now().isoformat(),
            'passed': len(self.issues['critical']) == 0
        }


def main():
    """ä¸»å‡½æ•°"""
    validator = DataStructureValidator()
    
    # å°è¯•åŠ è½½çœŸå®æ•°æ®
    try:
        from bma_models.polygon_client import PolygonDataProvider
        provider = PolygonDataProvider()
        
        # è·å–ä¸€äº›æµ‹è¯•æ•°æ®
        tickers = ['AAPL', 'GOOGL', 'MSFT']
        feature_data = provider.get_training_data(
            tickers=tickers,
            start_date='2024-01-01',
            end_date='2024-03-01'
        )
        
        logger.info("ä½¿ç”¨çœŸå®æ•°æ®è¿›è¡ŒéªŒè¯")
        
    except Exception as e:
        logger.warning(f"æ— æ³•åŠ è½½çœŸå®æ•°æ®: {e}")
        logger.info("ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡ŒéªŒè¯")
        feature_data = None
    
    # è¿è¡ŒéªŒè¯
    report = validator.run_comprehensive_validation(feature_data)
    
    # ä¿å­˜æŠ¥å‘Š
    import json
    with open('data_structure_validation_report.json', 'w') as f:
        json.dump(report, f, indent=2, default=str)
    
    logger.info(f"\néªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: data_structure_validation_report.json")
    
    # è¿”å›æ˜¯å¦é€šè¿‡
    return report['passed']


if __name__ == "__main__":
    passed = main()
    sys.exit(0 if passed else 1)