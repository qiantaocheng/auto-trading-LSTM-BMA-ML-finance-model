#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¨³å¥ç‰¹å¾é€‰æ‹©ç³»ç»Ÿç”Ÿäº§éƒ¨ç½²è®¡åˆ’
åŒ…å«é›†æˆã€ç›‘æ§ã€è°ƒä¼˜å’Œæ€§èƒ½è·Ÿè¸ªçš„å®Œæ•´æ–¹æ¡ˆ
"""

import pandas as pd
import numpy as np
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
import pickle
import warnings
warnings.filterwarnings('ignore')

class ProductionDeploymentManager:
    """
    ç”Ÿäº§éƒ¨ç½²ç®¡ç†å™¨
    è´Ÿè´£ç¨³å¥ç‰¹å¾é€‰æ‹©ç³»ç»Ÿçš„ç”Ÿäº§ç¯å¢ƒé›†æˆå’Œç®¡ç†
    """
    
    def __init__(self, base_path="production_feature_selection"):
        self.base_path = Path(base_path)
        self.base_path.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.base_path / "configs").mkdir(exist_ok=True)
        (self.base_path / "models").mkdir(exist_ok=True)
        (self.base_path / "logs").mkdir(exist_ok=True)
        (self.base_path / "reports").mkdir(exist_ok=True)
        (self.base_path / "monitoring").mkdir(exist_ok=True)
        (self.base_path / "backups").mkdir(exist_ok=True)
        
        self.logger = self._setup_logging()
        self.deployment_config = self._create_deployment_config()
        
    def _setup_logging(self):
        """è®¾ç½®ç”Ÿäº§ç¯å¢ƒæ—¥å¿—"""
        logger = logging.getLogger('ProductionFeatureSelection')
        logger.setLevel(logging.INFO)
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(
            self.base_path / "logs" / f"feature_selection_{datetime.now().strftime('%Y%m%d')}.log"
        )
        file_handler.setLevel(logging.INFO)
        
        # æ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
        
        return logger
    
    def _create_deployment_config(self):
        """åˆ›å»ºéƒ¨ç½²é…ç½®"""
        config = {
            "version": "1.0.0",
            "deployment_date": datetime.now().isoformat(),
            
            # é˜¶æ®µæ€§éƒ¨ç½²é…ç½®
            "deployment_stages": {
                "stage1_testing": {
                    "enabled": True,
                    "parallel_mode": True,  # ä¸åŸç³»ç»Ÿå¹¶è¡Œè¿è¡Œ
                    "comparison_enabled": True,
                    "rollback_threshold": 0.05  # æ€§èƒ½ä¸‹é™5%åˆ™å›æ»š
                },
                "stage2_gradual": {
                    "enabled": False,
                    "traffic_percentage": 30,  # 30%æµé‡ä½¿ç”¨æ–°ç³»ç»Ÿ
                    "monitoring_period_days": 30
                },
                "stage3_full": {
                    "enabled": False,
                    "full_replacement": True
                }
            },
            
            # ç‰¹å¾é€‰æ‹©å‚æ•°ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
            "feature_selection_params": {
                "target_features": 16,
                "ic_window": 126,  # 6ä¸ªæœˆ
                "min_ic_mean": 0.008,  # ç¨å¾®æé«˜æ ‡å‡†
                "min_ic_ir": 0.25,     # ç¨å¾®æé«˜æ ‡å‡†
                "max_correlation": 0.55,  # ç¨å¾®é™ä½ç›¸å…³æ€§
                "reselection_period": 90,  # 3ä¸ªæœˆé‡é€‰
                "emergency_fallback": True
            },
            
            # ç›‘æ§é…ç½®
            "monitoring": {
                "performance_check_frequency": "daily",
                "feature_health_check": "weekly",
                "full_reselection_check": "monthly",
                "alert_thresholds": {
                    "ic_degradation": 0.2,  # ICä¸‹é™20%æŠ¥è­¦
                    "prediction_error_increase": 0.15,  # é¢„æµ‹è¯¯å·®å¢åŠ 15%æŠ¥è­¦
                    "feature_correlation_spike": 0.8,  # ç‰¹å¾ç›¸å…³æ€§è¶…è¿‡80%æŠ¥è­¦
                    "processing_time_increase": 2.0  # å¤„ç†æ—¶é—´å¢åŠ 100%æŠ¥è­¦
                }
            },
            
            # æ€§èƒ½è·Ÿè¸ªé…ç½®
            "performance_tracking": {
                "metrics": [
                    "training_time",
                    "prediction_time", 
                    "memory_usage",
                    "feature_count",
                    "ic_quality",
                    "prediction_accuracy",
                    "model_stability"
                ],
                "baseline_period_days": 30,
                "comparison_period_days": 7
            }
        }
        
        # ä¿å­˜é…ç½®
        config_path = self.base_path / "configs" / "deployment_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        return config

class Stage1TestingDeployment:
    """
    ç¬¬ä¸€é˜¶æ®µï¼šæµ‹è¯•ç¯å¢ƒéƒ¨ç½²
    ä¸åŸç³»ç»Ÿå¹¶è¡Œè¿è¡Œï¼Œè¿›è¡Œæ€§èƒ½å¯¹æ¯”
    """
    
    def __init__(self, manager):
        self.manager = manager
        self.logger = manager.logger
        
    def deploy_parallel_testing(self):
        """éƒ¨ç½²å¹¶è¡Œæµ‹è¯•ç³»ç»Ÿ"""
        self.logger.info("ğŸš€ å¼€å§‹ç¬¬ä¸€é˜¶æ®µéƒ¨ç½²ï¼šå¹¶è¡Œæµ‹è¯•")
        
        deployment_code = '''
# ç¬¬ä¸€é˜¶æ®µéƒ¨ç½²ä»£ç 
def create_parallel_testing_setup():
    """åˆ›å»ºå¹¶è¡Œæµ‹è¯•ç¯å¢ƒ"""
    
    # 1. å¯¼å…¥å¿…è¦æ¨¡å—
    from bma_robust_feature_production import create_enhanced_bma_model
    from robust_feature_selection import RobustFeatureSelector
    
    class ParallelTestingManager:
        def __init__(self, original_model):
            self.original_model = original_model
            self.enhanced_model = create_enhanced_bma_model(original_model)
            self.comparison_results = []
            
        def run_parallel_analysis(self, *args, **kwargs):
            """å¹¶è¡Œè¿è¡ŒåŸç‰ˆå’Œå¢å¼ºç‰ˆï¼Œå¯¹æ¯”ç»“æœ"""
            
            start_time = time.time()
            
            # è¿è¡ŒåŸç‰ˆæ¨¡å‹
            original_start = time.time()
            try:
                original_result = self.original_model.run_complete_analysis(*args, **kwargs)
                original_time = time.time() - original_start
                original_success = True
            except Exception as e:
                original_result = {"error": str(e)}
                original_time = time.time() - original_start
                original_success = False
            
            # è¿è¡Œå¢å¼ºç‰ˆæ¨¡å‹
            enhanced_start = time.time()
            try:
                enhanced_result = self.enhanced_model.run_complete_analysis(*args, **kwargs)
                enhanced_time = time.time() - enhanced_start
                enhanced_success = True
            except Exception as e:
                enhanced_result = {"error": str(e)}
                enhanced_time = time.time() - enhanced_start
                enhanced_success = False
            
            # æ€§èƒ½å¯¹æ¯”
            comparison = self._compare_results(
                original_result, enhanced_result,
                original_time, enhanced_time,
                original_success, enhanced_success
            )
            
            self.comparison_results.append({
                "timestamp": datetime.now().isoformat(),
                "args": str(args),
                "kwargs": str(kwargs),
                "comparison": comparison
            })
            
            # æ ¹æ®é…ç½®è¿”å›ç»“æœ
            if enhanced_success and comparison["performance_improvement"] > 0:
                return enhanced_result, comparison
            else:
                return original_result, comparison
        
        def _compare_results(self, orig, enh, orig_time, enh_time, orig_success, enh_success):
            """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœ"""
            comparison = {
                "original_success": orig_success,
                "enhanced_success": enh_success,
                "original_time": orig_time,
                "enhanced_time": enh_time,
                "time_improvement": (orig_time - enh_time) / orig_time if orig_time > 0 else 0,
                "performance_improvement": 0
            }
            
            if orig_success and enh_success:
                # æ¯”è¾ƒé¢„æµ‹è´¨é‡
                if "predictions" in orig and "predictions" in enh:
                    orig_pred = orig["predictions"]
                    enh_pred = enh["predictions"]
                    
                    if orig_pred is not None and enh_pred is not None:
                        # è®¡ç®—é¢„æµ‹ä¸€è‡´æ€§
                        if len(orig_pred) == len(enh_pred):
                            correlation = np.corrcoef(orig_pred, enh_pred)[0, 1]
                            comparison["prediction_correlation"] = correlation
                            
                            # å¦‚æœç›¸å…³æ€§é«˜ä¸”æ—¶é—´æ”¹å–„ï¼Œè®¤ä¸ºæ˜¯æ”¹è¿›
                            if correlation > 0.9 and comparison["time_improvement"] > 0:
                                comparison["performance_improvement"] = comparison["time_improvement"]
            
            return comparison
        
        def get_performance_summary(self):
            """è·å–æ€§èƒ½æ€»ç»“"""
            if not self.comparison_results:
                return {"status": "no_data"}
            
            improvements = [r["comparison"]["time_improvement"] for r in self.comparison_results]
            successes = [r["comparison"]["enhanced_success"] for r in self.comparison_results]
            
            return {
                "total_runs": len(self.comparison_results),
                "success_rate": sum(successes) / len(successes),
                "avg_time_improvement": np.mean(improvements),
                "min_time_improvement": min(improvements),
                "max_time_improvement": max(improvements),
                "recommendation": "proceed" if np.mean(improvements) > 0.1 else "investigate"
            }
    
    return ParallelTestingManager

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºå¹¶è¡Œæµ‹è¯•ç®¡ç†å™¨
    original_bma = UltraEnhancedQuantitativeModel()
    parallel_manager = create_parallel_testing_setup()(original_bma)
    
    # è¿è¡Œæµ‹è¯•
    result, comparison = parallel_manager.run_parallel_analysis(
        tickers=['AAPL', 'MSFT'],
        start_date='2024-01-01',
        end_date='2024-12-01'
    )
    
    # æŸ¥çœ‹æ€§èƒ½æ€»ç»“
    summary = parallel_manager.get_performance_summary()
    print(f"æ€§èƒ½æ”¹è¿›: {summary}")
'''
        
        # ä¿å­˜éƒ¨ç½²ä»£ç 
        code_path = self.manager.base_path / "stage1_parallel_testing.py"
        with open(code_path, 'w', encoding='utf-8') as f:
            f.write(deployment_code)
        
        self.logger.info(f"âœ… ç¬¬ä¸€é˜¶æ®µéƒ¨ç½²ä»£ç å·²ç”Ÿæˆ: {code_path}")
        
        return deployment_code

class Stage2GradualDeployment:
    """
    ç¬¬äºŒé˜¶æ®µï¼šæ¸è¿›å¼éƒ¨ç½²
    éƒ¨åˆ†æµé‡ä½¿ç”¨æ–°ç³»ç»Ÿï¼Œé€æ­¥æ‰©å¤§èŒƒå›´
    """
    
    def __init__(self, manager):
        self.manager = manager
        self.logger = manager.logger
    
    def create_gradual_deployment(self):
        """åˆ›å»ºæ¸è¿›å¼éƒ¨ç½²ç³»ç»Ÿ"""
        self.logger.info("ğŸ¯ å‡†å¤‡ç¬¬äºŒé˜¶æ®µéƒ¨ç½²ï¼šæ¸è¿›å¼æ›¿æ¢")
        
        gradual_code = '''
class GradualDeploymentManager:
    """æ¸è¿›å¼éƒ¨ç½²ç®¡ç†å™¨"""
    
    def __init__(self, original_model, traffic_percentage=30):
        self.original_model = original_model
        self.enhanced_model = create_enhanced_bma_model(original_model)
        self.traffic_percentage = traffic_percentage
        self.deployment_metrics = []
        
    def route_request(self, request_hash, *args, **kwargs):
        """æ ¹æ®æµé‡ç™¾åˆ†æ¯”è·¯ç”±è¯·æ±‚"""
        
        # ä½¿ç”¨è¯·æ±‚å“ˆå¸Œå†³å®šè·¯ç”±
        if hash(request_hash) % 100 < self.traffic_percentage:
            # ä½¿ç”¨å¢å¼ºç‰ˆæ¨¡å‹
            try:
                result = self.enhanced_model.run_complete_analysis(*args, **kwargs)
                self._record_metrics("enhanced", True, result)
                return result
            except Exception as e:
                self.logger.error(f"å¢å¼ºç‰ˆæ¨¡å‹å¤±è´¥ï¼Œå›é€€åˆ°åŸç‰ˆ: {e}")
                result = self.original_model.run_complete_analysis(*args, **kwargs)
                self._record_metrics("enhanced", False, result)
                return result
        else:
            # ä½¿ç”¨åŸç‰ˆæ¨¡å‹
            result = self.original_model.run_complete_analysis(*args, **kwargs)
            self._record_metrics("original", True, result)
            return result
    
    def _record_metrics(self, model_type, success, result):
        """è®°å½•éƒ¨ç½²æŒ‡æ ‡"""
        self.deployment_metrics.append({
            "timestamp": datetime.now().isoformat(),
            "model_type": model_type,
            "success": success,
            "has_predictions": "predictions" in result and result["predictions"] is not None
        })
    
    def get_deployment_health(self):
        """è·å–éƒ¨ç½²å¥åº·çŠ¶å†µ"""
        if not self.deployment_metrics:
            return {"status": "no_data"}
        
        recent_metrics = [m for m in self.deployment_metrics 
                         if datetime.fromisoformat(m["timestamp"]) > datetime.now() - timedelta(days=7)]
        
        enhanced_metrics = [m for m in recent_metrics if m["model_type"] == "enhanced"]
        original_metrics = [m for m in recent_metrics if m["model_type"] == "original"]
        
        enhanced_success = sum(m["success"] for m in enhanced_metrics) / len(enhanced_metrics) if enhanced_metrics else 0
        original_success = sum(m["success"] for m in original_metrics) / len(original_metrics) if original_metrics else 0
        
        return {
            "enhanced_success_rate": enhanced_success,
            "original_success_rate": original_success,
            "enhanced_requests": len(enhanced_metrics),
            "original_requests": len(original_metrics),
            "health_status": "good" if enhanced_success >= original_success * 0.95 else "needs_attention"
        }
'''
        
        # ä¿å­˜æ¸è¿›éƒ¨ç½²ä»£ç 
        code_path = self.manager.base_path / "stage2_gradual_deployment.py"
        with open(code_path, 'w', encoding='utf-8') as f:
            f.write(gradual_code)
        
        self.logger.info(f"âœ… ç¬¬äºŒé˜¶æ®µéƒ¨ç½²ä»£ç å·²ç”Ÿæˆ: {code_path}")

class ParameterOptimizer:
    """
    å‚æ•°è°ƒä¼˜å™¨
    æ ¹æ®å®é™…æ•°æ®å’Œæ€§èƒ½åé¦ˆè°ƒä¼˜ç‰¹å¾é€‰æ‹©å‚æ•°
    """
    
    def __init__(self, manager):
        self.manager = manager
        self.logger = manager.logger
    
    def create_parameter_tuning_system(self):
        """åˆ›å»ºå‚æ•°è°ƒä¼˜ç³»ç»Ÿ"""
        self.logger.info("ğŸ›ï¸ åˆ›å»ºå‚æ•°è°ƒä¼˜ç³»ç»Ÿ")
        
        tuning_code = '''
class AdaptiveParameterTuner:
    """è‡ªé€‚åº”å‚æ•°è°ƒä¼˜å™¨"""
    
    def __init__(self, base_config):
        self.base_config = base_config.copy()
        self.tuning_history = []
        self.current_params = base_config.copy()
        
    def evaluate_current_parameters(self, model, test_data):
        """è¯„ä¼°å½“å‰å‚æ•°çš„æ•ˆæœ"""
        
        # è¿è¡Œç‰¹å¾é€‰æ‹©
        selector = RobustFeatureSelector(**self.current_params)
        
        try:
            X = test_data["features"]
            y = test_data["targets"]
            dates = test_data["dates"]
            
            X_selected = selector.fit_transform(X, y, dates)
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            report = selector.get_feature_report()
            selected_stats = report[report['selected']]
            
            evaluation = {
                "selected_feature_count": len(selected_stats),
                "avg_ic": selected_stats['ic_mean'].mean(),
                "avg_ic_ir": selected_stats['ic_ir'].mean(),
                "compression_ratio": len(selected_stats) / len(report),
                "max_correlation": self._calculate_max_correlation(X_selected),
                "timestamp": datetime.now().isoformat(),
                "parameters": self.current_params.copy()
            }
            
            self.tuning_history.append(evaluation)
            return evaluation
            
        except Exception as e:
            self.logger.error(f"å‚æ•°è¯„ä¼°å¤±è´¥: {e}")
            return None
    
    def suggest_parameter_adjustments(self):
        """åŸºäºå†å²æ•°æ®å»ºè®®å‚æ•°è°ƒæ•´"""
        
        if len(self.tuning_history) < 3:
            return {"message": "éœ€è¦æ›´å¤šå†å²æ•°æ®è¿›è¡Œè°ƒä¼˜"}
        
        recent_evals = self.tuning_history[-5:]  # æœ€è¿‘5æ¬¡è¯„ä¼°
        
        suggestions = []
        
        # åˆ†æç‰¹å¾æ•°é‡è¶‹åŠ¿
        feature_counts = [e["selected_feature_count"] for e in recent_evals]
        avg_features = np.mean(feature_counts)
        
        if avg_features < self.current_params["target_features"] * 0.7:
            suggestions.append({
                "parameter": "min_ic_mean",
                "current": self.current_params["min_ic_mean"],
                "suggested": self.current_params["min_ic_mean"] * 0.8,
                "reason": "é€‰æ‹©ç‰¹å¾è¿‡å°‘ï¼Œé™ä½ICé˜ˆå€¼"
            })
        elif avg_features > self.current_params["target_features"] * 1.2:
            suggestions.append({
                "parameter": "min_ic_ir", 
                "current": self.current_params["min_ic_ir"],
                "suggested": self.current_params["min_ic_ir"] * 1.2,
                "reason": "é€‰æ‹©ç‰¹å¾è¿‡å¤šï¼Œæé«˜IC_IRé˜ˆå€¼"
            })
        
        # åˆ†æICè´¨é‡è¶‹åŠ¿
        ic_values = [e["avg_ic"] for e in recent_evals]
        if np.mean(ic_values) < 0.01:
            suggestions.append({
                "parameter": "ic_window",
                "current": self.current_params["ic_window"],
                "suggested": max(60, self.current_params["ic_window"] - 30),
                "reason": "ICè´¨é‡è¾ƒä½ï¼Œç¼©çŸ­è¯„ä¼°çª—å£"
            })
        
        return suggestions
    
    def apply_parameter_adjustment(self, adjustments):
        """åº”ç”¨å‚æ•°è°ƒæ•´"""
        for adj in adjustments:
            if adj["parameter"] in self.current_params:
                old_value = self.current_params[adj["parameter"]]
                self.current_params[adj["parameter"]] = adj["suggested"]
                self.logger.info(f"å‚æ•°è°ƒæ•´: {adj['parameter']} {old_value} -> {adj['suggested']} ({adj['reason']})")
        
        return self.current_params
    
    def _calculate_max_correlation(self, X):
        """è®¡ç®—ç‰¹å¾é—´æœ€å¤§ç›¸å…³æ€§"""
        if len(X.columns) <= 1:
            return 0.0
        
        corr_matrix = X.corr().abs()
        upper_triangle = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )
        return upper_triangle.max().max()

# ä½¿ç”¨ç¤ºä¾‹
def run_parameter_optimization():
    """è¿è¡Œå‚æ•°ä¼˜åŒ–æµç¨‹"""
    
    base_params = {
        "target_features": 16,
        "ic_window": 126,
        "min_ic_mean": 0.008,
        "min_ic_ir": 0.25,
        "max_correlation": 0.55
    }
    
    tuner = AdaptiveParameterTuner(base_params)
    
    # æ¯å‘¨è¿è¡Œä¸€æ¬¡å‚æ•°è¯„ä¼°
    # è¿™é‡Œåº”è¯¥ç”¨å®é™…çš„æµ‹è¯•æ•°æ®
    test_data = load_recent_market_data()  # éœ€è¦å®ç°
    
    evaluation = tuner.evaluate_current_parameters(model, test_data)
    suggestions = tuner.suggest_parameter_adjustments()
    
    if suggestions:
        tuner.apply_parameter_adjustment(suggestions)
        save_updated_parameters(tuner.current_params)
'''
        
        # ä¿å­˜è°ƒä¼˜ä»£ç 
        code_path = self.manager.base_path / "parameter_optimizer.py"
        with open(code_path, 'w', encoding='utf-8') as f:
            f.write(tuning_code)
        
        self.logger.info(f"âœ… å‚æ•°è°ƒä¼˜ç³»ç»Ÿå·²ç”Ÿæˆ: {code_path}")

class MonitoringSystem:
    """
    ç›‘æ§ç³»ç»Ÿ
    å®ç°6-12ä¸ªæœˆçš„å®šæœŸç‰¹å¾"ä½“æ£€"
    """
    
    def __init__(self, manager):
        self.manager = manager
        self.logger = manager.logger
    
    def create_monitoring_system(self):
        """åˆ›å»ºç›‘æ§ç³»ç»Ÿ"""
        self.logger.info("ğŸ“Š åˆ›å»ºç›‘æ§ç³»ç»Ÿ")
        
        monitoring_code = '''
import schedule
import time
from datetime import datetime, timedelta

class FeatureHealthMonitor:
    """ç‰¹å¾å¥åº·ç›‘æ§å™¨"""
    
    def __init__(self, model_manager):
        self.model_manager = model_manager
        self.health_history = []
        self.alert_thresholds = {
            "ic_degradation": 0.2,
            "feature_count_change": 0.3,
            "correlation_spike": 0.8,
            "performance_drop": 0.15
        }
    
    def daily_health_check(self):
        """æ¯æ—¥å¥åº·æ£€æŸ¥"""
        self.logger.info("ğŸ” æ‰§è¡Œæ¯æ—¥ç‰¹å¾å¥åº·æ£€æŸ¥")
        
        try:
            # è·å–å½“å‰æ¨¡å‹çŠ¶æ€
            status = self.model_manager.get_feature_selection_status()
            
            # æ£€æŸ¥åŸºæœ¬çŠ¶æ€
            health_report = {
                "timestamp": datetime.now().isoformat(),
                "feature_count": status.get("selected_features_count", 0),
                "last_selection_age_days": self._calculate_selection_age(status),
                "selector_available": status.get("selector_available", False),
                "health_score": 0.0
            }
            
            # è®¡ç®—å¥åº·åˆ†æ•°
            health_score = 1.0
            
            if not status.get("selector_available", False):
                health_score -= 0.5
                health_report["alerts"] = ["ç‰¹å¾é€‰æ‹©å™¨ä¸å¯ç”¨"]
            
            if health_report["last_selection_age_days"] > 120:  # 4ä¸ªæœˆ
                health_score -= 0.3
                health_report.setdefault("alerts", []).append("ç‰¹å¾é€‰æ‹©è¿‡æœŸ")
            
            health_report["health_score"] = health_score
            self.health_history.append(health_report)
            
            # ä¿å­˜æŠ¥å‘Š
            self._save_health_report(health_report)
            
            # è§¦å‘è­¦æŠ¥
            if health_score < 0.7:
                self._send_alert(health_report)
            
        except Exception as e:
            self.logger.error(f"æ¯æ—¥å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
    
    def weekly_feature_analysis(self):
        """æ¯å‘¨ç‰¹å¾åˆ†æ"""
        self.logger.info("ğŸ“ˆ æ‰§è¡Œæ¯å‘¨ç‰¹å¾åˆ†æ")
        
        try:
            # è·å–æœ€è¿‘ä¸€å‘¨çš„æ¨¡å‹è¡¨ç°æ•°æ®
            recent_performance = self._get_recent_performance()
            
            analysis = {
                "timestamp": datetime.now().isoformat(),
                "analysis_period": "weekly",
                "performance_metrics": recent_performance,
                "recommendations": []
            }
            
            # åˆ†ææ€§èƒ½è¶‹åŠ¿
            if recent_performance:
                ic_trend = recent_performance.get("ic_trend", 0)
                if ic_trend < -0.1:  # ICä¸‹é™è¶…è¿‡10%
                    analysis["recommendations"].append("è€ƒè™‘é‡æ–°è¿›è¡Œç‰¹å¾é€‰æ‹©")
                
                processing_time_trend = recent_performance.get("processing_time_trend", 0)
                if processing_time_trend > 0.2:  # å¤„ç†æ—¶é—´å¢åŠ 20%
                    analysis["recommendations"].append("æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ")
            
            # ä¿å­˜åˆ†æç»“æœ
            self._save_analysis_report(analysis)
            
        except Exception as e:
            self.logger.error(f"æ¯å‘¨ç‰¹å¾åˆ†æå¤±è´¥: {e}")
    
    def monthly_comprehensive_review(self):
        """æ¯æœˆç»¼åˆè¯„ä¼°"""
        self.logger.info("ğŸ”¬ æ‰§è¡Œæ¯æœˆç»¼åˆè¯„ä¼°")
        
        try:
            # è·å–è¿‡å»ä¸€ä¸ªæœˆçš„æ‰€æœ‰æ•°æ®
            monthly_data = self._get_monthly_data()
            
            comprehensive_review = {
                "timestamp": datetime.now().isoformat(),
                "review_period": "monthly",
                "summary": self._generate_monthly_summary(monthly_data),
                "feature_stability": self._analyze_feature_stability(monthly_data),
                "performance_comparison": self._compare_monthly_performance(monthly_data),
                "reselection_recommendation": False
            }
            
            # å†³å®šæ˜¯å¦éœ€è¦é‡æ–°é€‰æ‹©ç‰¹å¾
            if self._should_trigger_reselection(comprehensive_review):
                comprehensive_review["reselection_recommendation"] = True
                self.logger.warning("ğŸš¨ å»ºè®®è¿›è¡Œç‰¹å¾é‡æ–°é€‰æ‹©")
            
            # ä¿å­˜ç»¼åˆè¯„ä¼°
            self._save_comprehensive_review(comprehensive_review)
            
        except Exception as e:
            self.logger.error(f"æ¯æœˆç»¼åˆè¯„ä¼°å¤±è´¥: {e}")
    
    def setup_scheduled_monitoring(self):
        """è®¾ç½®å®šæ—¶ç›‘æ§ä»»åŠ¡"""
        
        # æ¯æ—¥å¥åº·æ£€æŸ¥ï¼ˆå·¥ä½œæ—¥æ—©ä¸Š9ç‚¹ï¼‰
        schedule.every().monday.at("09:00").do(self.daily_health_check)
        schedule.every().tuesday.at("09:00").do(self.daily_health_check)
        schedule.every().wednesday.at("09:00").do(self.daily_health_check)
        schedule.every().thursday.at("09:00").do(self.daily_health_check)
        schedule.every().friday.at("09:00").do(self.daily_health_check)
        
        # æ¯å‘¨ç‰¹å¾åˆ†æï¼ˆå‘¨æ—¥æ™šä¸Š8ç‚¹ï¼‰
        schedule.every().sunday.at("20:00").do(self.weekly_feature_analysis)
        
        # æ¯æœˆç»¼åˆè¯„ä¼°ï¼ˆæ¯æœˆ1å·æ—©ä¸Š8ç‚¹ï¼‰
        schedule.every().month.do(self.monthly_comprehensive_review)
        
        self.logger.info("âœ… å®šæ—¶ç›‘æ§ä»»åŠ¡å·²è®¾ç½®")
        
        # å¯åŠ¨ç›‘æ§å¾ªç¯
        while True:
            schedule.run_pending()
            time.sleep(3600)  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
    
    def _calculate_selection_age(self, status):
        """è®¡ç®—ç‰¹å¾é€‰æ‹©çš„å¹´é¾„ï¼ˆå¤©æ•°ï¼‰"""
        if not status.get("last_selection_date"):
            return 999  # å¦‚æœæ²¡æœ‰è®°å½•ï¼Œè¿”å›å¾ˆå¤§çš„æ•°å­—
        
        last_date = datetime.fromisoformat(status["last_selection_date"])
        return (datetime.now() - last_date).days
    
    def _save_health_report(self, report):
        """ä¿å­˜å¥åº·æŠ¥å‘Š"""
        filename = f"health_report_{datetime.now().strftime('%Y%m%d')}.json"
        filepath = self.manager.base_path / "monitoring" / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
    
    def _send_alert(self, health_report):
        """å‘é€è­¦æŠ¥"""
        alert_message = f"""
        ğŸš¨ ç‰¹å¾é€‰æ‹©ç³»ç»Ÿå¥åº·è­¦æŠ¥
        
        æ—¶é—´: {health_report['timestamp']}
        å¥åº·åˆ†æ•°: {health_report['health_score']:.2f}
        é—®é¢˜: {health_report.get('alerts', [])}
        
        å»ºè®®ç«‹å³æ£€æŸ¥ç³»ç»ŸçŠ¶æ€ã€‚
        """
        
        self.logger.warning(alert_message)
        # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶ã€é’‰é’‰ã€å¾®ä¿¡ç­‰é€šçŸ¥ç³»ç»Ÿ
'''
        
        # ä¿å­˜ç›‘æ§ä»£ç 
        code_path = self.manager.base_path / "monitoring_system.py"
        with open(code_path, 'w', encoding='utf-8') as f:
            f.write(monitoring_code)
        
        self.logger.info(f"âœ… ç›‘æ§ç³»ç»Ÿå·²ç”Ÿæˆ: {code_path}")

class PerformanceTracker:
    """
    æ€§èƒ½è·Ÿè¸ªå™¨
    ç›‘æ§è®¡ç®—æ•ˆç‡å’Œé¢„æµ‹è´¨é‡æå‡
    """
    
    def __init__(self, manager):
        self.manager = manager
        self.logger = manager.logger
    
    def create_performance_tracking(self):
        """åˆ›å»ºæ€§èƒ½è·Ÿè¸ªç³»ç»Ÿ"""
        self.logger.info("âš¡ åˆ›å»ºæ€§èƒ½è·Ÿè¸ªç³»ç»Ÿ")
        
        tracking_code = '''
import psutil
import time
from collections import defaultdict

class PerformanceTracker:
    """æ€§èƒ½è·Ÿè¸ªå™¨"""
    
    def __init__(self):
        self.performance_data = []
        self.baseline_metrics = None
        self.current_session_metrics = defaultdict(list)
    
    def start_tracking(self, operation_name):
        """å¼€å§‹è·Ÿè¸ªæŸä¸ªæ“ä½œ"""
        return PerformanceContext(self, operation_name)
    
    def record_metrics(self, operation_name, metrics):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        record = {
            "timestamp": datetime.now().isoformat(),
            "operation": operation_name,
            "metrics": metrics
        }
        
        self.performance_data.append(record)
        self.current_session_metrics[operation_name].append(metrics)
    
    def set_baseline(self, baseline_data):
        """è®¾ç½®åŸºçº¿æ€§èƒ½æ•°æ®"""
        self.baseline_metrics = baseline_data
        self.logger.info("åŸºçº¿æ€§èƒ½æ•°æ®å·²è®¾ç½®")
    
    def get_performance_summary(self, days=7):
        """è·å–æ€§èƒ½æ€»ç»“"""
        cutoff_date = datetime.now() - timedelta(days=days)
        
        recent_data = [
            record for record in self.performance_data
            if datetime.fromisoformat(record["timestamp"]) > cutoff_date
        ]
        
        if not recent_data:
            return {"status": "no_recent_data"}
        
        # æŒ‰æ“ä½œç±»å‹åˆ†ç»„
        operation_metrics = defaultdict(list)
        for record in recent_data:
            operation_metrics[record["operation"]].append(record["metrics"])
        
        summary = {}
        for operation, metrics_list in operation_metrics.items():
            summary[operation] = self._calculate_operation_summary(metrics_list)
        
        # ä¸åŸºçº¿å¯¹æ¯”
        if self.baseline_metrics:
            summary["vs_baseline"] = self._compare_with_baseline(summary)
        
        return summary
    
    def _calculate_operation_summary(self, metrics_list):
        """è®¡ç®—æ“ä½œçš„æ€§èƒ½æ€»ç»“"""
        if not metrics_list:
            return {}
        
        # æå–å„ä¸ªæŒ‡æ ‡
        times = [m.get("execution_time", 0) for m in metrics_list]
        memory_peaks = [m.get("peak_memory_mb", 0) for m in metrics_list]
        feature_counts = [m.get("feature_count", 0) for m in metrics_list]
        
        return {
            "avg_execution_time": np.mean(times),
            "min_execution_time": min(times),
            "max_execution_time": max(times),
            "avg_memory_usage": np.mean(memory_peaks),
            "avg_feature_count": np.mean(feature_counts),
            "total_operations": len(metrics_list)
        }
    
    def _compare_with_baseline(self, current_summary):
        """ä¸åŸºçº¿å¯¹æ¯”"""
        comparison = {}
        
        for operation, current_metrics in current_summary.items():
            if operation in self.baseline_metrics:
                baseline = self.baseline_metrics[operation]
                
                time_improvement = (
                    baseline.get("avg_execution_time", 0) - 
                    current_metrics.get("avg_execution_time", 0)
                ) / baseline.get("avg_execution_time", 1)
                
                memory_improvement = (
                    baseline.get("avg_memory_usage", 0) - 
                    current_metrics.get("avg_memory_usage", 0)
                ) / baseline.get("avg_memory_usage", 1)
                
                comparison[operation] = {
                    "time_improvement_percent": time_improvement * 100,
                    "memory_improvement_percent": memory_improvement * 100,
                    "feature_reduction": baseline.get("avg_feature_count", 0) - current_metrics.get("avg_feature_count", 0)
                }
        
        return comparison
    
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        summary = self.get_performance_summary()
        
        report = {
            "report_date": datetime.now().isoformat(),
            "summary": summary,
            "recommendations": self._generate_recommendations(summary),
            "charts_data": self._prepare_charts_data()
        }
        
        # ä¿å­˜æŠ¥å‘Š
        filename = f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = self.manager.base_path / "reports" / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        return report
    
    def _generate_recommendations(self, summary):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if "vs_baseline" in summary:
            baseline_comparison = summary["vs_baseline"]
            
            for operation, comparison in baseline_comparison.items():
                time_improvement = comparison.get("time_improvement_percent", 0)
                memory_improvement = comparison.get("memory_improvement_percent", 0)
                
                if time_improvement < 10:  # æ—¶é—´æ”¹å–„ä¸è¶³10%
                    recommendations.append(f"{operation}: æ—¶é—´ä¼˜åŒ–æ•ˆæœæœ‰é™ï¼Œå»ºè®®æ£€æŸ¥å‚æ•°é…ç½®")
                
                if memory_improvement < 0:  # å†…å­˜ä½¿ç”¨å¢åŠ 
                    recommendations.append(f"{operation}: å†…å­˜ä½¿ç”¨å¢åŠ ï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ç®¡ç†")
                
                if time_improvement > 50:  # æ—¶é—´æ”¹å–„è¶…è¿‡50%
                    recommendations.append(f"{operation}: æ€§èƒ½ä¼˜åŒ–æ˜¾è‘—ï¼Œå¯è€ƒè™‘è¿›ä¸€æ­¥æ¨å¹¿")
        
        return recommendations

class PerformanceContext:
    """æ€§èƒ½è·Ÿè¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    def __init__(self, tracker, operation_name):
        self.tracker = tracker
        self.operation_name = operation_name
        self.start_time = None
        self.start_memory = None
        self.process = psutil.Process()
    
    def __enter__(self):
        self.start_time = time.time()
        self.start_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        end_time = time.time()
        end_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        
        metrics = {
            "execution_time": end_time - self.start_time,
            "peak_memory_mb": max(self.start_memory, end_memory),
            "memory_change_mb": end_memory - self.start_memory
        }
        
        self.tracker.record_metrics(self.operation_name, metrics)

# ä½¿ç”¨ç¤ºä¾‹
def track_model_performance():
    """è·Ÿè¸ªæ¨¡å‹æ€§èƒ½ç¤ºä¾‹"""
    
    tracker = PerformanceTracker()
    
    # è·Ÿè¸ªç‰¹å¾åˆ›å»ºæ€§èƒ½
    with tracker.start_tracking("feature_creation") as ctx:
        # è¿™é‡Œæ˜¯ç‰¹å¾åˆ›å»ºä»£ç 
        feature_data = model.create_traditional_features()
        ctx.metrics["feature_count"] = len(feature_data.columns) - 3
    
    # è·Ÿè¸ªæ¨¡å‹è®­ç»ƒæ€§èƒ½
    with tracker.start_tracking("model_training") as ctx:
        # è¿™é‡Œæ˜¯æ¨¡å‹è®­ç»ƒä»£ç 
        training_results = model.train_enhanced_models()
        ctx.metrics["model_count"] = len(training_results)
    
    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    report = tracker.generate_performance_report()
    return report
'''
        
        # ä¿å­˜è·Ÿè¸ªä»£ç 
        code_path = self.manager.base_path / "performance_tracker.py"
        with open(code_path, 'w', encoding='utf-8') as f:
            f.write(tracking_code)
        
        self.logger.info(f"âœ… æ€§èƒ½è·Ÿè¸ªç³»ç»Ÿå·²ç”Ÿæˆ: {code_path}")

def main():
    """ä¸»è¦éƒ¨ç½²æµç¨‹"""
    print("ğŸš€ ç¨³å¥ç‰¹å¾é€‰æ‹©ç³»ç»Ÿç”Ÿäº§éƒ¨ç½²è®¡åˆ’")
    print("=" * 60)
    
    # åˆ›å»ºéƒ¨ç½²ç®¡ç†å™¨
    manager = ProductionDeploymentManager()
    
    print("1. åˆ›å»ºç¬¬ä¸€é˜¶æ®µéƒ¨ç½²ï¼ˆå¹¶è¡Œæµ‹è¯•ï¼‰...")
    stage1 = Stage1TestingDeployment(manager)
    stage1.deploy_parallel_testing()
    
    print("2. åˆ›å»ºç¬¬äºŒé˜¶æ®µéƒ¨ç½²ï¼ˆæ¸è¿›å¼æ›¿æ¢ï¼‰...")
    stage2 = Stage2GradualDeployment(manager)
    stage2.create_gradual_deployment()
    
    print("3. åˆ›å»ºå‚æ•°è°ƒä¼˜ç³»ç»Ÿ...")
    optimizer = ParameterOptimizer(manager)
    optimizer.create_parameter_tuning_system()
    
    print("4. åˆ›å»ºç›‘æ§ç³»ç»Ÿ...")
    monitor = MonitoringSystem(manager)
    monitor.create_monitoring_system()
    
    print("5. åˆ›å»ºæ€§èƒ½è·Ÿè¸ªç³»ç»Ÿ...")
    tracker = PerformanceTracker(manager)
    tracker.create_performance_tracking()
    
    print("\nâœ… ç”Ÿäº§éƒ¨ç½²è®¡åˆ’åˆ›å»ºå®Œæˆï¼")
    print(f"ğŸ“ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {manager.base_path}")
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œï¼š")
    print("1. å®¡æŸ¥ç”Ÿæˆçš„éƒ¨ç½²ä»£ç ")
    print("2. æ ¹æ®å®é™…ç¯å¢ƒè°ƒæ•´é…ç½®å‚æ•°")
    print("3. åœ¨æµ‹è¯•ç¯å¢ƒè¿è¡Œç¬¬ä¸€é˜¶æ®µéƒ¨ç½²")
    print("4. ç›‘æ§æ€§èƒ½æŒ‡æ ‡å¹¶é€æ­¥æ¨è¿›")

if __name__ == "__main__":
    main()
