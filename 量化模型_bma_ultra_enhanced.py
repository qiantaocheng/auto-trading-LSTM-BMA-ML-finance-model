#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BMA Ultra Enhanced é‡åŒ–åˆ†ææ¨¡å‹ V4
é›†æˆAlphaç­–ç•¥ã€Learning-to-Rankã€ä¸ç¡®å®šæ€§æ„ŸçŸ¥BMAã€é«˜çº§æŠ•èµ„ç»„åˆä¼˜åŒ–
æä¾›å·¥ä¸šçº§çš„é‡åŒ–äº¤æ˜“è§£å†³æ–¹æ¡ˆ
"""

import pandas as pd
import numpy as np
# ä¿®å¤å‘½åç©ºé—´å†²çªï¼šä½¿ç”¨åˆ«åé¿å…ä¸å…¶ä»–åº“å†²çª
from polygon_client import polygon_client as pc, download as polygon_download, Ticker as PolygonTicker
import yaml
import warnings
import argparse
import os
import tempfile
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple, Union
import time

# åŸºç¡€ç§‘å­¦è®¡ç®—
from scipy.stats import spearmanr, entropy
from scipy.optimize import minimize
import statsmodels.api as sm
from dataclasses import dataclass, field
from scipy import stats
from sklearn.linear_model import HuberRegressor
from sklearn.covariance import LedoitWolf

# æœºå™¨å­¦ä¹ 
from sklearn.model_selection import TimeSeriesSplit, GroupKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from purged_time_series_cv import PurgedGroupTimeSeriesSplit, ValidationConfig, create_time_groups

# å¯è§†åŒ–
import matplotlib.pyplot as plt
import seaborn as sns

# å¯¼å…¥æˆ‘ä»¬çš„å¢å¼ºæ¨¡å—
try:
    from enhanced_alpha_strategies import AlphaStrategiesEngine
    from learning_to_rank_bma import LearningToRankBMA
    from advanced_portfolio_optimizer import AdvancedPortfolioOptimizer
    ENHANCED_MODULES_AVAILABLE = True
except ImportError as e:
    print(f"[WARN] å¢å¼ºæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    ENHANCED_MODULES_AVAILABLE = False

# ç»Ÿä¸€å¸‚åœºæ•°æ®ï¼ˆè¡Œä¸š/å¸‚å€¼/å›½å®¶ç­‰ï¼‰
try:
    from unified_market_data_manager import UnifiedMarketDataManager
    MARKET_MANAGER_AVAILABLE = True
except Exception:
    MARKET_MANAGER_AVAILABLE = False

# ä¸­æ€§åŒ–å·²ç»Ÿä¸€ç”±Alphaå¼•æ“å¤„ç†ï¼Œç§»é™¤é‡å¤ä¾èµ–

# å¯¼å…¥isotonicæ ¡å‡†
try:
    from sklearn.isotonic import IsotonicRegression
    ISOTONIC_AVAILABLE = True
except ImportError:
    ISOTONIC_AVAILABLE = False

# è‡ªé€‚åº”åŠ æ ‘ä¼˜åŒ–å™¨
try:
    from adaptive_tree_optimizer import AdaptiveTreeOptimizer
    ADAPTIVE_OPTIMIZER_AVAILABLE = True
except ImportError:
    ADAPTIVE_OPTIMIZER_AVAILABLE = False
    logging.warning("è‡ªé€‚åº”åŠ æ ‘ä¼˜åŒ–å™¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ ‡å‡†æ¨¡å‹è®­ç»ƒ")

# é«˜çº§æ¨¡å‹
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

# CatBoost removed due to compatibility issues
    CATBOOST_AVAILABLE = False

# é…ç½®
warnings.filterwarnings('ignore')

# ä¿®å¤matplotlibç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
try:
    import matplotlib
    if hasattr(matplotlib, '__version__') and matplotlib.__version__ >= '3.4.0':
        try:
            plt.style.use('seaborn-v0_8')
        except OSError:
            # å¦‚æœseaborn-v0_8ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ ·å¼
            plt.style.use('default')
            print("[WARN] seaborn-v0_8æ ·å¼ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ ·å¼")
    else:
        plt.style.use('seaborn')
except Exception as e:
    print(f"[WARN] matplotlibæ ·å¼è®¾ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æ ·å¼")
    plt.style.use('default')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€é…ç½®
DEFAULT_TICKER_LIST =["A", "AA", "AACB", "AACI", "AACT", "AAL", "AAMI", "AAOI", "AAON", "AAP", "AAPL", "AARD", "AAUC", "AB", "ABAT", "ABBV", "ABCB", "ABCL", "ABEO", "ABEV", "ABG", "ABL", "ABM", "ABNB", "ABSI", "ABT", "ABTS", "ABUS", "ABVC", "ABVX", "ACA", "ACAD", "ACB", "ACCO", "ACDC", "ACEL", "ACGL", "ACHC", "ACHR", "ACHV", "ACI", "ACIC", "ACIU", "ACIW", "ACLS", "ACLX", "ACM", "ACMR", "ACN", "ACNT", "ACOG", "ACRE", "ACT", "ACTG", "ACTU", "ACVA", "ACXP", "ADAG", "ADBE", "ADC", "ADCT", "ADEA", "ADI", "ADM", "ADMA", "ADNT", "ADP", "ADPT", "ADSE", "ADSK", "ADT", "ADTN", "ADUR", "ADUS", "ADVM", "AEBI", "AEE", "AEG", "AEHL", "AEHR", "AEIS", "AEM", "AEO", "AEP", "AER", "AES", "AESI", "AEVA", "AEYE", "AFCG", "AFG", "AFL", "AFRM", "AFYA", "AG", "AGCO", "AGD", "AGEN", "AGH", "AGI", "AGIO", "AGM", "AGNC", "AGO", "AGRO", "AGX", "AGYS", "AHCO", "AHH", "AHL", "AHR", "AI", "AIFF", "AIFU", "AIG", "AII", "AIM", "AIMD", "AIN", "AIOT", "AIP", "AIR", "AIRI", "AIRJ", "AIRO", "AIRS", "AISP", "AIT", "AIV", "AIZ", "AJG", "AKAM", "AKBA", "AKRO", "AL", "ALAB", "ALAR", "ALB", "ALBT", "ALC", "ALDF", "ALDX", "ALE", "ALEX", "ALF", "ALG", "ALGM", "ALGN", "ALGS", "ALGT", "ALHC", "ALIT", "ALK", "ALKS", "ALKT", "ALL", "ALLE", "ALLT", "ALLY", "ALM", "ALMS", "ALMU", "ALNT", "ALNY", "ALRM", "ALRS", "ALSN", "ALT", "ALTG", "ALTI", "ALTS", "ALUR", "ALV", "ALVO", "ALX", "ALZN", "AM", "AMAL", "AMAT", "AMBA", "AMBC", "AMBP", "AMBQ", "AMBR", "AMC", "AMCR", "AMCX", "AMD", "AME", "AMED", "AMG", "AMGN", "AMH", "AMKR", "AMLX", "AMN", "AMP", "AMPG", "AMPH", "AMPL", "AMPX", "AMPY", "AMR", "AMRC", "AMRK", "AMRN", "AMRX", "AMRZ", "AMSC", "AMSF", "AMST", "AMT", "AMTB", "AMTM", "AMTX", "AMWD", "AMWL", "AMX", "AMZE", "AMZN", "AN", "ANAB", "ANDE", "ANEB", "ANET", "ANF", "ANGH", "ANGI", "ANGO", "ANIK", "ANIP", "ANIX", "ANNX", "ANPA", "ANRO", "ANSC", "ANTA", "ANTE", "ANVS", "AOMR", "AON", "AORT", "AOS", "AOSL", "AOUT", "AP", "APA", "APAM", "APD", "APEI", "APG", "APGE", "APH", "API", "APLD", "APLE", "APLS", "APO", "APOG", "APP", "APPF", "APPN", "APPS", "APTV", "APVO", "AQN", "AQST", "AR", "ARAI", "ARCB", "ARCC", "ARCO", "ARCT", "ARDT", "ARDX", "ARE", "AREN", "ARES", "ARHS", "ARI", "ARIS", "ARKO", "ARLO", "ARLP", "ARM", "ARMK", "ARMN", "ARMP", "AROC", "ARQ", "ARQQ", "ARQT", "ARR", "ARRY", "ARTL", "ARTV", "ARVN", "ARW", "ARWR", "ARX", "AS", "ASA", "ASAN", "ASB", "ASC", "ASGN", "ASH", "ASIC", "ASIX", "ASLE", "ASM", "ASND", "ASO", "ASPI", "ASPN", "ASR", "ASST", "ASTE", "ASTH", "ASTI", "ASTL", "ASTS", "ASUR", "ASX", "ATAI", "ATAT", "ATEC", "ATEN", "ATEX", "ATGE", "ATHE", "ATHM", "ATHR", "ATI", "ATII", "ATKR", "ATLC", "ATLX", "ATMU", "ATNF", "ATO", "ATOM", "ATR", "ATRA", "ATRC", "ATRO", "ATS", "ATUS", "ATXS", "ATYR", "AU", "AUB", "AUDC", "AUGO", "AUID", "AUPH", "AUR", "AURA", "AUTL", "AVA", "AVAH", "AVAL", "AVAV", "AVB", "AVBC", "AVBP", "AVD", "AVDL", "AVDX", "AVGO", "AVIR", "AVNS", "AVNT", "AVNW", "AVO", "AVPT", "AVR", "AVT", "AVTR", "AVTX", "AVXL", "AVY", "AWI", "AWK", "AWR", "AX", "AXGN", "AXIN", "AXL", "AXP", "AXS", "AXSM", "AXTA", "AXTI", "AYI", "AYTU", "AZ", "AZN", "AZTA", "AZZ", "B", "BA", "BABA", "BAC", "BACC", "BACQ", "BAER", "BAH", "BAK", "BALL", "BALY", "BAM", "BANC", "BAND", "BANF", "BANR", "BAP", "BASE", "BATRA", "BATRK", "BAX", "BB", "BBAI", "BBAR", "BBCP", "BBD", "BBDC", "BBIO", "BBNX", "BBSI", "BBUC", "BBVA", "BBW", "BBWI", "BBY", "BC", "BCAL", "BCAX", "BCBP", "BCC", "BCE", "BCH", "BCO", "BCPC", "BCRX", "BCS", "BCSF", "BCYC", "BDC", "BDMD", "BDRX", "BDTX", "BDX", "BE", "BEAG", "BEAM", "BEEM", "BEEP", "BEKE", "BELFB", "BEN", "BEP", "BEPC", "BETR", "BF-A", "BF-B", "BFAM", "BFC", "BFH", "BFIN", "BFS", "BFST", "BG", "BGC", "BGL", "BGLC", "BGM", "BGS", "BGSF", "BHC", "BHE", "BHF", "BHFAP", "BHLB", "BHP", "BHR", "BHRB", "BHVN", "BIDU", "BIIB", "BILI", "BILL", "BIO", "BIOA", "BIOX", "BIP", "BIPC", "BIRD", "BIRK", "BJ", "BJRI", "BK", "BKD", "BKE", "BKH", "BKKT", "BKR", "BKSY", "BKTI", "BKU", "BKV", "BL", "BLBD", "BLBX", "BLCO", "BLD", "BLDE", "BLDR", "BLFS", "BLFY", "BLIV", "BLKB", "BLMN", "BLND", "BLNE", "BLRX", "BLUW", "BLX", "BLZE", "BMA", "BMBL", "BMGL", "BMHL", "BMI", "BMNR", "BMO", "BMR", "BMRA", "BMRC", "BMRN", "BMY", "BN", "BNC", "BNED", "BNGO", "BNL", "BNS", "BNTC", "BNTX", "BNZI", "BOC", "BOF", "BOH", "BOKF", "BOOM", "BOOT", "BORR", "BOSC", "BOW", "BOX", "BP", "BPOP", "BQ", "BR", "BRBR", "BRBS", "BRC", "BRDG", "BRFS", "BRK-B", "BRKL", "BRKR", "BRLS", "BRO", "BROS", "BRR", "BRSL", "BRSP", "BRX", "BRY", "BRZE", "BSAA", "BSAC", "BSBR", "BSET", "BSGM", "BSM", "BSX", "BSY", "BTAI", "BTBD", "BTBT", "BTCM", "BTCS", "BTCT", "BTDR", "BTE", "BTG", "BTI", "BTM", "BTMD", "BTSG", "BTU", "BUD", "BULL", "BUR", "BURL", "BUSE", "BV", "BVFL", "BVN", "BVS", "BWA", "BWB", "BWEN", "BWIN", "BWLP", "BWMN", "BWMX", "BWXT", "BX", "BXC", "BXP", "BY", "BYD", "BYND", "BYON", "BYRN", "BYSI", "BZ", "BZAI", "BZFD", "BZH", "BZUN", "C", "CAAP", "CABO", "CAC", "CACC", "CACI", "CADE", "CADL", "CAE", "CAEP", "CAG", "CAH", "CAI", "CAKE", "CAL", "CALC", "CALM", "CALX", "CAMT", "CANG", "CAPR", "CAR", "CARE", "CARG", "CARL", "CARR", "CARS", "CART", "CASH", "CASS", "CAT", "CATX", "CATY", "CAVA", "CB", "CBAN", "CBIO", "CBL", "CBLL", "CBNK", "CBOE", "CBRE", "CBRL", "CBSH", "CBT", "CBU", "CBZ", "CC", "CCAP", "CCB", "CCCC", "CCCS", "CCCX", "CCEP", "CCI", "CCIR", "CCIX", "CCJ", "CCK", "CCL", "CCLD", "CCNE", "CCOI", "CCRD", "CCRN", "CCS", "CCSI", "CCU", "CDE", "CDIO", "CDLR", "CDNA", "CDNS", "CDP", "CDRE", "CDRO", "CDTX", "CDW", "CDXS", "CDZI", "CE", "CECO", "CEG", "CELC", "CELH", "CELU", "CELZ", "CENT", "CENTA", "CENX", "CEP", "CEPO", "CEPT", "CEPU", "CERO", "CERT", "CEVA", "CF", "CFFN", "CFG", "CFLT", "CFR", "CG", "CGAU", "CGBD", "CGCT", "CGEM", "CGNT", "CGNX", "CGON", "CHA", "CHAC", "CHCO", "CHD", "CHDN", "CHE", "CHEF", "CHH", "CHKP", "CHMI", "CHPT", "CHRD", "CHRW", "CHT", "CHTR", "CHWY", "CHYM", "CI", "CIA", "CIB", "CIEN", "CIFR", "CIGI", "CIM", "CINF", "CING", "CINT", "CIO", "CION", "CIVB", "CIVI", "CL", "CLAR", "CLB", "CLBK", "CLBT", "CLCO", "CLDI", "CLDX", "CLF", "CLFD", "CLGN", "CLH", "CLLS", "CLMB", "CLMT", "CLNE", "CLNN", "CLOV", "CLPR", "CLPT", "CLRB", "CLRO", "CLS", "CLSK", "CLVT", "CLW", "CLX", "CM", "CMA", "CMBT", "CMC", "CMCL", "CMCO", "CMCSA", "CMDB", "CME", "CMG", "CMI", "CMP", "CMPO", "CMPR", "CMPS", "CMPX", "CMRC", "CMRE", "CMS", "CMTL", "CNA", "CNC", "CNCK", "CNDT", "CNEY", "CNH", "CNI", "CNK", "CNL", "CNM", "CNMD", "CNNE", "CNO", "CNOB", "CNP", "CNQ", "CNR", "CNS", "CNTA", "CNTB", "CNTY", "CNVS", "CNX", "CNXC", "CNXN", "COCO", "CODI", "COF", "COFS", "COGT", "COHR", "COHU", "COIN", "COKE", "COLB", "COLL", "COLM", "COMM", "COMP", "CON", "COO", "COOP", "COP", "COPL", "COR", "CORT", "CORZ", "COTY", "COUR", "COYA", "CP", "CPA", "CPAY", "CPB", "CPF", "CPIX", "CPK", "CPNG", "CPRI", "CPRT", "CPRX", "CPS", "CPSH", "CQP", "CR", "CRAI", "CRAQ", "CRBG", "CRBP", "CRC", "CRCL", "CRCT", "CRD-A", "CRDF", "CRDO", "CRE", "CRESY", "CREV", "CREX", "CRGO", "CRGX", "CRGY", "CRH", "CRI", "CRK", "CRL", "CRM", "CRMD", "CRML", "CRMT", "CRNC", "CRNX", "CRON", "CROX", "CRS", "CRSP", "CRSR", "CRTO", "CRUS", "CRVL", "CRVO", "CRVS", "CRWD", "CRWV", "CSAN", "CSCO", "CSGP", "CSGS", "CSIQ", "CSL", "CSR", "CSTL", "CSTM", "CSV", "CSW", "CSWC", "CSX", "CTAS", "CTEV", "CTGO", "CTKB", "CTLP", "CTMX", "CTNM", "CTO", "CTOS", "CTRA", "CTRI", "CTRM", "CTRN", "CTS", "CTSH", "CTVA", "CTW", "CUB", "CUBE", "CUBI", "CUK", "CUPR", "CURB", "CURI", "CURV", "CUZ", "CV", "CVAC", "CVBF", "CVCO", "CVE", "CVEO", "CVGW", "CVI", "CVLG", "CVLT", "CVM", "CVNA", "CVRX", "CVS", "CVX", "CW", "CWAN", "CWBC", "CWCO", "CWEN", "CWEN-A", "CWH", "CWK", "CWST", "CWT", "CX", "CXDO", "CXM", "CXT", "CXW", "CYBN", "CYBR", "CYCC", "CYD", "CYH", "CYN", "CYRX", "CYTK", "CZR", "CZWI", "D", "DAAQ", "DAC", "DAIC", "DAKT", "DAL", "DALN", "DAN", "DAO", "DAR", "DARE", "DASH", "DATS", "DAVA", "DAVE", "DAWN", "DAY", "DB", "DBD", "DBI", "DBRG", "DBX", "DC", "DCBO", "DCI", "DCO", "DCOM", "DCTH", "DD", "DDC", "DDI", "DDL", "DDOG", "DDS", "DEA", "DEC", "DECK", "DEFT", "DEI", "DELL", "DENN", "DEO", "DERM", "DEVS", "DFDV", "DFH", "DFIN", "DFSC", "DG", "DGICA", "DGII", "DGX", "DGXX", "DH", "DHI", "DHR", "DHT", "DHX", "DIBS", "DIN", "DINO", "DIOD", "DIS", "DJCO", "DJT", "DK", "DKL", "DKNG", "DKS", "DLB", "DLHC", "DLO", "DLTR", "DLX", "DLXY", "DMAC", "DMLP", "DMRC", "DMYY", "DNA", "DNB", "DNLI", "DNN", "DNOW", "DNTH", "DNUT", "DOC", "DOCN", "DOCS", "DOCU", "DOGZ", "DOLE", "DOMH", "DOMO", "DOOO", "DORM", "DOUG", "DOV", "DOW", "DOX", "DOYU", "DPRO", "DPZ", "DQ", "DRD", "DRDB", "DRH", "DRI", "DRS", "DRVN", "DSGN", "DSGR", "DSGX", "DSP", "DT", "DTE", "DTI", "DTIL", "DTM", "DTST", "DUK", "DUOL", "DUOT", "DV", "DVA", "DVAX", "DVN", "DVS", "DWTX", "DX", "DXC", "DXCM", "DXPE", "DXYZ", "DY", "DYN", "DYNX", "E", "EA", "EARN", "EAT", "EB", "EBAY", "EBC", "EBF", "EBMT", "EBR", "EBS", "EC", "ECC", "ECG", "ECL", "ECO", "ECOR", "ECPG", "ECVT", "ED", "EDBL", "EDIT", "EDN", "EDU", "EE", "EEFT", "EEX", "EFC", "EFSC", "EFX", "EFXT", "EG", "EGAN", "EGBN", "EGG", "EGO", "EGP", "EGY", "EH", "EHAB", "EHC", "EHTH", "EIC", "EIG", "EIX", "EKSO", "EL", "ELAN", "ELDN", "ELF", "ELMD", "ELME", "ELP", "ELPW", "ELS", "ELV", "ELVA", "ELVN", "ELWS", "EMA", "EMBC", "EMN", "EMP", "EMPD", "EMPG", "EMR", "EMX", "ENB", "ENGN", "ENGS", "ENIC", "ENOV", "ENPH", "ENR", "ENS", "ENSG", "ENTA", "ENTG", "ENVA", "ENVX", "EOG", "EOLS", "EOSE", "EPAC", "EPAM", "EPC", "EPD", "EPM", "EPR", "EPSM", "EPSN", "EQBK", "EQH", "EQNR", "EQR", "EQT", "EQV", "EQX", "ERIC", "ERIE", "ERII", "ERJ", "ERO", "ES", "ESAB", "ESE", "ESGL", "ESI", "ESLT", "ESNT", "ESOA", "ESQ", "ESTA", "ESTC", "ET", "ETD", "ETN", "ETNB", "ETON", "ETOR", "ETR", "ETSY", "EU", "EUDA", "EVAX", "EVC", "EVCM", "EVER", "EVEX", "EVGO", "EVH", "EVLV", "EVO", "EVOK", "EVR", "EVRG", "EVTC", "EVTL", "EW", "EWBC", "EWCZ", "EWTX", "EXAS", "EXC", "EXE", "EXEL", "EXK", "EXLS", "EXOD", "EXP", "EXPD", "EXPE", "EXPI", "EXPO", "EXR", "EXTR", "EYE", "EYPT", "EZPW", "F", "FA",
 "FACT", "FAF", "FANG", "FAST", "FAT", "FATN", "FBIN", "FBK", "FBLA", 
 "FBNC", "FBP", "FBRX", "FC", "FCBC", "FCEL", "FCF", "FCFS", "FCN", "FCX", "FDMT",
  "FDP", "FDS", "FDUS", "FDX", "FE", "FEIM", "FELE", "FENC", "FER", "FERA", "FERG", "FET", "FF", 
  "FFAI", "FFBC", "FFIC", "FFIN", "FFIV", "FFWM", "FG", "FGI", "FHB", "FHI", "FHN", "FHTX", "FI", "FIBK", "FIEE", "FIG", "FIGS", 
  "FIHL", "FINV", "FIP", "FIS", "FISI", "FITB", "FIVE", "FIVN", "FIZZ", "FL", "FLD", "FLEX", "FLG", "FLGT", "FLL", "FLNC", "FLNG", "FLO", "FLOC",
   "FLR", "FLS", "FLUT", "FLWS", "FLX", "FLY", "FLYE", "FLYW", "FLYY", "FMBH", "FMC", "FMFC", "FMNB", "FMS", "FMST", 
   "FMX", "FN", "FNB", "FND", "FNF", "FNGD", "FNKO", "FNV", "FOA", "FOLD", "FOR", "FORM", "FORR", "FOUR", "FOX", "FOXA", 
   "FOXF", "FPH", "FPI", "FRGE", "FRHC", "FRME", "FRO", "FROG", "FRPT", "FRSH", "FRST", "FSCO", "FSK", "FSLR", "FSLY",
    "FSM", "FSS", "FSUN", "FSV", "FTAI", "FTCI", "FTDR", "FTEK", "FTI", "FTK", "FTNT", "FTRE", "FTS", "FTV", "FUBO", "FUFU", "FUL", "FULC", "FULT", "FUN", "FUTU", "FVR", "FVRR", "FWONA", "FWONK", "FWRD", "FWRG", "FYBR", "G",
     "GABC", "GAIA", "GAIN", "GALT", "GAMB", "GAP", "GASS", "GATX", "GAUZ", "GB", "GBCI", "GBDC", "GBFH", "GBIO", "GBTG", "GBX", "GCI", "GCL", "GCMG", "GCO", "GCT", "GD", "GDC", "GDDY", "GDEN", "GDOT", "GDRX", "GDS", 
     "GDYN", "GE", "GEF", "GEHC", "GEL", "GEN", "GENI", "GENK", "GEO", "GEOS", "GES", "GFF", "GFI", "GFL", "GFR", "GFS", "GGAL", "GGB", "GGG", "GH", "GHLD", "GHM", "GHRS", "GIB", "GIC", "GIG", "GIII", "GIL", "GILD", "GILT", "GIS", "GITS", 
     "GKOS", "GL", "GLAD", "GLBE", "GLD", "GLDD", "GLIBA", "GLIBK", "GLNG", "GLOB", "GLP", "GLPG", "GLPI", "GLRE", "GLSI", "GLUE", "GLW", "GLXY", "GM", "GMAB", "GME", "GMED", "GMRE", "GMS", "GNE", "GNK", "GNL", "GNLX", "GNRC", "GNTX", "GNTY", "GNW", "GO", "GOCO", "GOGL", "GOGO", "GOLF", "GOOD", "GOOG", "GOOGL", "GOOS", "GORV", "GOTU", "GPAT", 
     "GPC", "GPCR", "GPI", "GPK", "GPN", "GPOR", "GPRE", "GPRK", "GRAB", "GRAL", "GRAN", "GRBK", "GRC", "GRCE", "GRDN", "GRFS", "GRMN", "GRND", "GRNT", "GROY", "GRPN", "GRRR", "GSAT", "GSBC", "GSBD", "GSHD", "GSIT", "GSK", "GSL", "GSM", "GSRT", "GT", "GTE", "GTEN", "GTERA", "GTES", "GTLB", "GTLS", "GTM", "GTN", "GTX", "GTY", "GVA", "GWRE", "GWRS", "GXO", "GYRE", "H", "HAE", "HAFC", "HAFN", "HAL", "HALO", "HAS", "HASI", "HAYW", "HBAN", "HBCP", "HBI", "HBM", "HBNC", "HCA", "HCAT", "HCC", "HCHL", "HCI", "HCKT", "HCM", "HCSG", "HCTI", "HCWB", "HD", "HDB", "HDSN", "HE", "HEI", "HEI-A", "HELE", "HEPS", "HESM", "HFFG", "HFWA", "HG", "HGTY", "HGV", "HHH", "HI", "HIFS", "HIG", "HII", "HIMS", "HIMX",
     "HIPO", "HIT", "HITI", "HIVE", "HIW", "HL", "HLF", "HLI", "HLIO", "HLIT", "HLLY", "HLMN", "HLN", "HLNE", "HLT", "HLVX", "HLX", "HLXB", "HMC", "HMN", "HMST", "HMY", "HNGE", "HNI", "HNRG", "HNST", "HOFT", "HOG", "HOLO", "HOLX", "HOMB", "HON", "HOND", "HONE", "HOOD", "HOPE", "HOUS", "HOV", "HP", "HPE", "HPK", "HPP", "HPQ", "HQH", "HQL", "HQY", "HRB", "HRI", "HRL", "HRMY", "HROW", "HRTG", "HRZN", "HSAI", "HSBC", "HSCS", "HSHP", "HSIC", "HSII", "HST", "HSTM", "HSY", "HTBK", "HTCO", "HTGC", "HTH", "HTHT", "HTLD", "HTO", "HTOO", "HTZ", "HUBB", "HUBC", "HUBG", "HUBS", "HUHU", "HUM", "HUMA", "HUN", "HURA", "HURN", "HUSA", "HUT", "HUYA", "HVII", "HVT", "HWC", "HWKN", "HWM", 
     "HXL", "HY", "HYAC", "HYMC", "HYPD", "HZO", "IAC", "IAG", "IART", "IAS", "IBCP", "IBEX", "IBKR", "IBM", "IBN", "IBOC", "IBP", "IBRX", "IBTA", "ICE", "ICFI", "ICG", "ICHR", "ICL", "ICLR", "ICUI", "IDA", "IDAI", "IDCC", "IDN", "IDR", "IDT", "IDYA", "IE", "IEP", "IESC", "IEX", "IFF", "IFS", "IGIC", "IHG", "IHS", "III", "IIIN", "IIIV", "IIPR", "ILMN", "IMAB", "IMAX", "IMCC", "IMCR", "IMDX", "IMKTA", "IMMR", "IMMX", "IMNM", "IMNN", "IMO", "IMPP", "IMRX", "IMTX", "IMVT", "IMXI", "INAB", "INAC", "INBK", "INBX", "INCY", "INDB", "INDI", "INDO", "INDP", "INDV", "INFA", "INFU", "INFY", "ING", "INGM", "INGN", "INGR", "INKT", "INMB", "INMD", "INN", "INOD", "INR", "INSE", "INSG", "INSM", "INSP", "INSW", "INTA", "INTC", "INTR", "INUV", "INV", "INVA", "INVE", "INVH", "INVX", "IONQ", "IONS", "IOSP", "IOT", "IOVA", "IP", "IPA", "IPAR", "IPDN", "IPG", "IPGP", "IPI", "IPX", "IQST", "IQV", "IR", "IRBT", "IRDM", "IREN", "IRM", "IRMD", "IROH", "IRON", "IRS", "IRTC", "ISPR", "ISRG", "ISSC", "IT", "ITGR", "ITIC", "ITOS", "ITRI", "ITRN", "ITT", "ITUB", "ITW", "IVR", "IVZ", "IX", "IZEA", "J", "JACK", "JACS", "JAKK", "JAMF", "JANX", "JAZZ", "JBGS", "JBHT", "JBI", "JBIO", "JBL", "JBLU", "JBS", "JBSS", "JBTM", "JCAP", "JCI", "JD", "JEF", "JELD", "JEM", "JENA", "JFIN", "JHG", "JHX", "JILL", "JJSF", "JKHY", "JKS", "JLHL", "JLL", "JMIA", "JNJ", "JOBY", "JOE", "JOUT", "JOYY", "JPM", "JRSH", "JRVR", "JSPR", "JTAI", "JVA", "JXN", "JYNT", "K", "KAI", "KALA", "KALU", 
     "KALV", "KAR", "KARO", "KB", "KBDC", "KBH", "KBR", "KC", "KCHV", "KD", "KDP", "KE", "KELYA", "KEP", "KEX", "KEY", "KEYS", "KFII", "KFRC", "KFS", "KFY", "KGC", "KGEI", "KGS", "KHC", "KIDS", "KIM", "KINS", "KKR", "KLC", "KLG", "KLIC", "KLRS", "KMB", "KMDA", "KMI", "KMPR", "KMT", "KMTS", "KMX", "KN", "KNF", "KNOP", "KNSA", "KNSL", "KNTK", "KNW", "KNX", "KO", "KOD", "KODK", "KOF", "KOP", "KOSS", "KPRX", "KPTI", "KR", "KRC", "KRMD", "KRMN", "KRNT", "KRNY", "KRO", "KROS", "KRP", "KRRO", "KRT", "KRUS", "KRYS", "KSCP", "KSPI", "KSS", "KT", "KTB", "KTOS", "KULR", "KURA", "KVUE", "KVYO", "KW", "KWM", "KWR", "KYMR", "KYTX", "KZIA", "L", "LAC", "LAD", "LADR", "LAES", "LAKE", "LAMR", "LAND", "LANV", "LAR", "LASE", "LASR", "LAUR", "LAW", "LAWR", "LAZ", "LAZR", "LB", "LBRDA", "LBRDK", "LBRT", "LBTYA", "LBTYK", "LC", "LCCC", "LCFY", "LCID", "LCII", "LCUT", "LDOS", "LE", "LEA", "LECO", "LEG", "LEGH", "LEGN", "LEN", "LENZ", "LEO", "LEU", "LEVI", "LFCR", "LFMD", "LFST", "LFUS", "LFVN", "LGCY", "LGIH", "LGND", "LH", "LHAI", "LHSW", "LHX", "LI", "LIDR", "LIF", "LILA", "LILAK", "LIMN", "LIN", "LINC", "LIND", "LINE", "LION", "LITE", "LITM", "LIVE", "LIVN", "LIXT", "LKFN", "LKQ", "LLYVA", "LLYVK", "LMAT", "LMB", "LMND", "LMNR", "LMT", "LNC", "LNG", "LNN", "LNSR", "LNT", "LNTH", "LNW", "LOAR", "LOB", "LOCO", "LODE", "LOGI", "LOKV", "LOMA", "LOPE", "LOT", "LOVE", "LOW", "LPAA", "LPBB", "LPCN", "LPG", "LPL", "LPLA", "LPRO", "LPTH", "LPX", "LQDA", "LQDT", "LRCX", "LRMR", "LRN", "LSCC", "LSE", "LSPD", "LSTR", "LTBR", "LTC", "LTH", "LTM", "LTRN", "LTRX", "LU", "LUCK", "LULU", "LUMN", "LUNR", "LUV", "LUXE", "LVLU", "LVS", "LVWR", "LW", "LWAY", "LWLG", "LX", "LXEH", "LXEO", "LXFR", "LXU", "LYB", "LYEL", "LYFT", "LYG", "LYRA", "LYTS", "LYV", "LZ", "LZB", "LZM", "LZMH", "M", "MAA", "MAAS", "MAC", "MACI", "MAG", "MAGN", "MAIN", "MAMA", "MAMK", "MAN", "MANH", "MANU", "MAR", "MARA", "MAS", "MASI", "MASS", "MAT", "MATH", "MATV", "MATW", "MATX", "MAX", "MAXN", "MAZE", "MB", "MBAV", "MBC", "MBI", "MBIN", "MBLY",
      "MBOT", "MBUU", "MBWM", "MBX", "MC", "MCB", "MCD", "MCFT", "MCHP", "MCRB", "MCRI", "MCRP", "MCS", "MCVT", "MCW", "MCY", "MD", "MDAI", "MDB", "MDCX", "MDGL", "MDLZ", "MDT", "MDU", "MDV", "MDWD", "MDXG", "MDXH", "MEC", "MED", "MEDP", "MEG", "MEI", "MEIP", "MENS", "MEOH", "MERC", "MESO", "MET", "METC", "METCB", "MFA", "MFC", "MFG", "MFH", "MFI", "MFIC", "MFIN", "MG", "MGA", "MGEE", "MGIC", "MGM", "MGNI", "MGPI", "MGRC", "MGRM", "MGRT", "MGTX", "MGY", "MH", "MHK", "MHO", "MIDD", "MIMI", "MIND", "MIR", "MIRM", "MITK", "MKC", "MKSI", "MKTX", "MLAB", "MLCO", "MLEC", "MLGO", "MLI", "MLKN", "MLNK", "MLR", "MLTX", "MLYS", "MMC", "MMI", "MMM", "MMS", "MMSI", "MMYT", "MNDY", "MNKD", "MNMD", "MNR", "MNRO", "MNSO", "MNST", "MNTN", "MO", "MOB", "MOD", "MODG", "MODV", "MOFG", "MOG-A", "MOH", "MOMO", "MORN", "MOS", "MOV", "MP", "MPAA", "MPB", "MPC", "MPLX", "MPTI", "MPU", "MQ", "MRAM", "MRBK", "MRC", "MRCC", "MRCY", "MRK", "MRNA", "MRP", "MRSN", "MRT", "MRTN", "MRUS", "MRVI", "MRVL", "MRX", "MS", "MSA", "MSBI", "MSEX", "MSGE", "MSGM", "MSGS", "MSGY", "MSI", "MSM", "MSTR", "MT", "MTA", "MTAL", "MTB", "MTCH", "MTDR", "MTEK", "MTEN", "MTG", "MTH", "MTLS", "MTN", "MTRN", "MTRX", "MTSI", "MTSR", "MTUS", "MTW", "MTX", "MTZ", "MU", "MUFG", "MUR", "MUSA", "MUX", "MVBF", "MVST", "MWA", "MX", "MXL", "MYE", "MYFW", "MYGN", "MYRG", "MZTI", "NA", "NAAS", "NABL", "NAGE", "NAKA", "NAMM", "NAMS", "NAT", "NATH", "NATL", "NATR", "NAVI", "NB", "NBBK", "NBHC", "NBIS", "NBIX", "NBN", "NBR", "NBTB", "NCDL", "NCLH", "NCMI", "NCNO", "NCPL", "NCT", "NCTY", "NDAQ", "NDSN", "NE", "NEE", "NEGG", "NEM", "NEO", "NEOG", "NEON", "NEOV", "NESR", "NET", "NETD", "NEWT", "NEXM", "NEXN", "NEXT", "NFBK", "NFE", "NFG", "NG", "NGD", "NGG", "NGL", "NGNE", "NGS", "NGVC", "NGVT", "NHC", "NHI", "NHIC", "NI", "NIC", "NICE", "NIO", "NIQ", "NISN", "NIU", "NJR", "NKE", "NKTR", "NLOP", "NLSP", "NLY", "NMAX", "NMFC", "NMIH", "NMM", "NMR", "NMRK", "NN", "NNBR", "NNE", "NNI", "NNN", "NNNN", "NNOX", "NOA", "NOAH", "NOG", "NOK", "NOMD", "NOV", "NOVT", "NPAC", "NPB", "NPCE", "NPK", "NPKI", "NPO", "NPWR", "NRC", "NRDS", "NRG", "NRIM", "NRIX", "NRXP", "NRXS", "NSC", "NSIT", "NSP", "NSPR", "NSSC", "NTAP", "NTB", "NTCT", "NTES", "NTGR", "NTHI", "NTLA", "NTNX", "NTR", "NTRA", "NTRB", "NTST", "NU", "NUE", "NUKK", "NUS", "NUTX", "NUVB", "NUVL", "NUWE", "NVAX", "NVCR", "NVCT", "NVDA", "NVEC", "NVGS", "NVMI", "NVNO", "NVO", "NVRI", "NVS", 
  "NVST", "NVT", "NVTS", "NWBI", "NWE", "NWG", "NWL", "NWN", "NWPX", "NWS", "NWSA", "NX", "NXE", "NXP", "NXPI", "NXST", "NXT", "NXTC", "NYT", "NYXH", "O", "OACC", "OBDC", "OBE", "OBIO", "OBK", "OBLG", "OBT", "OC", "OCC", "OCCI", "OCFC", "OCFT", "OCSL", "OCUL", "ODC", "ODD", "ODFL", "ODP", "ODV", "OEC", "OFG", "OFIX", "OGE", "OGN", "OGS", "OHI", "OI", "OII", "OIS", "OKE", "OKLO", "OKTA", "OKUR", "OKYO", "OLED", "OLLI", "OLMA", "OLN", "OLO", "OLP", "OM", "OMAB", "OMC", "OMCL", "OMDA", "OMER", "OMF", "OMI", "OMSE", "ON", "ONB", "ONC", "ONDS", "ONEG", "ONEW", "ONL", "ONON", "ONTF", "ONTO", "OOMA", "OPAL", "OPBK", "OPCH", "OPFI", "OPRA", "OPRT", "OPRX", "OPXS", "OPY", "OR", "ORA", "ORC", "ORCL", "ORGO", "ORI", "ORIC", "ORKA", "ORLA", "ORLY", "ORMP", "ORN", "ORRF", "OS", "OSBC", "OSCR", "OSIS", "OSK", "OSPN", "OSS", "OSUR", "OSW", "OTEX", "OTF", "OTIS", "OTLY", "OTTR", "OUST", "OUT", "OVV", "OWL", "OWLT", "OXLC", "OXM", "OXSQ", "OXY", "OYSE", "OZK", "PAA", "PAAS", "PAC", "PACK", "PACS", "PAG", "PAGP", "PAGS", "PAHC", "PAL", "PAM", "PANL", "PANW", "PAR", "PARR", "PATH", "PATK", "PAX", "PAY", "PAYC", "PAYO", "PAYS", "PAYX", "PB", "PBA", "PBF", "PBH", "PBI", "PBPB", "PBR", "PBR-A", "PBYI", "PC", "PCAP", "PCAR", "PCG", "PCH", "PCOR", "PCRX", "PCT", "PCTY", "PCVX", "PD", "PDD", "PDEX", "PDFS", "PDS", "PDYN", "PEBO", "PECO", "PEG", "PEGA", "PEN", "PENG", "PENN", "PEP", "PERI", "PESI", "PETS", "PEW", "PFBC", "PFE", "PFG", "PFGC", "PFLT", "PFS", "PFSI", "PG", "PGC", "PGNY", "PGR", "PGRE", "PGY", "PHAT", "PHG", "PHI", "PHIN", "PHIO", "PHLT", "PHM", "PHOE", "PHR", "PHUN", "PHVS", "PI", "PII", "PINC", "PINS", "PIPR", "PJT", "PK", "PKE", "PKG", "PKX", "PL", "PLAB", "PLAY", "PLCE", "PLD", "PLL", "PLMR", "PLNT", "PLOW", "PLPC", "PLSE", "PLTK", "PLTR", "PLUS", "PLXS", "PLYM", "PM", "PMTR", "PMTS", "PN", "PNC", "PNFP", "PNNT", "PNR", "PNRG", "PNTG", "PNW", "PODD", "POET", "PONY", "POOL", "POR", "POST", "POWI", "POWL", "PPBI", "PPBT", "PPC", "PPG", "PPIH", "PPL", "PPSI", "PPTA", "PR", "PRA", "PRAA", "PRAX", "PRCH", "PRCT", "PRDO", "PRE", "PRG", "PRGO", "PRGS", "PRI", "PRIM", "PRK", "PRKS", "PRLB", "PRM", "PRMB", "PRME", "PRO", "PROK", "PROP", "PRQR", "PRSU", "PRTA", "PRTG", "PRTH", "PRU", "PRVA", "PSA", "PSEC", "PSFE", "PSIX", "PSKY", "PSMT", "PSN", "PSNL", "PSO", "PSQH", "PSTG", "PSX", "PTC", "PTCT", "PTEN", "PTGX", "PTHS", "PTLO", "PTON", "PUBM", "PUK", "PUMP", "PVBC", "PVH", "PVLA", "PWP", "PWR", "PX", "PXLW", "PYPD", "PYPL", "PZZA", "QBTS", "QCOM", "QCRH", "QD", "QDEL", "QFIN", "QGEN", "QIPT", "QLYS", "QMCO", "QMMM", "QNST", "QNTM", "QRHC", "QRVO", "QS", "QSEA", "QSG", "QSR", "QTRX", "QTWO", "QUAD", "QUBT", "QUIK", "QURE", "QVCGA", "QXO", "R", "RAAQ", "RAC", "RACE", "RAIL", "RAL", "RAMP", "RAPP", "RAPT", "RARE", "RAY", "RBA", "RBB", "RBBN", "RBC", "RBCAA", "RBLX", "RBRK", "RC", "RCAT", "RCEL", "RCI", "RCKT", "RCKY", "RCL", "RCMT", "RCON", "RCT", 
    "RCUS", "RDAG", "RDAGU", "RDCM", "RDDT", "RDN", "RDNT", "RDVT", "RDW", "RDWR", "RDY", "REAL", "REAX", "REBN", "REFI", "REG", "RELX", "RELY", "RENT", "REPL", "REPX", "RERE", "RES", "RETO", "REVG", "REX", "REXR", "REYN", "REZI", "RF", "RFIL", "RGA", "RGC", "RGEN", "RGLD", "RGNX", "RGP", "RGR", "RGTI", "RH", "RHI", "RHLD", "RHP", "RICK", "RIG", "RIGL", "RILY", "RIME", "RIO", "RIOT", "RITM", "RITR", "RIVN", "RJF", "RKLB", "RKT", "RL", "RLAY", "RLGT", "RLI", "RLX", "RMAX", "RMBI", "RMBL", "RMBS", "RMD", "RMNI", "RMR", "RMSG", "RNA", "RNAC", "RNAZ", "RNG", "RNGR", "RNR", "RNST", "RNW", "ROAD", "ROCK", "ROG", "ROIV", "ROK", "ROKU", "ROL", "ROLR", "ROMA", "ROOT", "ROST", "RPAY", "RPD", "RPID", "RPM", "RPRX", "RPT", "RRC", "RRGB", "RRR", "RRX", "RS", "RSG", "RSI", "RSKD", "RSLS", "RSVR", "RTAC", "RTO", "RTX", "RUBI", "RUM", "RUN", "RUSHA", "RUSHB", "RVLV", "RVMD", "RVSB", "RVTY", "RWAY", "RXO", "RXRX", "RXST", "RY", "RYAAY", "RYAM", "RYAN", "RYI", "RYN", "RYTM", "RZB", "RZLT", "RZLV", "S", "SA", "SABS", "SAFE", "SAFT", "SAGT", "SAH", "SAIA", "SAIC", "SAIL", "SAM", "SAMG", "SAN", "SANA", "SAND", "SANM", "SAP", "SAR", "SARO", "SATL", "SATS", "SAVA", "SB", "SBAC", "SBC", "SBCF", "SBET", "SBGI", "SBH", "SBLK", "SBRA", "SBS", "SBSI", "SBSW", "SBUX", "SBXD", "SCAG", "SCCO", "SCHL", "SCHW", "SCI", "SCL", "SCLX", "SCM", "SCNX", "SCPH", "SCS", "SCSC", "SCVL", "SD", "SDA", "SDGR", "SDHC", "SDHI", "SDM", "SDRL", "SE", "SEAT", "SEDG", "SEE", "SEG", "SEI", "SEIC", "SEM", "SEMR", "SENEA", "SEPN", "SERA", "SERV", "SEZL", "SF", "SFBS", "SFD", "SFIX", "SFL", "SFM", "SFNC", "SG", "SGHC", "SGHT", "SGI", "SGML", "SGMT", "SGRY", "SHAK", "SHBI", "SHC", "SHCO", "SHEL", "SHEN", "SHG", "SHIP", "SHLS", "SHO", "SHOO", "SHOP", "SHW", "SI", "SIBN", "SIEB", "SIFY", "SIG", "SIGA", "SIGI", "SII", "SIMO", "SINT", "SION", "SIRI", "SITC", "SITE", "SITM", "SJM", "SKE", "SKLZ", "SKM", "SKT", "SKWD", "SKX", "SKY", "SKYE", "SKYH", "SKYT", "SKYW", "SLAB", "SLB", "SLDB", "SLDE", "SLDP", "SLF", "SLG", "SLGN", "SLI", "SLM", "SLN", "SLND", "SLNO", "SLP", "SLRC", "SLSN", "SLVM", "SM", "SMA", "SMBK", "SMC", "SMCI", "SMFG", "SMG", "SMHI", "SMLR", "SMMT", "SMP", "SMPL", "SMR", "SMTC", "SMWB", "SMX", "SN", "SNA", "SNAP", "SNBR", "SNCR", "SNCY", "SNDK", "SNDR", "SNDX", "SNES", "SNEX", "SNFCA", "SNGX", "SNN", "SNOW", "SNRE", "SNT", "SNV", "SNWV", "SNX", "SNY", "SNYR", "SO", "SOBO", "SOC", "SOFI", "SOGP", "SOHU", "SOLV", "SON", "SOND", "SONN", "SONO", "SONY", "SOPH", "SORA", "SOS", "SOUL", "SOUN", "SPAI", "SPB", "SPCB", "SPCE", "SPG", "SPH", "SPHR", "SPIR", "SPKL", "SPNS", "SPNT", "SPOK", "SPR", "SPRO", "SPRY", "SPSC", "SPT", "SPTN", "SPWH", "SPXC", "SQM", "SR", "SRAD", "SRBK", "SRCE", "SRDX", "SRE", "SRFM", "SRG", "SRI", "SRPT", "SRRK", "SRTS", "SSB", "SSD", "SSII", "SSL", "SSNC", "SSP", "SSRM", "SSSS", "SST", "SSTI", "SSTK", "SSYS", "ST", "STAA", "STAG", "STBA", "STC", "STE", "STEL", "STEM", "STEP", "STFS", "STGW", "STHO", "STI", "STIM", "STKL", "STKS", "STLA", "STLD", "STM", "STN", "STNE", "STNG", "STOK", "STR", "STRA", "STRD", "STRL", 
    "STRM", "STRT", "STRZ", "STSS", "STT", "STVN", "STX", "STXS", "STZ", "SU", "SUI", "SUN", "SUPN", "SUPV", "SUPX", "SURG", "SUZ", "SVCO", "SVM", "SVRA", "SVV", "SW", "SWBI", "SWIM", "SWIN", "SWK", "SWKS", "SWX", "SXC", "SXI", "SXT", "SY", "SYBT", "SYF", "SYK", "SYM", "SYNA", "SYRE", "SYTA", "SYY", "SZZL", "T", "TAC", "TACH", "TACO", "TAK", "TAL", "TALK", "TALO", "TAOX", "TAP", "TARA", "TARS", "TASK", "TATT", "TBB", "TBBB", "TBBK", "TBCH", "TBI", "TBLA", "TBPH", "TBRG", "TCBI", "TCBK", "TCBX", "TCMD", "TCOM", "TCPC", "TD", "TDC", "TDIC", "TDOC", "TDS", "TDUP", "TDW", "TEAM", "TECH", "TECK", "TECX", "TEF", "TEL", "TEM", "TEN", "TENB", "TEO", "TER", "TERN", "TEVA", "TEX", "TFC", "TFII", "TFIN", "TFPM", "TFSL", "TFX", "TG", "TGB", "TGE", "TGEN", "TGLS", "TGNA", "TGS", "TGT", "TGTX", "TH", "THC", "THFF", "THG", "THO", "THR", "THRM", "THRY", "THS", "THTX", "TIC", "TIGO", "TIGR", "TIL", "TILE", "TIMB", "TIPT", "TITN", "TIXT", "TJX", "TK", "TKC", "TKNO", "TKO", "TKR", "TLK", "TLN", "TLS", "TLSA", "TLSI", "TM", "TMC",
     "TMCI", "TMDX", "TME", "TMHC", "TMO", "TMUS", "TNC", "TNDM", "TNET", "TNGX", "TNK", "TNL", "TNXP", "TOI", "TOL", "TOPS", "TORO", "TOST", "TOWN", "TPB", "TPC", "TPCS", "TPG", "TPH", "TPR", "TPST", "TPVG", "TR", "TRAK", "TRC", "TRDA", "TREE", "TREX", "TRGP", "TRI", "TRIN", "TRIP", "TRMB", "TRMD", "TRML", "TRN", "TRNO", "TRNR", "TRNS", "TRON", "TROW", "TROX", "TRP", "TRS", "TRU", "TRUE", "TRUG", "TRUP", "TRV", "TRVG", "TRVI", "TS", "TSAT", "TSCO", "TSE", "TSEM", "TSHA", "TSLA", "TSLX", "TSM", "TSN", "TSQ", "TSSI", "TT", "TTAM", "TTAN", "TTC", "TTD", "TTE", "TTEC", "TTEK", "TTGT", "TTI", "TTMI", "TTSH", "TTWO", "TU", "TUSK", "TUYA", "TV", "TVA", "TVAI", "TVRD", "TVTX", "TW", "TWFG", "TWI", "TWIN", "TWLO", "TWNP", "TWO", "TWST", "TX", "TXG", "TXN", "TXNM", "TXO", "TXRH", "TXT", "TYG", "TYRA", "TZOO", "TZUP", "U", "UA", "UAA", "UAL", "UAMY", "UAVS", "UBER", "UBFO", "UBS", "UBSI", "UCAR", "UCB", "UCL", "UCTT", "UDMY", "UDR", "UE", "UEC", "UFCS", "UFG", "UFPI", "UFPT", 
     "UGI", "UGP", "UHAL", "UHAL-B", "UHG", "UHS", "UI", "UIS", "UL", "ULBI", "ULCC", "ULS", "ULY", "UMAC", "UMBF", "UMC", "UMH", "UNCY", "UNF", "UNFI", "UNH", "UNIT", "UNM", "UNP", "UNTY", "UPB", "UPBD", "UPS", "UPST", "UPWK", "UPXI", "URBN", "URGN", "UROY", "USAC", "USAR", "USAU", "USB", "USFD", "USLM", "USM", "USNA", "USPH", "UTHR", "UTI", "UTL", "UTZ", "UUUU", "UVE", "UVSP", "UVV", "UWMC", "UXIN", "V", "VAC", "VAL", "VALE", "VBIX", "VBNK", "VBTX", "VC", "VCEL", "VCTR", "VCYT", "VECO", "VEEV", "VEL", "VENU", "VEON", "VERA", "VERB", "VERI", "VERX", 
     "VET", "VFC", "VFS", "VG", "VIAV", "VICI", "VICR", "VIK", "VINP", "VIOT", "VIPS", "VIR", "VIRC", "VIRT", "VIST", "VITL", "VIV", "VKTX",
      "VLGEA", "VLN", "VLO", "VLRS", "VLTO", "VLY", "VMC", "VMD", "VMEO", "VMI", "VNDA", "VNET", "VNOM", "VNT", "VNTG", "VOD", "VOR", "VOXR", "VOYA", "VOYG", "VPG", "VRDN", "VRE",
       "VREX", "V", "WING", "WIT", "WIX", "WK", "WKC", "WKEY", "WKSP", "WLDN", "WLFC", "WLK", "WLY", "WM", "WMB", "WMG", "WMK", "WMS", "WMT", "WNC", "WNEB", "WNS", "WOOF", "WOR", "WOW", "WPC", "WPM", "WPP", "WRB", "WRBY", "WRD",
       "WS", "WSBC", "WSC", "WSFS", "WSM", "WSO", "WSR", "WST", "WT", "WTF", "WTG", "WTRG", "WTS", "WTTR", "WTW", "WU", "WULF", "WVE", "WW", "WWD", "WWW", "WXM", "WY", "WYFI", "WYNN", "WYY", "XAIR", "XBIT", "XCUR", "XEL", "XENE", "XERS", "XGN", "XHR", "XIFR", "XMTR", "XNCR", "XNET", "XOM", "XOMA", "XP", "XPEL", "XPER", "XPEV", "XPO",
        "XPOF", "XPRO", "XRAY", "XRX", "XTKG", "XYF", "XYL", "XYZ", "YALA", "YB", "YELP", "YETI", "YEXT", "YMAB", "YMAT", "YMM", "YORK", "YORW", "YOU", "YPF", "YRD", "YSG", "YSXT", "YUM", "YUMC", "YYAI", "YYGH", "Z",
         "ZBAI", "ZBH", "ZBIO", "ZBRA", "ZD", "ZDGE", "ZENA", "ZEO", "ZEPP", "ZETA", "ZEUS", 
         "ZG", "ZGN", "ZH", "ZIM", "ZIMV", "ZION", "ZIP", "ZJK", "ZK", "ZLAB", "ZM", "ZONE", "ZS", "ZSPC", "ZTO", "ZTS", "ZUMZ", "ZVIA", "ZVRA", "ZWS", "ZYBT", "ZYME"]


@dataclass
class MarketRegime:
    """å¸‚åœºçŠ¶æ€"""
    regime_id: int
    name: str
    probability: float
    characteristics: Dict[str, float]
    duration: int = 0

@dataclass 
class RiskFactorExposure:
    """é£é™©å› å­æš´éœ²"""
    market_beta: float
    size_exposure: float  
    value_exposure: float
    momentum_exposure: float
    volatility_exposure: float
    quality_exposure: float
    country_exposure: Dict[str, float] = field(default_factory=dict)
    sector_exposure: Dict[str, float] = field(default_factory=dict)

def sanitize_ticker(raw: Union[str, Any]) -> str:
    """æ¸…ç†è‚¡ç¥¨ä»£ç ä¸­çš„BOMã€å¼•å·ã€ç©ºç™½ç­‰æ‚è´¨ã€‚"""
    try:
        s = str(raw)
    except Exception:
        return ''
    # å»é™¤BOMä¸é›¶å®½å­—ç¬¦
    s = s.replace('\ufeff', '').replace('\u200b', '').replace('\u200c', '').replace('\u200d', '')
    # å»é™¤å¼•å·ä¸ç©ºç™½
    s = s.strip().strip("'\"")
    # ç»Ÿä¸€å¤§å†™
    s = s.upper()
    return s


def load_universe_from_file(file_path: str) -> Optional[List[str]]:
    try:
        if os.path.exists(file_path):
            # ä½¿ç”¨utf-8-sigä»¥è‡ªåŠ¨å»é™¤BOM
            with open(file_path, 'r', encoding='utf-8-sig') as f:
                tickers = []
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    # æ”¯æŒé€—å·æˆ–ç©ºæ ¼åˆ†éš”
                    parts = [p for token in line.split(',') for p in token.split()]
                    for p in parts:
                        t = sanitize_ticker(p)
                        if t:
                            tickers.append(t)
            # å»é‡å¹¶ä¿æŒé¡ºåº
            tickers = list(dict.fromkeys(tickers))
            return tickers if tickers else None
    except Exception:
        return None
    return None

def load_universe_fallback() -> List[str]:
    # ç»Ÿä¸€ä»é…ç½®æ–‡ä»¶è¯»å–è‚¡ç¥¨æ¸…å•ï¼Œç§»é™¤æ—§ç‰ˆä¾èµ–
    root_stocks = os.path.join(os.path.dirname(__file__), 'stocks.txt')
    tickers = load_universe_from_file(root_stocks)
    if tickers:
        return tickers
    
    logger.warning("æœªæ‰¾åˆ°stocks.txtæ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤è‚¡ç¥¨æ¸…å•")
    return DEFAULT_TICKER_LIST

class UltraEnhancedQuantitativeModel:
    """Ultra Enhanced é‡åŒ–æ¨¡å‹ï¼šé›†æˆæ‰€æœ‰é«˜çº§åŠŸèƒ½"""
    
    def __init__(self, config_path: str = "alphas_config.yaml"):
        """
        åˆå§‹åŒ–Ultra Enhancedé‡åŒ–æ¨¡å‹
        
        Args:
            config_path: Alphaç­–ç•¥é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.config = self._load_config()
        
        # ğŸ”¥ ç”Ÿäº§çº§åŠŸèƒ½ï¼šæ¨¡å‹ç‰ˆæœ¬æ§åˆ¶
        try:
            from model_version_control import ModelVersionControl
            self.version_control = ModelVersionControl("ultra_models")
            logger.info("æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿå·²å¯ç”¨")
        except ImportError as e:
            logger.warning(f"ç‰ˆæœ¬æ§åˆ¶æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
            self.version_control = None
        
        # æ ¸å¿ƒå¼•æ“
        if ENHANCED_MODULES_AVAILABLE:
            self.alpha_engine = AlphaStrategiesEngine(config_path)
            self.ltr_bma = LearningToRankBMA(
                ranking_objective=self.config.get('model_config', {}).get('ranking_objective', 'rank:pairwise'),
                temperature=self.config.get('temperature', 1.2),
                enable_regime_detection=self.config.get('model_config', {}).get('regime_detection', True)
            )
            self.portfolio_optimizer = AdvancedPortfolioOptimizer(
                risk_aversion=self.config.get('risk_config', {}).get('risk_aversion', 5.0),
                turnover_penalty=self.config.get('risk_config', {}).get('turnover_penalty', 1.0),
                max_turnover=self.config.get('max_turnover', 0.10),
                max_position=self.config.get('max_position', 0.03),
                max_sector_exposure=self.config.get('risk_config', {}).get('max_sector_exposure', 0.15),
                max_country_exposure=self.config.get('risk_config', {}).get('max_country_exposure', 0.20)
            )
        else:
            logger.warning("å¢å¼ºæ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€åŠŸèƒ½")
            self.alpha_engine = None
            self.ltr_bma = None
            self.portfolio_optimizer = None
        
        # ä¼ ç»ŸMLæ¨¡å‹ï¼ˆä½œä¸ºå¯¹æ¯”ï¼‰
        self.traditional_models = {}
        self.model_weights = {}
        
        # Professionalå¼•æ“åŠŸèƒ½
        self.risk_model_results = {}
        self.current_regime = None
        self.regime_weights = {}
        self.market_data_manager = UnifiedMarketDataManager() if MARKET_MANAGER_AVAILABLE else None
        
        # æ•°æ®å’Œç»“æœå­˜å‚¨
        self.raw_data = {}
        self.feature_data = None
        self.alpha_signals = None
        self.final_predictions = None
        self.portfolio_weights = None
        
        # æ€§èƒ½è·Ÿè¸ª
        self.performance_metrics = {}
        self.backtesting_results = {}
        
        # å¥åº·ç›‘æ§è®¡æ•°å™¨
        self.health_metrics = {
            'universe_load_fallbacks': 0,
            'risk_model_failures': 0,
            'optimization_fallbacks': 0,
            'alpha_computation_failures': 0,
            'neutralization_failures': 0,
            'prediction_failures': 0,
            'total_exceptions': 0
        }
        
        logger.info("UltraEnhancedé‡åŒ–æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def get_health_report(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿå¥åº·çŠ¶å†µæŠ¥å‘Š"""
        total_operations = sum(self.health_metrics.values())
        failure_rate = (self.health_metrics['total_exceptions'] / max(total_operations, 1)) * 100
        
        report = {
            'health_metrics': self.health_metrics.copy(),
            'failure_rate_percent': failure_rate,
            'risk_level': 'LOW' if failure_rate < 5 else 'MEDIUM' if failure_rate < 15 else 'HIGH',
            'recommendations': []
        }
        
        # æ ¹æ®å¤±è´¥ç±»å‹ç»™å‡ºå»ºè®®
        if self.health_metrics['universe_load_fallbacks'] > 0:
            report['recommendations'].append("æ£€æŸ¥è‚¡ç¥¨æ¸…å•æ–‡ä»¶æ ¼å¼å’Œç¼–ç ")
        if self.health_metrics['risk_model_failures'] > 2:
            report['recommendations'].append("æ£€æŸ¥UMDMé…ç½®å’Œå¸‚åœºæ•°æ®è¿æ¥")
        if self.health_metrics['optimization_fallbacks'] > 1:
            report['recommendations'].append("æ£€æŸ¥æŠ•èµ„ç»„åˆçº¦æŸè®¾ç½®")
        
        return report
    
    def build_risk_model(self) -> Dict[str, Any]:
        """æ„å»ºMulti-factoré£é™©æ¨¡å‹ï¼ˆæ¥è‡ªProfessionalå¼•æ“ï¼‰"""
        logger.info("æ„å»ºMulti-factoré£é™©æ¨¡å‹")
        
        if not self.raw_data:
            raise ValueError("Market data not loaded")
        
        # æ„å»ºæ”¶ç›Šç‡çŸ©é˜µ
        returns_data = []
        tickers = []
        
        for ticker, data in self.raw_data.items():
            if len(data) > 100:
                returns = data['close'].pct_change().fillna(0)
                returns_data.append(returns)
                tickers.append(ticker)
        
        if not returns_data:
            raise ValueError("No valid returns data")
        
        returns_matrix = pd.concat(returns_data, axis=1, keys=tickers)
        returns_matrix = returns_matrix.fillna(0.0)
        
        # æ„å»ºé£é™©å› å­
        risk_factors = self._build_risk_factors(returns_matrix)
        
        # ä¼°è®¡å› å­è½½è·
        factor_loadings = self._estimate_factor_loadings(returns_matrix, risk_factors)
        
        # ä¼°è®¡å› å­åæ–¹å·®
        factor_covariance = self._estimate_factor_covariance(risk_factors)
        
        # ä¼°è®¡ç‰¹å¼‚é£é™©
        specific_risk = self._estimate_specific_risk(returns_matrix, factor_loadings, risk_factors)
        
        self.risk_model_results = {
            'factor_loadings': factor_loadings,
            'factor_covariance': factor_covariance,
            'specific_risk': specific_risk,
            'risk_factors': risk_factors
        }
        
        logger.info("é£é™©æ¨¡å‹æ„å»ºå®Œæˆ")
        return self.risk_model_results
    
    def _build_risk_factors(self, returns_matrix: pd.DataFrame) -> pd.DataFrame:
        """æ„å»ºé£é™©å› å­ï¼ˆæ¥è‡ªProfessionalå¼•æ“ï¼‰"""
        factors = pd.DataFrame(index=returns_matrix.index)
        
        # 1. å¸‚åœºå› å­
        factors['market'] = returns_matrix.mean(axis=1)
        
        # 2. è§„æ¨¡å› å­ (ä½¿ç”¨UMDMçœŸå®å¸‚å€¼æ•°æ®)
        try:
            if self.market_data_manager is not None:
                # æ„å»ºç»Ÿä¸€ç‰¹å¾DataFrameï¼Œè·å–çœŸå®å¸‚å€¼æ•°æ®
                tickers = returns_matrix.columns.tolist()
                dates = returns_matrix.index.tolist()
                
                # åˆ›å»ºç”¨äºUMDMçš„è¾“å…¥DataFrame
                input_data = []
                for date in dates:
                    for ticker in tickers:
                        input_data.append({'date': date, 'ticker': ticker})
                
                if input_data:
                    input_df = pd.DataFrame(input_data)
                    features_df = self.market_data_manager.create_unified_features_dataframe(input_df)
                    
                    if 'free_float_market_cap' in features_df.columns:
                        # é‡å¡‘ä¸º[date, ticker]æ ¼å¼å¹¶å¯¹é½
                        features_pivot = features_df.set_index(['date', 'ticker'])['free_float_market_cap']
                        
                        #  ä¿®å¤æ—¶é—´æ³„éœ²ï¼šSizeå› å­ä½¿ç”¨å‰æœŸå¸‚å€¼åˆ†ç»„å½“æœŸæ”¶ç›Š
                        size_factor = []
                        dates_list = list(returns_matrix.index)
                        
                        for i, date in enumerate(dates_list):
                            try:
                                #  å…³é”®ä¿®å¤ï¼šä½¿ç”¨T-1æœŸçš„å¸‚å€¼è¿›è¡Œåˆ†ç»„ï¼Œè®¡ç®—TæœŸæ”¶ç›Š
                                if i == 0:
                                    # ç¬¬ä¸€ä¸ªæ—¥æœŸæ²¡æœ‰å‰æœŸæ•°æ®ï¼Œè·³è¿‡
                                    size_factor.append(0.0)
                                    continue
                                
                                prev_date = dates_list[i-1]
                                prev_date_caps = features_pivot.loc[prev_date]  # ä½¿ç”¨å‰ä¸€æœŸå¸‚å€¼
                                prev_date_caps = prev_date_caps.reindex(returns_matrix.columns)
                                
                                if prev_date_caps.notna().sum() > 2:  # è‡³å°‘éœ€è¦3åªè‚¡ç¥¨æœ‰å¸‚å€¼æ•°æ®
                                    cap_median = prev_date_caps.median()
                                    small_cap_mask = prev_date_caps < cap_median
                                    large_cap_mask = ~small_cap_mask
                                    
                                    # ä½¿ç”¨å½“æœŸæ”¶ç›Šç‡ï¼Œä½†åˆ†ç»„åŸºäºå‰æœŸå¸‚å€¼
                                    date_returns = returns_matrix.loc[date]
                                    small_ret = date_returns[small_cap_mask].mean()
                                    large_ret = date_returns[large_cap_mask].mean()
                                    
                                    size_factor.append(small_ret - large_ret)
                                    
                                    logger.debug(f"æ—¥æœŸ{date}: ä½¿ç”¨{prev_date}å¸‚å€¼åˆ†ç»„ï¼Œ"
                                               f"å°ç›˜è‚¡æ”¶ç›Š{small_ret:.4f}, å¤§ç›˜è‚¡æ”¶ç›Š{large_ret:.4f}")
                                else:
                                    size_factor.append(0.0)
                            except (KeyError, IndexError):
                                size_factor.append(0.0)
                        
                        factors['size'] = pd.Series(size_factor, index=returns_matrix.index)
                        logger.info("ä½¿ç”¨UMDMçœŸå®å¸‚å€¼æ•°æ®æ„å»ºSizeå› å­")
                    else:
                        logger.warning("UMDMä¸­ç¼ºå°‘free_float_market_capå­—æ®µï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                        raise ValueError("No market cap data available")
                else:
                    raise ValueError("No input data for UMDM")
            else:
                raise ValueError("UMDM not available")
                
        except (ValueError, KeyError, IndexError) as e:
            logger.exception(f"UMDM Sizeå› å­æ„å»ºå¤±è´¥: {e}, ä½¿ç”¨ç®€åŒ–å›é€€æ–¹æ¡ˆ")
            self.health_metrics['risk_model_failures'] += 1
            # å›é€€æ–¹æ¡ˆï¼šåŸºäºæˆäº¤é‡ä¼°ç®—è§„æ¨¡
            try:
                volume_data = {}
                for ticker in returns_matrix.columns:
                    if ticker in self.raw_data and 'volume' in self.raw_data[ticker].columns:
                        # ä½¿ç”¨æœ€è¿‘60å¤©å¹³å‡æˆäº¤é‡ä½œä¸ºè§„æ¨¡ä»£ç†
                        recent_volume = self.raw_data[ticker]['volume'].tail(60).mean()
                        volume_data[ticker] = recent_volume

                if volume_data:
                    volume_series = pd.Series(volume_data)
                    volume_median = volume_series.median()
                    small_vol_mask = volume_series < volume_median

                    small_vol_returns = returns_matrix.loc[:, small_vol_mask].mean(axis=1)
                    large_vol_returns = returns_matrix.loc[:, ~small_vol_mask].mean(axis=1)
                    factors['size'] = small_vol_returns - large_vol_returns
                    logger.info("ä½¿ç”¨æˆäº¤é‡ä»£ç†æ„å»ºSizeå› å­ï¼ˆå›é€€æ–¹æ¡ˆï¼‰")
                else:
                    # æœ€ç»ˆå›é€€ï¼šä½¿ç”¨é›¶å€¼
                    factors['size'] = 0.0
                    logger.warning("æ— æ³•æ„å»ºSizeå› å­ï¼Œä½¿ç”¨é›¶å€¼")
            except Exception as fallback_error:
                logger.error(f"Sizeå› å­å›é€€æ–¹æ¡ˆä¹Ÿå¤±è´¥: {fallback_error}")
                factors['size'] = 0.0
        
        # 3. åŠ¨é‡å› å­
        momentum_scores = {}
        for ticker in returns_matrix.columns:
            momentum_scores[ticker] = returns_matrix[ticker].rolling(252).sum().shift(21)
        
        momentum_df = pd.DataFrame(momentum_scores)
        high_momentum = momentum_df.rank(axis=1, pct=True) > 0.7
        low_momentum = momentum_df.rank(axis=1, pct=True) < 0.3
        
        factors['momentum'] = returns_matrix.where(high_momentum).mean(axis=1) - \
                             returns_matrix.where(low_momentum).mean(axis=1)
        
        # 4. æ³¢åŠ¨ç‡å› å­
        volatility_scores = returns_matrix.rolling(60).std()
        low_vol = volatility_scores.rank(axis=1, pct=True) < 0.3
        high_vol = volatility_scores.rank(axis=1, pct=True) > 0.7
        
        factors['volatility'] = returns_matrix.where(low_vol).mean(axis=1) - \
                               returns_matrix.where(high_vol).mean(axis=1)
        
        # 5. è´¨é‡å› å­
        quality_scores = returns_matrix.rolling(60).mean() / returns_matrix.rolling(60).std()
        high_quality = quality_scores.rank(axis=1, pct=True) > 0.7
        low_quality = quality_scores.rank(axis=1, pct=True) < 0.3
        
        factors['quality'] = returns_matrix.where(high_quality).mean(axis=1) - \
                            returns_matrix.where(low_quality).mean(axis=1)
        
        # 6. åè½¬å› å­
        reversal_scores = returns_matrix.rolling(21).sum()
        high_reversal = reversal_scores.rank(axis=1, pct=True) < 0.3
        low_reversal = reversal_scores.rank(axis=1, pct=True) > 0.7
        
        factors['reversal'] = returns_matrix.where(high_reversal).mean(axis=1) - \
                             returns_matrix.where(low_reversal).mean(axis=1)
        
        # æ ‡å‡†åŒ–å› å­
        factors = factors.fillna(0)
        for col in factors.columns:
            factors[col] = (factors[col] - factors[col].mean()) / (factors[col].std() + 1e-8)
        
        return factors
    
    def _estimate_factor_loadings(self, returns_matrix: pd.DataFrame, 
                                 risk_factors: pd.DataFrame) -> pd.DataFrame:
        """ä¼°è®¡å› å­è½½è·"""
        loadings = {}
        
        for ticker in returns_matrix.columns:
            stock_returns = returns_matrix[ticker].dropna()
            aligned_factors = risk_factors.loc[stock_returns.index].dropna().fillna(0)
            
            if len(stock_returns) < 50 or len(aligned_factors) < 50:
                loadings[ticker] = np.zeros(len(risk_factors.columns))
                continue
            
            try:
                # ç¡®ä¿æ•°æ®é•¿åº¦åŒ¹é…
                min_len = min(len(stock_returns), len(aligned_factors))
                stock_returns = stock_returns.iloc[:min_len]
                aligned_factors = aligned_factors.iloc[:min_len]
                
                # ä½¿ç”¨ç¨³å¥å›å½’ä¼°è®¡è½½è·
                model = HuberRegressor(epsilon=1.35, alpha=0.0001)
                model.fit(aligned_factors.values, stock_returns.values)
                
                loadings[ticker] = model.coef_
                
            except Exception as e:
                logger.warning(f"Failed to estimate loadings for {ticker}: {e}")
                loadings[ticker] = np.zeros(len(risk_factors.columns))
        
        loadings_df = pd.DataFrame(loadings, index=risk_factors.columns).T
        return loadings_df
    
    def _estimate_factor_covariance(self, risk_factors: pd.DataFrame) -> pd.DataFrame:
        """ä¼°è®¡å› å­åæ–¹å·®çŸ©é˜µ"""
        # ä½¿ç”¨Ledoit-Wolfæ”¶ç¼©ä¼°è®¡
        cov_estimator = LedoitWolf()
        factor_cov_matrix = cov_estimator.fit(risk_factors.fillna(0)).covariance_
        
        # ç¡®ä¿æ­£å®šæ€§
        eigenvals, eigenvecs = np.linalg.eigh(factor_cov_matrix)
        eigenvals = np.maximum(eigenvals, 1e-8)
        factor_cov_matrix = eigenvecs @ np.diag(eigenvals) @ eigenvecs.T
        
        return pd.DataFrame(factor_cov_matrix, 
                           index=risk_factors.columns, 
                           columns=risk_factors.columns)
    
    def _estimate_specific_risk(self, returns_matrix: pd.DataFrame,
                               factor_loadings: pd.DataFrame, 
                               risk_factors: pd.DataFrame) -> pd.Series:
        """ä¼°è®¡ç‰¹å¼‚é£é™©"""
        specific_risks = {}
        
        for ticker in returns_matrix.columns:
            if ticker not in factor_loadings.index:
                specific_risks[ticker] = 0.2  # é»˜è®¤ç‰¹å¼‚é£é™©
                continue
            
            stock_returns = returns_matrix[ticker].dropna()
            loadings = factor_loadings.loc[ticker]
            aligned_factors = risk_factors.loc[stock_returns.index].fillna(0)
            
            if len(stock_returns) < 50:
                specific_risks[ticker] = 0.2
                continue
            
            # è®¡ç®—æ®‹å·®
            min_len = min(len(stock_returns), len(aligned_factors))
            factor_returns = (aligned_factors.iloc[:min_len] @ loadings).values
            residuals = stock_returns.iloc[:min_len].values - factor_returns
            
            # ç‰¹å¼‚é£é™©ä¸ºæ®‹å·®æ ‡å‡†å·®
            specific_var = np.nan_to_num(np.var(residuals), nan=0.04)
            specific_risks[ticker] = np.sqrt(specific_var)
        
        return pd.Series(specific_risks)
    
    def detect_market_regime(self) -> MarketRegime:
        """æ£€æµ‹å¸‚åœºçŠ¶æ€ï¼ˆæ¥è‡ªProfessionalå¼•æ“ï¼‰"""
        logger.info("æ£€æµ‹å¸‚åœºçŠ¶æ€")
        
        if not self.raw_data:
            return MarketRegime(0, "Unknown", 0.5, {'volatility': 0.2, 'trend': 0.0})
        
        # æ„å»ºå¸‚åœºæŒ‡æ•°
        market_returns = []
        for ticker, data in self.raw_data.items():
            if len(data) > 100:
                returns = data['close'].pct_change().fillna(0)
                market_returns.append(returns)
        
        if not market_returns:
            return MarketRegime(0, "Unknown", 0.5, {'volatility': 0.2, 'trend': 0.0})
        
        market_index = pd.concat(market_returns, axis=1).mean(axis=1).dropna()
        
        if len(market_index) < 100:
            return MarketRegime(1, "Normal", 1.0, {'volatility': 0.15, 'trend': 0.0})
        
        # åŸºäºæ³¢åŠ¨ç‡å’Œè¶‹åŠ¿çš„çŠ¶æ€æ£€æµ‹
        rolling_vol = market_index.rolling(21).std()
        rolling_trend = market_index.rolling(21).mean()
        
        # å®šä¹‰çŠ¶æ€é˜ˆå€¼
        vol_low = rolling_vol.quantile(0.33)
        vol_high = rolling_vol.quantile(0.67)
        trend_low = rolling_trend.quantile(0.33)
        trend_high = rolling_trend.quantile(0.67)
        
        # å½“å‰çŠ¶æ€
        current_vol = rolling_vol.iloc[-1]
        current_trend = rolling_trend.iloc[-1]
        
        if current_vol < vol_low:
            if current_trend > trend_high:
                regime = MarketRegime(1, "Bull_Low_Vol", 0.8, 
                                    {'volatility': current_vol, 'trend': current_trend})
            elif current_trend < trend_low:
                regime = MarketRegime(2, "Bear_Low_Vol", 0.8,
                                    {'volatility': current_vol, 'trend': current_trend})
            else:
                regime = MarketRegime(3, "Normal_Low_Vol", 0.8,
                                    {'volatility': current_vol, 'trend': current_trend})
        elif current_vol > vol_high:
            if current_trend > trend_high:
                regime = MarketRegime(4, "Bull_High_Vol", 0.8,
                                    {'volatility': current_vol, 'trend': current_trend})
            elif current_trend < trend_low:
                regime = MarketRegime(5, "Bear_High_Vol", 0.8,
                                    {'volatility': current_vol, 'trend': current_trend})
            else:
                regime = MarketRegime(6, "Volatile", 0.8,
                                    {'volatility': current_vol, 'trend': current_trend})
        else:
            regime = MarketRegime(0, "Normal", 0.7,
                                {'volatility': current_vol, 'trend': current_trend})
        
        self.current_regime = regime
        logger.info(f"æ£€æµ‹åˆ°å¸‚åœºçŠ¶æ€: {regime.name} (æ¦‚ç‡: {regime.probability:.2f})")
        
        return regime
    
    def _get_regime_alpha_weights(self, regime: MarketRegime) -> Dict[str, float]:
        """æ ¹æ®å¸‚åœºçŠ¶æ€è°ƒæ•´Alphaæƒé‡ï¼ˆæ¥è‡ªProfessionalå¼•æ“ï¼‰"""
        if "Bull" in regime.name:
            # ç‰›å¸‚ï¼šåå¥½åŠ¨é‡
            return {
                'momentum_21d': 2.0, 'momentum_63d': 2.5, 'momentum_126d': 2.0,
                'reversion_5d': 0.5, 'reversion_10d': 0.5, 'reversion_21d': 0.5,
                'volatility_factor': 1.0, 'volume_trend': 1.5, 'quality_factor': 1.0
            }
        elif "Bear" in regime.name:
            # ç†Šå¸‚ï¼šåå¥½è´¨é‡å’Œé˜²å¾¡
            return {
                'momentum_21d': 0.5, 'momentum_63d': 0.5, 'momentum_126d': 1.0,
                'reversion_5d': 1.5, 'reversion_10d': 2.0, 'reversion_21d': 1.5,
                'volatility_factor': 2.0, 'volume_trend': 0.5, 'quality_factor': 2.0
            }
        elif "Volatile" in regime.name:
            # é«˜æ³¢åŠ¨ï¼šåå¥½å‡å€¼å›å½’
            return {
                'momentum_21d': 0.5, 'momentum_63d': 1.0, 'momentum_126d': 1.0,
                'reversion_5d': 2.5, 'reversion_10d': 2.0, 'reversion_21d': 1.5,
                'volatility_factor': 2.5, 'volume_trend': 1.0, 'quality_factor': 1.5
            }
        else:
            # æ­£å¸¸å¸‚åœºï¼šå‡è¡¡æƒé‡
            return {col: 1.0 for col in [
                'momentum_21d', 'momentum_63d', 'momentum_126d',
                'reversion_5d', 'reversion_10d', 'reversion_21d',
                'volatility_factor', 'volume_trend', 'quality_factor'
            ]}
    
    def generate_enhanced_predictions(self, training_results: Dict[str, Any], 
                                    market_regime: MarketRegime) -> pd.Series:
        """ç”ŸæˆRegime-Awareçš„å¢å¼ºé¢„æµ‹"""
        try:
            # è·å–åŸºç¡€é¢„æµ‹
            base_predictions = self.generate_ensemble_predictions(training_results)
            
            if not ENHANCED_MODULES_AVAILABLE or not self.alpha_engine:
                # å¦‚æœæ²¡æœ‰å¢å¼ºæ¨¡å—ï¼Œåº”ç”¨regimeæƒé‡åˆ°åŸºç¡€é¢„æµ‹
                regime_weights = self._get_regime_alpha_weights(market_regime)
                # ç®€å•åº”ç”¨æƒé‡ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                adjustment_factor = sum(regime_weights.values()) / len(regime_weights)
                enhanced_predictions = base_predictions * adjustment_factor
                logger.info(f"åº”ç”¨ç®€åŒ–çš„regimeè°ƒæ•´ï¼Œè°ƒæ•´å› å­: {adjustment_factor:.3f}")
                return enhanced_predictions
            
            # å¦‚æœæœ‰Alphaå¼•æ“ï¼Œç”ŸæˆAlphaä¿¡å·
            try:
                # ä¸ºAlphaå¼•æ“å‡†å¤‡æ•°æ®ï¼ˆåŒ…å«æ ‡å‡†åŒ–çš„ä»·æ ¼åˆ—ï¼‰
                alpha_input = self._prepare_alpha_data()
                # è®¡ç®—Alphaå› å­ï¼ˆç­¾ååªæ¥å—dfï¼‰
                alpha_signals = self.alpha_engine.compute_all_alphas(alpha_input)
                
                # æ ¹æ®å¸‚åœºçŠ¶æ€è°ƒæ•´Alphaæƒé‡
                regime_weights = self._get_regime_alpha_weights(market_regime)
                
                # åº”ç”¨regimeæƒé‡åˆ°alphaä¿¡å·
                weighted_alpha = pd.Series(0.0, index=alpha_signals.index)
                for alpha_name, weight in regime_weights.items():
                    if alpha_name in alpha_signals.columns:
                        weighted_alpha += alpha_signals[alpha_name] * weight
                
                # æ ‡å‡†åŒ–åŠ æƒåçš„alpha
                if weighted_alpha.std() > 0:
                    weighted_alpha = (weighted_alpha - weighted_alpha.mean()) / weighted_alpha.std()
                
                # ä¸åŸºç¡€MLé¢„æµ‹èåˆ
                alpha_weight = 0.3  # Alphaä¿¡å·æƒé‡
                ml_weight = 0.7     # MLé¢„æµ‹æƒé‡
                
                # ç¡®ä¿ç´¢å¼•å¯¹é½
                common_index = base_predictions.index.intersection(weighted_alpha.index)
                if len(common_index) > 0:
                    enhanced_predictions = (
                        ml_weight * base_predictions.reindex(common_index).fillna(0) +
                        alpha_weight * weighted_alpha.reindex(common_index).fillna(0)
                    )
                else:
                    enhanced_predictions = base_predictions
                
                logger.info(f"æˆåŠŸèåˆAlphaä¿¡å·å’ŒMLé¢„æµ‹ï¼Œmarket regime: {market_regime.name}")
                return enhanced_predictions
                
            except (ValueError, KeyError, AttributeError) as e:
                logger.exception(f"Alphaä¿¡å·ç”Ÿæˆå¤±è´¥: {e}")
                self.health_metrics['alpha_computation_failures'] += 1
                # å›é€€åˆ°åŸºç¡€é¢„æµ‹
                return base_predictions
                
        except Exception as e:
            logger.exception(f"å¢å¼ºé¢„æµ‹ç”Ÿæˆå¤±è´¥: {e}")
            self.health_metrics['prediction_failures'] += 1
            self.health_metrics['total_exceptions'] += 1
            # æœ€ç»ˆå›é€€
            return pd.Series(0.0, index=range(10))
    
    def optimize_portfolio_with_risk_model(self, predictions: pd.Series, 
                                          feature_data: pd.DataFrame) -> Dict[str, Any]:
        """ä½¿ç”¨é£é™©æ¨¡å‹çš„æŠ•èµ„ç»„åˆä¼˜åŒ–"""
        try:
            # å¦‚æœæœ‰Professionalçš„é£é™©æ¨¡å‹ç»“æœï¼Œä½¿ç”¨å®ƒä»¬
            if self.risk_model_results and 'factor_loadings' in self.risk_model_results:
                factor_loadings = self.risk_model_results['factor_loadings']
                factor_covariance = self.risk_model_results['factor_covariance']
                specific_risk = self.risk_model_results['specific_risk']
                
                # æ„å»ºåæ–¹å·®çŸ©é˜µ
                common_assets = list(set(predictions.index) & set(factor_loadings.index))
                if len(common_assets) >= 3:
                    # ä½¿ç”¨ä¸“ä¸šé£é™©æ¨¡å‹è¿›è¡Œä¼˜åŒ–
                    try:
                        # æ„å»ºæŠ•èµ„ç»„åˆåæ–¹å·®çŸ©é˜µ: B * F * B' + S
                        B = factor_loadings.reindex(common_assets).dropna()  # å› å­è½½è· - å®‰å…¨ç´¢å¼•
                        F = factor_covariance                   # å› å­åæ–¹å·®
                        S = specific_risk.reindex(common_assets).dropna()    # ç‰¹å¼‚é£é™© - å®‰å…¨ç´¢å¼•
                        
                        # è®¡ç®—åæ–¹å·®çŸ©é˜µ
                        portfolio_cov = B @ F @ B.T + np.diag(S**2)
                        portfolio_cov = pd.DataFrame(
                            portfolio_cov, 
                            index=common_assets, 
                            columns=common_assets
                        )
                        
                        # ä½¿ç”¨ç»Ÿä¸€çš„AdvancedPortfolioOptimizerè€Œéé‡å¤å®ç°
                        if self.portfolio_optimizer:
                            try:
                                # å‡†å¤‡é¢„æœŸæ”¶ç›Šç‡ - ä½¿ç”¨å®‰å…¨çš„ç´¢å¼•è®¿é—®
                                available_assets = predictions.index.intersection(common_assets)
                                if len(available_assets) == 0:
                                    raise ValueError("No common assets between predictions and risk model")
                                expected_returns = predictions.reindex(available_assets).dropna()
                                common_assets = list(expected_returns.index)  # æ›´æ–°common_assetsä¸ºå®é™…å¯ç”¨çš„èµ„äº§
                                
                                # é‡æ–°æ„å»ºåæ–¹å·®çŸ©é˜µä»¥åŒ¹é…å¯ç”¨èµ„äº§
                                B_updated = factor_loadings.reindex(common_assets).dropna()
                                S_updated = specific_risk.reindex(common_assets).dropna()
                                portfolio_cov = B_updated @ F @ B_updated.T + np.diag(S_updated**2)
                                portfolio_cov = pd.DataFrame(
                                    portfolio_cov, 
                                    index=common_assets, 
                                    columns=common_assets
                                )
                                
                                # å‡†å¤‡è‚¡ç¥¨æ± æ•°æ®ï¼ˆç”¨äºçº¦æŸï¼‰
                                universe_data = pd.DataFrame(index=common_assets)
                                # æ·»åŠ æ¨¡æ‹Ÿçš„è¡Œä¸š/å›½å®¶ä¿¡æ¯ç”¨äºçº¦æŸ
                                universe_data['COUNTRY'] = 'US'  # ç®€åŒ–
                                universe_data['SECTOR'] = 'TECH'  # ç®€åŒ– 
                                universe_data['liquidity_rank'] = 0.5  # ä¸­ç­‰æµåŠ¨æ€§
                                
                                # è°ƒç”¨ç»Ÿä¸€çš„ä¼˜åŒ–å™¨
                                optimization_result = self.portfolio_optimizer.optimize_portfolio(
                                    expected_returns=expected_returns,
                                    covariance_matrix=portfolio_cov,
                                    current_weights=None,  # å‡è®¾ä»ç©ºä»“å¼€å§‹
                                    universe_data=universe_data
                                )
                                
                                if optimization_result.get('success', False):
                                    optimal_weights = optimization_result['optimal_weights']
                                    portfolio_metrics = optimization_result['portfolio_metrics']

                                    # é£é™©å½’å› 
                                    risk_attribution = self.portfolio_optimizer.risk_attribution(
                                        optimal_weights, portfolio_cov
                                    )
                                    
                                    return {
                                        'success': True,
                                        'method': 'unified_portfolio_optimizer_with_risk_model',
                                        'weights': optimal_weights.to_dict(),
                                        'portfolio_metrics': portfolio_metrics,
                                        'risk_attribution': risk_attribution,
                                        'regime_context': self.current_regime.name if self.current_regime else "Unknown"
                                    }
                                else:
                                    logger.warning("ç»Ÿä¸€ä¼˜åŒ–å™¨ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                                    raise ValueError("Unified optimizer failed")
                            
                            except (ValueError, RuntimeError, np.linalg.LinAlgError) as optimizer_error:
                                logger.exception(f"ç»Ÿä¸€ä¼˜åŒ–å™¨è°ƒç”¨å¤±è´¥: {optimizer_error}, ä½¿ç”¨ç®€åŒ–ä¼˜åŒ–")
                                self.health_metrics['optimization_fallbacks'] += 1
                                # ç®€åŒ–å›é€€ï¼šç­‰æƒç»„åˆ - ä½¿ç”¨å®‰å…¨çš„ç´¢å¼•è®¿é—®
                                fallback_assets = predictions.index.intersection(common_assets)
                                if len(fallback_assets) == 0:
                                    # å¦‚æœæ²¡æœ‰äº¤é›†ï¼Œä½¿ç”¨predictionsçš„å‰å‡ ä¸ªèµ„äº§
                                    fallback_assets = predictions.index[:min(5, len(predictions.index))]
                                
                                n_assets = len(fallback_assets)
                                equal_weights = pd.Series(1.0/n_assets, index=fallback_assets)
                                
                                expected_returns = predictions.reindex(fallback_assets).dropna()
                                portfolio_return = expected_returns @ equal_weights.reindex(expected_returns.index)
                                
                                # åˆ›å»ºç®€åŒ–çš„åæ–¹å·®çŸ©é˜µç”¨äºé£é™©è®¡ç®—
                                try:
                                    portfolio_risk = np.sqrt(equal_weights.reindex(expected_returns.index) @ portfolio_cov.reindex(expected_returns.index, expected_returns.index).fillna(0.01) @ equal_weights.reindex(expected_returns.index))
                                except (KeyError, ValueError):
                                    # å¦‚æœåæ–¹å·®çŸ©é˜µè®¿é—®å¤±è´¥ï¼Œä½¿ç”¨ä¼°è®¡é£é™©
                                    portfolio_risk = 0.15  # å‡è®¾15%çš„å¹´åŒ–é£é™©
                                sharpe_ratio = portfolio_return / portfolio_risk if portfolio_risk > 0 else 0
                            
                            return {
                                'success': True,
                                    'method': 'equal_weight_fallback_with_risk_model',
                                    'weights': equal_weights.reindex(expected_returns.index).to_dict(),
                                'portfolio_metrics': {
                                    'expected_return': float(portfolio_return),
                                    'portfolio_risk': float(portfolio_risk),
                                    'sharpe_ratio': float(sharpe_ratio),
                                        'diversification_ratio': n_assets
                                },
                                    'risk_attribution': {},
                                'regime_context': self.current_regime.name if self.current_regime else "Unknown"
                            }
                        else:
                            logger.error("AdvancedPortfolioOptimizer ä¸å¯ç”¨")
                            raise ValueError("Portfolio optimizer not available")
                        
                    except Exception as e:
                        logger.warning(f"ä¸“ä¸šé£é™©æ¨¡å‹ä¼˜åŒ–å¤±è´¥: {e}")
            
            # å›é€€åˆ°åŸºç¡€ä¼˜åŒ–
            return self.optimize_portfolio(predictions, feature_data)
            
        except Exception as e:
            logger.error(f"é£é™©æ¨¡å‹ä¼˜åŒ–å¤±è´¥: {e}")
            # æœ€ç»ˆå›é€€åˆ°ç­‰æƒç»„åˆ
            top_assets = predictions.nlargest(min(10, len(predictions))).index
            equal_weights = pd.Series(1.0/len(top_assets), index=top_assets)
            
            return {
                'success': True,
                'method': 'equal_weight_fallback',
                'weights': equal_weights.to_dict(),
                'portfolio_metrics': {
                    'expected_return': predictions.reindex(top_assets).dropna().mean(),
                    'portfolio_risk': 0.15,  # å‡è®¾é£é™©
                    'sharpe_ratio': 1.0,
                    'diversification_ratio': len(top_assets)
                },
                'risk_attribution': {},
                'regime_context': self.current_regime.name if self.current_regime else "Unknown"
            }
    
    def _prepare_alpha_data(self) -> pd.DataFrame:
        """ä¸ºAlphaå¼•æ“å‡†å¤‡æ•°æ®"""
        if not self.raw_data:
            return pd.DataFrame()
        
        # å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºAlphaå¼•æ“éœ€è¦çš„æ ¼å¼
        all_data = []
        for ticker, data in self.raw_data.items():
            ticker_data = data.copy()
            ticker_data['ticker'] = ticker
            ticker_data['date'] = ticker_data.index
            # æ ‡å‡†åŒ–ä»·æ ¼åˆ—ï¼ŒAlphaå¼•æ“éœ€è¦ 'Close','High','Low'
            if 'Adj Close' in ticker_data.columns:
                ticker_data['Close'] = ticker_data['Adj Close']
            elif 'close' in ticker_data.columns:
                ticker_data['Close'] = ticker_data['close']
            elif 'Close' not in ticker_data.columns and 'close' not in ticker_data.columns:
                # è‹¥ç¼ºå°‘closeä¿¡æ¯ï¼Œè·³è¿‡è¯¥ç¥¨
                continue
            if 'High' not in ticker_data.columns and 'high' in ticker_data.columns:
                ticker_data['High'] = ticker_data['high']
            if 'Low' not in ticker_data.columns and 'low' in ticker_data.columns:
                ticker_data['Low'] = ticker_data['low']
            # æ·»åŠ æ¨¡æ‹Ÿçš„åŸºæœ¬ä¿¡æ¯
            ticker_data['COUNTRY'] = 'US'
            ticker_data['SECTOR'] = 'Technology'  # ç®€åŒ–å¤„ç†
            ticker_data['SUBINDUSTRY'] = 'Software'
            all_data.append(ticker_data)
        
        if all_data:
            combined_data = pd.concat(all_data, ignore_index=True)
            return combined_data
        else:
            return pd.DataFrame()
        
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except FileNotFoundError:
            logger.warning(f"é…ç½®æ–‡ä»¶{self.config_path}æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'universe': 'TOPDIV3000',
            'neutralization': ['COUNTRY'],
            'hump_levels': [0.003, 0.008],
            'winsorize_std': 2.5,
            'truncation': 0.10,
            'max_position': 0.03,
            'max_turnover': 0.10,
            'temperature': 1.2,
            'model_config': {
                'learning_to_rank': True,
                'ranking_objective': 'rank:pairwise',
                'uncertainty_aware': True,
                'quantile_regression': True
            },
            'risk_config': {
                'risk_aversion': 5.0,
                'turnover_penalty': 1.0,
                'max_sector_exposure': 0.15,
                'max_country_exposure': 0.20
            }
        }
    
    def download_stock_data(self, tickers: List[str], 
                           start_date: str, end_date: str) -> Dict[str, pd.DataFrame]:
        """
        ä¸‹è½½è‚¡ç¥¨æ•°æ®
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            è‚¡ç¥¨æ•°æ®å­—å…¸
        """
        logger.info(f"ä¸‹è½½{len(tickers)}åªè‚¡ç¥¨çš„æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {start_date} - {end_date}")

        # å°†è®­ç»ƒç»“æŸæ—¶é—´é™åˆ¶ä¸ºå½“å¤©çš„å‰ä¸€å¤©ï¼ˆT-1ï¼‰ï¼Œé¿å…ä½¿ç”¨æœªå®Œå…¨ç»“ç®—çš„æ•°æ®
        try:
            yesterday = (datetime.now() - timedelta(days=1)).date()
            end_dt = pd.to_datetime(end_date).date()
            if end_dt > yesterday:
                adjusted_end = yesterday.strftime('%Y-%m-%d')
                logger.info(f"ç»“æŸæ—¥æœŸ{end_date} è¶…è¿‡æ˜¨æ—¥ï¼Œå·²è°ƒæ•´ä¸º {adjusted_end}")
                end_date = adjusted_end
        except Exception as _e:
            logger.debug(f"ç»“æŸæ—¥æœŸè°ƒæ•´è·³è¿‡: {_e}")
        
        # æ•°æ®éªŒè¯
        if not tickers or len(tickers) == 0:
            logger.error("è‚¡ç¥¨ä»£ç åˆ—è¡¨ä¸ºç©º")
            return {}
        
        if not start_date or not end_date:
            logger.error("å¼€å§‹æ—¥æœŸæˆ–ç»“æŸæ—¥æœŸä¸ºç©º")
            return {}
        
        all_data = {}
        failed_downloads = []
        
        for ticker in tickers:
            try:
                # éªŒè¯è‚¡ç¥¨ä»£ç æ ¼å¼
                if not ticker or not isinstance(ticker, str) or len(ticker.strip()) == 0:
                    logger.warning(f"æ— æ•ˆçš„è‚¡ç¥¨ä»£ç : {ticker}")
                    failed_downloads.append(ticker)
                    continue
                
                ticker = ticker.strip().upper()  # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
                
                stock = PolygonTicker(ticker)
                # ä½¿ç”¨å¤æƒæ•°æ®ï¼Œé¿å…è‚¡åˆ©æ±¡æŸ“ï¼›å›ºå®šæ—¥é¢‘ï¼Œå…³é—­actionsåˆ—
                hist = stock.history(start=start_date, end=end_date, interval='1d')
                
                # æ•°æ®è´¨é‡æ£€æŸ¥
                if hist is None or len(hist) == 0:
                    logger.warning(f"{ticker}: æ— æ•°æ®")
                    failed_downloads.append(ticker)
                    continue
                
                # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
                required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
                missing_cols = [col for col in required_cols if col not in hist.columns]
                if missing_cols:
                    logger.warning(f"{ticker}: ç¼ºå°‘å¿…è¦åˆ— {missing_cols}")
                    failed_downloads.append(ticker)
                    continue
                
                # æ£€æŸ¥æ•°æ®è´¨é‡
                if hist['Close'].isna().all():
                    logger.warning(f"{ticker}: æ‰€æœ‰æ”¶ç›˜ä»·éƒ½æ˜¯NaN")
                    failed_downloads.append(ticker)
                    continue
                
                # æ ‡å‡†åŒ–åˆ—å
                hist = hist.rename(columns={
                    'Open': 'open', 'High': 'high', 'Low': 'low', 
                    'Close': 'close', 'Volume': 'volume'
                })
                
                # æ·»åŠ åŸºç¡€ç‰¹å¾
                hist['ticker'] = ticker
                hist['date'] = hist.index
                hist['amount'] = hist['close'] * hist['volume']  # æˆäº¤é¢
                
                # æ·»åŠ å…ƒæ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰
                hist['COUNTRY'] = self._get_country_for_ticker(ticker)
                hist['SECTOR'] = self._get_sector_for_ticker(ticker)
                hist['SUBINDUSTRY'] = self._get_subindustry_for_ticker(ticker)
                
                all_data[ticker] = hist
                
            except Exception as e:
                logger.warning(f"ä¸‹è½½{ticker}å¤±è´¥: {e}")
                failed_downloads.append(ticker)
        
        if failed_downloads:
            logger.warning(f"ä»¥ä¸‹è‚¡ç¥¨ä¸‹è½½å¤±è´¥: {failed_downloads}")
        
        logger.info(f"æˆåŠŸä¸‹è½½{len(all_data)}åªè‚¡ç¥¨çš„æ•°æ®")
        self.raw_data = all_data
        
        return all_data
    
    def _get_country_for_ticker(self, ticker: str) -> str:
        """è·å–è‚¡ç¥¨çš„å›½å®¶ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # è¿™é‡Œå¯ä»¥æ¥å…¥çœŸå®çš„è‚¡ç¥¨å…ƒæ•°æ®API
        if ticker in ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'TSLA', 'META']:
            return 'US'
        else:
            return np.random.choice(['US', 'EU', 'ASIA'])
    
    def _get_sector_for_ticker(self, ticker: str) -> str:
        """è·å–è‚¡ç¥¨çš„è¡Œä¸šï¼ˆç®€åŒ–å®ç°ï¼‰"""
        sector_mapping = {
            'AAPL': 'TECH', 'MSFT': 'TECH', 'GOOGL': 'TECH', 'NVDA': 'TECH',
            'AMZN': 'CONSUMER', 'TSLA': 'AUTO', 'META': 'TECH', 'NFLX': 'MEDIA'
        }
        return sector_mapping.get(ticker, np.random.choice(['TECH', 'FINANCE', 'ENERGY', 'HEALTH']))
    
    def _get_subindustry_for_ticker(self, ticker: str) -> str:
        """è·å–è‚¡ç¥¨çš„å­è¡Œä¸šï¼ˆç®€åŒ–å®ç°ï¼‰"""
        return np.random.choice(['SOFTWARE', 'HARDWARE', 'BIOTECH', 'RETAIL'])
    
    def create_traditional_features(self, data_dict: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """
        åˆ›å»ºä¼ ç»ŸæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        
        Args:
            data_dict: è‚¡ç¥¨æ•°æ®å­—å…¸
            
        Returns:
            ç‰¹å¾æ•°æ®æ¡†
        """
        logger.info("åˆ›å»ºä¼ ç»ŸæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾")
        
        all_features = []
        
        for ticker, df in data_dict.items():
            if len(df) < 60:  # è‡³å°‘éœ€è¦60å¤©æ•°æ®
                continue
            
            df_copy = df.copy().sort_values('date')
            
            # ä»·æ ¼ç‰¹å¾
            df_copy['returns'] = df_copy['close'].pct_change()
            df_copy['log_returns'] = np.log(df_copy['close'] / df_copy['close'].shift(1))
            
            # ç§»åŠ¨å¹³å‡
            for window in [5, 10, 20, 50]:
                df_copy[f'ma_{window}'] = df_copy['close'].rolling(window).mean()
                df_copy[f'ma_ratio_{window}'] = df_copy['close'] / df_copy[f'ma_{window}']
            
            # æ³¢åŠ¨ç‡
            for window in [10, 20, 50]:
                df_copy[f'vol_{window}'] = df_copy['log_returns'].rolling(window).std()
            
            # RSI
            def calculate_rsi(prices, window=14):
                delta = prices.diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
                rs = gain / loss
                return 100 - (100 / (1 + rs))
            
            df_copy['rsi_14'] = calculate_rsi(df_copy['close'])
            
            # æˆäº¤é‡ç‰¹å¾
            if 'volume' in df_copy.columns:
                df_copy['volume_ma_20'] = df_copy['volume'].rolling(20).mean()
                df_copy['volume_ratio'] = df_copy['volume'] / df_copy['volume_ma_20']
            
            # ä»·æ ¼ä½ç½®
            for window in [20, 50]:
                high_roll = df_copy['high'].rolling(window).max()
                low_roll = df_copy['low'].rolling(window).min()
                df_copy[f'price_position_{window}'] = (df_copy['close'] - low_roll) / (high_roll - low_roll + 1e-8)
            
            # åŠ¨é‡æŒ‡æ ‡
            for period in [5, 10, 20]:
                df_copy[f'momentum_{period}'] = df_copy['close'] / df_copy['close'].shift(period) - 1
            
            # ğŸ”´ ä¿®å¤ä¸¥é‡æ—¶é—´æ³„éœ²ï¼šå¢å¼ºçš„æ—¶é—´å¯¹é½å’ŒéªŒè¯
            FEATURE_LAG = 2        # ç‰¹å¾ä½¿ç”¨T-2åŠä¹‹å‰æ•°æ®
            SAFETY_GAP = 2         # é¢å¤–å®‰å…¨é—´éš”ï¼ˆé˜²æ­¢ä¿¡æ¯æ³„éœ²ï¼‰
            PRED_START = 1         # é¢„æµ‹ä»T+1å¼€å§‹  
            PRED_END = 5           # é¢„æµ‹åˆ°T+5ç»“æŸ
            prediction_horizon = PRED_END  # å‘åå…¼å®¹
            
            # éªŒè¯æ—¶é—´å¯¹é½æ­£ç¡®æ€§
            total_gap = FEATURE_LAG + SAFETY_GAP + PRED_START
            if total_gap <= 0:
                raise ValueError(f"æ—¶é—´å¯¹é½é”™è¯¯ï¼šæ€»é—´éš” {total_gap} <= 0ï¼Œå­˜åœ¨æ•°æ®æ³„éœ²é£é™©")
            
            logger.info(f"æ—¶é—´å¯¹é½é…ç½®: ç‰¹å¾lag={FEATURE_LAG}, å®‰å…¨gap={SAFETY_GAP}, é¢„æµ‹[T+{PRED_START}, T+{PRED_END}]")
            
            # å®‰å…¨çš„ç›®æ ‡æ„å»ºï¼šTæ—¶åˆ»ä½¿ç”¨T-2-2=T-4ç‰¹å¾ï¼Œé¢„æµ‹T+1åˆ°T+5æ”¶ç›Š
            # ç¡®ä¿ç‰¹å¾å’Œç›®æ ‡ä¹‹é—´æœ‰è¶³å¤Ÿçš„æ—¶é—´é—´éš”ï¼ˆè‡³å°‘6æœŸï¼‰
            df_copy['target'] = (
                df_copy['close'].shift(-PRED_END) / 
                df_copy['close'].shift(-PRED_START + 1) - 1
            )
            
            # æ—¶é—´éªŒè¯ï¼šç¡®ä¿æ²¡æœ‰é‡å 
            feature_max_time = -FEATURE_LAG - SAFETY_GAP  # ç‰¹å¾æœ€æ–°æ—¶é—´
            target_min_time = -PRED_START + 1             # ç›®æ ‡æœ€æ—©æ—¶é—´
            actual_gap = target_min_time - feature_max_time
            
            if actual_gap <= 0:
                raise ValueError(f"æ—¶é—´é‡å é”™è¯¯ï¼šç‰¹å¾æœ€æ–°æ—¶é—´{feature_max_time} >= ç›®æ ‡æœ€æ—©æ—¶é—´{target_min_time}")
            
            logger.info(f"âœ… æ—¶é—´å¯¹é½éªŒè¯é€šè¿‡ï¼šç‰¹å¾å’Œç›®æ ‡é—´éš” {actual_gap} æœŸ")
            
            # ğŸ”¥ å…³é”®ï¼šå¼ºåˆ¶ç‰¹å¾æ»åä»¥åŒ¹é…å¢å¼ºçš„æ—¶é—´çº¿
            # ç‰¹å¾ä½¿ç”¨T-4æ•°æ®ï¼Œç›®æ ‡ä½¿ç”¨T+1åˆ°T+5ï¼Œé—´éš”6æœŸï¼ˆå®‰å…¨ï¼‰
            feature_lag = FEATURE_LAG + SAFETY_GAP  # æ‰€æœ‰ç‰¹å¾é¢å¤–æ»å4æœŸ
            
            # åœ¨åç»­feature_colså¤„ç†ä¸­ä¼šç»Ÿä¸€åº”ç”¨æ»å
            
            # æ·»åŠ è¾…åŠ©ä¿¡æ¯
            df_copy['ticker'] = ticker
            df_copy['date'] = df_copy.index
            # æ¨¡æ‹Ÿè¡Œä¸šå’Œå›½å®¶ä¿¡æ¯ï¼ˆå®é™…åº”ä»æ•°æ®æºè·å–ï¼‰
            df_copy['COUNTRY'] = 'US'
            df_copy['SECTOR'] = ticker[:2] if len(ticker) >= 2 else 'TECH'  # ç®€åŒ–åˆ†ç±»
            df_copy['SUBINDUSTRY'] = ticker[:3] if len(ticker) >= 3 else 'SOFTWARE'
            
            all_features.append(df_copy)
        
        if all_features:
            combined_features = pd.concat(all_features, ignore_index=True)
            # é€‰å‡ºçº¯ç‰¹å¾åˆ—ï¼ˆæ’é™¤æ ‡è¯†/ç›®æ ‡/å…ƒæ•°æ®ï¼‰
            feature_cols = [col for col in combined_features.columns 
                            if col not in ['ticker','date','target','COUNTRY','SECTOR','SUBINDUSTRY']]
            # ğŸ”¥ å¼ºåŒ–ç‰¹å¾æ»åï¼šç¡®ä¿ä¸¥æ ¼çš„æ—¶é—´å¯¹é½
            try:
                # T-2åŸºç¡€æ»å + formation_lag(2) = æ€»å…±T-4æ»å
                # è¿™ç¡®ä¿ç‰¹å¾ä¿¡æ¯ä¸¥æ ¼æ—©äºç›®æ ‡æ—¶é—´çª—å£
                total_lag = 2 + 2  # base_lag + formation_lag
                combined_features[feature_cols] = combined_features.groupby('ticker')[feature_cols].shift(total_lag)
                logger.info(f"åº”ç”¨æ€»æ»åæœŸæ•°: {total_lag}ï¼Œç¡®ä¿ç‰¹å¾-ç›®æ ‡æ—¶é—´éš”ç¦»")
            except Exception as e:
                logger.warning(f"ç‰¹å¾æ»åå¤„ç†å¤±è´¥: {e}")
                # å›é€€åˆ°åŸºç¡€æ»å
                combined_features[feature_cols] = combined_features.groupby('ticker')[feature_cols].shift(2)
            # åŸºç¡€æ¸…æ´— - åªåˆ é™¤ç‰¹å¾å…¨ä¸ºNaNçš„è¡Œï¼Œä¿ç•™ç›®æ ‡å˜é‡
            # åˆ é™¤ç‰¹å¾å…¨ä¸ºNaNçš„è¡Œï¼Œä½†ä¿ç•™æœ‰æ•ˆç›®æ ‡çš„è¡Œ
            feature_na_mask = combined_features[feature_cols].isna().all(axis=1)
            combined_features = combined_features[~feature_na_mask]

            # ğŸ”— åˆå¹¶å®Œæ•´çš„Polygon 40+ä¸“ä¸šå› å­é›†ï¼ˆç»Ÿä¸€æ¥æº - T+5ä¼˜åŒ–ï¼‰
            try:
                from polygon_complete_factors import PolygonCompleteFactors
                from polygon_factors import PolygonShortTermFactors
                
                complete_factors = PolygonCompleteFactors()
                short_term_factors = PolygonShortTermFactors()
                symbols = sorted(combined_features['ticker'].unique().tolist())
                
                logger.info(f"å¼€å§‹é›†æˆPolygonå®Œæ•´å› å­åº“ï¼Œè‚¡ç¥¨æ•°é‡: {len(symbols)}")
                
                # è·å–å› å­åº“æ‘˜è¦
                factor_summary = complete_factors.get_factor_summary()
                logger.info(f"å®Œæ•´å› å­åº“åŒ…å« {factor_summary['total_factors']} ä¸ªä¸“ä¸šå› å­")
                
                # å®Œæ•´40+ä¸“ä¸šå› å­é›†åˆ
                all_polygon_factors = {}
                factor_calculation_success = {}
                
                # å¯¹å‰å‡ åªä»£è¡¨æ€§è‚¡ç¥¨è®¡ç®—å®Œæ•´å› å­
                sample_symbols = symbols[:min(3, len(symbols))]  # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥é¿å…APIé™åˆ¶
                
                for symbol in sample_symbols:
                    try:
                        logger.info(f"ä¸º {symbol} è®¡ç®—å®Œæ•´å› å­...")
                        
                        # è®¡ç®—æ‰€æœ‰ç±»åˆ«çš„å› å­
                        symbol_factors = complete_factors.calculate_all_complete_factors(
                            symbol, 
                            categories=['momentum', 'fundamental', 'profitability', 'quality', 'risk', 'microstructure']
                        )
                        
                        if symbol_factors:
                            logger.info(f"{symbol} æˆåŠŸè®¡ç®— {len(symbol_factors)} ä¸ªå› å­")
                            
                            # æå–å› å­å€¼ä½œä¸ºç‰¹å¾
                            for factor_name, result in symbol_factors.items():
                                if len(result.values) > 0 and result.data_quality > 0.5:
                                    col_name = f"polygon_{factor_name}"
                                    # ä½¿ç”¨æœ€æ–°å€¼
                                    factor_value = result.values.iloc[-1]
                                    if not np.isnan(factor_value) and np.isfinite(factor_value):
                                        all_polygon_factors[col_name] = factor_value
                                        factor_calculation_success[factor_name] = True
                        
                        # T+5çŸ­æœŸå› å­
                        try:
                            t5_results = short_term_factors.calculate_all_short_term_factors(symbol)
                            if t5_results:
                                prediction = short_term_factors.create_t_plus_5_prediction(symbol, t5_results)
                                
                                # T+5ä¸“ç”¨å› å­
                                for factor_name, result in t5_results.items():
                                    col_name = f"t5_{factor_name}"
                                    if hasattr(result, 't_plus_5_signal'):
                                        signal_value = result.t_plus_5_signal
                                        if not np.isnan(signal_value) and np.isfinite(signal_value):
                                            all_polygon_factors[col_name] = signal_value
                                
                                # T+5ç»¼åˆé¢„æµ‹ä¿¡å·
                                if 'signal_strength' in prediction:
                                    all_polygon_factors['t5_prediction_signal'] = prediction['signal_strength']
                                    all_polygon_factors['t5_prediction_confidence'] = prediction.get('confidence', 0.5)
                        except Exception as t5_e:
                            logger.warning(f"{symbol} T+5å› å­è®¡ç®—å¤±è´¥: {t5_e}")
                        
                        time.sleep(0.5)  # APIé™åˆ¶
                        
                    except Exception as e:
                        logger.warning(f"{symbol}å®Œæ•´å› å­è®¡ç®—å¤±è´¥: {e}")
                        continue
                
                # å°†è®¡ç®—æˆåŠŸçš„å› å­æ·»åŠ åˆ°ç‰¹å¾çŸ©é˜µ
                if all_polygon_factors:
                    logger.info(f"æˆåŠŸè®¡ç®—Polygonå› å­: {len(all_polygon_factors)} ä¸ª")
                    logger.info(f"å› å­ç±»å‹åˆ†å¸ƒ: {list(factor_calculation_success.keys())}")
                    
                    # æ·»åŠ åˆ°combined_features
                    for col_name, value in all_polygon_factors.items():
                        if col_name not in combined_features.columns:
                            # å¯¹æ‰€æœ‰è‚¡ç¥¨å¹¿æ’­è¯¥å› å­å€¼ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                            combined_features[col_name] = value
                    
                    # è®°å½•æˆåŠŸæ·»åŠ çš„å› å­æ•°é‡
                    added_factors = len(all_polygon_factors)
                    logger.info(f"âœ… æˆåŠŸæ·»åŠ  {added_factors} ä¸ªPolygonä¸“ä¸šå› å­åˆ°ç‰¹å¾çŸ©é˜µ")
                    
                    # æ˜¾ç¤ºå› å­åˆ†ç±»ç»Ÿè®¡
                    momentum_factors = len([k for k in all_polygon_factors.keys() if 'momentum' in k])
                    fundamental_factors = len([k for k in all_polygon_factors.keys() if any(x in k for x in ['earnings', 'ebit', 'yield'])])
                    quality_factors = len([k for k in all_polygon_factors.keys() if any(x in k for x in ['piotroski', 'altman', 'quality'])])
                    risk_factors = len([k for k in all_polygon_factors.keys() if any(x in k for x in ['volatility', 'beta', 'risk'])])
                    t5_factors = len([k for k in all_polygon_factors.keys() if 't5_' in k])
                    
                    logger.info(f"å› å­åˆ†å¸ƒ - åŠ¨é‡:{momentum_factors}, åŸºæœ¬é¢:{fundamental_factors}, è´¨é‡:{quality_factors}, é£é™©:{risk_factors}, T+5:{t5_factors}")
                else:
                    logger.warning("æœªèƒ½æˆåŠŸè®¡ç®—ä»»ä½•Polygonå› å­")
                
            except Exception as _e:
                logger.error(f"Polygonå®Œæ•´å› å­åº“é›†æˆå¤±è´¥: {_e}")
                import traceback
                logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
            # ========== ç®€åŒ–ä½†å¯é çš„ä¸­æ€§åŒ–å¤„ç† ==========
            logger.info("åº”ç”¨ç®€åŒ–ä¸­æ€§åŒ–å¤„ç†")
            try:
                # é¢„å…ˆè·å–ä¸€æ¬¡æ‰€æœ‰tickerçš„è¡Œä¸šä¿¡æ¯ï¼Œé¿å…åœ¨å¾ªç¯ä¸­é‡å¤è·å–
                all_tickers = combined_features['ticker'].unique().tolist()
                stock_info_cache = {}
                if self.market_data_manager:
                    try:
                        stock_info_cache = self.market_data_manager.get_batch_stock_info(all_tickers)
                        logger.info(f"é¢„è·å–{len(all_tickers)}åªè‚¡ç¥¨çš„è¡Œä¸šä¿¡æ¯å®Œæˆ")
                    except Exception as e:
                        logger.warning(f"é¢„è·å–è¡Œä¸šä¿¡æ¯å¤±è´¥: {e}")
                
                # æŒ‰æ—¥æœŸåˆ†ç»„ï¼Œé€æ—¥è¿›è¡Œç®€å•çš„æ ‡å‡†åŒ–å’Œwinsorization
                neutralized_features = []
                
                for date, group in combined_features.groupby('date'):
                    group_features = group[feature_cols].copy()
                    
                    # 1. Winsorization (1%-99%åˆ†ä½æ•°æˆªæ–­)
                    for col in feature_cols:
                        if group_features[col].notna().sum() > 2:
                            q01, q99 = group_features[col].quantile([0.01, 0.99])
                            group_features[col] = group_features[col].clip(lower=q01, upper=q99)
                    
                    # 2. æ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰
                    for col in feature_cols:
                        if group_features[col].notna().sum() > 2:
                            mean_val = group_features[col].mean()
                            std_val = group_features[col].std()
                            if std_val > 0:
                                group_features[col] = (group_features[col] - mean_val) / std_val
                            else:
                                group_features[col] = 0.0
                    
                    # 3. è¡Œä¸šä¸­æ€§åŒ–ï¼ˆä½¿ç”¨é¢„è·å–çš„è¡Œä¸šä¿¡æ¯ï¼‰
                    if stock_info_cache:
                        try:
                            tickers = group['ticker'].tolist()
                            industries = {}
                            for ticker in tickers:
                                info = stock_info_cache.get(ticker)
                                if info:
                                    sector = info.gics_sub_industry or info.gics_industry or info.sector
                                    industries[ticker] = sector or 'Unknown'
                                else:
                                    industries[ticker] = 'Unknown'
                            
                            # æŒ‰è¡Œä¸šå»å‡å€¼
                            group_with_industry = group_features.copy()
                            group_with_industry['industry'] = group['ticker'].map(industries)
                            
                            for col in feature_cols:
                                if group_with_industry[col].notna().sum() > 2:
                                    industry_means = group_with_industry.groupby('industry')[col].transform('mean')
                                    group_features[col] = group_features[col] - industry_means
                                    
                        except Exception as e:
                            logger.debug(f"è¡Œä¸šä¸­æ€§åŒ–è·³è¿‡: {e}")
                    
                    # ä¿ç•™éç‰¹å¾åˆ—
                    group_result = group[['date', 'ticker']].copy()
                    group_result[feature_cols] = group_features[feature_cols]
                    neutralized_features.append(group_result)
                
                # åˆå¹¶ç»“æœ
                neutralized_df = pd.concat(neutralized_features, ignore_index=True)
                combined_features[feature_cols] = neutralized_df[feature_cols]
                
                logger.info(f"ç®€åŒ–ä¸­æ€§åŒ–å®Œæˆï¼Œå¤„ç†{len(feature_cols)}ä¸ªç‰¹å¾")
                
            except Exception as e:
                logger.warning(f"ç®€åŒ–ä¸­æ€§åŒ–å¤±è´¥: {e}")
                logger.info("ä½¿ç”¨åŸå§‹ç‰¹å¾ï¼Œä»…è¿›è¡Œæ ‡å‡†åŒ–")
                # æœ€ç®€å•çš„å›é€€ï¼šå…¨å±€æ ‡å‡†åŒ–
                from sklearn.preprocessing import StandardScaler
                scaler = StandardScaler()
                try:
                    combined_features[feature_cols] = scaler.fit_transform(combined_features[feature_cols].fillna(0))
                except Exception:
                    pass
            
            logger.info(f"ä¼ ç»Ÿç‰¹å¾åˆ›å»ºå®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {combined_features.shape}")
            return combined_features
        else:
            logger.error("æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾æ•°æ®")
            return pd.DataFrame()
    
    def _validate_temporal_alignment(self, feature_data: pd.DataFrame) -> bool:
        """éªŒè¯ç‰¹å¾å’Œç›®æ ‡çš„æ—¶é—´å¯¹é½ï¼Œç¡®ä¿æ— æ•°æ®æ³„éœ²"""
        try:
            # æ£€æŸ¥æ¯ä¸ªtickerçš„æ—¶é—´å¯¹é½
            for ticker in feature_data['ticker'].unique()[:3]:  # æ ·æœ¬æ£€æŸ¥
                ticker_data = feature_data[feature_data['ticker'] == ticker].sort_values('date')
                if len(ticker_data) < 10:
                    continue
                    
                # æ£€æŸ¥ç‰¹å¾å’Œç›®æ ‡çš„æ—¶é—´å·®
                feature_dates = ticker_data['date'].iloc[:-5]  # ç‰¹å¾æ—¥æœŸ
                target_dates = ticker_data['date'].iloc[5:]    # ç›®æ ‡æ—¥æœŸ
                
                if len(feature_dates) > 0 and len(target_dates) > 0:
                    # éªŒè¯æ—¶é—´é—´éš”ç¬¦åˆé¢„æœŸï¼ˆåº”è¯¥æœ‰è¶³å¤Ÿçš„gapï¼‰
                    time_diff = (target_dates.iloc[0] - feature_dates.iloc[-1]).days
                    if time_diff < 7:  # è‡³å°‘7å¤©gap
                        logger.warning(f"æ—¶é—´å¯¹é½éªŒè¯å¤±è´¥ï¼š{ticker} ç‰¹å¾-ç›®æ ‡é—´éš”ä»…{time_diff}å¤©")
                        return False
            
            logger.info("âœ… æ—¶é—´å¯¹é½éªŒè¯é€šè¿‡ï¼šç‰¹å¾å’Œç›®æ ‡æ—¶é—´å……åˆ†éš”ç¦»")
            return True
        except Exception as e:
            logger.warning(f"æ—¶é—´å¯¹é½éªŒè¯å¼‚å¸¸: {e}")
            return False

    def train_enhanced_models(self, feature_data: pd.DataFrame, current_ticker: str = None) -> Dict[str, Any]:
        """
        è®­ç»ƒå¢å¼ºæ¨¡å‹ï¼ˆAlphaç­–ç•¥ + Learning-to-Rank + ä¼ ç»ŸMLï¼‰
        
        Args:
            feature_data: ç‰¹å¾æ•°æ®
            current_ticker: å½“å‰å¤„ç†çš„è‚¡ç¥¨ä»£ç ï¼ˆç”¨äºè‡ªé€‚åº”ä¼˜åŒ–ï¼‰
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        logger.info("å¼€å§‹è®­ç»ƒå¢å¼ºæ¨¡å‹")
        
        self.feature_data = feature_data
        training_results = {}
        
        # å‡†å¤‡æ•°æ®
        feature_cols = [col for col in feature_data.columns 
                       if col not in ['ticker', 'date', 'target', 'COUNTRY', 'SECTOR', 'SUBINDUSTRY']]
        
        X = feature_data[feature_cols]
        y = feature_data['target']
        dates = feature_data['date']
        tickers = feature_data['ticker']
        
        # å»é™¤ç¼ºå¤±å€¼ - æ”¹è¿›ç‰ˆï¼šåªå»é™¤ç‰¹å¾æˆ–ç›®æ ‡ä¸ºç©ºçš„æ ·æœ¬
        # å…ˆå¡«å……NaNå€¼ï¼Œç„¶åè¿‡æ»¤
        from sklearn.impute import SimpleImputer
        
        # å¯¹ç‰¹å¾è¿›è¡Œå®‰å…¨çš„ä¸­ä½æ•°å¡«å……ï¼ˆåªå¤„ç†æ•°å€¼åˆ—ï¼‰
        try:
            # è¯†åˆ«æ•°å€¼åˆ—å’Œéæ•°å€¼åˆ—
            numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
            non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()
            
            X_imputed = X.copy()
            
            # åªå¯¹æ•°å€¼åˆ—åº”ç”¨ä¸­ä½æ•°å¡«å……
            if numeric_cols:
                imputer = SimpleImputer(strategy='median')
                X_imputed[numeric_cols] = pd.DataFrame(
                    imputer.fit_transform(X[numeric_cols]), 
                    columns=numeric_cols, 
                    index=X.index
                )
            
            # å¯¹éæ•°å€¼åˆ—ä½¿ç”¨å¸¸æ•°å¡«å……
            if non_numeric_cols:
                for col in non_numeric_cols:
                    X_imputed[col] = X_imputed[col].fillna('Unknown')
                    
        except Exception as e:
            logger.warning(f"ç‰¹å¾å¡«å……å¤±è´¥ï¼Œä½¿ç”¨ç®€å•å¡«å……: {e}")
            X_imputed = X.fillna(0)
        
        # ç›®æ ‡å˜é‡å¿…é¡»æœ‰æ•ˆ
        target_valid = ~y.isna()
        
        X_clean = X_imputed[target_valid]
        y_clean = y[target_valid]
        dates_clean = dates[target_valid]
        tickers_clean = tickers[target_valid]
        
        if len(X_clean) == 0:
            logger.error("æ¸…æ´—åæ•°æ®ä¸ºç©º")
            return {}
        
        logger.info(f"è®­ç»ƒæ•°æ®: {len(X_clean)}æ ·æœ¬, {len(feature_cols)}ç‰¹å¾")
        
        # ğŸ”¥ æ—¶é—´å¯¹é½éªŒè¯ï¼šç¡®ä¿æ— æ•°æ®æ³„éœ²
        if not self._validate_temporal_alignment(feature_data):
            logger.error("âš ï¸ æ—¶é—´å¯¹é½éªŒè¯å¤±è´¥ï¼Œå­˜åœ¨æ•°æ®æ³„éœ²é£é™©ï¼")
        
        # 1. è®­ç»ƒAlphaç­–ç•¥å¼•æ“
        if self.alpha_engine and ENHANCED_MODULES_AVAILABLE:
            logger.info("è®­ç»ƒAlphaç­–ç•¥å¼•æ“")
            try:
                # é‡ç»„æ•°æ®æ ¼å¼ç”¨äºAlphaè®¡ç®—
                alpha_data = feature_data[['date', 'ticker', 'close', 'high', 'low', 'volume', 'amount',
                                         'COUNTRY', 'SECTOR', 'SUBINDUSTRY']].copy()
                # ä¸ºAlphaå¼•æ“æ ‡å‡†åŒ–åˆ—åå¹¶ä¼˜å…ˆä½¿ç”¨å¤æƒæ”¶ç›˜ä»·
                if 'Adj Close' in feature_data.columns:
                    alpha_data['Close'] = feature_data['Adj Close']
                else:
                    alpha_data['Close'] = feature_data['close']
                alpha_data['High'] = feature_data['high']
                alpha_data['Low'] = feature_data['low']
                
                # è®¡ç®—Alphaå› å­
                alpha_df = self.alpha_engine.compute_all_alphas(alpha_data)
                
                # è®¡ç®—OOFè¯„åˆ†
                if len(alpha_df) > 0:
                    alpha_scores = self.alpha_engine.compute_oof_scores(
                        alpha_df, y_clean, dates_clean, metric='ic'
                    )
                    
                    # è®¡ç®—BMAæƒé‡
                    alpha_weights = self.alpha_engine.compute_bma_weights(alpha_scores)
                    
                    # ç»„åˆAlphaä¿¡å·
                    alpha_signal = self.alpha_engine.combine_alphas(alpha_df, alpha_weights)
                    
                    # ç®€å•è¿‡æ»¤ï¼šå»é™¤æå€¼å’ŒNaN
                    filtered_signal = alpha_signal.copy()
                    filtered_signal = filtered_signal.replace([np.inf, -np.inf], np.nan)
                    filtered_signal = filtered_signal.fillna(0.0)
                    
                    # å¯é€‰ï¼šWinsorizeå¤„ç†æå€¼
                    q1, q99 = filtered_signal.quantile([0.01, 0.99])
                    filtered_signal = filtered_signal.clip(lower=q1, upper=q99)
                    
                    self.alpha_signals = filtered_signal
                    training_results['alpha_strategy'] = {
                        'alpha_scores': alpha_scores,
                        'alpha_weights': alpha_weights,
                        'alpha_signals': filtered_signal,
                        'alpha_stats': self.alpha_engine.get_stats()
                    }
                    
                    logger.info(f"Alphaç­–ç•¥è®­ç»ƒå®Œæˆï¼Œä¿¡å·è¦†ç›–: {(~filtered_signal.isna()).sum()}æ ·æœ¬")
                
            except Exception as e:
                logger.error(f"Alphaç­–ç•¥è®­ç»ƒå¤±è´¥: {e}")
                training_results['alpha_strategy'] = {'error': str(e)}
        
        # 2. è®­ç»ƒLearning-to-Rank BMA
        if self.ltr_bma and ENHANCED_MODULES_AVAILABLE:
            logger.info("è®­ç»ƒLearning-to-Rank BMA")
            try:
                ltr_results = self.ltr_bma.train_ranking_models(
                    X=X_clean, y=y_clean, dates=dates_clean,
                    cv_folds=3, optimize_hyperparams=False
                )
                
                training_results['learning_to_rank'] = {
                    'model_results': ltr_results,
                    'performance_summary': self.ltr_bma.get_performance_summary()
                }
                
                logger.info("Learning-to-Rankè®­ç»ƒå®Œæˆ")
                
            except Exception as e:
                logger.error(f"Learning-to-Rankè®­ç»ƒå¤±è´¥: {e}")
                training_results['learning_to_rank'] = {'error': str(e)}
        
        # 3. è®­ç»ƒä¼ ç»ŸMLæ¨¡å‹ï¼ˆä½œä¸ºåŸºå‡†ï¼‰
        logger.info("è®­ç»ƒä¼ ç»ŸMLæ¨¡å‹")
        try:
            # å°è¯•ä»ç‰¹å¾æ•°æ®ä¸­æå–è‚¡ç¥¨ä»£ç 
            if current_ticker is None and 'ticker' in feature_data.columns:
                tickers = feature_data['ticker'].unique()
                current_ticker = tickers[0] if len(tickers) > 0 else "MULTI_STOCK"
            elif current_ticker is None:
                current_ticker = "UNKNOWN"
            
            traditional_results = self._train_traditional_models(X_clean, y_clean, dates_clean, current_ticker)
            training_results['traditional_models'] = traditional_results
            
        except Exception as e:
            logger.error(f"ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            training_results['traditional_models'] = {'error': str(e)}
        
        logger.info("å¢å¼ºæ¨¡å‹è®­ç»ƒå®Œæˆ")
        return training_results
    
    def _train_traditional_models(self, X: pd.DataFrame, y: pd.Series, 
                                 dates: pd.Series, stock_symbol: str = "UNKNOWN") -> Dict[str, Any]:
        """è®­ç»ƒä¼ ç»ŸMLæ¨¡å‹ - æ”¯æŒè‡ªé€‚åº”åŠ æ ‘ä¼˜åŒ–"""
        
        # ğŸš€ ç¬¬äºŒå±‚ä¼˜åŒ–ï¼šè‡ªé€‚åº”åŠ æ ‘
        if ADAPTIVE_OPTIMIZER_AVAILABLE:
            logger.info(f"ä½¿ç”¨è‡ªé€‚åº”åŠ æ ‘ä¼˜åŒ–å™¨è®­ç»ƒ{stock_symbol}")
            return self._train_with_adaptive_optimizer(X, y, stock_symbol)
        else:
            logger.info(f"ä½¿ç”¨æ ‡å‡†æ¨¡å‹è®­ç»ƒ{stock_symbol}")
            return self._train_standard_models(X, y, dates)
    
    def _train_with_adaptive_optimizer(self, X: pd.DataFrame, y: pd.Series, 
                                     stock_symbol: str) -> Dict[str, Any]:
        """ä½¿ç”¨è‡ªé€‚åº”åŠ æ ‘ä¼˜åŒ–å™¨è®­ç»ƒæ¨¡å‹"""
        # åˆ›å»ºè‡ªé€‚åº”ä¼˜åŒ–å™¨
        optimizer = AdaptiveTreeOptimizer(
            slope_threshold_ic=0.002,    # ICæå‡æ–œç‡é˜ˆå€¼
            slope_threshold_mse=0.01,    # MSEä¸‹é™æ–œç‡é˜ˆå€¼(1%)
            tree_increment=20,           # æ¯æ¬¡å¢åŠ 20æ£µæ ‘
            top_percentile=0.2,          # é€‰æ‹©å‰20%çš„è‚¡ç¥¨
            max_trees_xgb=150,           # XGBæœ€å¤§150æ£µæ ‘
            max_trees_lgb=150,           # LightGBMæœ€å¤§150æ£µæ ‘
            max_trees_rf=200             # RandomForestæœ€å¤§200æ£µæ ‘
        )
        
        model_results = {}
        oof_predictions = {}
        
        # 1. è®­ç»ƒçº¿æ€§æ¨¡å‹ï¼ˆä¸éœ€è¦è‡ªé€‚åº”ä¼˜åŒ–ï¼‰
        linear_models = {
            'ridge': Ridge(alpha=1.0),
            'elastic': ElasticNet(alpha=0.05, l1_ratio=0.2, max_iter=5000)
        }
        
        for model_name, model in linear_models.items():
            try:
                model.fit(X, y)
                predictions = model.predict(X)
                score = r2_score(y, predictions)
                
                model_results[model_name] = {
                    'model': model,
                    'cv_score': score,
                    'feature_importance': getattr(model, 'coef_', None)
                }
                oof_predictions[model_name] = predictions
                
                logger.info(f"{stock_symbol} {model_name}: R2={score:.4f}")
            except Exception as e:
                logger.warning(f"{stock_symbol} {model_name}è®­ç»ƒå¤±è´¥: {e}")
        
        # 2. è‡ªé€‚åº”è®­ç»ƒRandomForest
        try:
            rf_model, rf_perf = optimizer.adaptive_train_random_forest(X, y, stock_symbol)
            if rf_model:
                predictions = rf_model.predict(X)
                model_results['rf'] = {
                    'model': rf_model,
                    'cv_score': rf_perf.get('oob_score', 0.0),
                    'feature_importance': rf_model.feature_importances_,
                    'adaptive_performance': rf_perf
                }
                oof_predictions['rf'] = predictions
                logger.info(f"{stock_symbol} RandomForestè‡ªé€‚åº”è®­ç»ƒå®Œæˆ: {rf_perf}")
        except Exception as e:
            logger.warning(f"{stock_symbol} RandomForestè‡ªé€‚åº”è®­ç»ƒå¤±è´¥: {e}")
        
        # 3. è‡ªé€‚åº”è®­ç»ƒXGBoost
        if XGBOOST_AVAILABLE:
            try:
                xgb_model, xgb_perf = optimizer.adaptive_train_xgboost(X, y, stock_symbol)
                if xgb_model:
                    predictions = xgb_model.predict(X)
                    model_results['xgboost'] = {
                        'model': xgb_model,
                        'cv_score': xgb_perf.get('ic', 0.0),
                        'feature_importance': xgb_model.feature_importances_,
                        'adaptive_performance': xgb_perf
                    }
                    oof_predictions['xgboost'] = predictions
                    logger.info(f"{stock_symbol} XGBoostè‡ªé€‚åº”è®­ç»ƒå®Œæˆ: {xgb_perf}")
            except Exception as e:
                logger.warning(f"{stock_symbol} XGBoostè‡ªé€‚åº”è®­ç»ƒå¤±è´¥: {e}")
        
        # 4. è‡ªé€‚åº”è®­ç»ƒLightGBM
        if LIGHTGBM_AVAILABLE:
            try:
                lgb_model, lgb_perf = optimizer.adaptive_train_lightgbm(X, y, stock_symbol)
                if lgb_model:
                    predictions = lgb_model.predict(X)
                    model_results['lightgbm'] = {
                        'model': lgb_model,
                        'cv_score': lgb_perf.get('ic', 0.0),
                        'feature_importance': lgb_model.feature_importances_,
                        'adaptive_performance': lgb_perf
                    }
                    oof_predictions['lightgbm'] = predictions
                    logger.info(f"{stock_symbol} LightGBMè‡ªé€‚åº”è®­ç»ƒå®Œæˆ: {lgb_perf}")
            except Exception as e:
                logger.warning(f"{stock_symbol} LightGBMè‡ªé€‚åº”è®­ç»ƒå¤±è´¥: {e}")
        
        return {
            'models': model_results,
            'oof_predictions': oof_predictions,
            'optimizer_summary': optimizer.get_optimization_summary()
        }
    
    def _train_standard_models(self, X: pd.DataFrame, y: pd.Series, 
                             dates: pd.Series) -> Dict[str, Any]:
        """æ ‡å‡†æ¨¡å‹è®­ç»ƒï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        models = {
            'ridge': Ridge(alpha=1.0),
            'elastic': ElasticNet(alpha=0.05, l1_ratio=0.2, max_iter=5000),
            'rf': RandomForestRegressor(
                n_estimators=100,        # ä»200å‡åˆ°100 (BMAä¼˜åŒ–)
                max_depth=10,            # æ–°å¢æ·±åº¦é™åˆ¶
                max_features=0.8,        # ç‰¹å¾é‡‡æ ·80%
                min_samples_leaf=10,     # å¢åŠ å¶å­æœ€å°æ ·æœ¬
                max_samples=0.8,         # æ ·æœ¬é‡‡æ ·80%
                n_jobs=1,                # é™åˆ¶å¹¶è¡Œåº¦
                random_state=42
            )
        }
        
        # æ·»åŠ é«˜çº§æ¨¡å‹
        if XGBOOST_AVAILABLE:
            models['xgboost'] = xgb.XGBRegressor(
                n_estimators=70,         # ä»100å‡åˆ°70 (BMAä¼˜åŒ–)
                max_depth=4,             # ä»6å‡åˆ°4 (BMAä¼˜åŒ–)
                learning_rate=0.2,       # ä»0.1å¢åˆ°0.2 (BMAä¼˜åŒ–)
                subsample=0.8,           # æ ·æœ¬é‡‡æ ·
                colsample_bytree=0.8,    # ç‰¹å¾é‡‡æ ·
                reg_alpha=0.1,           # L1æ­£åˆ™åŒ–
                reg_lambda=1.0,          # L2æ­£åˆ™åŒ–
                tree_method='hist',      # é«˜æ•ˆç®—æ³•
                early_stopping_rounds=15, # æ—©åœæœºåˆ¶ (BMAä¼˜åŒ–)
                random_state=42,
                n_jobs=1                 # é™åˆ¶å¹¶è¡Œåº¦
            )
        
        if LIGHTGBM_AVAILABLE:
            models['lightgbm'] = lgb.LGBMRegressor(
                n_estimators=80,         # ä»100å‡åˆ°80 (BMAä¼˜åŒ–)
                max_depth=5,             # ä»6å‡åˆ°5 (BMAä¼˜åŒ–)
                num_leaves=31,           # ä¸¥æ ¼æ§åˆ¶å¶å­æ•°
                learning_rate=0.2,       # ä»0.1å¢åˆ°0.2 (BMAä¼˜åŒ–)
                feature_fraction=0.8,    # ç‰¹å¾é‡‡æ ·
                bagging_fraction=0.8,    # æ ·æœ¬é‡‡æ ·
                bagging_freq=1,
                min_data_in_leaf=50,     # å¢åŠ å¶å­æœ€å°æ•°æ®
                force_row_wise=True,     # å†…å­˜ä¼˜åŒ– (BMAä¼˜åŒ–)
                early_stopping_rounds=15, # æ—©åœæœºåˆ¶ (BMAä¼˜åŒ–)
                random_state=42,
                verbose=-1,
                n_jobs=1                 # é™åˆ¶å¹¶è¡Œåº¦
            )
        
        # CatBoost removed due to compatibility issues
        
        # ğŸ”¥ åŠ å¼ºæ—¶åºéªŒè¯ï¼šå¢åŠ embargoé˜²æ­¢ç›®æ ‡æ³„éœ²
        cv_config = ValidationConfig(
            n_splits=3,    # å‡å°‘æŠ˜æ•°é€‚åº”å°æ•°æ®é›†
            test_size=42,  # å‡å°‘æµ‹è¯•é›†å¤§å°
            gap=5,         # é€‚ä¸­çš„gap
            embargo=3,     # é€‚ä¸­çš„embargo
            group_freq='W',
            min_train_size=50  # é™ä½æœ€å°è®­ç»ƒé›†è¦æ±‚
        )
        purged_cv = PurgedGroupTimeSeriesSplit(cv_config)
        groups = create_time_groups(dates, freq=cv_config.group_freq)
        
        model_results = {}
        oof_predictions = {}
        
        for model_name, model in models.items():
            logger.info(f"è®­ç»ƒ{model_name}æ¨¡å‹")
            
            fold_predictions = np.full(len(X), np.nan)
            fold_models = []
            
            for train_idx, test_idx in purged_cv.split(X, y, groups):
                # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…ï¼ˆå…ˆè½¬ä¸ºndarrayå†æ¯”è¾ƒï¼‰
                train_idx = np.asarray(train_idx)
                test_idx = np.asarray(test_idx)
                train_idx = train_idx[train_idx < len(X)]
                test_idx = test_idx[test_idx < len(X)]
                
                if len(train_idx) == 0 or len(test_idx) == 0:
                    continue
                
                train_mask = np.zeros(len(X), dtype=bool)
                train_mask[train_idx] = True
                test_mask = np.zeros(len(X), dtype=bool) 
                test_mask[test_idx] = True
                
                X_train, y_train = X[train_mask], y[train_mask]
                X_test = X[test_mask]
                
                try:
                    # æ ‡å‡†åŒ–
                    scaler = StandardScaler()
                    X_train_scaled = scaler.fit_transform(X_train)
                    X_test_scaled = scaler.transform(X_test)
                    
                    # è®­ç»ƒæ¨¡å‹
                    if model_name in ['xgboost', 'lightgbm', 'rf']:
                        # Tree-basedæ¨¡å‹ä¸éœ€è¦æ ‡å‡†åŒ–
                        model_copy = type(model)(**model.get_params())
                        model_copy.fit(X_train, y_train)
                        test_pred = model_copy.predict(X_test)
                    else:
                        model_copy = type(model)(**model.get_params())
                        model_copy.fit(X_train_scaled, y_train)
                        test_pred = model_copy.predict(X_test_scaled)
                    
                    fold_predictions[test_mask] = test_pred
                    fold_models.append((model_copy, scaler))
                    
                except Exception as e:
                    logger.warning(f"{model_name}æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
                    continue
            
            oof_predictions[model_name] = fold_predictions
            self.traditional_models[model_name] = fold_models
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            valid_mask = ~np.isnan(fold_predictions)
            if valid_mask.sum() > 0:
                oof_ic = np.corrcoef(y[valid_mask], fold_predictions[valid_mask])[0, 1]
                oof_rank_ic = spearmanr(y[valid_mask], fold_predictions[valid_mask])[0]
                
                model_results[model_name] = {
                    'oof_ic': oof_ic if not np.isnan(oof_ic) else 0.0,
                    'oof_rank_ic': oof_rank_ic if not np.isnan(oof_rank_ic) else 0.0,
                    'valid_predictions': valid_mask.sum()
                }
                
                logger.info(f"{model_name} - IC: {oof_ic:.4f}, RankIC: {oof_rank_ic:.4f}")
        
        # ğŸ”´ ä¿®å¤Stackingæ³„éœ²ï¼šäºŒå±‚Stackingå…ƒå­¦ä¹ å™¨æ—¶é—´å®‰å…¨è®­ç»ƒ
        try:
            logger.info("è®­ç»ƒæ—¶é—´å®‰å…¨çš„äºŒå±‚Stackingå…ƒå­¦ä¹ å™¨")
            base_pred_df = pd.DataFrame({name: preds for name, preds in oof_predictions.items()})
            
            # ğŸ”´ å…³é”®ä¿®å¤ï¼šç¡®ä¿OOFé¢„æµ‹æ¥è‡ªä¸¥æ ¼çš„æ—¶é—´åˆ†å‰²
            # ç¬¬ä¸€å±‚æ¨¡å‹çš„OOFé¢„æµ‹å¿…é¡»æ˜¯çœŸæ­£çš„out-of-foldï¼Œä¸èƒ½æœ‰æ—¶é—´æ³„éœ²
            base_pred_df = base_pred_df.reset_index(drop=True)
            y_reset = y.reset_index(drop=True)
            dates_reset = dates.reset_index(drop=True)
            
            # éªŒè¯ç¬¬ä¸€å±‚OOFé¢„æµ‹çš„å®Œæ•´æ€§
            base_valid_mask = ~base_pred_df.isna().any(axis=1) & ~y_reset.isna()
            
            # ğŸ”´ æ—¶é—´éªŒè¯ï¼šç¡®ä¿åªä½¿ç”¨æœ‰å®Œæ•´OOFé¢„æµ‹çš„æ ·æœ¬
            if base_valid_mask.sum() < len(base_pred_df) * 0.8:
                logger.warning(f"OOFé¢„æµ‹å®Œæ•´æ€§ä¸è¶³: {base_valid_mask.sum()}/{len(base_pred_df)} ({base_valid_mask.mean():.1%})")
            
            X_meta = base_pred_df.loc[base_valid_mask].copy()
            y_meta = y_reset.loc[base_valid_mask].copy()
            dates_meta = dates_reset.loc[base_valid_mask].copy()

            # ğŸ”´ ç¬¬äºŒå±‚CVå¿…é¡»ä¸¥æ ¼æ™šäºç¬¬ä¸€å±‚ï¼šæ›´å¤§çš„gapå’Œembargo
            groups = create_time_groups(dates_meta, freq='W')
            stacking_cv_config = ValidationConfig(
                n_splits=3,       # æ›´å°‘çš„foldï¼ˆé¿å…è¿‡åº¦åˆ‡åˆ†ï¼‰
                test_size=84,     # æ›´å¤§çš„æµ‹è¯•é›†ï¼ˆ4å‘¨ï¼‰
                gap=14,           # æ›´å¤§çš„gapï¼ˆ2å‘¨ï¼Œç¡®ä¿è¶…è¿‡ç¬¬ä¸€å±‚çš„gapï¼‰
                embargo=10,       # æ›´å¤§çš„embargoï¼ˆé¿å…ç›®æ ‡æ³„éœ²ï¼‰
                min_train_size=126  # ç¡®ä¿è¶³å¤Ÿçš„è®­ç»ƒæ ·æœ¬
            )
            pgts = PurgedGroupTimeSeriesSplit(stacking_cv_config)
            
            logger.info(f"ç¬¬äºŒå±‚CVé…ç½®: n_splits={stacking_cv_config.n_splits}, "
                       f"gap={stacking_cv_config.gap}, embargo={stacking_cv_config.embargo}")

            meta_models = {
                'meta_ridge': Ridge(alpha=0.5),
                'meta_elastic': ElasticNet(alpha=0.05, l1_ratio=0.3, max_iter=5000)
            }

            meta_oof = {name: np.full(len(X_meta), np.nan) for name in meta_models.keys()}
            trained_meta = {}

            # ğŸ”´ ä¸¥æ ¼çš„æ—¶é—´éªŒè¯ï¼šç¡®ä¿ç¬¬äºŒå±‚CVä¸ä¼šæ³„éœ²
            for fold_idx, (train_idx, test_idx) in enumerate(pgts.split(X_meta, y_meta, groups)):
                # æ—¶é—´éªŒè¯ï¼šè®­ç»ƒé›†æœ€å¤§æ—¥æœŸ + gap + embargo < æµ‹è¯•é›†æœ€å°æ—¥æœŸ
                train_dates = dates_meta.iloc[train_idx]
                test_dates = dates_meta.iloc[test_idx]
                
                train_max_date = train_dates.max()
                test_min_date = test_dates.min()
                gap_days = (test_min_date - train_max_date).days
                
                # éªŒè¯æ—¶é—´é—´éš”
                required_gap = stacking_cv_config.gap + stacking_cv_config.embargo
                if gap_days < required_gap:
                    logger.error(f"ç¬¬äºŒå±‚CV Fold {fold_idx}: æ—¶é—´é—´éš”ä¸è¶³ {gap_days} < {required_gap}")
                    raise ValueError(f"Stacking CVæ—¶é—´æ³„éœ²é£é™©: fold {fold_idx}")
                
                logger.debug(f"ç¬¬äºŒå±‚CV Fold {fold_idx}: æ—¶é—´é—´éš” {gap_days}å¤© >= {required_gap}å¤© âœ…")
                
                X_tr, X_te = X_meta.iloc[train_idx], X_meta.iloc[test_idx]
                y_tr = y_meta.iloc[train_idx]
                
                for mname, m in meta_models.items():
                    m_fit = type(m)(**m.get_params())
                    m_fit.fit(X_tr, y_tr)
                    meta_oof[mname][test_idx] = m_fit.predict(X_te)
                    trained_meta.setdefault(mname, []).append(m_fit)

            # è®°å½•å…ƒå­¦ä¹ å™¨æ€§èƒ½
            meta_perf = {}
            for mname, preds in meta_oof.items():
                valid = ~np.isnan(preds)
                ic = np.corrcoef(y_meta.values[valid], np.array(preds)[valid])[0, 1]
                rank_ic = spearmanr(y_meta.values[valid], np.array(preds)[valid])[0]
                meta_perf[mname] = {
                    'oof_ic': float(ic) if not np.isnan(ic) else 0.0,
                    'oof_rank_ic': float(rank_ic) if not np.isnan(rank_ic) else 0.0,
                    'valid_predictions': int(valid.sum())
                }

            # ä¿å­˜åˆ°å®ä¾‹ä»¥ä¾›åç»­é¢„æµ‹
            self.meta_learners = trained_meta
            self.meta_oof_predictions = meta_oof
            model_results.update({f'stacking_{k}': v for k, v in meta_perf.items()})
        except Exception as e:
            logger.warning(f"äºŒå±‚Stackingè®­ç»ƒå¤±è´¥: {e}")

        return {
            'model_performance': model_results,
            'oof_predictions': oof_predictions,
            'stacking': {
                'meta_oof': meta_oof if 'meta_oof' in locals() else {},
                'meta_performance': meta_perf if 'meta_perf' in locals() else {}
            }
        }
    
    def generate_ensemble_predictions(self, training_results: Dict[str, Any]) -> pd.Series:
        """
        ç”Ÿæˆé›†æˆé¢„æµ‹
        
        Args:
            training_results: è®­ç»ƒç»“æœ
            
        Returns:
            é›†æˆé¢„æµ‹åºåˆ—
        """
        logger.info("ç”Ÿæˆé›†æˆé¢„æµ‹")
        
        predictions_dict = {}
        weights_dict = {}
        
        # 1. Alphaç­–ç•¥é¢„æµ‹
        if 'alpha_strategy' in training_results and 'alpha_signals' in training_results['alpha_strategy']:
            alpha_signals = training_results['alpha_strategy']['alpha_signals']
            if alpha_signals is not None and len(alpha_signals) > 0:
                predictions_dict['alpha'] = alpha_signals
                # åŸºäºAlphaè¯„åˆ†è®¾ç½®æƒé‡
                alpha_scores = training_results['alpha_strategy'].get('alpha_scores', pd.Series())
                if len(alpha_scores) > 0:
                    avg_alpha_score = alpha_scores.mean()
                    weights_dict['alpha'] = max(0.1, min(0.5, avg_alpha_score * 5))  # æƒé‡åœ¨0.1-0.5ä¹‹é—´
                else:
                    weights_dict['alpha'] = 0.2
        
        # 2. Learning-to-Ranké¢„æµ‹
        if (self.ltr_bma and 'learning_to_rank' in training_results and 
            'model_results' in training_results['learning_to_rank']):
            try:
                if self.feature_data is not None:
                    feature_cols = [col for col in self.feature_data.columns 
                                   if col not in ['ticker', 'date', 'target', 'COUNTRY', 'SECTOR', 'SUBINDUSTRY']]
                    X_for_prediction = self.feature_data[feature_cols].dropna()
                    
                    ltr_pred, ltr_uncertainty = self.ltr_bma.predict_with_uncertainty(X_for_prediction)
                    
                    # æƒé‡åŸºäºä¸ç¡®å®šæ€§å’ŒLTRæ€§èƒ½
                    avg_uncertainty = np.nanmean(ltr_uncertainty)
                    base_ltr_weight = 1.0 / (1.0 + avg_uncertainty * 10)
                    
                    # æ£€æŸ¥LTRæ€§èƒ½ï¼Œå¦‚æœæœ‰è´ŸICé€šé“åˆ™é™æƒ
                    performance_penalty = 1.0
                    try:
                        ltr_results = training_results['learning_to_rank']
                        if isinstance(ltr_results, dict):
                            ltr_performance = ltr_results.get('performance_summary', {})
                            if ltr_performance and isinstance(ltr_performance, dict):
                                avg_ic = np.mean([p.get('ic', 0.0) for p in ltr_performance.values() if isinstance(p, dict)])
                                if avg_ic < 0:
                                    performance_penalty = 0.3  # è´ŸICæ—¶å¤§å¹…é™æƒ
                                elif avg_ic < 0.05:
                                    performance_penalty = 0.6  # å¼±ICæ—¶ä¸­åº¦é™æƒ
                    except Exception as e:
                        logger.debug(f"LTRæ€§èƒ½æ£€æŸ¥å¤±è´¥: {e}")
                        performance_penalty = 0.8  # å®‰å…¨çš„ä¸­ç­‰æƒé‡
                    
                    final_ltr_weight = base_ltr_weight * performance_penalty
                    predictions_dict['ltr'] = pd.Series(ltr_pred, index=X_for_prediction.index)
                    weights_dict['ltr'] = max(0.05, min(0.25, final_ltr_weight))  # é™ä½ä¸Šé™ä»0.4åˆ°0.25
                    
            except Exception as e:
                logger.warning(f"Learning-to-Ranké¢„æµ‹å¤±è´¥: {e}")
        
        # 3. ä¼ ç»Ÿæ¨¡å‹é¢„æµ‹
        if 'traditional_models' in training_results and 'oof_predictions' in training_results['traditional_models']:
            oof_preds = training_results['traditional_models']['oof_predictions']
            model_perfs = training_results['traditional_models'].get('model_performance', {})
            stacking_info = training_results['traditional_models'].get('stacking', {})
            
            # è·å–è®­ç»ƒæ•°æ®çš„ç´¢å¼•ä½œä¸ºå‚è€ƒ
            if hasattr(self, 'feature_data') and self.feature_data is not None:
                ref_index = self.feature_data.index
            else:
                ref_index = None
            
            for model_name, pred_array in oof_preds.items():
                if pred_array is not None and not np.all(np.isnan(pred_array)):
                    # ç¡®ä¿é¢„æµ‹ä¸ç‰¹å¾æ•°æ®ç´¢å¼•å¯¹é½
                    if ref_index is not None and len(pred_array) == len(ref_index):
                        predictions_dict[f'traditional_{model_name}'] = pd.Series(pred_array, index=ref_index)
                    else:
                        # å›é€€åˆ°é»˜è®¤ç´¢å¼•ï¼Œä½†è¦ç¡®ä¿é•¿åº¦åŒ¹é…
                        logger.warning(f"ä¼ ç»Ÿæ¨¡å‹{model_name}é¢„æµ‹é•¿åº¦{len(pred_array)}ä¸ç‰¹å¾æ•°æ®ä¸åŒ¹é…")
                        continue
                    
                    # åŠ¨æ€æƒé‡ï¼šåŸºäºICå€¼çš„æ™ºèƒ½åˆ†é…ï¼ˆè´ŸICæ¨¡å‹æ’é™¤æˆ–åå‘ä½¿ç”¨ï¼‰
                    if model_name in model_perfs:
                        ic = model_perfs[model_name].get('oof_ic', 0.0)
                        ic_abs = abs(ic)
                        
                        if ic < -0.05:
                            # å¼ºè´ŸICï¼šå®Œå…¨æ’é™¤ï¼Œä¸ç»™äºˆæƒé‡
                            weights_dict[f'traditional_{model_name}'] = 0.0
                            logger.warning(f"æ¨¡å‹ {model_name} IC={ic:.4f} < -0.05ï¼Œå·²æ’é™¤")
                        elif ic < -0.02:
                            # ä¸­ç­‰è´ŸICï¼šæä½æƒé‡æˆ–è€ƒè™‘åå‘ä¿¡å·
                            weights_dict[f'traditional_{model_name}'] = 0.0
                            logger.info(f"æ¨¡å‹ {model_name} IC={ic:.4f} è´Ÿç›¸å…³æ€§è¾ƒå¼ºï¼Œå·²æ’é™¤")
                        elif ic < 0.02:
                            # å™ªéŸ³åŒºé—´ï¼šICæ¥è¿‘0ï¼Œä¸ä½¿ç”¨
                            weights_dict[f'traditional_{model_name}'] = 0.0
                            logger.debug(f"æ¨¡å‹ {model_name} IC={ic:.4f} åœ¨å™ªéŸ³åŒºé—´ï¼Œå·²æ’é™¤")
                        elif ic > 0.15:
                            # å¼ºæ­£ICï¼šæœ€é«˜æƒé‡
                            weights_dict[f'traditional_{model_name}'] = 0.30
                        elif ic > 0.1:
                            # è¾ƒå¼ºæ­£ICï¼šé«˜æƒé‡
                            weights_dict[f'traditional_{model_name}'] = 0.25
                        elif ic > 0.05:
                            # ä¸­ç­‰æ­£ICï¼šä¸­ç­‰æƒé‡
                            weights_dict[f'traditional_{model_name}'] = 0.15
                        elif ic >= 0.02:
                            # å¼±æ­£ICï¼šä½æƒé‡
                            weights_dict[f'traditional_{model_name}'] = 0.08
                        else:
                            # é»˜è®¤æƒ…å†µï¼šæä½æƒé‡
                            weights_dict[f'traditional_{model_name}'] = 0.0
                    else:
                        weights_dict[f'traditional_{model_name}'] = 0.05

            # åŠ å…¥äºŒå±‚Stackingå…ƒå­¦ä¹ å™¨çš„é¢„æµ‹ï¼ˆä½œä¸ºé¢å¤–é€šé“ï¼‰
            try:
                if stacking_info and 'meta_oof' in stacking_info and hasattr(self, 'feature_data'):
                    base_models = [f"{k}" for k in oof_preds.keys()]
                    base_pred_df = pd.DataFrame({name: predictions_dict.get(f'traditional_{name}', pd.Series(dtype=float)) for name in base_models})
                    # å¯¹é½åˆ°å‚è€ƒç´¢å¼•
                    base_pred_df = base_pred_df.reindex(ref_index)
                    # ä½¿ç”¨å·²è®­ç»ƒçš„meta learnersåšä¸€å±‚é¢„æµ‹å¹³å‡
                    if hasattr(self, 'meta_learners') and isinstance(self.meta_learners, dict):
                        for mname, mlist in self.meta_learners.items():
                            # å¯¹å¤šä¸ªæŠ˜çš„metaæ¨¡å‹å–å¹³å‡é¢„æµ‹
                            meta_preds = np.nanmean([m.predict(base_pred_df.fillna(0.0)) for m in mlist], axis=0)
                            predictions_dict[f'stacking_{mname}'] = pd.Series(meta_preds, index=ref_index)
                            perf = stacking_info.get('meta_performance', {}).get(mname, {})
                            ic = perf.get('oof_ic', 0.0)
                            weights_dict[f'stacking_{mname}'] = max(0.05, min(0.35, ic * 6))
            except Exception as e:
                logger.warning(f"Stackingé€šé“é›†æˆå¤±è´¥: {e}")
        
        # é›†æˆé¢„æµ‹
        if not predictions_dict:
            logger.error("æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
            return pd.Series()
        
        # æ ‡å‡†åŒ–æƒé‡
        total_weight = sum(weights_dict.values())
        if total_weight > 0:
            for key in weights_dict:
                weights_dict[key] /= total_weight
        
        logger.info(f"é›†æˆæƒé‡: {weights_dict}")
        
        # ç»Ÿä¸€æ‰€æœ‰é¢„æµ‹çš„ç´¢å¼•åˆ°feature_dataçš„ç´¢å¼•
        if hasattr(self, 'feature_data') and self.feature_data is not None:
            reference_index = self.feature_data.index
        else:
            # å¦‚æœæ²¡æœ‰å‚è€ƒç´¢å¼•ï¼Œå–æ‰€æœ‰é¢„æµ‹çš„äº¤é›†
            all_indices = set(list(predictions_dict.values())[0].index)
            for pred in list(predictions_dict.values())[1:]:
                all_indices = all_indices.intersection(set(pred.index))
            reference_index = sorted(all_indices)
        
        if len(reference_index) == 0:
            logger.error("æ²¡æœ‰å¯ç”¨çš„å‚è€ƒç´¢å¼•è¿›è¡Œé›†æˆ")
            return pd.Series()
        
        # æ„å»ºé¢„æµ‹çŸ©é˜µï¼ˆä¸å¡«å……ä¸º0ï¼Œä¿ç•™NaNï¼‰
        preds_df = pd.DataFrame({
            name: series.reindex(reference_index) for name, series in predictions_dict.items()
        })

        # å°†æƒé‡å‘é‡ä¸åˆ—å¯¹é½
        weights_vec = np.array([weights_dict.get(name, 0.0) for name in preds_df.columns], dtype=float)
        # æ¯è¡Œæœ‰æ•ˆæƒé‡ä¹‹å’Œï¼ˆå¿½ç•¥è¯¥è¡Œä¸­çš„NaNï¼‰
        mask = ~preds_df.isna().values
        weights_matrix = np.tile(weights_vec, (len(preds_df), 1))
        denom = (weights_matrix * mask).sum(axis=1)
        numer = np.nansum(preds_df.values * weights_matrix, axis=1)
        with np.errstate(invalid='ignore', divide='ignore'):
            ensemble_values = np.where(denom > 0, numer / denom, np.nan)
        ensemble_prediction = pd.Series(ensemble_values, index=reference_index)
        
        self.final_predictions = ensemble_prediction
        
        logger.info(f"é›†æˆé¢„æµ‹å®Œæˆï¼Œè¦†ç›–{len(ensemble_prediction)}ä¸ªæ ·æœ¬")
        
        return ensemble_prediction
    
    def optimize_portfolio(self, predictions: pd.Series, 
                          feature_data: pd.DataFrame) -> Dict[str, Any]:
        """
        ä¼˜åŒ–æŠ•èµ„ç»„åˆ
        
        Args:
            predictions: é›†æˆé¢„æµ‹
            feature_data: ç‰¹å¾æ•°æ®
            
        Returns:
            æŠ•èµ„ç»„åˆä¼˜åŒ–ç»“æœ
        """
        if not self.portfolio_optimizer or not ENHANCED_MODULES_AVAILABLE:
            logger.warning("æŠ•èµ„ç»„åˆä¼˜åŒ–å™¨ä¸å¯ç”¨ï¼Œæ— æ³•ç”ŸæˆæŠ•èµ„å»ºè®®")
            return {'success': False, 'error': 'Portfolio optimizer not available'}
        
        logger.info("å¼€å§‹æŠ•èµ„ç»„åˆä¼˜åŒ–")
        
        try:
            # å°†é¢„æµ‹ä¸æ ·æœ¬å…ƒæ•°æ®(date,ticker)å¯¹é½ï¼Œå†ç­›é€‰æœ€æ–°æˆªé¢
            if self.feature_data is None or len(self.feature_data) == 0:
                logger.error("ç¼ºå°‘ç‰¹å¾å…ƒæ•°æ®ç”¨äºå¯¹é½é¢„æµ‹")
                return {}
            
            # åªå–é¢„æµ‹ç´¢å¼•ä¸­å­˜åœ¨äºfeature_dataä¸­çš„éƒ¨åˆ†
            valid_pred_indices = predictions.index.intersection(self.feature_data.index)
            if len(valid_pred_indices) == 0:
                logger.error("é¢„æµ‹ç´¢å¼•ä¸ç‰¹å¾æ•°æ®ç´¢å¼•æ²¡æœ‰äº¤é›†")
                return {}
            
            # è·å–æœ‰æ•ˆé¢„æµ‹
            valid_predictions = predictions.reindex(valid_pred_indices)
            meta = self.feature_data.loc[valid_pred_indices, ['date', 'ticker']].copy()
            pred_df = meta.assign(pred=valid_predictions.values)
            
            # ä»…ä¿ç•™æœ‰æ•ˆé¢„æµ‹
            pred_df = pred_df.replace([np.inf, -np.inf], np.nan).dropna(subset=['pred'])
            if pred_df.empty:
                logger.error("æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ä¿¡å·")
                return {}
            
            latest_date = pred_df['date'].max()
            latest_pred = pred_df[pred_df['date'] == latest_date]
            if latest_pred.empty:
                logger.error("æœ€æ–°æˆªé¢æ²¡æœ‰é¢„æµ‹ä¿¡å·")
                return {}
            
            # èšåˆåˆ°tickerå±‚é¢
            ticker_pred = latest_pred.groupby('ticker')['pred'].mean()
            
            # å¯¹é½åˆ°æœ€æ–°æˆªé¢ç‰¹å¾
            latest_slice = feature_data[feature_data['date'] == latest_date].copy()
            if latest_slice.empty:
                logger.error("æ²¡æœ‰æœ€æ–°æˆªé¢æ•°æ®")
                return {}
            
            latest_slice = latest_slice.set_index('ticker')
            predictions_valid = ticker_pred.reindex(latest_slice.index)
            
            # è¿‡æ»¤NaNï¼ˆä½†ä¸æŠŠä¿¡å·å¼ºè¡Œç½®é›¶ï¼Œé¿å…å…¨é›¶ï¼‰
            valid_mask = (~predictions_valid.isna())
            if valid_mask.sum() == 0:
                logger.error("æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ä¿¡å·")
                return {}
                
            latest_data_valid = latest_slice[valid_mask]
            predictions_valid = predictions_valid[valid_mask]

            # å¦‚æœé¢„æµ‹ä¸ºå¸¸æ•°ï¼ˆstdä¸º0ï¼‰ï¼Œç”¨å¤‡ç”¨æ‰“åˆ†ç ´å¹³ï¼ˆå¦‚è¿‘20æ—¥åŠ¨é‡ï¼‰ï¼Œå¹¶åšæˆªé¢æ ‡å‡†åŒ–
            try:
                if float(predictions_valid.std()) == 0.0:
                    backup_scores = []
                    for tk in latest_data_valid.index:
                        try:
                            df_hist = self.raw_data.get(tk)
                            if df_hist is not None and 'close' in df_hist.columns:
                                df_hist = df_hist.sort_values('date')
                                df_hist['ret'] = df_hist['close'].pct_change()
                                mom20 = (1.0 + df_hist['ret']).rolling(21).apply(lambda x: np.prod(1.0 + x) - 1.0).iloc[-1]
                                backup_scores.append(mom20 if pd.notna(mom20) else 0.0)
                            else:
                                backup_scores.append(0.0)
                        except Exception:
                            backup_scores.append(0.0)
                    backup_series = pd.Series(backup_scores, index=latest_data_valid.index)
                    # æˆªé¢æ ‡å‡†åŒ–
                    if backup_series.std() > 0:
                        backup_series = (backup_series - backup_series.mean()) / backup_series.std()
                    predictions_valid = backup_series
                    logger.info("æ£€æµ‹åˆ°é¢„æµ‹ä¸ºå¸¸æ•°ï¼Œå·²ä½¿ç”¨è¿‘20æ—¥åŠ¨é‡ä½œä¸ºå¤‡ç”¨ä¿¡å·å¹¶æ ‡å‡†åŒ–")
            except Exception:
                pass

            # è®°å½•æœ€æ–°æˆªé¢ä¿¡å·ç»Ÿè®¡ï¼Œè¯Šæ–­æ˜¯å¦å‡ºç°å…¨0
            try:
                nz_ratio = float((predictions_valid != 0).sum()) / float(len(predictions_valid))
                logger.info(f"æœ€æ–°æˆªé¢ä¿¡å·éé›¶æ¯”ç‡: {nz_ratio:.2%}, å‡å€¼: {predictions_valid.mean():.6f}, æ ‡å‡†å·®: {predictions_valid.std():.6f}")
                self.latest_ticker_predictions = predictions_valid.copy()
            except Exception:
                self.latest_ticker_predictions = predictions_valid
            
            logger.info(f"æœ‰æ•ˆé¢„æµ‹ä¿¡å·æ•°é‡: {len(predictions_valid)}, æ¶µç›–è‚¡ç¥¨: {list(predictions_valid.index)}")
            
            # æ„å»ºé¢„æœŸæ”¶ç›Šç‡ï¼ˆåŸºäºé¢„æµ‹ä¿¡å·ï¼‰ã€‚
            # å¢å¼ºä¿¡å·å¤„ç†ï¼šæ ‡å‡†åŒ– + æ”¾å¤§ + æŠ–åŠ¨
            expected_returns = predictions_valid.copy()
            
            # æ ‡å‡†åŒ–
            if expected_returns.std() > 1e-12:
                expected_returns = (expected_returns - expected_returns.mean()) / expected_returns.std()
            else:
                # ä¿¡å·è¿‡äºå¹³å¦ï¼Œåˆ›å»ºäººå·¥æ¢¯åº¦
                expected_returns = pd.Series(
                    np.linspace(-1, 1, len(expected_returns)), 
                    index=expected_returns.index
                )
            
            # æ”¾å¤§ä¿¡å·å¼ºåº¦ï¼ˆæ”¹å–„ä¼˜åŒ–å™¨æ•°å€¼ç¨³å®šæ€§ï¼‰
            expected_returns = expected_returns * 0.02  # ç›®æ ‡å¹´åŒ–æ”¶ç›Š2%çš„é‡çº§
            
            # å¾®æŠ–åŠ¨ç¡®ä¿éç­‰æƒè§£
            rng = np.random.RandomState(42)
            expected_returns = expected_returns + rng.normal(0, 1e-4, size=len(expected_returns))
            expected_returns.name = 'expected_returns'
            
            # æ„å»ºå†å²æ”¶ç›Šç‡çŸ©é˜µç”¨äºåæ–¹å·®ä¼°è®¡
            returns_data = []
            tickers_for_cov = expected_returns.index.tolist()
            
            # è·å–å†å²æ”¶ç›Šç‡
            for ticker in tickers_for_cov:
                if ticker in self.raw_data:
                    hist_data = self.raw_data[ticker].copy()
                    hist_data['returns'] = hist_data['close'].pct_change()
                    returns_data.append(hist_data[['date', 'returns']].set_index('date')['returns'].rename(ticker))
            
            if returns_data:
                returns_matrix = pd.concat(returns_data, axis=1).dropna()
                
                # ä¼°è®¡åæ–¹å·®çŸ©é˜µ
                cov_matrix = self.portfolio_optimizer.estimate_covariance_matrix(returns_matrix)
                
                # ç»Ÿä¸€èµ„äº§é¡ºåºï¼Œé¿å…ç»´åº¦ä¸ä¸€è‡´ï¼ˆä½¿ç”¨returns_matrixåˆ—ä½œä¸ºæƒå¨é¡ºåºï¼‰
                cov_tickers = list(returns_matrix.columns)
                expected_returns = expected_returns.reindex(cov_tickers).dropna()
                universe_data = latest_data_valid[['COUNTRY', 'SECTOR', 'SUBINDUSTRY']].copy()
                universe_data = universe_data.reindex(expected_returns.index)

                # è‡³å°‘éœ€è¦2åªè‚¡ç¥¨ä»¥è¿›è¡Œä¼˜åŒ–
                if len(expected_returns) < 2:
                    logger.error("æœ‰æ•ˆè‚¡ç¥¨æ•°é‡ä¸è¶³ä»¥è¿›è¡Œä¼˜åŒ–")
                    return {}
                if 'volume' in latest_data_valid.columns:
                    # ç®€å•çš„æµåŠ¨æ€§æ’å
                    universe_data['liquidity_rank'] = latest_data_valid['volume'].reindex(expected_returns.index).rank(pct=True)
                else:
                    universe_data['liquidity_rank'] = 0.5
                
                # æ‰§è¡ŒæŠ•èµ„ç»„åˆä¼˜åŒ–
                optimization_result = self.portfolio_optimizer.optimize_portfolio(
                    expected_returns=expected_returns,
                    covariance_matrix=cov_matrix,
                    current_weights=None,  # å‡è®¾ä»ç©ºä»“å¼€å§‹
                    universe_data=universe_data
                )
                
                if optimization_result.get('success', False):
                    optimal_weights = optimization_result['optimal_weights']
                    portfolio_metrics = optimization_result['portfolio_metrics']
                    
                    # é£é™©å½’å› 
                    risk_attribution = self.portfolio_optimizer.risk_attribution(
                        optimal_weights, cov_matrix
                    )
                    
                    # å‹åŠ›æµ‹è¯•
                    from advanced_portfolio_optimizer import create_stress_scenarios
                    stress_scenarios = create_stress_scenarios(optimal_weights.index.tolist())
                    stress_results = self.portfolio_optimizer.stress_test(
                        optimal_weights, cov_matrix, stress_scenarios
                    )
                    
                    self.portfolio_weights = optimal_weights
                    
                    return {
                        'success': True,
                        'optimal_weights': optimal_weights,
                        'portfolio_metrics': portfolio_metrics,
                        'risk_attribution': risk_attribution,
                        'stress_test': stress_results,
                        'optimization_info': optimization_result.get('optimization_info', {})
                    }
                else:
                    logger.warning("é«˜çº§æŠ•èµ„ç»„åˆä¼˜åŒ–æœªè¾¾åˆ°æœ€ä¼˜ï¼Œä½†å·²è¿”å›æœ€ä½³å¯ç”¨ç»“æœ")
                    return optimization_result
            else:
                logger.error("æ— æ³•æ„å»ºåæ–¹å·®çŸ©é˜µ")
                return {}
                
        except Exception as e:
            logger.error(f"æŠ•èµ„ç»„åˆä¼˜åŒ–å¼‚å¸¸: {e}")
            return {'error': str(e)}
    

    
    def generate_investment_recommendations(self, portfolio_result: Dict[str, Any],
                                          top_n: int = 10) -> List[Dict[str, Any]]:
        """
        ç”ŸæˆæŠ•èµ„å»ºè®®
        
        Args:
            portfolio_result: æŠ•èµ„ç»„åˆä¼˜åŒ–ç»“æœ
            top_n: è¿”å›å‰Nä¸ªæ¨è
            
        Returns:
            æŠ•èµ„å»ºè®®åˆ—è¡¨
        """
        logger.info(f"ç”Ÿæˆå‰{top_n}ä¸ªæŠ•èµ„å»ºè®®")
        
        if not portfolio_result.get('success', False):
            logger.error("æŠ•èµ„ç»„åˆä¼˜åŒ–å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆå»ºè®®")
            return []
        
        optimal_weights = portfolio_result['optimal_weights']
        portfolio_metrics = portfolio_result.get('portfolio_metrics', {})
        
        # è·å–æœ€æ–°çš„è‚¡ç¥¨æ•°æ®
        recommendations = []
        
        # æŒ‰æƒé‡æ’åº
        sorted_weights = optimal_weights[optimal_weights > 0.001].sort_values(ascending=False)
        
        for i, (ticker, weight) in enumerate(sorted_weights.head(top_n).items()):
            try:
                # è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
                if ticker in self.raw_data:
                    stock_data = self.raw_data[ticker]
                    latest_price = stock_data['close'].iloc[-1]
                    
                    # è®¡ç®—ä¸€äº›åŸºæœ¬æŒ‡æ ‡
                    price_change_1d = stock_data['close'].pct_change().iloc[-1]
                    price_change_5d = (stock_data['close'].iloc[-1] / stock_data['close'].iloc[-6] - 1) if len(stock_data) > 5 else 0
                    
                    avg_volume = stock_data['volume'].tail(20).mean() if 'volume' in stock_data.columns else 0
                    
                    # è·å–é¢„æµ‹ä¿¡å·ï¼ˆä¼˜å…ˆä½¿ç”¨æŒ‰tickerèšåˆè¿‡çš„æœ€æ–°æˆªé¢ä¿¡å·ï¼‰
                    if hasattr(self, 'latest_ticker_predictions') and isinstance(self.latest_ticker_predictions, pd.Series):
                        prediction_signal = float(self.latest_ticker_predictions.get(ticker, np.nan))
                    else:
                        # å›é€€ï¼šä»é€è¡Œé¢„æµ‹èšåˆï¼ˆæœ€æ–°æ—¥æœŸï¼‰
                        try:
                            if self.final_predictions is not None and hasattr(self, 'feature_data'):
                                ref_idx = self.feature_data.index
                                preds = pd.Series(self.final_predictions).reindex(ref_idx)
                                latest_date = self.feature_data['date'].max()
                                latest_mask = self.feature_data['date'] == latest_date
                                latest_tickers = self.feature_data.loc[latest_mask, 'ticker']
                                grouped = pd.DataFrame({'ticker': latest_tickers, 'pred': preds[latest_mask]}).groupby('ticker')['pred'].mean()
                                prediction_signal = float(grouped.get(ticker, np.nan))
                            else:
                                prediction_signal = np.nan
                        except Exception:
                            prediction_signal = np.nan
                    if np.isnan(prediction_signal):
                        prediction_signal = 0.0
                    
                    recommendation = {
                        'rank': i + 1,
                        'ticker': ticker,
                        'weight': weight,
                        'latest_price': latest_price,
                        'price_change_1d': price_change_1d,
                        'price_change_5d': price_change_5d,
                        'avg_volume_20d': avg_volume,
                        'prediction_signal': prediction_signal,
                        'recommendation_reason': self._get_recommendation_reason(ticker, weight, prediction_signal)
                    }
                    
                    recommendations.append(recommendation)
                
            except Exception as e:
                logger.warning(f"ç”Ÿæˆ{ticker}æ¨èä¿¡æ¯å¤±è´¥: {e}")
                continue
        
        return recommendations
    
    def _get_recommendation_reason(self, ticker: str, weight: float, signal: float) -> str:
        """ç”Ÿæˆæ¨èç†ç”±"""
        reasons = []
        
        if weight > 0.05:
            reasons.append("é«˜æƒé‡é…ç½®")
        elif weight > 0.03:
            reasons.append("ä¸­ç­‰æƒé‡é…ç½®")
        else:
            reasons.append("ä½æƒé‡é…ç½®")
        
        if signal > 0.1:
            reasons.append("å¼ºçƒˆä¹°å…¥ä¿¡å·")
        elif signal > 0.05:
            reasons.append("ä¹°å…¥ä¿¡å·")
        elif signal > 0:
            reasons.append("å¼±ä¹°å…¥ä¿¡å·")
        else:
            reasons.append("ä¸­æ€§ä¿¡å·")
        
        return "; ".join(reasons)
    
    def save_results(self, recommendations: List[Dict[str, Any]], 
                    portfolio_result: Dict[str, Any]) -> str:
        """
        ä¿å­˜ç»“æœ
        
        Args:
            recommendations: æŠ•èµ„å»ºè®®
            portfolio_result: æŠ•èµ„ç»„åˆç»“æœ
            
        Returns:
            ä¿å­˜æ–‡ä»¶è·¯å¾„
        """
        logger.info("ä¿å­˜åˆ†æç»“æœ")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_dir = Path("result")
        result_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜æŠ•èµ„å»ºè®®
        if recommendations:
            # Excelæ ¼å¼ï¼ˆç¡®ä¿åˆ—é¡ºåºä¸æ•°æ®ç±»å‹ç¨³å®šï¼‰
            excel_file = result_dir / f"ultra_enhanced_recommendations_{timestamp}.xlsx"
            rec_df = pd.DataFrame(recommendations)
            # è§„èŒƒticker
            if 'ticker' in rec_df.columns:
                rec_df['ticker'] = rec_df['ticker'].map(sanitize_ticker)
            # è®¾å®šåˆ—é¡ºåº
            preferred_cols = ['rank','ticker','weight','latest_price','price_change_1d','price_change_5d','avg_volume_20d','prediction_signal','recommendation_reason']
            ordered_cols = [c for c in preferred_cols if c in rec_df.columns] + [c for c in rec_df.columns if c not in preferred_cols]
            rec_df = rec_df[ordered_cols]
            # ä»…å¯¼å‡ºå‰200æ¡åˆ°Excel
            rec_df = rec_df.head(200)
            # Excelä¼˜å…ˆï¼›å¤±è´¥æ—¶å›é€€CSV
            try:
                rec_df.to_excel(excel_file, index=False)
            except Exception:
                excel_file = result_dir / f"ultra_enhanced_recommendations_{timestamp}.csv"
                rec_df.to_csv(excel_file, index=False, encoding='utf-8')
            
            # ç®€åŒ–çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨
            tickers_file = result_dir / f"top_tickers_{timestamp}.txt"
            top_tickers = [sanitize_ticker(rec.get('ticker','')) for rec in recommendations[:7] if rec.get('ticker')]
            with open(tickers_file, 'w', encoding='utf-8') as f:
                f.write(", ".join([f"'{ticker}'" for ticker in top_tickers]))

            # ä»…è‚¡ç¥¨ä»£ç ï¼ˆJSONå­˜å‚¨ä¸ºå•ä¸ªå­—ç¬¦ä¸²ï¼Œå½¢å¦‚: 'NVDA', 'AAPL'ï¼‰ï¼ŒTop7
            top7_json = result_dir / f"top7_tickers_{timestamp}.json"
            top7_string = ", ".join([f"'{t}'" for t in top_tickers])
            with open(top7_json, 'w', encoding='utf-8') as f:
                json.dump(top7_string, f, ensure_ascii=False)
        
            # ä¿å­˜æŠ•èµ„ç»„åˆè¯¦æƒ…
        if portfolio_result.get('success', False):
            portfolio_file = result_dir / f"portfolio_details_{timestamp}.json"
            portfolio_data = {
                'timestamp': timestamp,
                'portfolio_metrics': portfolio_result.get('portfolio_metrics', {}),
                'optimization_info': portfolio_result.get('optimization_info', {}),
                    'weights': {sanitize_ticker(k): float(v) for k, v in portfolio_result.get('optimal_weights', pd.Series(dtype=float)).to_dict().items()}
            }
            
            with open(portfolio_file, 'w', encoding='utf-8') as f:
                json.dump(portfolio_data, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ° {result_dir}")
        return str(excel_file) if recommendations else str(result_dir)
    
    def run_complete_analysis(self, tickers: List[str], 
                             start_date: str, end_date: str,
                             top_n: int = 10) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´åˆ†ææµç¨‹
        
        Args:
            tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            top_n: è¿”å›æ¨èæ•°é‡
            
        Returns:
            å®Œæ•´åˆ†æç»“æœ
        """
        logger.info("å¼€å§‹å®Œæ•´åˆ†ææµç¨‹")
        
        analysis_results = {
            'start_time': datetime.now(),
            'config': self.config,
            'tickers': tickers,
            'date_range': f"{start_date} to {end_date}"
        }
        
        try:
            # 1. ä¸‹è½½æ•°æ®
            stock_data = self.download_stock_data(tickers, start_date, end_date)
            if not stock_data:
                raise ValueError("æ— æ³•è·å–è‚¡ç¥¨æ•°æ®")
            
            analysis_results['data_download'] = {
                'success': True,
                'stocks_downloaded': len(stock_data)
            }
            
            # 2. åˆ›å»ºç‰¹å¾
            feature_data = self.create_traditional_features(stock_data)
            if len(feature_data) == 0:
                raise ValueError("ç‰¹å¾åˆ›å»ºå¤±è´¥")
            
            analysis_results['feature_engineering'] = {
                'success': True,
                'feature_shape': feature_data.shape,
                'feature_columns': len([col for col in feature_data.columns 
                                      if col not in ['ticker', 'date', 'target']])
            }
            
            # 3. æ„å»ºMulti-factoré£é™©æ¨¡å‹
            try:
                risk_model = self.build_risk_model()
                analysis_results['risk_model'] = {
                    'success': True,
                    'factor_count': len(risk_model['risk_factors'].columns),
                    'assets_covered': len(risk_model['factor_loadings'])
                }
                logger.info("é£é™©æ¨¡å‹æ„å»ºå®Œæˆ")
            except Exception as e:
                logger.warning(f"é£é™©æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
                analysis_results['risk_model'] = {'success': False, 'error': str(e)}
            
            # 4. æ£€æµ‹å¸‚åœºçŠ¶æ€
            try:
                market_regime = self.detect_market_regime()
                analysis_results['market_regime'] = {
                    'success': True,
                    'regime': market_regime.name,
                    'probability': market_regime.probability,
                    'characteristics': market_regime.characteristics
                }
                logger.info(f"å¸‚åœºçŠ¶æ€æ£€æµ‹å®Œæˆ: {market_regime.name}")
            except Exception as e:
                logger.warning(f"å¸‚åœºçŠ¶æ€æ£€æµ‹å¤±è´¥: {e}")
                analysis_results['market_regime'] = {'success': False, 'error': str(e)}
                market_regime = MarketRegime(0, "Normal", 0.7, {'volatility': 0.15, 'trend': 0.0})
            
            # 5. è®­ç»ƒæ¨¡å‹
            training_results = self.train_enhanced_models(feature_data)
            analysis_results['model_training'] = training_results
            
            # 6. ç”Ÿæˆé¢„æµ‹ï¼ˆç»“åˆregime-awareæƒé‡ï¼‰
            ensemble_predictions = self.generate_enhanced_predictions(training_results, market_regime)
            if len(ensemble_predictions) == 0:
                raise ValueError("é¢„æµ‹ç”Ÿæˆå¤±è´¥")
            
            analysis_results['prediction_generation'] = {
                'success': True,
                'predictions_count': len(ensemble_predictions),
                'prediction_stats': {
                    'mean': ensemble_predictions.mean(),
                    'std': ensemble_predictions.std(),
                    'min': ensemble_predictions.min(),
                    'max': ensemble_predictions.max()
                },
                'regime_adjusted': True
            }
            
            # 7. æŠ•èµ„ç»„åˆä¼˜åŒ–ï¼ˆå¸¦é£é™©æ¨¡å‹ï¼‰
            portfolio_result = self.optimize_portfolio_with_risk_model(ensemble_predictions, feature_data)
            analysis_results['portfolio_optimization'] = portfolio_result
            
            # 6. ç”ŸæˆæŠ•èµ„å»ºè®®
            recommendations = self.generate_investment_recommendations(portfolio_result, top_n)
            analysis_results['recommendations'] = recommendations
            
            # 7. ä¿å­˜ç»“æœ
            result_file = self.save_results(recommendations, portfolio_result)
            analysis_results['result_file'] = result_file
            
            analysis_results['end_time'] = datetime.now()
            analysis_results['total_time'] = (analysis_results['end_time'] - analysis_results['start_time']).total_seconds()
            analysis_results['success'] = True
            
            # æ·»åŠ å¥åº·ç›‘æ§æŠ¥å‘Š
            analysis_results['health_report'] = self.get_health_report()
            
            logger.info(f"å®Œæ•´åˆ†ææµç¨‹å®Œæˆï¼Œè€—æ—¶: {analysis_results['total_time']:.1f}ç§’")
            logger.info(f"ç³»ç»Ÿå¥åº·çŠ¶å†µ: {analysis_results['health_report']['risk_level']}, "
                       f"å¤±è´¥ç‡: {analysis_results['health_report']['failure_rate_percent']:.2f}%")
            
            return analysis_results
            
        except Exception as e:
            logger.error(f"åˆ†ææµç¨‹å¤±è´¥: {e}")
            analysis_results['error'] = str(e)
            analysis_results['success'] = False
            analysis_results['end_time'] = datetime.now()
            
            return analysis_results


def main():
    """ä¸»å‡½æ•°"""
    print("=== BMA Ultra Enhanced é‡åŒ–åˆ†ææ¨¡å‹ V4 ===")
    print("é›†æˆAlphaç­–ç•¥ã€Learning-to-Rankã€é«˜çº§æŠ•èµ„ç»„åˆä¼˜åŒ–")
    print(f"å¢å¼ºæ¨¡å—å¯ç”¨: {ENHANCED_MODULES_AVAILABLE}")
    print(f"é«˜çº§æ¨¡å‹: XGBoost={XGBOOST_AVAILABLE}, LightGBM={LIGHTGBM_AVAILABLE}")
    
    # è®¾ç½®å…¨å±€è¶…æ—¶ä¿æŠ¤
    start_time = time.time()
    MAX_EXECUTION_TIME = 300  # 5åˆ†é’Ÿè¶…æ—¶
    
    # å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='BMA Ultra Enhancedé‡åŒ–æ¨¡å‹V4')
    parser.add_argument('--start-date', type=str, default='2023-01-01', help='å¼€å§‹æ—¥æœŸ')
    parser.add_argument('--end-date', type=str, default='2024-12-31', help='ç»“æŸæ—¥æœŸ')
    parser.add_argument('--top-n', type=int, default=200, help='è¿”å›top Nä¸ªæ¨è')
    parser.add_argument('--config', type=str, default='alphas_config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--tickers', type=str, nargs='+', default=None, help='è‚¡ç¥¨ä»£ç åˆ—è¡¨')
    parser.add_argument('--tickers-file', type=str, default='stocks.txt', help='è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªä»£ç ï¼‰')
    parser.add_argument('--tickers-limit', type=int, default=0, help='å…ˆç”¨å‰Nåªåšå°æ ·æœ¬æµ‹è¯•ï¼Œå†å…¨é‡è®­ç»ƒï¼ˆ0è¡¨ç¤ºç›´æ¥å…¨é‡ï¼‰')
    
    args = parser.parse_args()
    
    # ç¡®å®šè‚¡ç¥¨åˆ—è¡¨
    if args.tickers:
        tickers = args.tickers
    else:
        tickers = load_universe_from_file(args.tickers_file) or load_universe_fallback()
    
    print(f"åˆ†æå‚æ•°:")
    print(f"  æ—¶é—´èŒƒå›´: {args.start_date} - {args.end_date}")
    print(f"  è‚¡ç¥¨æ•°é‡: {len(tickers)}")
    print(f"  æ¨èæ•°é‡: {args.top_n}")
    print(f"  é…ç½®æ–‡ä»¶: {args.config}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = UltraEnhancedQuantitativeModel(config_path=args.config)
    
    # ä¸¤é˜¶æ®µï¼šå°æ ·æœ¬æµ‹è¯• â†’ å…¨é‡
    if args.tickers_limit and args.tickers_limit > 0 and len(tickers) > args.tickers_limit:
        print("\nğŸ§ª å…ˆè¿è¡Œå°æ ·æœ¬æµ‹è¯•...")
        small_tickers = tickers[:args.tickers_limit]
        _ = model.run_complete_analysis(
            tickers=small_tickers,
            start_date=args.start_date,
            end_date=args.end_date,
            top_n=min(args.top_n, len(small_tickers))
        )
        print("\nâœ… å°æ ·æœ¬æµ‹è¯•å®Œæˆï¼Œå¼€å§‹å…¨é‡è®­ç»ƒ...")

    # è¿è¡Œå®Œæ•´åˆ†æ (å¸¦è¶…æ—¶ä¿æŠ¤)
    try:
        results = model.run_complete_analysis(
            tickers=tickers,
            start_date=args.start_date,
            end_date=args.end_date,
            top_n=args.top_n
        )
        
        # æ£€æŸ¥æ‰§è¡Œæ—¶é—´
        execution_time = time.time() - start_time
        if execution_time > MAX_EXECUTION_TIME:
            print(f"\nâš ï¸ æ‰§è¡Œæ—¶é—´è¶…è¿‡{MAX_EXECUTION_TIME}ç§’ï¼Œä½†å·²å®Œæˆ")
            
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        results = {'success': False, 'error': 'ç”¨æˆ·ä¸­æ–­'}
    except Exception as e:
        execution_time = time.time() - start_time
        print(f"\nâŒ æ‰§è¡Œå¼‚å¸¸ (è€—æ—¶{execution_time:.1f}s): {e}")
        results = {'success': False, 'error': str(e)}
    
    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    print("\n" + "="*60)
    print("åˆ†æç»“æœæ‘˜è¦")
    print("="*60)
    
    if results.get('success', False):
        # é¿å…æ§åˆ¶å°ç¼–ç é”™è¯¯ï¼ˆGBKï¼‰
        print(f"åˆ†ææˆåŠŸå®Œæˆï¼Œè€—æ—¶: {results['total_time']:.1f}ç§’")
        
        if 'data_download' in results:
            print(f"æ•°æ®ä¸‹è½½: {results['data_download']['stocks_downloaded']}åªè‚¡ç¥¨")
        
        if 'feature_engineering' in results:
            fe_info = results['feature_engineering']
            print(f"ç‰¹å¾å·¥ç¨‹: {fe_info['feature_shape'][0]}æ ·æœ¬, {fe_info['feature_columns']}ç‰¹å¾")
        
        if 'prediction_generation' in results:
            pred_info = results['prediction_generation']
            stats = pred_info['prediction_stats']
            print(f"é¢„æµ‹ç”Ÿæˆ: {pred_info['predictions_count']}ä¸ªé¢„æµ‹ (å‡å€¼: {stats['mean']:.4f})")
        
        if 'portfolio_optimization' in results and results['portfolio_optimization'].get('success', False):
            port_metrics = results['portfolio_optimization']['portfolio_metrics']
            print(f"æŠ•èµ„ç»„åˆ: é¢„æœŸæ”¶ç›Š{port_metrics.get('expected_return', 0):.4f}, "
                  f"å¤æ™®æ¯”{port_metrics.get('sharpe_ratio', 0):.4f}")
        
        if 'recommendations' in results:
            recommendations = results['recommendations']
            print(f"\næŠ•èµ„å»ºè®® (Top {len(recommendations)}):")
            for i, rec in enumerate(recommendations[:5], 1):
                print(f"  {i}. {rec['ticker']}: æƒé‡{rec['weight']:.3f}, "
                      f"ä¿¡å·{rec['prediction_signal']:.4f}")
        
        if 'result_file' in results:
            print(f"\nç»“æœå·²ä¿å­˜è‡³: {results['result_file']}")
    
    else:
        print(f"åˆ†æå¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    print("="*60)


if __name__ == "__main__":
    main()
